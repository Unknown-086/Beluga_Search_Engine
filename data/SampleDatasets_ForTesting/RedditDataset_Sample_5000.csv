DocID,date,subreddit,title,id,author,author_created_utc,full_link,score,num_comments,post,language
1105352,2008-03-19 10:08:43,statistics,Chance News 34,6cm5l,koolkao,1134882000.0,https://www.reddit.com/r/statistics/comments/6cm5l/chance_news_34/,3.0,0.0,,en
1105353,2008-03-29 04:27:22,statistics,Advanced R Tips and Tricks,6dsxt,mathsuu,1197264006.0,https://www.reddit.com/r/statistics/comments/6dsxt/advanced_r_tips_and_tricks/,1.0,0.0,,en
1105354,2008-04-23 20:55:52,statistics,fdrtool: new R package for FDR estimation using mixture models,6gupy,koolkao,1134882000.0,https://www.reddit.com/r/statistics/comments/6gupy/fdrtool_new_r_package_for_fdr_estimation_using/,4.0,0.0,,en
1105355,2008-04-25 08:26:02,statistics,R 2.7.0 released,6h1wu,koolkao,1134882000.0,https://www.reddit.com/r/statistics/comments/6h1wu/r_270_released/,5.0,1.0,,en
1105356,2008-06-12 08:38:02,statistics,"Cancer survival rates: tables, graphics, and PP",6n1hm,hitsman,1135832400.0,https://www.reddit.com/r/statistics/comments/6n1hm/cancer_survival_rates_tables_graphics_and_pp/,5.0,0.0,,en
1105357,2008-06-23 18:50:07,computerscience,List of Computer Science Video Lectures,6ol9b,syam1224,1201951154.0,https://www.reddit.com/r/computerscience/comments/6ol9b/list_of_computer_science_video_lectures/,1.0,0.0,,en
1105358,2008-06-30 07:55:24,statistics,Chris Anderson: Aware of All Statistical Traditions (with bonus fall course announcement),6pjwt,koolkao,1134882000.0,https://www.reddit.com/r/statistics/comments/6pjwt/chris_anderson_aware_of_all_statistical/,5.0,1.0,,en
1105359,2008-07-22 08:42:13,statistics,Statistics of One Year of Blogging,6suws,pkrumins,1136437200.0,https://www.reddit.com/r/statistics/comments/6suws/statistics_of_one_year_of_blogging/,0.0,0.0,,en
1105360,2008-08-27 16:26:50,artificial,History of artificial intelligence,6y98d,[deleted],,https://www.reddit.com/r/artificial/comments/6y98d/history_of_artificial_intelligence/,5.0,0.0,,en
1105361,2008-09-17 15:31:11,statistics,Fast sparse regression and classification,71xba,prider,1207918699.0,https://www.reddit.com/r/statistics/comments/71xba/fast_sparse_regression_and_classification/,2.0,1.0,,en
1105362,2008-10-12 00:40:40,artificial,The Single Layer Perceptron,76ljt,liamQ,1223676768.0,https://www.reddit.com/r/artificial/comments/76ljt/the_single_layer_perceptron/,2.0,1.0,,en
1105363,2008-11-24 05:39:20,artificial,IBM to buil brain-like machines - AI goes for a homerun with bases loaded,7f8vk,liamQ,1223676768.0,https://www.reddit.com/r/artificial/comments/7f8vk/ibm_to_buil_brainlike_machines_ai_goes_for_a/,0.0,0.0,,en
1105364,2008-11-26 10:58:19,statistics,"How to choose a statistical test, in table form",7frlb,speciousfool,1151058853.0,https://www.reddit.com/r/statistics/comments/7frlb/how_to_choose_a_statistical_test_in_table_form/,5.0,0.0,,en
1105365,2008-12-19 08:27:10,statistics,Learning Statistics Through Case Studies : StatLabs,7khor,juneaftn,1174841860.0,https://www.reddit.com/r/statistics/comments/7khor/learning_statistics_through_case_studies_statlabs/,2.0,0.0,,en
1105366,2008-12-19 08:38:52,statistics,Teaching of Statistics (Edward Tufte's site),7khr5,juneaftn,1174841860.0,https://www.reddit.com/r/statistics/comments/7khr5/teaching_of_statistics_edward_tuftes_site/,5.0,0.0,,en
1105367,2008-12-22 20:02:02,artificial,Documentary - The Hunt for A.I.,7l4ee,[deleted],,https://www.reddit.com/r/artificial/comments/7l4ee/documentary_the_hunt_for_ai/,0.0,0.0,,en
1105368,2008-12-26 17:09:16,statistics,Statistics forums,7ls5c,juneaftn,1174841860.0,https://www.reddit.com/r/statistics/comments/7ls5c/statistics_forums/,3.0,0.0,,en
1105369,2008-12-28 00:17:09,statistics,Statistical Literacy for doctors and patients by Gerd Gigerenzer,7lz7k,juneaftn,1174841860.0,https://www.reddit.com/r/statistics/comments/7lz7k/statistical_literacy_for_doctors_and_patients_by/,2.0,0.0,,en
1105370,2009-01-05 10:59:05,statistics,"Ask Stats: what single, technical text on statistics would you recommend reading above all others",7ng2l,speciousfool,1151058853.0,https://www.reddit.com/r/statistics/comments/7ng2l/ask_stats_what_single_technical_text_on/,7.0,7.0,,en
1105371,2009-01-07 15:35:20,statistics,NYTimes article on R,7o02e,[deleted],,https://www.reddit.com/r/statistics/comments/7o02e/nytimes_article_on_r/,1.0,0.0,,en
1105372,2009-01-07 15:45:23,statistics,Journal of Statistics Education - Teaching Bits,7o058,juneaftn,1174841860.0,https://www.reddit.com/r/statistics/comments/7o058/journal_of_statistics_education_teaching_bits/,2.0,0.0,,en
1105373,2009-01-08 04:25:18,statistics,The Best of Teaching Statistics,7o5uu,juneaftn,1174841860.0,https://www.reddit.com/r/statistics/comments/7o5uu/the_best_of_teaching_statistics/,0.0,0.0,,en
1105374,2009-01-08 04:48:05,statistics,RISK Mismanagement - What Led to the Financial Meltdown - NYTimes.com,7o5z7,juneaftn,1174841860.0,https://www.reddit.com/r/statistics/comments/7o5z7/risk_mismanagement_what_led_to_the_financial/,4.0,0.0,,en
1105375,2009-01-12 09:01:30,statistics,RPy : Python meets R,7p2ag,juneaftn,1174841860.0,https://www.reddit.com/r/statistics/comments/7p2ag/rpy_python_meets_r/,8.0,1.0,,en
1105376,2009-01-13 05:16:44,statistics,"If you're a chart nerd, you'll love how this author tears apart poorly designed charts",7pa6s,[deleted],,https://www.reddit.com/r/statistics/comments/7pa6s/if_youre_a_chart_nerd_youll_love_how_this_author/,2.0,0.0,,en
1105377,2009-01-28 20:27:01,statistics,The Earth is Round: A Critque of Hypothesis Testing [PDF],7t4dq,yggdrasilly,1219444184.0,https://www.reddit.com/r/statistics/comments/7t4dq/the_earth_is_round_a_critque_of_hypothesis/,9.0,1.0,,en
1105378,2009-02-08 04:25:44,statistics,Anscombe's Quartet,7vpm7,yggdrasilly,1219444184.0,https://www.reddit.com/r/statistics/comments/7vpm7/anscombes_quartet/,4.0,1.0,,en
1105379,2009-02-09 20:04:30,statistics,Ask Stats: Is There a Test to Show Binomial Population is all True from a Sample?,7w1s8,ST2K,1173236363.0,https://www.reddit.com/r/statistics/comments/7w1s8/ask_stats_is_there_a_test_to_show_binomial/,2.0,8.0,,en
1105380,2009-03-04 07:04:33,statistics,Need Help! Travel Stats for Market Research,81xha,adaminc,1182073050.0,https://www.reddit.com/r/statistics/comments/81xha/need_help_travel_stats_for_market_research/,0.0,1.0,,en
1105381,2009-03-11 12:54:17,statistics,Everything begins with “p” ,83rrb,BioGeek,1124683200.0,https://www.reddit.com/r/statistics/comments/83rrb/everything_begins_with_p/,0.0,0.0,,en
1105382,2009-03-13 08:20:28,statistics,"Statistics A Very Short Introduction, short and sweet.",84ba9,juneaftn,1174841860.0,https://www.reddit.com/r/statistics/comments/84ba9/statistics_a_very_short_introduction_short_and/,2.0,4.0,,en
1105383,2009-03-19 18:07:46,statistics,Bracketology gone overboard: analysis shows Obama's picks favor swing states,85xk7,AngelaMotorman,1189708654.0,https://www.reddit.com/r/statistics/comments/85xk7/bracketology_gone_overboard_analysis_shows_obamas/,0.0,0.0,,en
1105384,2009-03-21 06:50:54,artificial,John Searle interview about computers thinking and the Chinese Room,86cei,CuteAlien,1179241120.0,https://www.reddit.com/r/artificial/comments/86cei/john_searle_interview_about_computers_thinking/,3.0,0.0,,en
1105385,2009-03-21 09:19:49,statistics,"Ask statistics: how could one generate a set of representative ""personas"" from a social site like reddit? (if one had full access to db)",86d22,andresmh,1174603751.0,https://www.reddit.com/r/statistics/comments/86d22/ask_statistics_how_could_one_generate_a_set_of/,4.0,5.0,,en
1105386,2009-03-25 21:47:35,statistics,Fun Frequentist Paper on Convergence Rates for Bayesian Methods,87h3d,mstoehr,1185155370.0,https://www.reddit.com/r/statistics/comments/87h3d/fun_frequentist_paper_on_convergence_rates_for/,0.0,0.0,,en
1105387,2009-03-31 06:35:18,statistics,Ask statistics: What is the probability of the following events? [see comment],88rpf,[deleted],,https://www.reddit.com/r/statistics/comments/88rpf/ask_statistics_what_is_the_probability_of_the/,0.0,3.0,,en
1105388,2009-04-03 10:01:42,statistics,Statistical Modeling: The Two Cultures [PDF],89nwt,_underscore,1237136681.0,https://www.reddit.com/r/statistics/comments/89nwt/statistical_modeling_the_two_cultures_pdf/,11.0,4.0,,en
1105389,2009-04-08 20:03:10,statistics,"Ask Stats: Suppose you have a normally distributed variable X, with an estimated mean and variance. What's the bayesian distrbution for the next sample you draw from X?",8azxw,moultano,1160099427.0,https://www.reddit.com/r/statistics/comments/8azxw/ask_stats_suppose_you_have_a_normally_distributed/,0.0,1.0,,en
1105390,2009-04-21 00:13:08,statistics,"Ask Stats: Do You Wanna Smack the Next Person That Says ""Correlation is not Causation?""",8e0mm,ST2K,1173236363.0,https://www.reddit.com/r/statistics/comments/8e0mm/ask_stats_do_you_wanna_smack_the_next_person_that/,18.0,14.0,,en
1105391,2009-04-21 02:09:14,statistics,Ask Stats: What Are Good Ways to Prepare for a Master's in Applied Statistics?,8e1j1,ST2K,1173236363.0,https://www.reddit.com/r/statistics/comments/8e1j1/ask_stats_what_are_good_ways_to_prepare_for_a/,2.0,6.0,,en
1105392,2009-05-29 20:11:32,statistics,Can You Recommend a Good Book/Site to Learn Bayesian Statistics?,8o9u4,ST2K,1173236363.0,https://www.reddit.com/r/statistics/comments/8o9u4/can_you_recommend_a_good_booksite_to_learn/,13.0,5.0,,en
1105393,2009-06-01 04:56:58,statistics,"Is it possible to be Rational, Bayesian, and Non-racist?",8oqty,moultano,1160099427.0,https://www.reddit.com/r/statistics/comments/8oqty/is_it_possible_to_be_rational_bayesian_and/,6.0,10.0,,en
1105394,2009-06-02 01:23:40,statistics,Ask Stats: I'm getting an M.S. in stats and I'm looking for jobs where you apply stats to social sciences.  Any ideas?,8p01y,mathsuu,1197264006.0,https://www.reddit.com/r/statistics/comments/8p01y/ask_stats_im_getting_an_ms_in_stats_and_im/,4.0,4.0,,en
1105395,2009-06-09 03:21:39,artificial,Pleats Please by Issey Miyake.,8qvc1,acriacao,1219784932.0,https://www.reddit.com/r/artificial/comments/8qvc1/pleats_please_by_issey_miyake/,0.0,0.0,,en
1105396,2009-06-16 18:07:38,statistics,"[Q] Where can I get statistics on usage patterns (requests per second, peak load etc.) for sites like reddit? I just need a performane profile",8sz79,[deleted],,https://www.reddit.com/r/statistics/comments/8sz79/q_where_can_i_get_statistics_on_usage_patterns/,1.0,0.0,,en
1105397,2009-06-19 21:29:31,statistics,"Very cool analysis of Iran election, with links to the data and code...",8tz3a,[deleted],,https://www.reddit.com/r/statistics/comments/8tz3a/very_cool_analysis_of_iran_election_with_links_to/,0.0,0.0,,en
1105398,2009-07-01 23:59:58,statistics,Is a masters in Statistics worth it?,8xeyy,nazghash,1207678424.0,https://www.reddit.com/r/statistics/comments/8xeyy/is_a_masters_in_statistics_worth_it/,8.0,20.0,"I'm working on an MS in Statistics at a state school, and I'm curious how useful it will ultimately be.  Assuming no further schooling upon completion, would I be doomed to be a statistics ""technician"" or SAS programmer?  Is a Phd more-or-less required to get to do interesting/reasonably paid work?  Of course I haven't provided enough information for a definative answer, but when has that stopped redditers from posting opinions ...",en
1105399,2009-07-02 18:39:26,statistics,"For all you statistics geeks, an interactive explorer with all kinds of graphs and pictures",8xno0,[deleted],,https://www.reddit.com/r/statistics/comments/8xno0/for_all_you_statistics_geeks_an_interactive/,9.0,0.0,,en
1105400,2009-07-05 07:39:04,statistics,Books/resources that discuss change point analysis,8yasp,Abhishek_Ghose,1240309481.0,https://www.reddit.com/r/statistics/comments/8yasp/booksresources_that_discuss_change_point_analysis/,2.0,3.0,"I am looking for books/online-resources that discuss change point analysis at length (and its applications). For starters I have looked some of the stuff online and would like to go deeper. I am pretty new to this, so an 'introductory' sort-of book is preferred. Thanks!",en
1105401,2009-07-21 16:55:04,statistics,Ask Stats: I got a BS in stats and I'm starting my MA this fall.  I'm looking for a job/internship in NY for next summer.  What keywords do I need to search for to optimize my search results?  Any website recommendations?,935ne,mathsuu,1197264006.0,https://www.reddit.com/r/statistics/comments/935ne/ask_stats_i_got_a_bs_in_stats_and_im_starting_my/,0.0,1.0,"I should mention that I know SAS and R pretty well (not SAS certified yet).  I've taken classes in Regression, ANOVA, nonparametric methods, experimental design, intro to stochastic processes, and time series analysis.  I don't have work experience outside of working for the department as a TA and RA.",en
1105402,2009-07-27 00:07:34,statistics,List of Blogs : Data Mining Research,94r59,[deleted],,https://www.reddit.com/r/statistics/comments/94r59/list_of_blogs_data_mining_research/,3.0,1.0,,en
1105403,2009-07-29 17:35:16,MachineLearning,"Three Lessons in Analytic Strategy 
from the Netflix Prize",95nji,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/95nji/three_lessons_in_analytic_strategy_from_the/,8.0,0.0,,en
1105404,2009-07-29 18:27:18,MachineLearning,"Tom Mitchell on Learning. A good 
rationale  for this subreddit too.[PDF]",95o59,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/95o59/tom_mitchell_on_learning_a_good_rationale_for/,15.0,0.0,,en
1105405,2009-07-29 18:29:41,statistics,"Statisticians@Reddit:A new Machine 
Learning Subreddit. Let us discuss 
Data mining, machine learning, 
applied statistics, info retrieval 
issues.",95o6a,kunjaan,1223255404.0,https://www.reddit.com/r/statistics/comments/95o6a/statisticiansreddita_new_machine_learning/,6.0,0.0,,en
1105406,2009-07-29 18:36:58,MachineLearning,Introduction to Machine Learning (20 Lectures) – Stanford [AE],95o9l,RShnike,1234160162.0,https://www.reddit.com/r/MachineLearning/comments/95o9l/introduction_to_machine_learning_20_lectures/,46.0,5.0,,en
1105407,2009-07-29 18:53:26,MachineLearning,"Lot of talks on Machine Learning 
techniques and Learning Theory from 
leading experts.",95ohd,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/95ohd/lot_of_talks_on_machine_learning_techniques_and/,6.0,0.0,,en
1105408,2009-07-29 19:16:41,MachineLearning,"Research on a Game Based Approach 
to Label Geographical Relevance to 
Web Images",95or6,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/95or6/research_on_a_game_based_approach_to_label/,1.0,0.0,,en
1105409,2009-07-29 19:18:52,MachineLearning,Predicting the next viral tweet.,95os4,vpetro,1179701644.0,https://www.reddit.com/r/MachineLearning/comments/95os4/predicting_the_next_viral_tweet/,4.0,0.0,,en
1105410,2009-07-29 19:22:20,MachineLearning,"StatSnowball- a new framework for 
Relationship extraction.",95ots,[deleted],,https://www.reddit.com/r/MachineLearning/comments/95ots/statsnowball_a_new_framework_for_relationship/,1.0,0.0,,en
1105411,2009-07-29 19:22:56,MachineLearning,"StatSnowball- a new framework for 
Relationship extraction[PDF]",95ou9,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/95ou9/statsnowball_a_new_framework_for_relationship/,3.0,0.0,,en
1105412,2009-07-29 20:06:35,MachineLearning,Statistical Data Mining Tutorials,95pc6,isenfal,1248887136.0,https://www.reddit.com/r/MachineLearning/comments/95pc6/statistical_data_mining_tutorials/,12.0,0.0,,en
1105413,2009-07-29 21:22:20,MachineLearning,Ask Machine Learning: MLB Analysis,95q7p,[deleted],,https://www.reddit.com/r/MachineLearning/comments/95q7p/ask_machine_learning_mlb_analysis/,4.0,6.0,,en
1105414,2009-07-29 22:50:42,MachineLearning,"Collection of Open Source Softwares 
for Machine Learning algorithms, 
modeling etc.









All entries",95r79,[deleted],,https://www.reddit.com/r/MachineLearning/comments/95r79/collection_of_open_source_softwares_for_machine/,1.0,0.0,,en
1105415,2009-07-29 22:52:08,MachineLearning,"A good collection of Open Source 
Machine Learning Softwares.",95r7q,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/95r7q/a_good_collection_of_open_source_machine_learning/,8.0,0.0,,en
1105416,2009-07-29 23:51:37,MachineLearning,"'Maps of Science' using Lucene, Semantic Vectors and full-text of 5.7 million journal articles",95ru5,gnewton,1248900554.0,https://www.reddit.com/r/MachineLearning/comments/95ru5/maps_of_science_using_lucene_semantic_vectors_and/,4.0,0.0,,en
1105417,2009-07-30 00:00:48,MachineLearning,(cheap plug) Machine learning blog with code!,95rxe,[deleted],,https://www.reddit.com/r/MachineLearning/comments/95rxe/cheap_plug_machine_learning_blog_with_code/,1.0,0.0,,en
1105418,2009-07-30 00:11:35,MachineLearning,Machine learning blog with code snippets...,95s1e,ml123,1248901166.0,https://www.reddit.com/r/MachineLearning/comments/95s1e/machine_learning_blog_with_code_snippets/,9.0,0.0,,en
1105419,2009-07-30 16:24:32,MachineLearning,"JMLR provides ""high-quality scholarly 
articles in all areas of machine 
learning"" freely online.",95zac,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/95zac/jmlr_provides_highquality_scholarly_articles_in/,25.0,0.0,,en
1105420,2009-07-30 17:00:05,MachineLearning,"""Gaussian Processes for Machine 
Learning"" book freely available 
online from MIT Press.",95zpj,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/95zpj/gaussian_processes_for_machine_learning_book/,13.0,0.0,,en
1105421,2009-07-30 20:02:12,MachineLearning,"Introductory yet extensive tutorial on 
the basic ideas behind Support Vector 
Machines (SVMs).",961up,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/961up/introductory_yet_extensive_tutorial_on_the_basic/,20.0,3.0,,en
1105422,2009-07-31 07:20:53,MachineLearning,"David MacKay: Information Theory, 
Inference, and Learning Algorithms. 
[Free Book]",967n1,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/967n1/david_mackay_information_theory_inference_and/,17.0,2.0,,en
1105423,2009-07-31 15:51:04,MachineLearning,"Supervised Learning Method 
Comparison: ""Which supervised 
learning method works best for 
what?"" [Vid+Slide]",96bcd,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/96bcd/supervised_learning_method_comparison_which/,3.0,2.0,,en
1105424,2009-07-31 18:26:25,MachineLearning,"Crafting Papers on Machine Learning: 
Advice to authors of papers on 
Machine Learning.",96cw5,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/96cw5/crafting_papers_on_machine_learning_advice_to/,8.0,0.0,,en
1105425,2009-07-31 18:29:43,MachineLearning,Machine Learning Library for Hadoop,96cxc,[deleted],,https://www.reddit.com/r/MachineLearning/comments/96cxc/machine_learning_library_for_hadoop/,11.0,0.0,,en
1105426,2009-07-31 21:30:28,MachineLearning,"Some elementary mathematical 
concepts and their application in the 
development of the Vector Model of 
Information Retrieval.",96ewr,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/96ewr/some_elementary_mathematical_concepts_and_their/,3.0,0.0,,en
1105427,2009-07-31 22:12:16,MachineLearning,"Learning Algorithms implemented in 
Ruby",96fcw,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/96fcw/learning_algorithms_implemented_in_ruby/,13.0,0.0,,en
1105428,2009-07-31 22:16:05,MachineLearning,"Lots of Neural Network related 
resources",96fem,[deleted],,https://www.reddit.com/r/MachineLearning/comments/96fem/lots_of_neural_network_related_resources/,1.0,0.0,,en
1105429,2009-07-31 22:22:02,MachineLearning,"Neural Network Resources (nice 
collection but many links may be 
outdated)",96fgt,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/96fgt/neural_network_resources_nice_collection_but_many/,9.0,0.0,,en
1105430,2009-07-31 22:50:44,MachineLearning,Machine Learning Forum (re-post from compsci and sysor),96frc,[deleted],,https://www.reddit.com/r/MachineLearning/comments/96frc/machine_learning_forum_repost_from_compsci_and/,8.0,0.0,,en
1105431,2009-08-01 15:32:02,MachineLearning,"Kernel-Machines.org: Site devoted to 
learning methods building on kernels.",96lrh,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/96lrh/kernelmachinesorg_site_devoted_to_learning/,6.0,0.0,,en
1105432,2009-08-01 15:33:32,MachineLearning,"List of usage of Support Vector 
Machines.",96lrv,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/96lrv/list_of_usage_of_support_vector_machines/,6.0,0.0,,en
1105433,2009-08-01 15:37:19,MachineLearning,R reference Card [PDF],96lsp,[deleted],,https://www.reddit.com/r/MachineLearning/comments/96lsp/r_reference_card_pdf/,0.0,0.0,,en
1105434,2009-08-02 08:39:45,MachineLearning,"Does Machine Learning Really Work? 
- Tom Mitchell ",96rez,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/96rez/does_machine_learning_really_work_tom_mitchell/,8.0,0.0,,en
1105435,2009-08-02 08:41:04,MachineLearning,"An Draf of  ""Introduction to Machine 
Learning"" by Nils J. Nilsson",96rf7,[deleted],,https://www.reddit.com/r/MachineLearning/comments/96rf7/an_draf_of_introduction_to_machine_learning_by/,2.0,0.0,,en
1105436,2009-08-02 08:42:52,MachineLearning,"Several Review articles, Encyclopedia 
articles, and other Introductory 
information on Machine Learning.",96rfn,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/96rfn/several_review_articles_encyclopedia_articles_and/,6.0,0.0,,en
1105437,2009-08-02 08:46:04,MachineLearning,Reinforcement Learning Tutorial,96rga,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/96rga/reinforcement_learning_tutorial/,6.0,0.0,,en
1105438,2009-08-02 08:49:39,MachineLearning,"Applying Metrics to
Machine-Learning Tools
A Knowledge Engineering Approach
[PDF]",96rgs,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/96rgs/applying_metrics_to_machinelearning_tools_a/,3.0,0.0,,en
1105439,2009-08-02 08:50:58,MachineLearning,"""Machine  Learning: 
A  Historical  and  Methodological  
Analysis "". An gem on Machine 
Learning. [PDF]",96rgw,[deleted],,https://www.reddit.com/r/MachineLearning/comments/96rgw/machine_learning_a_historical_and_methodological/,1.0,0.0,,en
1105440,2009-08-02 08:57:19,MachineLearning,"""Machine Learning: A Historical and 
Methodological Analysis "". An old 
gem by Tom Mitchell. [PDF]",96rif,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/96rif/machine_learning_a_historical_and_methodological/,3.0,0.0,,en
1105441,2009-08-02 09:05:48,MachineLearning,"Self-Improving AI: Systems that 
improve themselves by learning from 
their own operations [Stanford VID]",96rjj,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/96rjj/selfimproving_ai_systems_that_improve_themselves/,3.0,0.0,,en
1105442,2009-08-02 09:07:15,MachineLearning,"Introduction to Robotics course at 
Stanford [16 Lecture VIDs]",96rjt,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/96rjt/introduction_to_robotics_course_at_stanford_16/,5.0,0.0,,en
1105443,2009-08-02 13:02:29,MachineLearning,Online Clustering?,96shq,gromgull,1195768827.0,https://www.reddit.com/r/MachineLearning/comments/96shq/online_clustering/,9.0,7.0,"(An attempt at getting some comments in the ML reddit, not just posts.)

I've been excited by online learning recently, i.e. algorithms where examples are processed one by one and only some aggregates are kept in memory. 

For example, the just open-sourced [Vowpal Wabbit](http://hunch.net/~vw/) or Leon Bottou's [svmsgd](http://leon.bottou.org/projects/sgd). These are both linear classifiers, Leon also has an online CRF implementation. 

Does anyone know of any online clustering algorithms? It seems a SVM can do clustering using an appropriate kernel? ",en
1105444,2009-08-02 20:53:54,MachineLearning,Artificial Intelligence: The Future Of Moral Debate!,96uyq,earlswynn,1216751059.0,https://www.reddit.com/r/MachineLearning/comments/96uyq/artificial_intelligence_the_future_of_moral_debate/,5.0,0.0,,en
1105445,2009-08-02 21:10:53,MachineLearning,"Using Machine Learning Techniques 
to Generate Compilers automatically.",96v38,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/96v38/using_machine_learning_techniques_to_generate/,7.0,0.0,,en
1105446,2009-08-02 21:19:29,MachineLearning,"A new and scalable click model based
on Bayesian framework that 
outperforms most of the other models
[PDF]",96v5y,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/96v5y/a_new_and_scalable_click_model_based_on_bayesian/,7.0,1.0,,en
1105447,2009-08-02 21:25:14,MachineLearning,"Hi Machine Learning enthusiasts, if 
you were to design a ""Subreddit 
Suggestion"" for Reddit , what 
methods would you use?",96v74,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/96v74/hi_machine_learning_enthusiasts_if_you_were_to/,18.0,19.0,"We have ""Suggest a Title"" for our links. But if you were to design a ""Suggest a Subreddit"" that  suggested an appropriate subreddit for your document, how would you develop that classifier?
If you refer to a paper, please provide a link to the original paper also. Thanks.",en
1105448,2009-08-03 15:19:03,statistics,"Lies, Damned Lies, and Statistics (4): Manipulating the Y-axis Scale in Graphs",9724z,[deleted],,https://www.reddit.com/r/statistics/comments/9724z/lies_damned_lies_and_statistics_4_manipulating/,3.0,0.0,,en
1105449,2009-08-03 23:12:18,MachineLearning,Ask MachineLearning: What do you consider to be the most exciting/innovative ideas in machine learning right now?,976pe,freyrs3,1209657115.0,https://www.reddit.com/r/MachineLearning/comments/976pe/ask_machinelearning_what_do_you_consider_to_be/,38.0,16.0,"As someone who is very intrigued by machine learning (although not very experienced) I am interested in finding out what you guys think are the ""hot"" areas of research.",en
1105450,2009-08-04 03:34:26,MachineLearning,"Vowpal Wabbit an open source 
infrastructure for implementing fast 
online algorithms from Yahoo! 
Research.",97897,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/97897/vowpal_wabbit_an_open_source_infrastructure_for/,14.0,0.0,,en
1105451,2009-08-04 17:05:40,MachineLearning,"Develop the best Learning agent for 
MARIO AI Competition.",97eqb,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/97eqb/develop_the_best_learning_agent_for_mario_ai/,16.0,1.0,,en
1105452,2009-08-05 08:12:09,MachineLearning,Hello MLreddit. Does anyone know of a good tutorial to get started on non-linear regression?,97nao,kungpaochicken,1202529408.0,https://www.reddit.com/r/MachineLearning/comments/97nao/hello_mlreddit_does_anyone_know_of_a_good/,12.0,2.0,,en
1105453,2009-08-05 17:26:30,MachineLearning,"The first paper written on Machine 
Learning written in 1956 that 
emphasized the importance of 
training sequences in constructing 
trial solutions to new problems.",97rjf,[deleted],,https://www.reddit.com/r/MachineLearning/comments/97rjf/the_first_paper_written_on_machine_learning/,1.0,0.0,,en
1105454,2009-08-05 17:27:57,MachineLearning,"The first Machine Learning Paper 
written on 1956 that ""emphasized 
the importance of training 
sequences, and the use of parts of 
previous solutions to problems in 
constructing trial solutions to new 
problems""",97rk3,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/97rk3/the_first_machine_learning_paper_written_on_1956/,5.0,1.0,,en
1105455,2009-08-06 08:32:09,MachineLearning,"""For Today’s Graduate, Just One 
Word: Statistics"" - NYTIMES.com",97ztu,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/97ztu/for_todays_graduate_just_one_word_statistics/,34.0,2.0,,en
1105456,2009-08-06 15:53:46,MachineLearning,"Summary of Machine Translation 
Papers at EMNLP",983b6,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/983b6/summary_of_machine_translation_papers_at_emnlp/,9.0,0.0,,en
1105457,2009-08-06 16:14:50,MachineLearning,"Papers: Spiking neural networks, liquid state machines, STDP, cortical microcircuit models, and more",983ir,[deleted],,https://www.reddit.com/r/MachineLearning/comments/983ir/papers_spiking_neural_networks_liquid_state/,11.0,0.0,,en
1105458,2009-08-06 21:27:38,MachineLearning,"Modification of  many collaborative 
filtering systems to make them 
scale , translation invariant and 
generally improve their accuracy 
without increasing their 
computational cost.",9872j,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9872j/modification_of_many_collaborative_filtering/,7.0,0.0,,en
1105459,2009-08-06 23:20:18,statistics,Social Network Analysis in R,9885m,agconway,1228006618.0,https://www.reddit.com/r/statistics/comments/9885m/social_network_analysis_in_r/,6.0,0.0,,en
1105460,2009-08-08 01:05:57,statistics,The rise of the statisticians,98lha,fstorino,1199712288.0,https://www.reddit.com/r/statistics/comments/98lha/the_rise_of_the_statisticians/,7.0,0.0,,en
1105461,2009-08-09 22:50:52,MachineLearning,"Do people accept recommender 
systems , under what circumstances 
they do and what can still be 
improved.",990k2,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/990k2/do_people_accept_recommender_systems_under_what/,8.0,0.0,,en
1105462,2009-08-10 21:34:55,MachineLearning,Data Mining and the Stock Market,99b43,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/99b43/data_mining_and_the_stock_market/,24.0,0.0,,en
1105463,2009-08-11 01:35:08,MachineLearning,YouTube - Theo Jansen: The art of creating creatures,99dh0,crucialfelix,1216084313.0,https://www.reddit.com/r/MachineLearning/comments/99dh0/youtube_theo_jansen_the_art_of_creating_creatures/,14.0,2.0,,en
1105464,2009-08-11 23:54:31,MachineLearning,Digital Intuition: Applying Common Sense Using Dimensionality Reduction and the Open Mind Common Sense project,99pf9,[deleted],,https://www.reddit.com/r/MachineLearning/comments/99pf9/digital_intuition_applying_common_sense_using/,1.0,0.0,,en
1105465,2009-08-12 00:04:59,MachineLearning,Digital Intuition: Applying Common Sense Using Dimensionality Reduction and the Open Mind Common Sense project,99pij,mycall,1183346313.0,https://www.reddit.com/r/MachineLearning/comments/99pij/digital_intuition_applying_common_sense_using/,9.0,0.0,,en
1105466,2009-08-12 08:53:31,MachineLearning,Data Mining for Fraud Detection.,99tsb,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/99tsb/data_mining_for_fraud_detection/,10.0,0.0,,en
1105467,2009-08-13 17:11:42,MachineLearning,"Complete Machine Learning course taught by Andrew Ng
on Youtube.",9aain,[deleted],,https://www.reddit.com/r/MachineLearning/comments/9aain/complete_machine_learning_course_taught_by_andrew/,37.0,6.0,,en
1105468,2009-08-14 00:45:16,MachineLearning,Surface Roughness Gage For Measuring Crankshafts &amp; Camshafts,9afjo,aovtask,1226443851.0,https://www.reddit.com/r/MachineLearning/comments/9afjo/surface_roughness_gage_for_measuring_crankshafts/,0.0,1.0,,en
1105469,2009-08-14 01:09:36,statistics,West/Middle east ass/tits-search difference.,9afqk,[deleted],,https://www.reddit.com/r/statistics/comments/9afqk/westmiddle_east_asstitssearch_difference/,0.0,0.0,,en
1105470,2009-08-14 18:44:00,MachineLearning,All-Purpose Crankshaft/Camshaft Gage - ADCOLE Model 1100,9anx9,aovtask,1226443851.0,https://www.reddit.com/r/MachineLearning/comments/9anx9/allpurpose_crankshaftcamshaft_gage_adcole_model/,0.0,1.0,,en
1105471,2009-08-15 11:08:16,statistics,Bernie Madoff and Data Visualization,9auza,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/9auza/bernie_madoff_and_data_visualization/,0.0,0.0,,en
1105472,2009-08-17 23:14:44,statistics,Anyone want to calculate the odds of this?,9bho3,delkarnu,1195599217.0,https://www.reddit.com/r/statistics/comments/9bho3/anyone_want_to_calculate_the_odds_of_this/,0.0,0.0,,en
1105473,2009-08-19 04:53:33,MachineLearning,"Lean Manufacturing Certification
",9bx7l,rockingking99,1229363224.0,https://www.reddit.com/r/MachineLearning/comments/9bx7l/lean_manufacturing_certification/,0.0,0.0,,en
1105474,2009-08-21 13:44:17,statistics,Pick your fantasy football team with Solver,9cqvl,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/9cqvl/pick_your_fantasy_football_team_with_solver/,0.0,0.0,,en
1105475,2009-08-21 20:08:35,MachineLearning,Face Recognition/Authentication Using Support Vector Machines,9cuvc,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/9cuvc/face_recognitionauthentication_using_support/,14.0,0.0,,en
1105476,2009-08-22 06:50:50,MachineLearning,[Repost]: Face Recognition using Eigenfaces and Distance Classifiers: A Tutorial,9d00p,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9d00p/repost_face_recognition_using_eigenfaces_and/,11.0,1.0,,en
1105477,2009-08-22 17:31:31,statistics,Are women discriminated against in graduate admissions? Simpson's paradox via R in three easy steps!,9d3m5,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/9d3m5/are_women_discriminated_against_in_graduate/,10.0,1.0,,en
1105478,2009-08-23 22:28:28,artificial,/r/artificial: Solution better than AIML for bots?,9dda3,epicRelic,1203055754.0,https://www.reddit.com/r/artificial/comments/9dda3/rartificial_solution_better_than_aiml_for_bots/,7.0,1.0,,en
1105479,2009-08-24 18:27:22,MachineLearning,Growing Neural Gas [PDF],9dm4f,AndreasBWagner,1192582160.0,https://www.reddit.com/r/MachineLearning/comments/9dm4f/growing_neural_gas_pdf/,9.0,1.0,,en
1105480,2009-08-26 09:16:56,MachineLearning,"Zipf's Law, Benford's Law from Interactive Mathematics Miscellany and Puzzles",9e7mx,wangfengmadking,1227232928.0,https://www.reddit.com/r/MachineLearning/comments/9e7mx/zipfs_law_benfords_law_from_interactive/,3.0,0.0,,en
1105481,2009-08-26 16:05:08,statistics,Portal:Statistics,9eavg,[deleted],,https://www.reddit.com/r/statistics/comments/9eavg/portalstatistics/,12.0,1.0,,en
1105482,2009-08-31 00:14:37,MachineLearning,IEEE Computer Magazine: The Pervasiveness of Data Mining and Machine Learning,9fncn,[deleted],,https://www.reddit.com/r/MachineLearning/comments/9fncn/ieee_computer_magazine_the_pervasiveness_of_data/,7.0,1.0,,en
1105483,2009-08-31 12:08:19,MachineLearning,Dirichlet Processes: Tutorial and Practical Course,9fsew,wangfengmadking,1227232928.0,https://www.reddit.com/r/MachineLearning/comments/9fsew/dirichlet_processes_tutorial_and_practical_course/,20.0,0.0,,en
1105484,2009-08-31 21:37:54,statistics,"MeanSquaredError.com, StackOverflow interface for stats, ML, data analysis questions",9fxz8,agconway,1228006618.0,https://www.reddit.com/r/statistics/comments/9fxz8/meansquarederrorcom_stackoverflow_interface_for/,9.0,8.0,,en
1105485,2009-09-01 17:21:50,statistics,"Lies, Damned Lies, and Statistics (6): Statistical Bias in the Design and Execution of Surveys",9g7s5,[deleted],,https://www.reddit.com/r/statistics/comments/9g7s5/lies_damned_lies_and_statistics_6_statistical/,5.0,1.0,,en
1105486,2009-09-02 15:49:32,MachineLearning,Ensemble learning,9gjub,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/9gjub/ensemble_learning/,13.0,1.0,,en
1105487,2009-09-03 22:29:34,statistics,"Lies, Damned Lies, and Statistics (5): (Not) Using a Left and a Right Y-axis",9h1rr,[deleted],,https://www.reddit.com/r/statistics/comments/9h1rr/lies_damned_lies_and_statistics_5_not_using_a/,3.0,0.0,,en
1105488,2009-09-04 14:01:45,MachineLearning,[Repost] Machine learning classifier gallery,9h9ao,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9h9ao/repost_machine_learning_classifier_gallery/,19.0,2.0,,en
1105489,2009-09-04 18:44:43,statistics,Can anyone recommend a tutorial on implementing decision trees in R?,9hc36,evandec,1226613867.0,https://www.reddit.com/r/statistics/comments/9hc36/can_anyone_recommend_a_tutorial_on_implementing/,0.0,1.0,,en
1105490,2009-09-04 23:55:36,MachineLearning,"Peter Norvig on ""Innovations in AI and Search"". [VID]",9hfbl,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9hfbl/peter_norvig_on_innovations_in_ai_and_search_vid/,31.0,0.0,,en
1105491,2009-09-05 05:58:37,MachineLearning,"Pattern Recognition Information: bibliographies, jobs, conferences and news.",9hhs2,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9hhs2/pattern_recognition_information_bibliographies/,1.0,0.0,,en
1105492,2009-09-05 05:59:07,MachineLearning,"Reinforcement Learning Repository at University of Massachusetts, Amherst",9hhsa,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9hhsa/reinforcement_learning_repository_at_university/,3.0,1.0,,en
1105493,2009-09-05 06:00:32,MachineLearning,"Machine Learning Resources: Applications, Books, Courses, Data Repositories , ML Search Engines , Software , Tutorials...",9hhsp,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9hhsp/machine_learning_resources_applications_books/,10.0,1.0,,en
1105494,2009-09-05 06:01:54,MachineLearning,K-Mean Clustering Tutorial,9hht5,[deleted],,https://www.reddit.com/r/MachineLearning/comments/9hht5/kmean_clustering_tutorial/,0.0,0.0,,en
1105495,2009-09-05 06:02:40,MachineLearning,"Information for researchers and practitioners interested in the use of learning techniques in intelligent, user-adaptive systems. ",9hht9,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9hht9/information_for_researchers_and_practitioners/,1.0,0.0,,en
1105496,2009-09-05 06:11:14,MachineLearning,Free book which compares and evaluates a range of classification techniques and introduces complex classification techniques.,9hhux,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9hhux/free_book_which_compares_and_evaluates_a_range_of/,2.0,0.0,,en
1105497,2009-09-07 03:15:35,MachineLearning,UC Irvine's Machine Learning Datasets,9hxxz,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9hxxz/uc_irvines_machine_learning_datasets/,18.0,2.0,,en
1105498,2009-09-07 03:24:49,statistics,American Vice: Mapping The 7 Deadly Sins,9hxzq,blogannath,1246840412.0,https://www.reddit.com/r/statistics/comments/9hxzq/american_vice_mapping_the_7_deadly_sins/,5.0,1.0,,en
1105499,2009-09-07 16:42:26,MachineLearning,Open source data visualization and analysis. Extensions for bioinformatics and text mining. [Python],9i3p7,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9i3p7/open_source_data_visualization_and_analysis/,12.0,0.0,,en
1105500,2009-09-07 16:49:43,MachineLearning,"By combining techniques from game theory and artificial intelligence, computer scientists at the University of Michigan have developed a better way to find the best bidding strategy in a simulated auction modeled after commodity and financial securities markets.[PDF]",9i3rm,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9i3rm/by_combining_techniques_from_game_theory_and/,2.0,0.0,,en
1105501,2009-09-07 16:52:19,MachineLearning, MCL(Markov Cluster Algorithm):Fast &amp; scalable unsupervised cluster algorithm for graphs based on simulation of (stochastic) flow in graphs. ,9i3s4,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9i3s4/mclmarkov_cluster_algorithmfast_scalable/,3.0,0.0,,en
1105502,2009-09-07 16:55:04,MachineLearning,"Set of lightweight UNIX/POSIX utilities to classify text documents automatically, according to Bayesian statistical principles.",9i3t5,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9i3t5/set_of_lightweight_unixposix_utilities_to/,4.0,0.0,,en
1105503,2009-09-07 16:58:55,MachineLearning,Morphix-NLP : Live CD Linux distro with a rich collection of NLP applications. ,9i3uj,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9i3uj/morphixnlp_live_cd_linux_distro_with_a_rich/,1.0,0.0,,en
1105504,2009-09-07 17:00:31,MachineLearning,OpenNLP : Set of tools for NLP.,9i3v0,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9i3v0/opennlp_set_of_tools_for_nlp/,9.0,8.0,,en
1105505,2009-09-07 17:02:00,MachineLearning,Predicting Real-valued Outputs: An introduction to regression. [Tutorial Slide],9i3v7,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9i3v7/predicting_realvalued_outputs_an_introduction_to/,1.0,0.0,,en
1105506,2009-09-07 17:50:48,MachineLearning,Dataset from Caltech for Image Recognition.,9i4eb,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9i4eb/dataset_from_caltech_for_image_recognition/,4.0,1.0,,en
1105507,2009-09-07 17:52:09,MachineLearning,Reinforcement Learning Introduction by example [VID],9i4ev,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9i4ev/reinforcement_learning_introduction_by_example_vid/,2.0,0.0,,en
1105508,2009-09-07 17:54:09,MachineLearning,"The Next Generation of Neural Networks from Geoffrey Hinton, a leading AI researcher. [ Google Talk]",9i4fp,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9i4fp/the_next_generation_of_neural_networks_from/,23.0,3.0,,en
1105509,2009-09-07 17:59:40,MachineLearning,Why People think Computers Can't by Marvin Minsky (1982)[PDF],9i4hl,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9i4hl/why_people_think_computers_cant_by_marvin_minsky/,7.0,0.0,,en
1105510,2009-09-07 18:07:00,MachineLearning,Ten problems for the next 10 year. [VID],9i4kb,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9i4kb/ten_problems_for_the_next_10_year_vid/,1.0,0.0,,en
1105511,2009-09-07 18:21:28,MachineLearning,Comparison for 5 approaches to collecting Tags for music.[PDF],9i4pb,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9i4pb/comparison_for_5_approaches_to_collecting_tags/,1.0,0.0,,en
1105512,2009-09-07 23:34:47,statistics,Whither Data Mining?,9i7bx,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/9i7bx/whither_data_mining/,1.0,0.0,,en
1105513,2009-09-07 23:35:00,MachineLearning,Whither Data Mining?,9i7c0,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/9i7c0/whither_data_mining/,3.0,1.0,,en
1105514,2009-09-09 02:26:30,statistics,Measuring Human Rights (6): Don't Make Governments Do It,9iltr,[deleted],,https://www.reddit.com/r/statistics/comments/9iltr/measuring_human_rights_6_dont_make_governments_do/,1.0,0.0,,en
1105515,2009-09-09 12:36:57,MachineLearning,"Machine Learning in R, in a nutshell",9iqyq,StompChicken,1248945961.0,https://www.reddit.com/r/MachineLearning/comments/9iqyq/machine_learning_in_r_in_a_nutshell/,12.0,0.0,,en
1105516,2009-09-09 16:03:02,MachineLearning,"Some interesting papers from NLP conferences , ACL and EMNLP.",9isre,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9isre/some_interesting_papers_from_nlp_conferences_acl/,6.0,0.0,,en
1105517,2009-09-09 16:04:55,MachineLearning,Doug Rivers: Second Thoughts About Internet Surveys,9iss2,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9iss2/doug_rivers_second_thoughts_about_internet_surveys/,2.0,0.0,,en
1105518,2009-09-09 17:43:17,statistics,A statistical look at when the express lane is faster,9itwf,z0rr0,1248205910.0,https://www.reddit.com/r/statistics/comments/9itwf/a_statistical_look_at_when_the_express_lane_is/,7.0,1.0,,en
1105519,2009-09-10 12:22:27,statistics,Datasets for Statistical Analysis,9j476,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/9j476/datasets_for_statistical_analysis/,4.0,0.0,,en
1105520,2009-09-10 12:30:00,artificial,Aesthetics in chess,9j49n,m00natic,1239549327.0,https://www.reddit.com/r/artificial/comments/9j49n/aesthetics_in_chess/,2.0,0.0,,en
1105521,2009-09-10 17:16:25,statistics,"How does one compute the sample size for a goodness of fit test (Kolmogorov–Smirnov or Chi-squared, etc.)?",9j6v7,freireib,1250189626.0,https://www.reddit.com/r/statistics/comments/9j6v7/how_does_one_compute_the_sample_size_for_a/,6.0,17.0,,en
1105522,2009-09-10 22:43:35,MachineLearning,Machine Learners: what applications of ML do you find most exciting?,9jap0,bmiguy,1223053823.0,https://www.reddit.com/r/MachineLearning/comments/9jap0/machine_learners_what_applications_of_ml_do_you/,11.0,16.0,,en
1105523,2009-09-11 02:46:29,statistics,"Lies, Damned Lies, and Statistics (9): Too Small Sample Sizes in Surveys",9jcug,[deleted],,https://www.reddit.com/r/statistics/comments/9jcug/lies_damned_lies_and_statistics_9_too_small/,5.0,1.0,,en
1105524,2009-09-12 09:15:38,statistics,Relationships Between Univariate Distributions (awesome figure),9jsjf,[deleted],,https://www.reddit.com/r/statistics/comments/9jsjf/relationships_between_univariate_distributions/,16.0,2.0,,en
1105525,2009-09-12 16:35:53,MachineLearning,White House Clarifies:  Private Insurance Market will shrink once the Government Exchange is implemented.,9juxx,[deleted],,https://www.reddit.com/r/MachineLearning/comments/9juxx/white_house_clarifies_private_insurance_market/,1.0,0.0,,en
1105526,2009-09-13 04:08:38,statistics,Fitting multilevel models with Ajax: a webinterface for R's lme4-package,9jzew,mhermans,1169219262.0,https://www.reddit.com/r/statistics/comments/9jzew/fitting_multilevel_models_with_ajax_a/,0.0,0.0,,en
1105527,2009-09-13 19:40:04,MachineLearning,"A recommendation algorithm using probabilistic matrix factorization, with full Python implementation.",9k4gk,danger_t,1249412021.0,https://www.reddit.com/r/MachineLearning/comments/9k4gk/a_recommendation_algorithm_using_probabilistic/,29.0,3.0,,en
1105528,2009-09-13 21:29:39,MachineLearning,"Graphical Models, Exponential Families, and Variational Inference - a 300 page advanced tutorial",9k577,urish,1221689900.0,https://www.reddit.com/r/MachineLearning/comments/9k577/graphical_models_exponential_families_and/,12.0,3.0,,en
1105529,2009-09-14 14:25:58,statistics,How Sam L. Savage is righting statistical wrongs,9kchr,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/9kchr/how_sam_l_savage_is_righting_statistical_wrongs/,7.0,0.0,,en
1105530,2009-09-14 19:32:46,statistics,Advanced NFL Stats,9kfpq,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/9kfpq/advanced_nfl_stats/,1.0,0.0,,en
1105531,2009-09-15 03:50:35,statistics,"Ask Stats: (probably stupid) normal distribution question, in R",9kksv,[deleted],,https://www.reddit.com/r/statistics/comments/9kksv/ask_stats_probably_stupid_normal_distribution/,1.0,0.0,,en
1105532,2009-09-15 04:00:43,statistics,Ask Stats: (probably stupid) R question concerning (double) normal distributions,9kkw4,Vulpius,1236278735.0,https://www.reddit.com/r/statistics/comments/9kkw4/ask_stats_probably_stupid_r_question_concerning/,2.0,7.0,"Hi r/Statistics, quick question, I've created a sample dataset in R as such:

    set1 &lt;- rnorm(1000, mean=0, sd=1)
    set2 &lt;- rnorm(100, mean=5, sd=1)
    set3 &lt;- rnorm(100, mean=10, sd=1)
    allpoints &lt;- c(set1, set2, set3)

When you plot the density of allpoints, you get a nice big hill, followed by two small one, as expected, of course. Basically my data comes from a set of multiple normal distributions, but I assume these could be other distributions as well.

Now what I want to do: starting from allpoints, give me a set of three (let's say we know the amount beforehand) mean/sdpairs which form normal distributions which fits the given dataset in the best way. So basically, I want means=c(0, 5, 10) and sd=(0, 5, 10) returned to me (or something in the vicinity, a best guess).

A feel that there is probably a function to do this exact thing but could not find it due to my inexperience. Can anybody help me out here? The closest I came was using kmeans to find three clusters, and using the mean/sd of each cluster to construct the three distributions, still, this feels as the wrong way to do this.

Thanks!",en
1105533,2009-09-15 10:51:29,statistics,"""The Flaw of Averages""—Why we underestimate risk in the face of certainty",9kofy,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/9kofy/the_flaw_of_averageswhy_we_underestimate_risk_in/,0.0,0.0,,en
1105534,2009-09-17 20:00:07,MachineLearning,"Dear MLeddit, I have a few questions about Artificial Neural Networks that I don't trust my professor to answer. Ayudame!",9ljrd,authorblues,1211915825.0,https://www.reddit.com/r/MachineLearning/comments/9ljrd/dear_mleddit_i_have_a_few_questions_about/,15.0,17.0,"So we are studying Artificial Neural Networks as one of our first steps in my machine learning course, and I have some questions for which I don't think I want to go to my professor:

**Background:** I am developing the same old alpha-beta search-tree pruning chess playing program that everyone else in the world has. I plan to, at some point, set it up to run on a Beowulf cluster or some other sort of distributed network in order to push it faster. I would like, however to set up its ""base case"" (evaluation of a single position in a chess game as the leaf of the tree) to be evaluated through a neural network, one that I have trained with the large amount of expert data (LARGE amount) I have collected. Evaluation will be the process of assigning a ""score"" to a position, with a maximum score of K, where a score of +K is the best possible score in favor of white, and -K is the best possible score in favor of black.

**Expert Data:** A bit more information about my expert data: chess games are often stored in a notation called PGN, the most important details given in this expert data is A) who won the game (which I think I can use to decide whose moves to categorize positively and whose moves to categorize negatively), and B) every move made, which I already have a system capable of parsing these moves.

1) I know that the basis of training a neural network (and really the basis of any machine learning training phase) is attempting to achieve results as close to a target function as possible. In this case, I don't have a ""known"" target function. I just want to be as ""correct"" as possible. In cases like this, where the target function isn't known up-front, but can really only be assessed later (as in, this player won, so his moves should possibly be scored higher), how is the best way to achieve this goal?

2) For a chessboard representation, there are many pieces of information that can be gathered: what piece is on a square, how many of one piece are on a board, where is a particular piece located (sort of the reverse of ""what piece is on a square""), what is the ""positional value"" (some pieces are more suited at particular locations on the board) or ""material value"" (some pieces are worth more than others) of a board, things of this nature. What should the input to a neural network given the abundance of data available regarding each position? is it feasible to expect the input nodes to be 64 nodes representing each position on the board, and this be enough for the network to be able to learn the subtle nuances between pieces and positions and strength?

In regards to (2), I’m not asking what my input set should be. I don’t want you guys to do my work for me. I just don’t know whether or not ONLY providing the 64 squares of the board as input data is a realistic input set to expect the results I’m wanting.

3) Is it problematic -- or really, does it even matter -- that the input set is likely to be discrete, while I’m wanting continuous output? I don’t think I quite grasp whether or not this is an issue with neural networks.

4) Is it possible to give a neural network “negative” training data? I’m not sure if it would help or hurt, but with this wealth of expert data I have, I could potentially use the loser as negative training data. If I can’t, I honestly have more than enough “positive” data that this shouldn’t be a problem.",en
1105535,2009-09-17 20:39:33,statistics,"Scrabble rants - Statistical Modeling, Causal Inference, and Social Science",9lk6q,sidsavara,1221328297.0,https://www.reddit.com/r/statistics/comments/9lk6q/scrabble_rants_statistical_modeling_causal/,7.0,0.0,,en
1105536,2009-09-20 07:34:42,statistics,FMRI response from mature Atlantic Salmon assessing the emotions of human subjects. The salmon was not alive at the time of scanning.,9ma8u,[deleted],,https://www.reddit.com/r/statistics/comments/9ma8u/fmri_response_from_mature_atlantic_salmon/,3.0,0.0,,en
1105537,2009-09-20 15:13:49,statistics,FMRI of mature Atlantic Salmon assessing the emotions of human subjects. The salmon was not alive at the time of scanning.,9mc4e,moultano,1160099427.0,https://www.reddit.com/r/statistics/comments/9mc4e/fmri_of_mature_atlantic_salmon_assessing_the/,13.0,0.0,,en
1105538,2009-09-20 19:36:16,statistics,Lies and Statistics: Defense Spending in the U.S.,9mdkq,[deleted],,https://www.reddit.com/r/statistics/comments/9mdkq/lies_and_statistics_defense_spending_in_the_us/,7.0,1.0,,en
1105539,2009-09-22 02:27:01,MachineLearning,StatLib---Datasets Archive,9mrkx,athosprime,1251776941.0,https://www.reddit.com/r/MachineLearning/comments/9mrkx/statlibdatasets_archive/,12.0,0.0,,en
1105540,2009-09-22 04:30:03,artificial,The New Science of Causality,9msi2,last_useful_man,1241273287.0,https://www.reddit.com/r/artificial/comments/9msi2/the_new_science_of_causality/,5.0,1.0,,en
1105541,2009-09-22 16:46:56,MachineLearning,How Much Is That? Data on inflation,9my6x,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/9my6x/how_much_is_that_data_on_inflation/,2.0,0.0,,en
1105542,2009-09-24 21:27:45,statistics,Measuring Human Rights (7): Don’t Let Governments Make it Easy on Themselves,9nr3k,[deleted],,https://www.reddit.com/r/statistics/comments/9nr3k/measuring_human_rights_7_dont_let_governments/,3.0,1.0,,en
1105543,2009-09-26 10:37:18,MachineLearning,Some members of the Machine Learning department at Carnegie Mellon University took to the streets for the G20 protest.,9o9dn,dwf,1180746219.0,https://www.reddit.com/r/MachineLearning/comments/9o9dn/some_members_of_the_machine_learning_department/,7.0,8.0,,en
1105544,2009-09-26 11:02:48,MachineLearning,What are some good schools to go to for a Masters in machine learning? In US &amp; Canada.,9o9j9,Dman003,1203582639.0,https://www.reddit.com/r/MachineLearning/comments/9o9j9/what_are_some_good_schools_to_go_to_for_a_masters/,13.0,15.0,,en
1105545,2009-09-27 07:33:48,MachineLearning,A Machine Learning-ish Take on the Analysis of Pollster Fraud and Oklahoma Students,9ognb,danger_t,1249412021.0,https://www.reddit.com/r/MachineLearning/comments/9ognb/a_machine_learningish_take_on_the_analysis_of/,8.0,0.0,,en
1105546,2009-10-01 00:58:28,MachineLearning,"Any large, free EEG databases out there?",9pnpi,bmiguy,1223053823.0,https://www.reddit.com/r/MachineLearning/comments/9pnpi/any_large_free_eeg_databases_out_there/,8.0,10.0,,en
1105547,2009-10-01 11:10:41,MachineLearning,Data-Mining Medical Records Could Predict Domestic Violence,9psfo,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/9psfo/datamining_medical_records_could_predict_domestic/,15.0,0.0,,en
1105548,2009-10-02 05:41:32,artificial,The End of AI Winter,9q25j,last_useful_man,1241273287.0,https://www.reddit.com/r/artificial/comments/9q25j/the_end_of_ai_winter/,8.0,1.0,,en
1105549,2009-10-02 06:01:47,rstats,How to use a Google Spreadsheet as data in R,9q2b0,ffualo,1203929903.0,https://www.reddit.com/r/rstats/comments/9q2b0/how_to_use_a_google_spreadsheet_as_data_in_r/,3.0,0.0,,en
1105550,2009-10-02 06:10:32,rstats,Google's R Style Guide,9q2d5,ffualo,1203929903.0,https://www.reddit.com/r/rstats/comments/9q2d5/googles_r_style_guide/,6.0,0.0,,en
1105551,2009-10-02 14:42:41,statistics,Mahalanobis distance comes from 1920's race research?,9q5px,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/9q5px/mahalanobis_distance_comes_from_1920s_race/,3.0,0.0,,en
1105552,2009-10-02 16:20:11,MachineLearning,nirvdrum's weblog :: GitHub Contest Recap,9q6h7,wangfengmadking,1227232928.0,https://www.reddit.com/r/MachineLearning/comments/9q6h7/nirvdrums_weblog_github_contest_recap/,4.0,0.0,,en
1105553,2009-10-02 17:19:48,MachineLearning,Linear Regression program of Team Gravity,9q717,wangfengmadking,1227232928.0,https://www.reddit.com/r/MachineLearning/comments/9q717/linear_regression_program_of_team_gravity/,11.0,0.0,,en
1105554,2009-10-03 02:32:53,MachineLearning,Averaging Models - what do you goes do? (attempted repost),9qcfl,petewilko,1239354518.0,https://www.reddit.com/r/MachineLearning/comments/9qcfl/averaging_models_what_do_you_goes_do_attempted/,4.0,6.0,Do you train multiple models and combine/average the classification scores from each - or attempt to average the actual models themselves? Any good pointers for varying approaches as trying to broaden out horizons.,en
1105555,2009-10-04 02:07:37,statistics,Please help with my conceptual fog around simple Correlation.,9qkva,suckeggmule,1250642053.0,https://www.reddit.com/r/statistics/comments/9qkva/please_help_with_my_conceptual_fog_around_simple/,1.0,8.0,"When I consider correlation conceptually, I find myself thinking about the “common” variance in each pair of measures.  I think I should be able to find the related amount of shared variance in each pair, then take an average to get a correlation score.

For example (see image http://imgur.com/0hfDY.jpg) the first pair has about 49% of deviation in common, while the 7th pair shares about 15% (but in opposite directions).  

But no matter how I do this kind of math, I never get the Pearson correlation score.  What is my conceptual mistake?

I think it might be that correlation is actually a measure of “part to whole” (observed covariance relative to maximum possible covariance), whereas my conceptual approach is “pattern of common variance amongst pairs”, which is NOT correlation, but something else??
",en
1105556,2009-10-05 12:31:27,statistics,Random Walks and Anomalous Diffusion,9qx1z,[deleted],,https://www.reddit.com/r/statistics/comments/9qx1z/random_walks_and_anomalous_diffusion/,0.0,0.0,,en
1105557,2009-10-06 05:43:08,artificial,CMU Machine Learning Lab G-20 Protest,9r6j2,last_useful_man,1241273287.0,https://www.reddit.com/r/artificial/comments/9r6j2/cmu_machine_learning_lab_g20_protest/,1.0,0.0,,en
1105558,2009-10-07 04:01:29,statistics,Ask Stats: What method allows me to determine the proportion of a distribution that is skewed relative to another distribution?,9riqs,quaternion,1252017693.0,https://www.reddit.com/r/statistics/comments/9riqs/ask_stats_what_method_allows_me_to_determine_the/,4.0,16.0,"I have two sets of observations (let's say n=50 for both sets).  The first set is normally distributed.  The second set is drawn from the same distribution as the first, but differs in one crucial way: all observations in the set that are larger than some value (N) are transformed such that X = X + B, where B is another normally distributed positive variable.

Is there some method I could use to find N, without knowing B?

For example:
Set 1:

1

1

2

2

2

3

3

Set 2:

1

1

2

2

2

6

6

Here the value of N is 2 and the value of B is 3.  In my actual data I have &gt;100 such sets, one per human subject.

I thought about finding N as the value that minimizes the p-value of a t-test between the two sets, but I realized that the data used in that process wouldn't conform to the normality assumptions of the t-test.

Then I thought about using k-means clustering with k=2 on the second set and then looking at the proportion of cases which belong to the cluster with a larger value, but this seems like a sloppy solution.

Please, stats geniuses, help me out...

EDIT: fixed problem description per [deleted]'s comment. ",en
1105559,2009-10-07 22:35:52,MachineLearning,Beautiful Data book chapter,9rsww,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/9rsww/beautiful_data_book_chapter/,5.0,0.0,,en
1105560,2009-10-08 00:48:55,statistics,"What do /r/stat subscribers ""do""?",9ru6x,slacker22,1220431135.0,https://www.reddit.com/r/statistics/comments/9ru6x/what_do_rstat_subscribers_do/,17.0,30.0,Personally I'm reading for a degree (undergraduate) in actuarial science.,en
1105561,2009-10-08 15:42:17,statistics,"A Gorgeous Chart: Facebook, Twitter, MySpace [Stats]",9s15b,williswee,1250960172.0,https://www.reddit.com/r/statistics/comments/9s15b/a_gorgeous_chart_facebook_twitter_myspace_stats/,1.0,3.0,,en
1105562,2009-10-08 18:58:08,datasets,New York City Data Mine - 170 datasets from 30 agencies,9s3dg,antitheftdevice,1197662102.0,https://www.reddit.com/r/datasets/comments/9s3dg/new_york_city_data_mine_170_datasets_from_30/,6.0,0.0,,en
1105563,2009-10-10 22:55:14,statistics,Time series in R,9ssuk,[deleted],,https://www.reddit.com/r/statistics/comments/9ssuk/time_series_in_r/,0.0,0.0,,en
1105564,2009-10-10 23:23:24,statistics,AskStats: Weight tracking using time series in R,9st0b,Tafkas,1231818191.0,https://www.reddit.com/r/statistics/comments/9st0b/askstats_weight_tracking_using_time_series_in_r/,0.0,5.0,"Hi,

I am tracking my body weight in a spread sheet but I want to improve the experience by using R. I was trying to find some information about time series analysis in R but I was not succesful.

The data I have here is in the following format:

date -&gt; weight -&gt; body-fat-percentage -&gt; water-percentage

e.g. 10/08/09 -&gt; 84.30 -&gt; 18.20 -&gt; 55.3

What I want to do

plot weight and exponential moving average against time

How can I achieve that?
",en
1105565,2009-10-14 06:23:29,statistics,Ask /r/statistics: Can anyone recommend a good piece of software for drawing plate diagrams / graphical models?,9tsuc,lmaurits,1215232856.0,https://www.reddit.com/r/statistics/comments/9tsuc/ask_rstatistics_can_anyone_recommend_a_good_piece/,0.0,4.0,"I'm not talking about a graphic editing program like The Gimp or Photoshop which happens to make the task not too hard, I'm really hoping for something specifically designed for this task, where I can just specify the model using some sort text file format and then have it spit out a pretty .png, .eps, whatever, suitable for including in a journal paper.

Something that runs on Linux would be preferable, but if it absolutely has to be Windows then so be it.",en
1105566,2009-10-14 17:04:47,MachineLearning,What are the best schools for a Master degree in Machine Learning in Europe?,9tygv,[deleted],,https://www.reddit.com/r/MachineLearning/comments/9tygv/what_are_the_best_schools_for_a_master_degree_in/,1.0,0.0,,en
1105567,2009-10-14 17:21:08,MachineLearning,What are the best schools for a Master degree in Machine Learning in Europe?,9tyow,nphrk,1255395412.0,https://www.reddit.com/r/MachineLearning/comments/9tyow/what_are_the_best_schools_for_a_master_degree_in/,8.0,9.0,,en
1105568,2009-10-14 19:51:02,MachineLearning,"The Elements of Statistical Learning (Hastie, Tibshirani and Friedman) is now available as free PDF
	
",9u0ot,StompChicken,1248945961.0,https://www.reddit.com/r/MachineLearning/comments/9u0ot/the_elements_of_statistical_learning_hastie/,51.0,6.0,,en
1105569,2009-10-15 07:21:01,statistics,AskStats: Any time series modeling (ARMA/ARIMA) tools for Python?,9u78f,roger_,1178076247.0,https://www.reddit.com/r/statistics/comments/9u78f/askstats_any_time_series_modeling_armaarima_tools/,6.0,2.0,"I've come across scikits.timeseries, but I think that just does manipulation and plotting. Does anyone know of any modeling tools?",en
1105570,2009-10-15 11:48:10,MachineLearning,I'm doing a graduation thesis in multi agent systems and I can propose a problem: can you give me some advice?,9u9jn,krat,1192222914.0,https://www.reddit.com/r/MachineLearning/comments/9u9jn/im_doing_a_graduation_thesis_in_multi_agent/,1.0,0.0,"I'm about to begin my graduation thesis and my professor asked me to come up with a problem to solve in the MAS domain. Basically, I can propose quite everything, the only requirements are that the whole thing will make extensive use of reinforcement learning (better if it is online RL) and the domain is *not* the robotic soccer. So I am asking: do you have any idea on what cool domain/problem to consider for my graduation thesis?",en
1105571,2009-10-15 16:17:37,statistics,Understanding the Monty Hall problem,9ubpg,[deleted],,https://www.reddit.com/r/statistics/comments/9ubpg/understanding_the_monty_hall_problem/,10.0,6.0,,en
1105572,2009-10-15 19:50:20,statistics,Ask Stats: Maximum Likelihood Estimation of the ExGaussian parameters... Do I have it right?,9uecu,[deleted],,https://www.reddit.com/r/statistics/comments/9uecu/ask_stats_maximum_likelihood_estimation_of_the/,0.0,2.0,,en
1105573,2009-10-15 21:06:20,statistics,"The Elements of Statistical Learning (Hastie, Tibshirani and Friedman) is now available as free PDF",9ufa2,StompChicken,1248945961.0,https://www.reddit.com/r/statistics/comments/9ufa2/the_elements_of_statistical_learning_hastie/,14.0,1.0,,en
1105574,2009-10-15 23:42:40,rstats,"Book: Elements of Statistical Learning: data mining, inference, and prediction. (Free pdf and R functions.)",9uh3c,novembera,1217357683.0,https://www.reddit.com/r/rstats/comments/9uh3c/book_elements_of_statistical_learning_data_mining/,4.0,0.0,,en
1105575,2009-10-16 01:48:53,statistics,"Ask Stats: Suppose you have two classes, and a distribution on labels in each. You know nothing about the distribution of the intersection of the classes. What should you conclude about an unlabelled item that is in both classes?",9uiab,moultano,1160099427.0,https://www.reddit.com/r/statistics/comments/9uiab/ask_stats_suppose_you_have_two_classes_and_a/,0.0,5.0,"To clarify, I don't want to make any assumptions about relationship between the two classes, so no conditional independence assumptions or anything like that. I'm also not restricting how these classes might be related, so one might be a subset of the other, etc.

I'm wonder if there's a known general result here. My instinct would be to pick the distribution among the two classes which gives the lowest entropy prediction of the distribution of the labels, but that contradicts my intuition in a lot of ways for some cases.

The problem restated. You have many labels, and you know their distribution for items in set A. You also know their distribution for items in set B. You want to predict the label of an item in both sets, without making any more assumptions. Can you do it? Is there a general way to pick one distribution over the other?",en
1105576,2009-10-16 06:05:34,MachineLearning,tRuEcasIng : A process to restore  case information to badly-cased  or noncased text.,9ukdr,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9ukdr/truecasing_a_process_to_restore_case_information/,7.0,2.0,,en
1105577,2009-10-16 09:25:14,statistics,"""the direction of causation (implies) that the use of cannabis leads to increases in levels of psychotic symptoms rather than psychotic symptoms increasing the use of cannabis."" I don't understand. How does equation modelling conclude direction of causation? [see footnote 7]",9um04,reddituser780,1252600063.0,https://www.reddit.com/r/statistics/comments/9um04/the_direction_of_causation_implies_that_the_use/,1.0,1.0,,en
1105578,2009-10-17 20:05:04,MachineLearning,"This book is Awesome. I read it last week and Highly Recommend it to MLreddit. ""Introduction to Information Retrieval"". [Free Book]",9v1eh,[deleted],,https://www.reddit.com/r/MachineLearning/comments/9v1eh/this_book_is_awesome_i_read_it_last_week_and/,1.0,0.0,,en
1105579,2009-10-19 02:58:29,MachineLearning,"TIL Excite had a full Relevance Feedback, which was dropped because people didnt use it. Here are some results of RF in Excite. [PDF]",9vcc7,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9vcc7/til_excite_had_a_full_relevance_feedback_which/,4.0,0.0,,en
1105580,2009-10-19 03:26:25,MachineLearning,Analyzing clickstream data (what users click in the results page) to provide Implicit Feedback for retrieving relevant documents. [PDF],9vchg,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9vchg/analyzing_clickstream_data_what_users_click_in/,5.0,0.0,,en
1105581,2009-10-19 04:51:53,MachineLearning,Statistical NLP to RF [PDF],9vd0c,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/9vd0c/statistical_nlp_to_rf_pdf/,8.0,0.0,,en
1105582,2009-10-19 04:56:44,statistics,When Does Correlation Imply Causation? When you design your experiment to.,9vd1r,reddituser780,1252600063.0,https://www.reddit.com/r/statistics/comments/9vd1r/when_does_correlation_imply_causation_when_you/,1.0,0.0,,en
1105583,2009-10-20 21:36:29,statistics,AskStats: Is the Fourier transform of the partial autocorrelation function anything useful?,9vzf5,[deleted],,https://www.reddit.com/r/statistics/comments/9vzf5/askstats_is_the_fourier_transform_of_the_partial/,0.0,3.0,,en
1105584,2009-10-22 09:43:26,statistics,What are some good graduate programs in statistics or applied statistics?,9wjpp,[deleted],,https://www.reddit.com/r/statistics/comments/9wjpp/what_are_some_good_graduate_programs_in/,0.0,0.0,,en
1105585,2009-10-22 09:53:44,statistics,Why It's Hard to Rank National Health-Care Systems - The Numbers Guy - WSJ,9wjsp,snipewiz,1256189534.0,https://www.reddit.com/r/statistics/comments/9wjsp/why_its_hard_to_rank_national_healthcare_systems/,4.0,1.0,,en
1105586,2009-10-22 18:17:23,statistics,"Exploratory Data Analysis in MatLab [Partial Book, Online]",9wonj,[deleted],,https://www.reddit.com/r/statistics/comments/9wonj/exploratory_data_analysis_in_matlab_partial_book/,0.0,4.0,,en
1105587,2009-10-23 10:35:49,MachineLearning,Is it possible to get an interesting job doing ML without an advanced degree?,9wxqe,machinelearner,1256282351.0,https://www.reddit.com/r/MachineLearning/comments/9wxqe/is_it_possible_to_get_an_interesting_job_doing_ml/,22.0,13.0,"Hi ML'ers,
I am currently in my last quarter of undergrad at a 2nd-tier UC doing computer science, and am interested in machine learning. I've taken a couple grad-level Machine Learning classes and have been having a lot of fun with my homework and projects. I also have several years' experience working as a software engineer and am currently working (nearly) full-time.

Since I am supporting my fiancee who is also in school right now (and will be for a while), I don't think grad school is an immediate option for me without taking on enormous amounts of debt. Does anyone know of an employer who will consider someone without an advanced degree for a position doing data mining / machine learning? Any tips on getting into the specialty with only a BS in CS?

Ultimately, I would like to be doing work creating/adapting algorithms to solve interesting problems (recommendation engines, advertising optimization, IR, NLP, etc), but wouldn't mind maintaining/enhancing existing systems at first to get my feet wet if needed.

Edit: Thanks for the interesting and insightful comments everyone!

What I'm going to do is find work doing some type of ML ASAP (or at least in proximity to the guys doing it) and then figure out how to afford to get a Master's once I have a little bit more experience applying the stuff. BTW, I got some anonymous comments that there are companies out there that are not too hung up on the piece of paper... so I guess it's like the rest of the industry: if you're good at it, someone will hire you to do it!",en
1105588,2009-10-23 22:14:16,MachineLearning,Ask /r/MachineLearning: what is your view on the current relations between AI and ML?,9x4s5,[deleted],,https://www.reddit.com/r/MachineLearning/comments/9x4s5/ask_rmachinelearning_what_is_your_view_on_the/,1.0,0.0,,en
1105589,2009-10-24 08:56:07,artificial,"Envirolawn, artificial grass suppliers an artificial yet natural lands
",9x9c0,mackindesouza,1252566188.0,https://www.reddit.com/r/artificial/comments/9x9c0/envirolawn_artificial_grass_suppliers_an/,1.0,1.0,,en
1105590,2009-10-25 21:17:39,MachineLearning,"Generation 5 SDK [Java, AI, ML, Image Proc.]",9xmkz,dearsomething,1210808677.0,https://www.reddit.com/r/MachineLearning/comments/9xmkz/generation_5_sdk_java_ai_ml_image_proc/,4.0,3.0,,en
1105591,2009-10-28 04:37:28,statistics,Numerology: When Digits Get Personal (WSJ),9ygni,snipewiz,1256189534.0,https://www.reddit.com/r/statistics/comments/9ygni/numerology_when_digits_get_personal_wsj/,1.0,0.0,,en
1105592,2009-10-28 05:30:52,statistics,What is the expectation maximization algorithm?,9yh20,indraniel,1214879371.0,https://www.reddit.com/r/statistics/comments/9yh20/what_is_the_expectation_maximization_algorithm/,9.0,2.0,,en
1105593,2009-10-28 17:12:38,statistics,Book of Odds - The Odds of Everyday Life,9ynd7,snipewiz,1256189534.0,https://www.reddit.com/r/statistics/comments/9ynd7/book_of_odds_the_odds_of_everyday_life/,8.0,0.0,,en
1105594,2009-10-28 20:28:27,statistics,Ask Stats: Good Introductory Book (or websites) on Bayesian Statistics?,9yprl,ST2K,1173236363.0,https://www.reddit.com/r/statistics/comments/9yprl/ask_stats_good_introductory_book_or_websites_on/,9.0,9.0,"I own a copy of [Bayesian Statistics: An Introduction](http://www.amazon.com/Bayesian-Statistics-Introduction-Arnold-Publication/dp/0340814055/), but it's just a little bit too difficult for me just yet.  Are there any easier books or websites for introductory Bayesian statistics that you could recommend?  Thank-you so much!!",en
1105595,2009-10-29 19:18:14,MachineLearning,A taxonomy of web search[pdf],9z2iz,salinon,1194906340.0,https://www.reddit.com/r/MachineLearning/comments/9z2iz/a_taxonomy_of_web_searchpdf/,1.0,0.0,,en
1105596,2009-10-30 04:11:21,statistics,Ask statistics: Interesting ideas for an International Studies Honors Thesis? (undergrad),9z7cd,[deleted],,https://www.reddit.com/r/statistics/comments/9z7cd/ask_statistics_interesting_ideas_for_an/,4.0,4.0,,en
1105597,2009-10-30 07:20:36,statistics,[PDF] Draft of ggplot2 book by Hadley Wickham,9z8qk,indraniel,1214879371.0,https://www.reddit.com/r/statistics/comments/9z8qk/pdf_draft_of_ggplot2_book_by_hadley_wickham/,14.0,1.0,,en
1105598,2009-10-31 17:47:33,statistics,Mission Improbable: A Concise and Precise Definition of P-Value,9zot4,[deleted],,https://www.reddit.com/r/statistics/comments/9zot4/mission_improbable_a_concise_and_precise/,1.0,0.0,,en
1105599,2009-11-02 13:02:22,MachineLearning,Popular Ensemble Methods: An Empirical Study,a03v9,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/a03v9/popular_ensemble_methods_an_empirical_study/,5.0,0.0,,en
1105600,2009-11-02 13:36:11,MachineLearning,"Boosting algorithms: regularization, prediction and model fitting [pdf]",a0446,mstoehr,1185155370.0,https://www.reddit.com/r/MachineLearning/comments/a0446/boosting_algorithms_regularization_prediction_and/,17.0,0.0,,en
1105601,2009-11-02 14:36:59,MachineLearning,"MLcomp, a site for comparing machine learning algorithms.",a04km,[deleted],,https://www.reddit.com/r/MachineLearning/comments/a04km/mlcomp_a_site_for_comparing_machine_learning/,1.0,0.0,,en
1105602,2009-11-03 01:52:53,MachineLearning,How Important Is Knowing Mathematical Matrices To Understanding Neural Networks?,a0bih,ST2K,1173236363.0,https://www.reddit.com/r/MachineLearning/comments/a0bih/how_important_is_knowing_mathematical_matrices_to/,0.0,3.0,"I've started reading a book on Neural Network programming in C#, and it quickly dove into using matrices.  I'm thinking about taking a local junior college class in linear algebra/differential equations in order to get a better understanding of matrices.  Do you think a class like that would be overkill for what's required in neural networks?",en
1105603,2009-11-03 06:56:52,statistics,Principal Components Analysis in 7 lines of MatLab,a0dyl,[deleted],,https://www.reddit.com/r/statistics/comments/a0dyl/principal_components_analysis_in_7_lines_of_matlab/,0.0,2.0,,en
1105604,2009-11-03 20:57:48,statistics,Hans Rosling shows the best stats you've ever seen - TED TALK (VIDEO),a0lb8,[deleted],,https://www.reddit.com/r/statistics/comments/a0lb8/hans_rosling_shows_the_best_stats_youve_ever_seen/,0.0,0.0,,en
1105605,2009-11-03 23:31:26,MachineLearning,"Dear Reddit Machine Learning, Is Jeff Hawkings, ""inventor"" of HTMs a Genius of a Rambling Egomaniac? ",a0mwd,gabgoh,1252945089.0,https://www.reddit.com/r/MachineLearning/comments/a0mwd/dear_reddit_machine_learning_is_jeff_hawkings/,26.0,27.0,,en
1105606,2009-11-04 10:11:44,MachineLearning,MLComp: a comparison site for machine learning algorithms,a0skc,[deleted],,https://www.reddit.com/r/MachineLearning/comments/a0skc/mlcomp_a_comparison_site_for_machine_learning/,1.0,0.0,,en
1105607,2009-11-05 13:23:29,MachineLearning,MLComp: a comparison site for machine learning algorithms,a17pe,StompChicken,1248945961.0,https://www.reddit.com/r/MachineLearning/comments/a17pe/mlcomp_a_comparison_site_for_machine_learning/,16.0,0.0,,en
1105608,2009-11-05 20:19:59,statistics,Cracking the Code Math for Schwarzenegger's Veto Message - The Numbers Guy - WSJ,a1c7t,snipewiz,1256189534.0,https://www.reddit.com/r/statistics/comments/a1c7t/cracking_the_code_math_for_schwarzeneggers_veto/,4.0,0.0,,en
1105609,2009-11-06 04:04:08,statistics,Please help. I need to transform life span distribution to age distribution.,a1gme,spgarbet,1191617059.0,https://www.reddit.com/r/statistics/comments/a1gme/please_help_i_need_to_transform_life_span/,0.0,4.0,"I have a life span distribution, and the population has been propagating for several generations. How do I transform this into an expected age distribution? I was thinking convolution, but then with what other distribution? Does that even make sense?

There's probably an easy answer to this, but I'm not finding it at the moment and I'm sick of searching PubMed and finding papers on dairy cow population statistics. 

P.S. This is not a homework problem.",en
1105610,2009-11-06 09:16:14,statistics,What is your favourite R IDE?,a1ja7,gabgoh,1252945089.0,https://www.reddit.com/r/statistics/comments/a1ja7/what_is_your_favourite_r_ide/,0.0,3.0,"I am looking for something with at least syntax highlighting, and optimally code completion. Does anything like that exist?",en
1105611,2009-11-07 18:45:16,MachineLearning,"Self-Improving Systems that Learn Through Human Interaction
By Yisong Yue",a1zcf,[deleted],,https://www.reddit.com/r/MachineLearning/comments/a1zcf/selfimproving_systems_that_learn_through_human/,15.0,0.0,,en
1105612,2009-11-09 01:37:49,MachineLearning,How many of you have industry jobs that primarily require machine learning knowledge? What is your academic background?,a2acv,timml,1257723334.0,https://www.reddit.com/r/MachineLearning/comments/a2acv/how_many_of_you_have_industry_jobs_that_primarily/,10.0,2.0,"I have a research-oriented masters in machine learning, and my job for the last two years has been in ""analytics"" and ""modeling"", which is a pretty machine learning heavy job. 

I believe job mobility is important, but my entire industry is pretty small and geographically concentrated in California. Basically, I'm curious what other industries out there use machine learning. I know ""applied statistics"" is in demand, but I'd specifically like to stay in some sort of AI because I find the field very interesting.",en
1105613,2009-11-09 10:23:19,statistics,Survey - Slower Broadband Speeds Could Hinder UK House Sales,a2e7a,ispreview,1237390263.0,https://www.reddit.com/r/statistics/comments/a2e7a/survey_slower_broadband_speeds_could_hinder_uk/,0.0,0.0,,en
1105614,2009-11-10 21:06:02,MachineLearning,Help. I need a data set of books and their ratings (similar to Neflix but for books),a2ys8,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/a2ys8/help_i_need_a_data_set_of_books_and_their_ratings/,8.0,10.0,"I am thinking of  implementing, testing and information retrieval systems to find the similarity between books. Do you guys know of any data set/repository/database with books and their ratings? Thanks.

Edit: I need to somehow test my system for precision too. I dont know if there are sample evaluated corpus. 

I am not just looking for a corpus of books but a small evaluated sample like trec-eval so that I can test my stems.",en
1105615,2009-11-10 22:09:04,MachineLearning,"How to Beat Google at Search for $50,000 in 10 Easy Steps | drstarcat.com",a2zij,drstarcat,1257876536.0,https://www.reddit.com/r/MachineLearning/comments/a2zij/how_to_beat_google_at_search_for_50000_in_10_easy/,2.0,4.0,,en
1105616,2009-11-11 05:02:56,statistics,New programming language.,a3381,[deleted],,https://www.reddit.com/r/statistics/comments/a3381/new_programming_language/,8.0,36.0,"I'm looking to replace R as my primary language.  Quite frankly I do a lot of simulation and R is just too damn slow.  What languages are good for this.  My only criteria are, 

1  Good mathematical functions and basic statistical ones
2  Fast
3  'Easy' to program
4  Wide variety of fast random number generators

Thanks in advance.  ",en
1105617,2009-11-11 19:27:44,statistics,Can people tell the difference between stouts?,a3bp1,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/a3bp1/can_people_tell_the_difference_between_stouts/,2.0,14.0,,en
1105618,2009-11-12 00:33:47,statistics,Need Reddit's help solving this mystery about SPSS: Cross tabs vs. Split File,a3f23,[deleted],,https://www.reddit.com/r/statistics/comments/a3f23/need_reddits_help_solving_this_mystery_about_spss/,0.0,7.0,"Today, I used the split file function in conjunction with a plain frequency. Then i run the same analysis using only a cross tab, and get the mostly the same numbers but a few exceptions, but they are not just small rounding errors, they are off by  5% or 6%

I run into this problem only with weighted data.

Anyone have an explanation?

P.S. it also will do this with Table of frequencies vs. frequency as well.",en
1105619,2009-11-12 10:20:41,statistics,UK ISP O2 Tops Half a Million Broadband Subscribers Milesone,a3k1d,ispreview,1237390263.0,https://www.reddit.com/r/statistics/comments/a3k1d/uk_isp_o2_tops_half_a_million_broadband/,0.0,0.0,,en
1105620,2009-11-12 13:51:36,statistics,Question for /r/statistics.,a3lm4,Dangger,1242487715.0,https://www.reddit.com/r/statistics/comments/a3lm4/question_for_rstatistics/,5.0,21.0,Is money discrete or continuous? Particularly weekly or monthly income. ,en
1105621,2009-11-12 16:27:11,statistics,How to calculate standard deviation,a3n34,gmatguru82,1244720697.0,https://www.reddit.com/r/statistics/comments/a3n34/how_to_calculate_standard_deviation/,1.0,0.0,,en
1105622,2009-11-12 22:18:30,statistics,Microsoft Access Tips &amp; Tricks: Finding The Mode,a3rdm,blogannath,1246840412.0,https://www.reddit.com/r/statistics/comments/a3rdm/microsoft_access_tips_tricks_finding_the_mode/,0.0,0.0,,en
1105623,2009-11-16 17:14:51,MachineLearning,How can I deal with uncertain and not equally important training data?,a4wm0,CyberByte,1234510184.0,https://www.reddit.com/r/MachineLearning/comments/a4wm0/how_can_i_deal_with_uncertain_and_not_equally/,6.0,4.0,"I have a signal processing problem that involves classifying time samples into one of several categories. The labels in the train set look more or less like this:

    000000???111111???00000????111??0000??22222????00000
    abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ

The x-axis here is time, the numbers on the first row are the labels I know about and the question marks are there because I don't exactly know what the labels there should be. The second row is just used to refer to timepoints in this example. The question marks really show transitions from one label into another, but I don't know exactly where the transition is. This means that a question mark should be one of the two surrounding labels (and not anything else) and that if we have AA????BB and the second question mark is a B, then the next ones should also be B's.
My first problem is basically how to incorporate the information I do have about these samples in my algorithm.

The second problem has to do with the fact that not all labels are equally important. If for every group of non-zero's just one would be classified correctly and the rest would be classified as zero, that would be great. So in the above example I would be happier if all samples would be classified as zeros except *m*, *B* and *O* than if for instance *j-o* would be classified correctly and the rest would be zeros, even though the latter would technically give a higher accuracy.
My second problem is thus how to deal with this unequal preference for different labels.

I guess I could just ignore all of this information I have, but the problem is that when I do that, performance is very bad. Of course, there is no guarantee that using it will make things much better, but I'm hoping it will.

I'm still quite new to this field, so if anyone could point me in the direction of algorithms that can be used for this, suggest resources that apply to this or even (like books, webpages or other places to ask questions like this; a StackOverflow for science questions would be great) come up with solutions for my problems, that would be great! I've tried to keep this as abstract and general as possible and I hope it is still clear what the problem is. If not, please ask and I will try to clarify. If you feel it's inappropriate for me to ask you to help with my problems, feel free to downvote. Thanks for your time!",en
1105624,2009-11-17 04:32:19,MachineLearning,"I'm trying to make an order preserving estimate of a probability distribution.  If people could throw some Ideas at me, it would be grand.",a53g1,TheMonkeyOfLove,1253761836.0,https://www.reddit.com/r/MachineLearning/comments/a53g1/im_trying_to_make_an_order_preserving_estimate_of/,8.0,22.0,"I have a bunch of (multivariate) samples from a probability density, and at the moment I'm using a kernel density estimator to recreate the distribution.  My eventual goal is to be able to take a series of points and order them by decreasing likelihood.  In other words, I don't need to estimate the probability function, but an order preserving transform of the probability function.  My first thought was to just estimate the original function, because it's obviously order preserving with itself, but as I started to tune the smoothing (bandwidth) parameters I was finding that I needed to over-smooth the distribution quite a bit to get good results.  It looks like having a bit of variance is just fine if I'm trying to minimize the error between my estimate and the underlying distribution, but if I'm concerned about ordering, it completely kills me.  

Does anyone out there know a better technique for reconstructing a likelihood function for a set of samples?  Or maybe a modification to my kernel estimator that would help?  Please ask me questions, if you don't understand exactly what I'm going for.  I doubt my explanation is all that clear.

Edit: I can't spell.",en
1105625,2009-11-19 01:11:05,MachineLearning,"PyBrain 0.3 released - very flexible Python library for FFNs, RNNs, Reinforcement Learning, Evolutionary Algorithms ",a5tqi,[deleted],,https://www.reddit.com/r/MachineLearning/comments/a5tqi/pybrain_03_released_very_flexible_python_library/,1.0,0.0,,en
1105626,2009-11-19 14:15:56,MachineLearning,30 Resources to Find the Data You Need,a60fv,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/a60fv/30_resources_to_find_the_data_you_need/,12.0,1.0,,en
1105627,2009-11-19 14:17:14,MachineLearning,theinfo data sets,a60ge,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/a60ge/theinfo_data_sets/,3.0,0.0,,en
1105628,2009-11-19 15:19:45,statistics,Rapid Flu Testing: Why It Isn't As Bad As It's Made Out To Be (A Lesson in Statistics and Epidemiology),a610x,prionattack,1257312771.0,https://www.reddit.com/r/statistics/comments/a610x/rapid_flu_testing_why_it_isnt_as_bad_as_its_made/,3.0,0.0,,en
1105629,2009-11-19 15:31:46,MachineLearning,PyBrain 0.3 -- swiss army knife for neural networking grew up a little,a614v,visionlessvisionary,1255005680.0,https://www.reddit.com/r/MachineLearning/comments/a614v/pybrain_03_swiss_army_knife_for_neural_networking/,30.0,1.0,,en
1105630,2009-11-19 22:38:01,statistics,Microsoft Access Tips &amp; Tricks: Geometric And Harmonic Means,a666f,blogannath,1246840412.0,https://www.reddit.com/r/statistics/comments/a666f/microsoft_access_tips_tricks_geometric_and/,0.0,0.0,,en
1105631,2009-11-20 11:43:53,statistics,Basic advice regarding data projection.,a6d42,MathewPerry,1258709159.0,https://www.reddit.com/r/statistics/comments/a6d42/basic_advice_regarding_data_projection/,0.0,5.0,"This is a particular type of problem that I have come across a number of times, and don't really know how to go about. 

In this case, I'm trying to project the number of bookings that will be made at a venue for a particular day in the future, given the number of bookings I have today or at any day in the past. There are other variables that have an effect on the shape of growth, for example the day of the week in question and the particular venue (of which there are several dozen).

I have ample past data on which to base the model.

It would also nice to have some sort of probabilistic margin of error for the projection.

I've studied maths throughout my undergraduate, but only went as far as 1st year with statistics. Basically: I know I need to learn more in order to do this, but I don't even know where to start. I thought it would be best to ask here rather than risk wasting my time on completely the wrong angle (non-linear regression, neural nets... yegads!). 

I'm not expecting someone to simply tell me how to do it, but it would good to have a nudge in the right direction or some recommended reading etc.",en
1105632,2009-11-25 03:03:15,statistics,All of New York City's Data,a7va0,swiz0r,1235971035.0,https://www.reddit.com/r/statistics/comments/a7va0/all_of_new_york_citys_data/,7.0,1.0,,en
1105633,2009-11-25 12:32:34,MachineLearning," Elefant (Efficient Learning, Large-scale Inference, and Optimisation Toolkit) is an open source library for machine learning ",a804a,BioGeek,1124683200.0,https://www.reddit.com/r/MachineLearning/comments/a804a/elefant_efficient_learning_largescale_inference/,12.0,3.0,,en
1105634,2009-11-26 16:43:49,statistics,STATS: B2B Marketers &amp;#038; Social Media Marketing,a8evq,williswee,1250960172.0,https://www.reddit.com/r/statistics/comments/a8evq/stats_b2b_marketers_038_social_media_marketing/,2.0,0.0,,en
1105635,2009-11-27 19:46:27,statistics,RECAP: 5 Important Statistics For Marketers,a8qzu,williswee,1250960172.0,https://www.reddit.com/r/statistics/comments/a8qzu/recap_5_important_statistics_for_marketers/,1.0,0.0,,en
1105636,2009-11-29 01:34:15,statistics,"Time Series Analysis by Michael Sampson, a free 239 page book",a924j,[deleted],,https://www.reddit.com/r/statistics/comments/a924j/time_series_analysis_by_michael_sampson_a_free/,0.0,0.0,,en
1105637,2009-11-29 04:05:04,statistics,Time Series Analysis by Michael Sampson ,a9313,roger_,1178076247.0,https://www.reddit.com/r/statistics/comments/a9313/time_series_analysis_by_michael_sampson/,16.0,0.0,,en
1105638,2009-11-29 22:34:17,statistics,"Sutton &amp; Barto : Reinforcement Learning: An
Introduction (free ebook)",a99bc,xamdam,1129780800.0,https://www.reddit.com/r/statistics/comments/a99bc/sutton_barto_reinforcement_learning_an/,9.0,2.0,,en
1105639,2009-11-30 09:45:07,statistics,Questions about applying to grad school for Statistics with a B.A.,a9e6g,whatdfc,1245352256.0,https://www.reddit.com/r/statistics/comments/a9e6g/questions_about_applying_to_grad_school_for/,0.0,1.0,"I'd send an e-mail to a counselor/rep from the school I plan to apply to concerning some of this stuff, but the site is littered with broken links (bad sign, I know...cal poly pomona if anyone is wondering), and the individual assigned to graduate school queries in particular only schedules in-person appointments.

I'm going to be graduating next spring.  I'm an English/Criminology major, and I've been introduced to statistics through the latter.  I've decided that I'm really interested in pursuing a Master's degree for it.   I'm mainly interested in data visualization after realizing that I really like graphs and charts (alot).

Unfortunately, not counting the stats in behavioral science class I took recently, my mathematical education basically ended after Algebra 2 in high school.  How far do I need to go in order to apply for a graduate program for Statistics without having to take remedial math classes while completing it(if that's even possible)?  I'm assuming up to the equivalent of Calculus I at my school, but I really have no clue. 

I'd assume the progression would be trigonometry-precal-beginning calculus, since that's what it was in high school.  I intend to learn on my own, since taking classes won't be an option as I'm already taking the maximum number of classes after asking for a unit extension.  If anyone could recommend specific textbooks for me to plow through, or even some sort of amazing condensed book (or books), that would be grand.

I'm assuming that I'll be given a proficiency test as part of the admissions process, since I won't have a B.S. or even any math at the collegiate level.  At least I'm hoping that this will be the case, as opposed to having to take math classes at a CC beforehand or something.

I also don't know anything about the GRE aside from the fact that it's a test that I have to take.  Are there different types?  Is there a specific math or statistics GRE that I'd be taking?  If so, would doing sufficiently well on that possibly be enough to satisfy the presumed undergraduate mathematical requirements?

Aside from math, how much programming is expected?  Presumably I'll be expected to know R, but will that be enough?

Once again, I know this is mostly stuff I should be asking a representative, but I'd appreciate any advice.  Thanks.",en
1105640,2009-12-02 22:33:20,statistics,Resources on 2+ dimensional confidence intervals?,aaejt,[deleted],,https://www.reddit.com/r/statistics/comments/aaejt/resources_on_2_dimensional_confidence_intervals/,0.0,5.0,,en
1105641,2009-12-03 19:07:12,MachineLearning,My Five Rules for Data Visualization ,aaqve,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/aaqve/my_five_rules_for_data_visualization/,23.0,0.0,,en
1105642,2009-12-03 19:21:15,statistics,My Five Rules for Data Visualization,aar1k,[deleted],,https://www.reddit.com/r/statistics/comments/aar1k/my_five_rules_for_data_visualization/,0.0,4.0,,en
1105643,2009-12-04 23:26:39,statistics,"Statistics, damn statistics and well kept secrets in medical trials",ab7o3,prionattack,1257312771.0,https://www.reddit.com/r/statistics/comments/ab7o3/statistics_damn_statistics_and_well_kept_secrets/,13.0,3.0,,en
1105644,2009-12-05 18:05:29,MachineLearning,MLReddit - What do you think are the trade offs in prediction using ML and Simlation?,abfix,[deleted],,https://www.reddit.com/r/MachineLearning/comments/abfix/mlreddit_what_do_you_think_are_the_trade_offs_in/,0.0,0.0,,en
1105645,2009-12-05 18:15:58,MachineLearning,What do you think are the top 10 most influential algorithm in Data Mining?,abfln,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/abfln/what_do_you_think_are_the_top_10_most_influential/,19.0,13.0,"Do you agree or disagree with these lists:

http://www.cs.uvm.edu/~icdm/algorithms/index.shtml",en
1105646,2009-12-05 22:16:58,statistics,Parapsychology: the control group for science,abhhm,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/abhhm/parapsychology_the_control_group_for_science/,4.0,2.0,,en
1105647,2009-12-06 13:35:12,statistics,"Hey StatsReddit, have you heard of a data bias that gives false correlation for two oscillating functions of different frequency?",abmu0,AndrewKemendo,1190929741.0,https://www.reddit.com/r/statistics/comments/abmu0/hey_statsreddit_have_you_heard_of_a_data_bias/,0.0,3.0,"I have ruled autocorrelation out for this one. I am speaking of two different functions that periodically will indicate perfect or near perfect correlation.

For example: http://i45.tinypic.com/2j3iq9h.jpg

Anyone have an answer?",en
1105648,2009-12-06 19:50:17,MachineLearning,Visualizing the Structure of Venture Capital Co-Investments,abp7t,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/abp7t/visualizing_the_structure_of_venture_capital/,7.0,0.0,,en
1105649,2009-12-07 07:32:17,MachineLearning,Which similarity coefficient and weights should I try in order to find the similarity of two movies?,abugw,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/abugw/which_similarity_coefficient_and_weights_should_i/,4.0,8.0,"I built the basic framework to access the IMDB data for movies and now I can get a bunch of attributes for each movie like the genre_list/actors_list/director_list.

Right now I just have a small script that runs the Jaccard Coefficient and uses arbritray weight for each attribute(genre is weighted more for example). I have something like this : ((jaccard_by_genre * GENRE_WEIGHT)+ (jaccard_by_actors * ACTOR_WEIGHT))

I want to experiment more with various similarity coefficient and various weights for each categories. Please write down your suggestion for similarity coefficient and weights and I will try to find the best one that minimizes the RMSE. I am beginning Information Retrieval and I really want to experiment with this data. Please suggest. Thank you.",en
1105650,2009-12-07 19:50:56,statistics,21st Century Scientist: Validating your statistical methods,ac1y2,Rich121,1221670184.0,https://www.reddit.com/r/statistics/comments/ac1y2/21st_century_scientist_validating_your/,0.0,0.0,,en
1105651,2009-12-08 17:43:43,MachineLearning,Lot of links to recommender system,aceuk,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/aceuk/lot_of_links_to_recommender_system/,10.0,0.0,,en
1105652,2009-12-09 15:44:36,statistics,Bundled UK Broadband Services Gain Popularity and ISP Satisfaction Rises,acrwz,ispreview,1237390263.0,https://www.reddit.com/r/statistics/comments/acrwz/bundled_uk_broadband_services_gain_popularity_and/,0.0,0.0,,en
1105653,2009-12-11 13:08:06,MachineLearning,Robbins: Some aspects of the sequential design of experiments [pdf] - classic paper,adjf0,mstoehr,1185155370.0,https://www.reddit.com/r/MachineLearning/comments/adjf0/robbins_some_aspects_of_the_sequential_design_of/,6.0,0.0,,en
1105654,2009-12-11 15:57:47,statistics,Gen. McChrystal doesn't seem to understand dominated strategies in game theory,adkzq,AndrewKemendo,1190929741.0,https://www.reddit.com/r/statistics/comments/adkzq/gen_mcchrystal_doesnt_seem_to_understand/,5.0,0.0,,en
1105655,2009-12-11 23:00:33,statistics,Detecting Cylons,adq25,GrumpySimon,1138450376.0,https://www.reddit.com/r/statistics/comments/adq25/detecting_cylons/,0.0,0.0,,en
1105656,2009-12-12 10:57:43,statistics,UK 5th in OECD Global Broadband ISP Subscriber League Table,adve5,ispreview,1237390263.0,https://www.reddit.com/r/statistics/comments/adve5/uk_5th_in_oecd_global_broadband_isp_subscriber/,1.0,0.0,,en
1105657,2009-12-12 20:24:12,statistics,"the R Graphical Manual takes a different tack on exploring the functions in R: rather than browsing by the name or description of the functions, you browse by the graphics generated by the functions.",adz9w,BioGeek,1124683200.0,https://www.reddit.com/r/statistics/comments/adz9w/the_r_graphical_manual_takes_a_different_tack_on/,23.0,1.0,,en
1105658,2009-12-12 20:29:20,statistics,Suggestions about textbooks on Statistics for Computer Science,adzb9,ogmus,1216776577.0,https://www.reddit.com/r/statistics/comments/adzb9/suggestions_about_textbooks_on_statistics_for/,9.0,9.0,"Hello folks,

I've recently got my B.S. degree in Computer Science, and one thing that bothers me is this feeling that I know very little or, to better put it, that I should know more about Statistics.

I mean, as an academia-oriented and mathematically-inclined person who's daily running several experiments, plotting graphs, making tables, writing technical reports and research papers, I find that Statistics is a vital tool in collecting, analyzing and exhibiting results, especially when they are produced in large amounts. I've certainly taken Statistics in college, although to me that did not seem to suffice.

So I would like you to point me to some good textbooks concerning preferably, but not only, Statistics for Computer Science so that I can:

1) Refresh my mind on the most central concepts in Statistics and 2) Learn more things that may help me to interpret results in a more thoroughly, more ""profoundly"" (and not just ""qualitatively"") way.

Thanks in advance :D",en
1105659,2009-12-14 14:53:20,MachineLearning,Neural Networks suck. Here is bipedal robot with   GA-optimized initial parameters. It has been in constant learning mode for almost two years. Good luck for getting Darpa funding. [gif animation] ,aeh9t,[deleted],,https://www.reddit.com/r/MachineLearning/comments/aeh9t/neural_networks_suck_here_is_bipedal_robot_with/,3.0,2.0,,en
1105660,2009-12-15 04:22:19,statistics,Question on the derivation of the Poisson Distribution through a Poisson Process,aepxv,ffualo,1203929903.0,https://www.reddit.com/r/statistics/comments/aepxv/question_on_the_derivation_of_the_poisson/,0.0,7.0,"Hi r/statistics,

I learned the origins of the Poisson distribution primarily through its derivation from the Law of Rare Events, which is a beautiful and nifty derivation. Recently, I was reading *Statistical Methods in Bioinformatics* when I saw another derivation that uses asymptotic notation (which I am not familiar with outside of the context of algorithms). I can't seem to grasp intuitively what's going on with the asymptotic term in this derivation - can you folks help? The material begins [here](http://books.google.com/books?id=ENvojTAm0TwC&amp;lpg=PP1&amp;dq=statistical%20methods%20in%20bioinformatics&amp;pg=PA155#v=onepage&amp;q=&amp;f=false) (I can post by hand if there are problems). I'm unclear on what the meaning of *o(h)* is in this context (described in (2)).",en
1105661,2009-12-16 10:53:51,statistics,Worldwide Broadband ISP Subscribers Top 451 Million in Q3 2009,af8hq,ispreview,1237390263.0,https://www.reddit.com/r/statistics/comments/af8hq/worldwide_broadband_isp_subscribers_top_451/,1.0,0.0,,en
1105662,2009-12-16 20:48:01,MachineLearning,Torrent tracker fo public datasets,aff7n,splix,1201182160.0,https://www.reddit.com/r/MachineLearning/comments/aff7n/torrent_tracker_fo_public_datasets/,12.0,0.0,,en
1105663,2009-12-17 02:24:32,MachineLearning,"Dan Siroker, Director of Analytics for the Obama Presidential campaign explains how they used data to win the election.",afis4,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/afis4/dan_siroker_director_of_analytics_for_the_obama/,5.0,0.0,,en
1105664,2009-12-17 18:09:40,MachineLearning,Survey of the existing methods of boosting multi-class classification algorithms. ,afs1w,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/afs1w/survey_of_the_existing_methods_of_boosting/,10.0,1.0,,en
1105665,2009-12-20 06:02:54,artificial,Rethinking artificial intelligence,agnk2,hyp3rVigi1ant,1259608259.0,https://www.reddit.com/r/artificial/comments/agnk2/rethinking_artificial_intelligence/,21.0,0.0,,en
1105666,2009-12-20 15:45:52,artificial,Very impressive 3d business card with augmented reality,agqvd,ahmadinad,1259346585.0,https://www.reddit.com/r/artificial/comments/agqvd/very_impressive_3d_business_card_with_augmented/,30.0,4.0,,en
1105667,2009-12-21 05:00:22,MachineLearning,Probabilistic Matrix Factorization for CF which scales linearly and performs well on very imbalanced and sparse datasets.,agwos,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/agwos/probabilistic_matrix_factorization_for_cf_which/,20.0,1.0,,en
1105668,2009-12-25 20:24:00,MachineLearning,Structured Machine Learning: Ten Problems for the Next Ten Years [pdf],aii73,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/aii73/structured_machine_learning_ten_problems_for_the/,13.0,1.0,,en
1105669,2009-12-25 20:37:10,MachineLearning,What is the current research in making Automatic Learning Software Debuggers?,aiibs,[deleted],,https://www.reddit.com/r/MachineLearning/comments/aiibs/what_is_the_current_research_in_making_automatic/,1.0,0.0,,en
1105670,2009-12-27 20:06:14,artificial,DANN - Open source Java AI library - implementations of old techniques as well as new novel ones,aiy52,koryk,1183043237.0,https://www.reddit.com/r/artificial/comments/aiy52/dann_open_source_java_ai_library_implementations/,18.0,0.0,,en
1105671,2009-12-29 19:55:59,MachineLearning,Algorithms for Graphical Models draft PDF [uses Python!] ,ajlx9,xamdam,1129780800.0,https://www.reddit.com/r/MachineLearning/comments/ajlx9/algorithms_for_graphical_models_draft_pdf_uses/,16.0,2.0,,en
1105672,2009-12-30 17:01:53,statistics,I there a good free data analysis software with a GUI out there?,ajx3j,fatman_apollo,1258507026.0,https://www.reddit.com/r/statistics/comments/ajx3j/i_there_a_good_free_data_analysis_software_with_a/,0.0,2.0,"So I was a big data geek in college. I like modeling and regression analysis. Being a philosophy major, though, I never really got in to the hard-core programming languages for statistics. Mostly, I used data desk for all of my statistics needs. Now I'm a poor post-grad and I still want to be able to indulge in my hobby. I was wondering if there was a good free data analysis software out there with a GUI. If anyone could let me know where to find one, I'd really appreciate it!",en
1105673,2009-12-31 14:10:20,MachineLearning,Brief Analysis of Christmas Day bomber's Web Posting Data ,ak8fc,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/ak8fc/brief_analysis_of_christmas_day_bombers_web/,0.0,0.0,,en
1105674,2010-01-01 07:30:50,MachineLearning,Parallel Metropolis-Hasting implementation?,akfv8,[deleted],,https://www.reddit.com/r/MachineLearning/comments/akfv8/parallel_metropolishasting_implementation/,0.0,0.0,I was wondering if anyone knew of any implementation of paralell metropolis hastings? I've seen a few papers on it but I really would prefer to not actually implement it myself.,en
1105675,2010-01-01 21:40:17,MachineLearning,Ask ML: non-stationary reinforcement learning?,akl1w,[deleted],,https://www.reddit.com/r/MachineLearning/comments/akl1w/ask_ml_nonstationary_reinforcement_learning/,1.0,1.0,,en
1105676,2010-01-01 23:43:41,statistics,Very Rare Terrorists Are Very Hard to Find,akly8,Dangger,1242487715.0,https://www.reddit.com/r/statistics/comments/akly8/very_rare_terrorists_are_very_hard_to_find/,1.0,0.0,,en
1105677,2010-01-04 17:23:56,MachineLearning,Opinion mining and sentiment analysis - ebook,alel7,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/alel7/opinion_mining_and_sentiment_analysis_ebook/,23.0,1.0,,en
1105678,2010-01-04 19:29:53,MachineLearning,Analytics X Prize,alg6f,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/alg6f/analytics_x_prize/,17.0,7.0,,en
1105679,2010-01-04 19:33:40,statistics,How To: Graph Your Country's Facebook Demographics,alg83,williswee,1250960172.0,https://www.reddit.com/r/statistics/comments/alg83/how_to_graph_your_countrys_facebook_demographics/,1.0,0.0,,en
1105680,2010-01-05 00:50:48,statistics,Microsoft Access Tips &amp; Tricks: Probability Density,aljqp,blogannath,1246840412.0,https://www.reddit.com/r/statistics/comments/aljqp/microsoft_access_tips_tricks_probability_density/,1.0,0.0,,en
1105681,2010-01-05 17:23:28,MachineLearning,"Data Mining, Operations Research, and Predicting Murders",alth3,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/alth3/data_mining_operations_research_and_predicting/,15.0,0.0,,en
1105682,2010-01-05 17:46:14,statistics,"Climate change as a religion, and the more general use of ""religion"" as a term of insult",altrp,[deleted],,https://www.reddit.com/r/statistics/comments/altrp/climate_change_as_a_religion_and_the_more_general/,0.0,0.0,,en
1105683,2010-01-06 04:56:57,MachineLearning,Why Engine need Oil ?,am1el,bahman13,1246870130.0,https://www.reddit.com/r/MachineLearning/comments/am1el/why_engine_need_oil/,1.0,1.0,,en
1105684,2010-01-06 23:58:10,MachineLearning,data mining course (same one as given at Stanford as stats 202) - look for video links towards the bottom,amfji,xamdam,1129780800.0,https://www.reddit.com/r/MachineLearning/comments/amfji/data_mining_course_same_one_as_given_at_stanford/,20.0,0.0,,en
1105685,2010-01-07 05:04:09,MachineLearning,How to achieve Diesel Engine Compression Ratio ?,amip7,bahman13,1246870130.0,https://www.reddit.com/r/MachineLearning/comments/amip7/how_to_achieve_diesel_engine_compression_ratio/,1.0,1.0,,en
1105686,2010-01-09 20:13:02,MachineLearning,A Peek Into Netflix Queues,anlcd,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/anlcd/a_peek_into_netflix_queues/,13.0,2.0,,en
1105687,2010-01-10 01:59:22,statistics,What are open source alternatives to Excel Solver?,anoa5,[deleted],,https://www.reddit.com/r/statistics/comments/anoa5/what_are_open_source_alternatives_to_excel_solver/,1.0,0.0,,en
1105688,2010-01-10 02:12:47,statistics,What is your favorite (free) statistics package?,anodo,citizenjr,1261499662.0,https://www.reddit.com/r/statistics/comments/anodo/what_is_your_favorite_free_statistics_package/,1.0,1.0,,en
1105689,2010-01-10 19:25:50,MachineLearning,Machine Learning books discovery,anvas,vidurNigam,1263144180.0,https://www.reddit.com/r/MachineLearning/comments/anvas/machine_learning_books_discovery/,1.0,0.0,,en
1105690,2010-01-11 00:52:51,statistics,Learning R....any recommendations?,any59,spraynard,1208797382.0,https://www.reddit.com/r/statistics/comments/any59/learning_rany_recommendations/,1.0,0.0,,en
1105691,2010-01-11 01:35:45,MachineLearning,Evaluating Spatial Predictions,anyhp,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/anyhp/evaluating_spatial_predictions/,5.0,0.0,,en
1105692,2010-01-11 23:50:16,statistics,Good reading list for Bayesian inference,aode7,xamdam,1129780800.0,https://www.reddit.com/r/statistics/comments/aode7/good_reading_list_for_bayesian_inference/,1.0,0.0,,en
1105693,2010-01-12 00:57:47,MachineLearning,What might predict murder?,aoe6v,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/aoe6v/what_might_predict_murder/,4.0,0.0,,en
1105694,2010-01-12 18:19:00,statistics,DAE want to create a regression analysis of what creates popular posts on Reddit?,aopec,citizenjr,1261499662.0,https://www.reddit.com/r/statistics/comments/aopec/dae_want_to_create_a_regression_analysis_of_what/,1.0,0.0,,en
1105695,2010-01-12 19:39:48,MachineLearning,"Peter Norvig's library (AI slant, obviously)",aoqjw,xamdam,1129780800.0,https://www.reddit.com/r/MachineLearning/comments/aoqjw/peter_norvigs_library_ai_slant_obviously/,24.0,2.0,,en
1105696,2010-01-12 20:19:46,MachineLearning,Stanford Machine Learning (CS229) - Who wants to work through this together?,aor5z,FantasticPants,1263152053.0,https://www.reddit.com/r/MachineLearning/comments/aor5z/stanford_machine_learning_cs229_who_wants_to_work/,30.0,19.0,"I'm loving the course so far. I'm about 1/4 of the way done. I think it's really valuable knowledge so I'm going through every detail and developing a deep understanding. 

I'm hungry for real problems to solve, for real projects. It's exciting to see there are plenty of them I can engage on online.

However, learning and exploring potential projects is much more fun, productive and efficient when done in a group.

We could work through this course together. We now have a CrunchCourse page! We can start using the forum http://www.crunchcourse.com/class/stanford-cs229-machine-learning/2010/jan/

Lecture materials and videos: [Stanford CS229 Machine Learning](http://see.stanford.edu/see/courseinfo.aspx?coll=348ca38a-3a6d-4052-937d-cb017338d7b1)

Summary of the course:
This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs; VC theory; large margins); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.",en
1105697,2010-01-12 20:20:52,statistics,Need help generating a plot of a year's worth of historical stock prices.,aor6q,Cartosys,1214581635.0,https://www.reddit.com/r/statistics/comments/aor6q/need_help_generating_a_plot_of_a_years_worth_of/,0.0,2.0,"So I have a large dataset of historical stock prices (1 year worth of daily closing price).  Basically in the format

Date,Ticker,Price
1-13-09,MSFT,43.21
1-14-09,MSFT,41.83
1-15-09,MSFT,39.17
1-01-09,GOOG,450.12
1-02-09,GOOG,453.52
etc. Except a couple hundred rows per ticker.

What I would like to do is plot price over time for all tickers, but since the data set is so huge, I can't do it in Excel.  I would prefer R, since it has manipulatable graphics outputs, so a large image would be preferable.  Can anybody point me in the right direction?  
",en
1105698,2010-01-14 02:25:39,statistics,Trying to get data from Philadelphians for analytics x prize. If you can help it would be great.,apbxp,[deleted],,https://www.reddit.com/r/statistics/comments/apbxp/trying_to_get_data_from_philadelphians_for/,1.0,0.0,,en
1105699,2010-01-14 04:00:13,rstats,R Virtual Workbench,apcx2,[deleted],,https://www.reddit.com/r/rstats/comments/apcx2/r_virtual_workbench/,1.0,1.0,,en
1105700,2010-01-14 19:42:21,computervision,List of papers from recent vision conferences,apn00,[deleted],,https://www.reddit.com/r/computervision/comments/apn00/list_of_papers_from_recent_vision_conferences/,14.0,0.0,,en
1105701,2010-01-14 19:44:45,MachineLearning,computer vision subreddit,apn1d,[deleted],,https://www.reddit.com/r/MachineLearning/comments/apn1d/computer_vision_subreddit/,16.0,0.0,,en
1105702,2010-01-14 19:46:55,computervision,Natural Image Statistics -- A probabilistic approach to early computational vision (ebook),apn2g,[deleted],,https://www.reddit.com/r/computervision/comments/apn2g/natural_image_statistics_a_probabilistic_approach/,10.0,0.0,,en
1105703,2010-01-14 21:46:18,MachineLearning,"Honing machine Manufacturer
",apopx,charlesroth123,1263498141.0,https://www.reddit.com/r/MachineLearning/comments/apopx/honing_machine_manufacturer/,1.0,0.0,,en
1105704,2010-01-14 22:19:39,computervision,Beginner's guide to computer vision?,app4t,[deleted],,https://www.reddit.com/r/computervision/comments/app4t/beginners_guide_to_computer_vision/,1.0,0.0,,en
1105705,2010-01-15 01:01:55,computervision,Real Time 3D Scanning Using off the Shelf Equipment,apr09,theschwa,1221492946.0,https://www.reddit.com/r/computervision/comments/apr09/real_time_3d_scanning_using_off_the_shelf/,11.0,0.0,,en
1105706,2010-01-15 01:28:45,computervision,Point Cloud Data from Radiohead - House Of Cards,apr9p,[deleted],,https://www.reddit.com/r/computervision/comments/apr9p/point_cloud_data_from_radiohead_house_of_cards/,13.0,1.0,,en
1105707,2010-01-15 04:27:30,computervision,Bunch of great articles on Machine Vision,apt25,chime,1134104400.0,https://www.reddit.com/r/computervision/comments/apt25/bunch_of_great_articles_on_machine_vision/,4.0,0.0,,en
1105708,2010-01-15 04:42:51,computervision,Motion-detection using Flash + web-cam,apt7n,chime,1134104400.0,https://www.reddit.com/r/computervision/comments/apt7n/motiondetection_using_flash_webcam/,5.0,0.0,,en
1105709,2010-01-15 07:36:27,computervision,Wikipedia Computer Vision Article,apv2a,Phrack,1255298352.0,https://www.reddit.com/r/computervision/comments/apv2a/wikipedia_computer_vision_article/,0.0,0.0,,en
1105710,2010-01-15 12:41:07,statistics,A Random-Walk on tombstones,apy1n,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/apy1n/a_randomwalk_on_tombstones/,1.0,0.0,,en
1105711,2010-01-15 15:01:09,computervision,Hyperlinking Reality via Camera Phones [a bad ass project from my faculty] ,apzag,tomazk,1229769587.0,https://www.reddit.com/r/computervision/comments/apzag/hyperlinking_reality_via_camera_phones_a_bad_ass/,11.0,1.0,,en
1105712,2010-01-15 22:12:55,MachineLearning,Machine Learning with Quantum Algorithms,aq4tk,FantasticPants,1263152053.0,https://www.reddit.com/r/MachineLearning/comments/aq4tk/machine_learning_with_quantum_algorithms/,12.0,4.0,,en
1105713,2010-01-16 00:32:03,computervision,siftgpu: GPU implementation of SIFT,aq6by,[deleted],,https://www.reddit.com/r/computervision/comments/aq6by/siftgpu_gpu_implementation_of_sift/,16.0,0.0,,en
1105714,2010-01-16 01:06:57,computervision,"""The VLFeat open source library implements 
popular computer vision algorithms including SIFT, 
MSER, k-means, hierarchical k-means, 
agglomerative information bottleneck, and quick 
shift.""",aq6ny,Gusfoo,1235679690.0,https://www.reddit.com/r/computervision/comments/aq6ny/the_vlfeat_open_source_library_implements_popular/,10.0,0.0,,en
1105715,2010-01-16 08:39:02,MachineLearning,Homicide in Philadelphia as a Networked Process,aqa9y,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/aqa9y/homicide_in_philadelphia_as_a_networked_process/,8.0,1.0,,en
1105716,2010-01-16 23:50:41,computervision,gPb boundary detection (papers + online demo),aqhdg,[deleted],,https://www.reddit.com/r/computervision/comments/aqhdg/gpb_boundary_detection_papers_online_demo/,8.0,0.0,"Papers: 

*Using Contours to Detect and Localize Junctions in Natural Images [pdf](http://www.cs.berkeley.edu/~arbelaez/publications/Maire_Arbelaez_Fowlkes_Malik_CVPR2008.pdf)

*From Contours to Regions: An Empirical Evaluation
[pdf](http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/papers/amfm-cvpr2009.pdf)

Results: [link](http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/bench/color/gPb_color/main.html)

Code (Gpu): [link](http://www.cs.berkeley.edu/~catanzar/Damascene/)

Demo: [link](http://telperion.eecs.berkeley.edu/)",en
1105717,2010-01-17 01:46:48,statistics,What free alternatives are there to Excel Solver's PrecisionTree and Influence Diagram?,aqi7b,citizenjr,1261499662.0,https://www.reddit.com/r/statistics/comments/aqi7b/what_free_alternatives_are_there_to_excel_solvers/,1.0,0.0,,en
1105718,2010-01-17 17:56:07,artificial,"The grandmasters: Intelligent machines are about to revolutionize the world, says Harvard economist KENNETH ROGOFF",aqo9p,queensnake,1183482087.0,https://www.reddit.com/r/artificial/comments/aqo9p/the_grandmasters_intelligent_machines_are_about/,12.0,3.0,,en
1105719,2010-01-17 19:28:42,computervision,Caltech Pedestrian Dataset (Also contains links to other pedestrian datasets),aqp4a,[deleted],,https://www.reddit.com/r/computervision/comments/aqp4a/caltech_pedestrian_dataset_also_contains_links_to/,1.0,0.0,,en
1105720,2010-01-17 19:30:34,computervision,Caltech Pedestrian Dataset. (Also contains links to other datasets),aqp4z,[deleted],,https://www.reddit.com/r/computervision/comments/aqp4z/caltech_pedestrian_dataset_also_contains_links_to/,7.0,0.0,,en
1105721,2010-01-18 00:15:35,MachineLearning,Data munging with SQL and R (video),aqrkr,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/aqrkr/data_munging_with_sql_and_r_video/,0.0,2.0,,en
1105722,2010-01-18 10:39:03,rstats,R videos,aqx6p,[deleted],,https://www.reddit.com/r/rstats/comments/aqx6p/r_videos/,3.0,4.0,The are many videos about R online let's have a list of them here,en
1105723,2010-01-18 21:20:19,computervision,Camera Calibration Toolbox for Matlab,ar4mm,[deleted],,https://www.reddit.com/r/computervision/comments/ar4mm/camera_calibration_toolbox_for_matlab/,8.0,0.0,,en
1105724,2010-01-18 23:16:18,MachineLearning,Spatial Autocorrelation of Homicide in Philadelphia ,ar61k,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/ar61k/spatial_autocorrelation_of_homicide_in/,8.0,0.0,,en
1105725,2010-01-19 19:10:32,computervision,"Understanding versus Interpretation -- a 
philosophical distinction",ariwe,jeanlucpikachu,1159634949.0,https://www.reddit.com/r/computervision/comments/ariwe/understanding_versus_interpretation_a/,5.0,0.0,,en
1105726,2010-01-19 20:56:04,computervision,Image-based Facial Hair Shaving,arkee,[deleted],,https://www.reddit.com/r/computervision/comments/arkee/imagebased_facial_hair_shaving/,0.0,0.0,,en
1105727,2010-01-19 20:59:23,computervision,Image-based Facial Hair Shaving,arkg5,[deleted],,https://www.reddit.com/r/computervision/comments/arkg5/imagebased_facial_hair_shaving/,22.0,1.0,,en
1105728,2010-01-20 03:09:29,statistics,Heatmap of predicted probability of homicide in Philadelphia,aror6,[deleted],,https://www.reddit.com/r/statistics/comments/aror6/heatmap_of_predicted_probability_of_homicide_in/,1.0,0.0,,en
1105729,2010-01-20 14:01:17,computervision,handprint : color vision,arvos,jeanlucpikachu,1159634949.0,https://www.reddit.com/r/computervision/comments/arvos/handprint_color_vision/,2.0,0.0,,en
1105730,2010-01-20 19:49:15,computervision,Automatic Photo Pop-up (creates 3D pop-up model from image),as0dg,[deleted],,https://www.reddit.com/r/computervision/comments/as0dg/automatic_photo_popup_creates_3d_popup_model_from/,17.0,1.0,,en
1105731,2010-01-21 05:20:44,MachineLearning,Utilizing GM as a robotics factory.,as71v,rsho,1236996757.0,https://www.reddit.com/r/MachineLearning/comments/as71v/utilizing_gm_as_a_robotics_factory/,0.0,0.0,"Since the government to a large extent owns GM and the factories, is there anything that you would want produced that would make sense given the tooling they have or could retrofit? I'm thinking something along the lines of using it as a robotics factory, creating unconventional vehicles or transports. These could just be one-off prototypes until there is a need to actually mass produce one of the designs.",en
1105732,2010-01-21 10:53:21,statistics,"OKCupid demonstrates that men are attracted to women who look at them and act flirty, while women are attracted to men who look away and look bored.",asanv,moultano,1160099427.0,https://www.reddit.com/r/statistics/comments/asanv/okcupid_demonstrates_that_men_are_attracted_to/,13.0,0.0,,en
1105733,2010-01-21 12:45:25,MachineLearning,UK Launches Open Data Site; Puts Data.gov to Shame,asbrh,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/asbrh/uk_launches_open_data_site_puts_datagov_to_shame/,17.0,0.0,,en
1105734,2010-01-21 18:14:05,MachineLearning,"AskML: Best/fastest way to learn R (with ML/AI/Data Mining orientation) - book, course, anything? - thanks!",asfsh,xamdam,1129780800.0,https://www.reddit.com/r/MachineLearning/comments/asfsh/askml_bestfastest_way_to_learn_r_with_mlaidata/,28.0,6.0,,en
1105735,2010-01-21 18:46:34,computervision,Seam Carving For Content-Aware Image Resizing (Paper + Video),asgbh,[deleted],,https://www.reddit.com/r/computervision/comments/asgbh/seam_carving_for_contentaware_image_resizing/,13.0,2.0,"Paper: [pdf](http://www.shaiavidan.org/papers/imretFinal.pdf)

Video: [link](http://www.youtube.com/watch?v=6NcIJXTlugc)

Gimp Plugin: [link](http://liquidrescale.wikidot.com/)",en
1105736,2010-01-21 23:09:43,computervision,I've got a question for you...,ask4p,[deleted],,https://www.reddit.com/r/computervision/comments/ask4p/ive_got_a_question_for_you/,5.0,7.0,"I am trying to implement a path planning algorithm that uses satellite imagery as one of its inputs (the other inputs are sensors on the vehicle, none of which are cameras, so not CV related).  I am playing around with different image segmentation techniques to find obstacles (just the big ones like trees and buildings) so the path can be routed around them.  It doesn't have to be 100% accurate, if the other sensors detect something in the way the path can be changed, but it would be nice if it could still give a good high level view of the best route to get to the destination so that the amount of changes would be minimal.  Right now I think cluster-based segmentation is the way to go (so far the best results have come from a self-organizing feature map).  This is my first experience doing anything with computer vision, however, so I was wondering if maybe anyone here has good suggestions for other strategies to tackle this problem.",en
1105737,2010-01-22 21:33:44,MachineLearning,Data mining competition for predicting drug reactions from medical records (with prizes),aszb6,bucanrabi,1264110458.0,https://www.reddit.com/r/MachineLearning/comments/aszb6/data_mining_competition_for_predicting_drug/,17.0,1.0,,en
1105738,2010-01-23 02:49:58,computervision,If your face were made of cereal [Image Quilting for Texture Synthesis and Transfer - paper and results],at2qi,[deleted],,https://www.reddit.com/r/computervision/comments/at2qi/if_your_face_were_made_of_cereal_image_quilting/,1.0,0.0,,en
1105739,2010-01-23 02:54:57,computervision,If your face was made of cereal [Image Quilting for Texture Synthesis and Transfer - paper and results],at2s5,[deleted],,https://www.reddit.com/r/computervision/comments/at2s5/if_your_face_was_made_of_cereal_image_quilting/,10.0,1.0,"Texture Transfer Example (cereal): 
[before](http://graphics.cs.cmu.edu/people/efros/research/quilting/figs/transfer/bill-big.jpg) , 
[after]( http://graphics.cs.cmu.edu/people/efros/research/quilting/figs/transfer/bill-rice.gif)

Paper: [pdf](http://graphics.cs.cmu.edu/people/efros/research/quilting/quilting.pdf)

Project Website: [link](http://graphics.cs.cmu.edu/people/efros/research/quilting.html)",en
1105740,2010-01-23 21:39:11,MachineLearning,Government posting wealth of data to Internet,atc37,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/atc37/government_posting_wealth_of_data_to_internet/,6.0,3.0,,en
1105741,2010-01-24 12:57:36,computervision,"PTAM: open source, state of the art visual localization and mapping",atidx,0ld,1261047157.0,https://www.reddit.com/r/computervision/comments/atidx/ptam_open_source_state_of_the_art_visual/,7.0,0.0,,en
1105742,2010-01-26 08:43:15,statistics,"R, Help! EM estimation of thresholded mixture of gaussians",au85a,[deleted],,https://www.reddit.com/r/statistics/comments/au85a/r_help_em_estimation_of_thresholded_mixture_of/,0.0,0.0,,en
1105743,2010-01-26 20:00:09,computervision,Human Detection Using Partial Least Squares [paper + video],auf1x,[deleted],,https://www.reddit.com/r/computervision/comments/auf1x/human_detection_using_partial_least_squares_paper/,13.0,1.0,"paper: [pdf](http://www.umiacs.umd.edu/~lsd/papers/PLS-ICCV09.pdf)

video: [link](http://www.youtube.com/watch?v=4OmNpJvkIek)",en
1105744,2010-01-26 21:03:56,computervision,PatchMatch: A Randomized Correspondence Algorithm for Structural Image Editing [site + paper + video],aufy0,[deleted],,https://www.reddit.com/r/computervision/comments/aufy0/patchmatch_a_randomized_correspondence_algorithm/,2.0,0.0,"site: [link]( http://www.cs.princeton.edu/gfx/pubs/Barnes_2009_PAR/index.php)

paper: [pdf](http://www.cs.princeton.edu/gfx/pubs/Barnes_2009_PAR/patchmatch.pdf)

video: [link](http://www.youtube.com/watch?v=dgKjs8ZjQNg)",en
1105745,2010-01-27 17:56:49,artificial,"One of my more recent genetic programming images.

Take a look at the full image.  It doesn't use that many colors, mostly strange patterns.
",ausvv,fireduck,1247511188.0,https://www.reddit.com/r/artificial/comments/ausvv/one_of_my_more_recent_genetic_programming_images/,0.0,0.0,,en
1105746,2010-01-27 21:13:26,MachineLearning,NLTK Regular Expression Parser (RegexpParser),auvqi,qbproger,1199026684.0,https://www.reddit.com/r/MachineLearning/comments/auvqi/nltk_regular_expression_parser_regexpparser/,6.0,0.0,,en
1105747,2010-01-28 07:17:31,artificial,What are good Classification algs for protein structures?,av2rq,[deleted],,https://www.reddit.com/r/artificial/comments/av2rq/what_are_good_classification_algs_for_protein/,1.0,0.0,,en
1105748,2010-01-28 10:03:11,MachineLearning,[fMRI Analysis; Machine Learning] Machine Learning Classifiers and fMRI: A tutorial overview [Link to PDF],av4mo,dearsomething,1210808677.0,https://www.reddit.com/r/MachineLearning/comments/av4mo/fmri_analysis_machine_learning_machine_learning/,15.0,0.0,,en
1105749,2010-01-28 16:07:21,MachineLearning,"Two Centuries On, a Cryptologist Cracks a Presidential Code ",av8ak,[deleted],,https://www.reddit.com/r/MachineLearning/comments/av8ak/two_centuries_on_a_cryptologist_cracks_a/,1.0,0.0,,en
1105750,2010-01-28 18:24:38,computervision,Unsupervised Face Alignment by Nonrigid Mapping [paper + project site],ava7e,[deleted],,https://www.reddit.com/r/computervision/comments/ava7e/unsupervised_face_alignment_by_nonrigid_mapping/,9.0,1.0,"paper: [pdf](http://www.vision.ee.ethz.ch/~zhuji/pdfs/iccv09.pdf)

site: [link](http://www.vision.ee.ethz.ch/~zhuji/facealign/index.html)",en
1105751,2010-01-28 21:28:27,artificial,Mapping the brain: MIT neuroscientists are making computers smart enough to see the connections between the brain's neurons,avd0i,yaserbuntu,1207657636.0,https://www.reddit.com/r/artificial/comments/avd0i/mapping_the_brain_mit_neuroscientists_are_making/,8.0,1.0,,en
1105752,2010-02-01 17:14:39,statistics,Ask Stats: any good courseware on stats?,awpay,xamdam,1129780800.0,https://www.reddit.com/r/statistics/comments/awpay/ask_stats_any_good_courseware_on_stats/,1.0,0.0,,en
1105753,2010-02-02 00:32:25,statistics,How do I become a biostatistician?,awuua,[deleted],,https://www.reddit.com/r/statistics/comments/awuua/how_do_i_become_a_biostatistician/,1.0,0.0,,en
1105754,2010-02-02 05:02:25,statistics,"Statistics book recommendations in general, specifically for analyzing algorithms",awxgp,[deleted],,https://www.reddit.com/r/statistics/comments/awxgp/statistics_book_recommendations_in_general/,5.0,2.0,,en
1105755,2010-02-03 14:32:33,MachineLearning,Kaggle: a new platform for data-related competitions,axjcc,StompChicken,1248945961.0,https://www.reddit.com/r/MachineLearning/comments/axjcc/kaggle_a_new_platform_for_datarelated_competitions/,14.0,0.0,,en
1105756,2010-02-03 18:11:04,MachineLearning,Zettair - 101 - Open Source Search Engine in C.,axm07,saneverse,1265213316.0,https://www.reddit.com/r/MachineLearning/comments/axm07/zettair_101_open_source_search_engine_in_c/,6.0,1.0,,en
1105757,2010-02-03 19:10:27,MachineLearning,"Predicting the locations of 'Emergency' Ushahidi reports in Port-au-Prince, and implications for crowdsourcing",axmyo,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/axmyo/predicting_the_locations_of_emergency_ushahidi/,0.0,2.0,,en
1105758,2010-02-04 18:59:25,computervision,Virtual Makeover site developed by some folks from UCSD,ay2tk,[deleted],,https://www.reddit.com/r/computervision/comments/ay2tk/virtual_makeover_site_developed_by_some_folks/,11.0,0.0,,en
1105759,2010-02-08 20:52:29,computervision,Reconstructing Building Interiors from Images [project site - contains paper + videos],azkr8,[deleted],,https://www.reddit.com/r/computervision/comments/azkr8/reconstructing_building_interiors_from_images/,15.0,0.0,,en
1105760,2010-02-08 20:54:21,MachineLearning,iPhone app based on CALO - a huge DARPA AI project (markov logic networks),azks0,xamdam,1129780800.0,https://www.reddit.com/r/MachineLearning/comments/azks0/iphone_app_based_on_calo_a_huge_darpa_ai_project/,0.0,1.0,,en
1105761,2010-02-09 14:38:07,MachineLearning,Spatial Statistics in R: An Introduction,azw2w,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/azw2w/spatial_statistics_in_r_an_introduction/,13.0,1.0,,en
1105762,2010-02-10 05:32:50,statistics,A look at the stats behind the accidents.,b06qe,[deleted],,https://www.reddit.com/r/statistics/comments/b06qe/a_look_at_the_stats_behind_the_accidents/,1.0,0.0,,en
1105763,2010-02-10 18:49:16,MachineLearning,"MS patents ""personal data mining"". Curious what their strategy is, considering how broad the patent seems; they are not known to troll",b0fng,xamdam,1129780800.0,https://www.reddit.com/r/MachineLearning/comments/b0fng/ms_patents_personal_data_mining_curious_what/,7.0,4.0,,en
1105764,2010-02-10 22:06:53,analytics,November Sees Number of U.S. Videos Viewed Online Surpass 30 Billion for First Time on Record [comScore],b0ihf,salvage,1184143405.0,https://www.reddit.com/r/analytics/comments/b0ihf/november_sees_number_of_us_videos_viewed_online/,1.0,0.0,,en
1105765,2010-02-11 19:47:22,analytics,So what do you guys all do related to analytics? Why the interest?,b0x63,xtom,1227475677.0,https://www.reddit.com/r/analytics/comments/b0x63/so_what_do_you_guys_all_do_related_to_analytics/,7.0,4.0,"There's a lot of reasons to want to know all this stuff, so I figured I'd get to know the others that are on this subreddit.

So let's hear it: Webmasters? Coders? Marketers? Work for an analytics software company? You get the idea.",en
1105766,2010-02-12 18:10:36,analytics,10 Web Analytics Tools For Tracking Your Visitors,b1bbg,[deleted],,https://www.reddit.com/r/analytics/comments/b1bbg/10_web_analytics_tools_for_tracking_your_visitors/,4.0,1.0,,en
1105767,2010-02-13 11:51:11,MachineLearning,Introduction to Information Retrieval,b1lu6,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/b1lu6/introduction_to_information_retrieval/,22.0,1.0,,en
1105768,2010-02-14 10:20:43,MachineLearning,Splitting up the USA: clustering 210 million Facebook profiles,b1vh6,fxj,1169393085.0,https://www.reddit.com/r/MachineLearning/comments/b1vh6/splitting_up_the_usa_clustering_210_million/,18.0,7.0,,en
1105769,2010-02-14 15:17:38,artificial,"Neuropilot Demo (Reinforcement learning with neural nets, requires java)",b1x10,[deleted],,https://www.reddit.com/r/artificial/comments/b1x10/neuropilot_demo_reinforcement_learning_with/,1.0,0.0,,en
1105770,2010-02-15 16:12:42,MachineLearning,Algebraic statistics for random graph models: Markov bases and their uses ,b29q3,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/b29q3/algebraic_statistics_for_random_graph_models/,17.0,0.0,,en
1105771,2010-02-17 23:35:50,statistics,Anyone know any good Operations Research freeware/shareware/code libraries?,b3b66,ST2K,1173236363.0,https://www.reddit.com/r/statistics/comments/b3b66/anyone_know_any_good_operations_research/,5.0,5.0,Back in the day I used to use a DOS program called Storm to do my OR work.  I was wondering if anyone doing OR knows of a more modern application or even a code library that has OR functionality.  Thanks.,en
1105772,2010-02-18 00:11:56,statistics,Free tools to visualize statistical information: Tableau Public as an alternative to ManyEyes,b3bmd,almodozo,1227269244.0,https://www.reddit.com/r/statistics/comments/b3bmd/free_tools_to_visualize_statistical_information/,1.0,0.0,,en
1105773,2010-02-19 16:00:19,MachineLearning,Ask ML: Which machine learning algorithm(SL or SSL) would you suggest for high performance and accurate classification?,b3zq7,personanongrata,1200898594.0,https://www.reddit.com/r/MachineLearning/comments/b3zq7/ask_ml_which_machine_learning_algorithmsl_or_ssl/,15.0,24.0,"My dataset has 7 tuples, 11 classes and I should classify at least 80k items under 5 mins. And of course there is noise in the dataset. I'm thinking of using a semi-supervised learning technique for my case. Because i have a lot of unlabeled samples. But semi-supervised techniques, usually tend to have higher computational cost when compared to other techniques. But cluster and label or cotraining approaches are still in my mind. I have tested SVM(C-SVC in libsvm) on my dataset and got 91% percent accuracy rate but lasted about 13 min. Also tested C4.5 and got 85 percent accuracy rate but it finished under 5 min. Both algorithms are tested with default parameters without fine-tuning. C4.5 is quick and it can get better accuracy by using boosting. But i don't want to use C4.5 for personal reasons :). Bayesian techniques that i have tested are also too slow and have long training time. What is you opinion? 

Edit: I have also written KNN but it took longer than 10 mins and it was the least accurate one.
Edit 2: I meant 7 tuples=7 features.",en
1105774,2010-02-19 18:59:32,MachineLearning,AskML: statistical models for studying historical evidence?,b422m,xamdam,1129780800.0,https://www.reddit.com/r/MachineLearning/comments/b422m/askml_statistical_models_for_studying_historical/,2.0,0.0,"I am curious if someone has come across statistical theory being used to evaluate alternative historical possibilities. E.g. you can use genetic, linguistic and archeological evidence to support certain theories of population migration. Googling did not help much ;(",en
1105775,2010-02-20 22:02:41,statistics,Before You Go Locking Up All Of Those “Crazy” People…,b4ggo,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/b4ggo/before_you_go_locking_up_all_of_those_crazy_people/,1.0,0.0,,en
1105776,2010-02-23 03:24:23,computervision,ARToolKit: A software library for optical glyph tracking,b58s5,ModernRonin,1201320304.0,https://www.reddit.com/r/computervision/comments/b58s5/artoolkit_a_software_library_for_optical_glyph/,6.0,0.0,,en
1105777,2010-02-23 06:12:51,computervision,Almost every piece of software scales images incorrectly (including GIMP and Photoshop.),b5amt,moultano,1160099427.0,https://www.reddit.com/r/computervision/comments/b5amt/almost_every_piece_of_software_scales_images/,19.0,0.0,,en
1105778,2010-02-23 09:40:48,computervision,Fill in the Blanks: Using Math to Turn Lo-Res Datasets Into Hi-Res Samples,b5cy0,moultano,1160099427.0,https://www.reddit.com/r/computervision/comments/b5cy0/fill_in_the_blanks_using_math_to_turn_lores/,13.0,3.0,,en
1105779,2010-02-23 13:57:35,MachineLearning,A simple R interface to Google Documents,b5fep,fxj,1169393085.0,https://www.reddit.com/r/MachineLearning/comments/b5fep/a_simple_r_interface_to_google_documents/,2.0,0.0,,en
1105780,2010-02-24 18:53:04,MachineLearning,"Ask ML: Where can I find information about executive education, conferences or workshops for Machine Learning",b5znf,RPI,1267029784.0,https://www.reddit.com/r/MachineLearning/comments/b5znf/ask_ml_where_can_i_find_information_about/,6.0,1.0,"It looks like the company I work for would like to help me get some formal training in the subject matter of my choice. I come from a finance / economics background and am doing data analysis on large consumer consumption pattern data sets. 

I have a degree in statistics but am very interested in learning more about machine learning and how it can be applied to prediction, forecasting and market segmentation problems.

Can anyone recommend machine learning executive education programs, conferences or workshops? 

Geographic location is not an issue.
",en
1105781,2010-02-24 21:10:21,computervision,How to clean a scanner,b61ot,jogkely007,1267038118.0,https://www.reddit.com/r/computervision/comments/b61ot/how_to_clean_a_scanner/,1.0,0.0,,en
1105782,2010-02-24 22:29:35,MachineLearning,Good (slightly outdated) open-source DM tool comparison,b62q7,xamdam,1129780800.0,https://www.reddit.com/r/MachineLearning/comments/b62q7/good_slightly_outdated_opensource_dm_tool/,2.0,1.0,,en
1105783,2010-02-25 03:29:49,computervision,Looking for a good face landmarking software,b65ye,aepr88,1263123694.0,https://www.reddit.com/r/computervision/comments/b65ye/looking_for_a_good_face_landmarking_software/,5.0,2.0,"Does anyone know one? I have looked over http://www.robots.ox.ac.uk/~vgg/research/nface/, but it has some undesired features.",en
1105784,2010-02-25 06:21:02,computervision,Compsoft Depot,b67r6,compsoft,1267071463.0,https://www.reddit.com/r/computervision/comments/b67r6/compsoft_depot/,0.0,0.0,,en
1105785,2010-02-25 22:58:39,statistics,r/statistics can you help point me toward some info on combining uncertainties in cost estimates?,b6jcl,[deleted],,https://www.reddit.com/r/statistics/comments/b6jcl/rstatistics_can_you_help_point_me_toward_some/,1.0,0.0,,en
1105786,2010-02-26 20:26:18,analytics,Improving Your Sense of Site,b6x0n,[deleted],,https://www.reddit.com/r/analytics/comments/b6x0n/improving_your_sense_of_site/,2.0,0.0,,en
1105787,2010-02-27 01:17:21,MachineLearning,Yahoo! Learning to Rank Challenge,b707j,cpdomina,1262293486.0,https://www.reddit.com/r/MachineLearning/comments/b707j/yahoo_learning_to_rank_challenge/,6.0,3.0,,en
1105788,2010-03-01 14:39:01,computervision,IVT - Integrating Vision Toolkit,b7r95,mebrahim,1219505995.0,https://www.reddit.com/r/computervision/comments/b7r95/ivt_integrating_vision_toolkit/,12.0,3.0,,en
1105789,2010-03-02 07:20:13,statistics,Representational dissimilarity matrix (a cognitive neuroscience method). Very cool stuff.,b82uy,dearsomething,1210808677.0,https://www.reddit.com/r/statistics/comments/b82uy/representational_dissimilarity_matrix_a_cognitive/,2.0,5.0,,en
1105790,2010-03-03 12:47:33,statistics,Internet Statistics in Video by jess3,b8nbs,nacene,1239769630.0,https://www.reddit.com/r/statistics/comments/b8nbs/internet_statistics_in_video_by_jess3/,1.0,1.0,,en
1105791,2010-03-04 20:17:26,analytics,"Google's Invasive, non-Anonymized Ad Targeting: A Quick Confirmation of previously suspected privacy issues",b9ab7,xtom,1227475677.0,https://www.reddit.com/r/analytics/comments/b9ab7/googles_invasive_nonanonymized_ad_targeting_a/,2.0,1.0,"I'm cross posting this from /r/cyberlaw, hopefully you guys find it as interesting as I did(it deals with Google Analytics):

So quite awhile ago, I ordered a Papa John's pizza online. My job largely involves looking at ads that appear online, so afterwards I was quick to notice *I was getting a LOT* of Papa Johns ads (especially at night) being served through a Google owned company (DoubleClick media). Yesterday one of these ads popped up again on Youtube (a place that typically serves using the adwords program, not doubleclick), so I decided to copy the URL. 

For those not in the advertising field: Making full use of Google's analytics tool means that certain information about the advertising campaign is leaked in the URL.

So let's break it apart: 

&gt;http://ad.doubleclick.net/click;h=(junk here);~sscs=?http://googleads.g.doubleclick.net/aclk?sa=l&amp;ai=(junk here)&amp;adurl=http://www.papajohns.com/index.shtm?utm_source=googlenetwork&amp;utm_medium=DisplayCPC&amp;utm_campaign=GoogleRemarketing

First off, we see ~sscs: ~sscs is doubleclick's redirect variable. So rather than directly serving adwords ads, they overrode it to serve through doubleclick, then redirect through what would otherwise be an adwords link(http://googleads.g.doubleclick.net). This is tighter integration than is generally seen with adwords/doubleclick.

* The interesting part is the end variables utm_source=**googlenetwork**&amp;utm_medium=**DisplayCPC**&amp;utm_campaign=**GoogleRemarketing**

* DisplayCPC/googlenetwork - Confirmation that doubleclick is now more finely integrated with adwords.

* ""GoogleRemarketing"", huh? Let's take a look at the definition for ""Remarketing""

&gt;Using past campaign information to target a particular message to an audience.

While in the past behavioral targetting has largely been based on the sum of your use, this is an interesting(though no doubt more widespread than is known) change in that; explicitly targeting old customers though a *massive* network of sites.

-----------------------------------

Just thought I'd put this out there. I'm sure it's not new to a lot of people, but at least to me it was interesting to see concepts like this actually put into practice on such a large scale. 

-----------------------------

PS: I did a quick survey across several thousand domains, and for the record: right now, the most common external resource locations on the internet are(Google owned is bolded):

**www.google-analytics.com**

**pagead2.googlesyndication.com**

**googleads.g.doubleclick.net**

edge.quantserve.com

**ad.doubleclick.net**

**www.youtube.com**

b.scorecardresearch.com

s0.2mdn.net

dg.specificclick.net

view.atdmt.com

**www.google.com**

**ajax.googleapis.com**

**partner.googleadservices.com**

That's a lot of data.",en
1105792,2010-03-04 21:29:12,statistics,Really Terrible Charts (Charts March Chart Madness | EagerEyes.org),b9bbd,cbg,1135746000.0,https://www.reddit.com/r/statistics/comments/b9bbd/really_terrible_charts_charts_march_chart_madness/,0.0,0.0,,en
1105793,2010-03-05 02:39:32,statistics,"Ask Stats: Why can't I use Pearson's R to examine the correlation between two autocorrelated time-series?  Moreover, what should I do instead (""ccf()""?)",b9eyi,quaternion,1252017693.0,https://www.reddit.com/r/statistics/comments/b9eyi/ask_stats_why_cant_i_use_pearsons_r_to_examine/,4.0,10.0,"I'd like to know how much variance is shared by time series 1 (225 time points x 129 different spatial locations) and time series 2 (also 225 time points x 129 different spatial locations).  I have 35 different pairs of these time series; each pair is independent from every other pair, and each series within a pair is independent of the other series.  However, each series is spatially and temporally autocorrelated - the data is voltage fluctuations on the scalp collected from 35 individuals in two different tasks.  

I've read that Pearson's R shouldn't be used in such situations, but I don't know why.  Should I use R's ccf? Do I just need to prewhiten the data?  Please have mercy!

EDIT: If any stats people can help out, then I have a juicy paper idea for you: if I'm correct in thinking that t-tests between two autocorrelated timeseries are also inappropriately skewed by autocorrelations, then you can single-handedly take down the #1 most common practice in ERP research - which is t-tests between two highly correlated and autocorrelated timeseries.  Voila, instant paper in *Neuroimage*, *Psychophysiology*, or *J Neurosci Methods* - your pick, if that floats your boat.

2nd Edit: In a certain manner of thinking, this is simply a non-independence of errors problem.  While it's standard to do prewhitening in fMRI, I've never seen this in ERP research.",en
1105794,2010-03-05 18:08:28,MachineLearning,"Predictive Analytics: 8 Things to Keep in 
Mind (Part 1)",b9oyw,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/b9oyw/predictive_analytics_8_things_to_keep_in_mind/,11.0,0.0,,en
1105795,2010-03-06 18:16:54,MachineLearning,"Agglomerative Clustering : Quite intuitive 
way of reducing the number of variables ",ba21s,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/ba21s/agglomerative_clustering_quite_intuitive_way_of/,2.0,0.0,,en
1105796,2010-03-06 18:18:48,MachineLearning,"Make the Data flat with Principal 
Component Analysis ",ba22d,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/ba22d/make_the_data_flat_with_principal_component/,6.0,1.0,,en
1105797,2010-03-06 18:28:49,MachineLearning,"Re-unifying the several clusters AI has 
been divided into, and to build a 
general-purpose, consistent, and 
realistic system: Markov Logic.[Vid 
Lecture]",ba252,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/ba252/reunifying_the_several_clusters_ai_has_been/,21.0,8.0,,en
1105798,2010-03-06 18:31:00,MachineLearning,A quick look at RapidMiner[VID],ba25x,[deleted],,https://www.reddit.com/r/MachineLearning/comments/ba25x/a_quick_look_at_rapidminervid/,0.0,0.0,,en
1105799,2010-03-07 01:51:02,statistics,"I built this wiki-survey site with a blog on correlations. Please check it out, it needs larger samples!",ba5n9,changokun,1203270808.0,https://www.reddit.com/r/statistics/comments/ba5n9/i_built_this_wikisurvey_site_with_a_blog_on/,0.0,0.0,,en
1105800,2010-03-07 06:57:18,MachineLearning,The Synthetic Visual Reasoning Test (Machine Learning &amp; Computer Vision Challenge),ba7q4,cpdomina,1262293486.0,https://www.reddit.com/r/MachineLearning/comments/ba7q4/the_synthetic_visual_reasoning_test_machine/,14.0,4.0,,en
1105801,2010-03-07 21:46:44,MachineLearning,"(AI) Minski's course on OCW, with partial video",badvx,xamdam,1129780800.0,https://www.reddit.com/r/MachineLearning/comments/badvx/ai_minskis_course_on_ocw_with_partial_video/,3.0,0.0,,en
1105802,2010-03-08 08:45:13,MachineLearning,Hey everyone I got accepted to grad school for machine learning!!!,bajoa,[deleted],,https://www.reddit.com/r/MachineLearning/comments/bajoa/hey_everyone_i_got_accepted_to_grad_school_for/,22.0,20.0,,en
1105803,2010-03-08 17:13:18,artificial,Kojiro musculoskeletal system mimics the human body,baow5,[deleted],,https://www.reddit.com/r/artificial/comments/baow5/kojiro_musculoskeletal_system_mimics_the_human/,1.0,0.0,,en
1105804,2010-03-09 15:38:22,rstats,How to compile R code to lisp and accelerate by a factor of 1000,bb4me,fxj,1169393085.0,https://www.reddit.com/r/rstats/comments/bb4me/how_to_compile_r_code_to_lisp_and_accelerate_by_a/,4.0,1.0,,en
1105805,2010-03-10 05:37:14,MachineLearning,Data for 2009 + 2010 March Madness. Can you use machine learning predict the tournament?,bbf7d,danger_t,1249412021.0,https://www.reddit.com/r/MachineLearning/comments/bbf7d/data_for_2009_2010_march_madness_can_you_use/,16.0,1.0,,en
1105806,2010-03-12 16:23:27,MachineLearning,"Horizontal Honing Machine
",bcjhr,william_san12,1268403655.0,https://www.reddit.com/r/MachineLearning/comments/bcjhr/horizontal_honing_machine/,0.0,1.0,,en
1105807,2010-03-12 18:42:38,MachineLearning,The Machine Learning Algorithm with Capital A - three approaches that solve all your problems (disclaimer: my own post),bclfh,gromgull,1195768827.0,https://www.reddit.com/r/MachineLearning/comments/bclfh/the_machine_learning_algorithm_with_capital_a/,29.0,13.0,,en
1105808,2010-03-13 08:04:51,statistics,the overall upvote and downvote behavior on reddit is statistically interesting,bctws,lambdaq,1267448436.0,https://www.reddit.com/r/statistics/comments/bctws/the_overall_upvote_and_downvote_behavior_on/,0.0,0.0,"http://www.reddit.com/r/funny/comments/bag80/

In the end the ratio began to drop

http://www.reddit.com/comments/bag80/_/c0lsftc",en
1105809,2010-03-14 08:45:58,statistics,"Reddit statisiticians, what do you think of these criticisms of geostatistics?  I'm of the impression that Mr. Merk is ""throwing the baby out with the bathwater"", but this is not my field of expertise.",bd5k9,[deleted],,https://www.reddit.com/r/statistics/comments/bd5k9/reddit_statisiticians_what_do_you_think_of_these/,2.0,2.0,,en
1105810,2010-03-15 23:02:47,MachineLearning,An open letter to Netflix from the authors of the de-anonymization paper,bdrkw,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/bdrkw/an_open_letter_to_netflix_from_the_authors_of_the/,35.0,4.0,,en
1105811,2010-03-16 16:24:07,statistics,Evaluating College Basketball Players,be2at,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/be2at/evaluating_college_basketball_players/,5.0,0.0,,en
1105812,2010-03-17 15:23:12,statistics,Why you only need to test with five users,beioe,AndrewKemendo,1190929741.0,https://www.reddit.com/r/statistics/comments/beioe/why_you_only_need_to_test_with_five_users/,9.0,0.0,,en
1105813,2010-03-19 01:13:56,MachineLearning,STAMINA: finite state machine learning competition,bf82e,cpdomina,1262293486.0,https://www.reddit.com/r/MachineLearning/comments/bf82e/stamina_finite_state_machine_learning_competition/,16.0,0.0,,en
1105814,2010-03-19 21:32:49,statistics,An article on the pitfalls and misunderstandings of statistics.,bfm6v,[deleted],,https://www.reddit.com/r/statistics/comments/bfm6v/an_article_on_the_pitfalls_and_misunderstandings/,1.0,0.0,,en
1105815,2010-03-20 22:57:27,statistics,Statistics People:  I need help with excel and doing some things involving trade data. ,bfzw3,highfrequencyT,1269011019.0,https://www.reddit.com/r/statistics/comments/bfzw3/statistics_people_i_need_help_with_excel_and/,0.0,5.0,"Please help me.  It would help if you were in Calgary, but not a must.  I need to merge excel files together that share a time stamp.  I then need to make the file merge any cells that have the exact same time stamp.  I also have some other things I need to do, but I Have no experience using excel really.  HELP!",en
1105816,2010-03-21 01:42:40,statistics,I need some help with a project that I'm doing in R.,bg17u,whiplash3,1264890213.0,https://www.reddit.com/r/statistics/comments/bg17u/i_need_some_help_with_a_project_that_im_doing_in_r/,0.0,3.0,"I have some data that contains a number of variables. I need to pull out the data where X &lt; 10 and Y &lt; 8. This seems like it should be relatively simple, but I can't seem to find how to do it. I'm new at R, so please forgive my ignorance.  

EDIT: Maybe more information is required. My data set consists of 9 columns and around 500 rows. Each row is a different sample. The first 2 columns are sample identification, and the rest are various test results. I need to extract the data where both test X is less than 10 and test Y is less than 8. ",en
1105817,2010-03-21 10:51:48,analytics,700 Million Monthly YouTube Visitors by 2016,bg4xt,salvage,1184143405.0,https://www.reddit.com/r/analytics/comments/bg4xt/700_million_monthly_youtube_visitors_by_2016/,2.0,0.0,,en
1105818,2010-03-22 17:49:40,computervision,I'm developing a driver that will allow you to move the mouse cursor by having a webcam track your hand movements. I need your contributions!,bgnfw,Blaidd_Dwrg,,https://www.reddit.com/r/computervision/comments/bgnfw/im_developing_a_driver_that_will_allow_you_to/,16.0,9.0,"Crossposted from /r/programming

[Example image](http://imgur.com/3qYpA.png)

The inspiration for this project is simple: I can't stand using laptop touchpads, and most laptops these days have an integrated webcam. I think it would be neat to be able to point at things simply by moving my hand in front of the screen.

Part of the tracking software is a Viola-Jones type detector that will search an area of the screen to determine the presence and location of the user's hand. To train the detector I need a large amount of samples. I'd be ever so grateful if you could take a minute and provide me with some samples.

What I need: photos of your hand in the specific posture shown in [this pic](http://imgur.com/3qYpA.png). Even if you can only provide one photo that'll be a great help. Don't fret about quality; so long as it's not all blurry it's fine. If you want to be even more helpful though, you could take a short video clip of 50-100 frames (a few seconds worth) with you moving your hand around. If you want to be totally amazing you could take multiple clips at different locations so I have different backgrounds and lighting to train with. I'd also like to have a shot of the background on its own with no hand so I can take negative samples from it.

When you've made your sample(s) you can either upload them somewhere and post links in this thread or email them to me at blaidd.of.reddit@googlemail.com.

Thanks in advance for your help - reddit is an awesome community so I'm pretty confident you guys will deliver!",en
1105819,2010-03-22 22:40:23,MachineLearning,Lecture on nonparametric models and other stuff by Peter Norvig (long video),bgrzk,xamdam,1129780800.0,https://www.reddit.com/r/MachineLearning/comments/bgrzk/lecture_on_nonparametric_models_and_other_stuff/,30.0,3.0,,en
1105820,2010-03-23 08:20:30,MachineLearning,Prof. Bishop's lecture: Embracing Uncertainty: The New Machine Intelligence ,bgyng,nearest_neighbor,1248941833.0,https://www.reddit.com/r/MachineLearning/comments/bgyng/prof_bishops_lecture_embracing_uncertainty_the/,16.0,5.0,,en
1105821,2010-03-24 17:25:20,MachineLearning,Damn impressive visualization/DM tool demo (at least the visualization part),bhm8t,xamdam,1129780800.0,https://www.reddit.com/r/MachineLearning/comments/bhm8t/damn_impressive_visualizationdm_tool_demo_at/,0.0,4.0,,en
1105822,2010-03-24 17:26:15,MachineLearning,Python lib for rule extraction,bhm99,xamdam,1129780800.0,https://www.reddit.com/r/MachineLearning/comments/bhm99/python_lib_for_rule_extraction/,19.0,2.0,,en
1105823,2010-03-25 11:47:01,statistics,Global Fixed Line Broadband Customers Top 466 Million,bhzzs,ispreview,1237390263.0,https://www.reddit.com/r/statistics/comments/bhzzs/global_fixed_line_broadband_customers_top_466/,1.0,0.0,,en
1105824,2010-03-25 20:39:44,MachineLearning,Learning and predicting a sequence of numbers?,bi7sm,sli,1255213302.0,https://www.reddit.com/r/MachineLearning/comments/bi7sm/learning_and_predicting_a_sequence_of_numbers/,3.0,19.0,"I'm looking to create an algorithm that will learn and predict (in this case) the Fibonacci sequence. Ideally, I'm looking for a way to do this with Python, even if it requires me to write a C extension. Anyone mind pointing me in the right direction? Even just theory, or general concepts on learning and guessing patterns is totally acceptable.

EDIT: Came up with a better way to describe what I'm looking do to: number forecasting. If I feed it just a random series of numbers (like 54, 8, 2, 7, 3, 3 ,7, 8, 34, 87, 3, 8, 41, 8 or whatever, with no range limits), it should report its guess, then learn from the next number I give it, which I'll predetermine before I start feeding numbers. Put simply: it should be able to give me a forecast of tomorrow's average temperature, based on, say, a few month's (or year's) worth of data.",en
1105825,2010-03-25 21:43:41,MachineLearning,What does vanilla mean?,bi8q9,wwkk,1247666644.0,https://www.reddit.com/r/MachineLearning/comments/bi8q9/what_does_vanilla_mean/,0.0,1.0,I heard this term a few times but I'm not sure what it refers to. Any help is appreciated. Thanks.,en
1105826,2010-03-26 15:05:11,statistics,Hans Rosling: Asia's rise - how and when,bikcq,[deleted],,https://www.reddit.com/r/statistics/comments/bikcq/hans_rosling_asias_rise_how_and_when/,1.0,0.0,,en
1105827,2010-03-27 04:24:59,statistics,repost from cogsci- the statistical myth of *g* (posted here to spark debate about EFA vs CFA),biud1,jjrs,1170375384.0,https://www.reddit.com/r/statistics/comments/biud1/repost_from_cogsci_the_statistical_myth_of_g/,7.0,2.0,,en
1105828,2010-03-27 04:42:38,statistics,"""Attack of the psychometricians""- a second look at the dated techniques used by many psychologists",biuiy,jjrs,1170375384.0,https://www.reddit.com/r/statistics/comments/biuiy/attack_of_the_psychometricians_a_second_look_at/,13.0,8.0,,en
1105829,2010-03-27 12:41:35,statistics,"askstats: Multiple Imputation and Penalized Regression, how to combine?",biygx,derwisch,1126584000.0,https://www.reddit.com/r/statistics/comments/biygx/askstats_multiple_imputation_and_penalized/,0.0,1.0,"My wife has a data set from a case-control study with a small number of interesting SNPs. The number of SNPs is still large enough to warrant some penalized regression method, such as LASSO (which she prefers because of the inherent selection mechanism). Some of the SNPs have not been obtained for the whole data set. So she is thinking about performing MI on the explanatory variables.

Penalized methods are largely about minimising the sum of the deviance of the model plus a penalty term which is nondecreasing in the absolute value of the regression coefficients. MI introduces another source of variance brought about by the uncertainty over the precise values of the missings. In simple models, this term is simply added to the mean variance of the estimate in the model, one does not try to refit the model in the attempt of minimising the sum of these variances. It seems that it is assumed (I don't have a textbook at hand here) that these two variances are independent.

My question is: Could this simple approach be taken with penalised regression too, or do I have to include the imputation variance into the minimisation problem, i.e. have to go through an additional cycle?


",en
1105830,2010-03-29 01:26:02,MachineLearning,Reddit Machine Learning:  Anyone want to work on and apply machine learning to something?  ,bjfs9,atlas245,1257794550.0,https://www.reddit.com/r/MachineLearning/comments/bjfs9/reddit_machine_learning_anyone_want_to_work_on/,22.0,33.0,"I've been applying machine learning algorithms to everything I can think of.  I've had a few ideas for a few projects, but would be slightly too large for one person.  I have finally learned when something is beyond the scope of one person :).  So anyone working on anything cool and need a partner,  or send me a line sometime, would love to discuss a few things/projects/ideas.    
Edit:
Basically do you have a fair amount of time and have decent skills?  Send me a message lets get a group of people together skype it up, discuss ideas, go with a project we like and crank out something awesome in 3 weeks.  My skillset is mech eng, machine learning algorithms, and data mining/collecting, preferred language python, matlab. I could really use a front end person, someone that could create a dynamic website.  I've been messing with cherrypy but realize one person can't do everything.",en
1105831,2010-03-29 16:14:54,MachineLearning,Naive Bayes Classifiers: A fairly intuitive explanation of naive Bayes models.,bjoph,[deleted],,https://www.reddit.com/r/MachineLearning/comments/bjoph/naive_bayes_classifiers_a_fairly_intuitive/,18.0,7.0,,en
1105832,2010-03-29 17:54:12,statistics,Multivariate outliers detection using the mahalanobis distance?,bjq3t,[deleted],,https://www.reddit.com/r/statistics/comments/bjq3t/multivariate_outliers_detection_using_the/,4.0,6.0,,en
1105833,2010-03-30 16:31:25,artificial,"A grand unified theory of AI - 
A new approach unites two prevailing but often opposed strains in the history of artificial-intelligence research.",bk6r9,cocoon56,1200778338.0,https://www.reddit.com/r/artificial/comments/bk6r9/a_grand_unified_theory_of_ai_a_new_approach/,14.0,1.0,,en
1105834,2010-03-30 21:40:50,MachineLearning,Crosspost:  A grand unified theory of AI - A new approach unites two prevailing but often opposed strains in the history of artificial-intelligence research.,bkbfy,resiros,1216493757.0,https://www.reddit.com/r/MachineLearning/comments/bkbfy/crosspost_a_grand_unified_theory_of_ai_a_new/,8.0,12.0,,en
1105835,2010-04-01 05:40:41,statistics,Visualize the relation between population (contingency table) and individual risk (Bayesian table),bkylw,[deleted],,https://www.reddit.com/r/statistics/comments/bkylw/visualize_the_relation_between_population/,8.0,0.0,,en
1105836,2010-04-02 22:08:35,MachineLearning,OK r/machinelearning: after talking with quite a few of you.  We have decided to make a reddit recommendation site/app based of links visited/recommended.,blqnz,atlas245,1257794550.0,https://www.reddit.com/r/MachineLearning/comments/blqnz/ok_rmachinelearning_after_talking_with_quite_a/,10.0,7.0,"Kudos to urish for the idea, so what is my goal for this project?  To finish in one week.  

I am starting a simple startup after this and am looking for some co-founders.   Mainly 1 Web developer and 1 Machine Learning/Web Developer if possible.  This project will serve as a very simple spring board.  Think of this as a trial in which the outcome doesn’t matter and cost nothing and we build something cool.   After we work together we will have a good idea of what people would be good to work with and we can discuss where to go from there.  
If you want to join send me a message and lets get started!
Edit: Ok I created a google group, as its simplest and fastest way to collaborate.

Current web address:
http://groups.google.com/group/machine-learning-apps

Current email address:
machine-learning-apps@googlegroups.com",en
1105837,2010-04-02 22:40:57,MachineLearning,Recent Developments in Deep Learning (Geoff Hinton),blr0g,jb55,1155593815.0,https://www.reddit.com/r/MachineLearning/comments/blr0g/recent_developments_in_deep_learning_geoff_hinton/,38.0,13.0,,en
1105838,2010-04-04 13:40:20,analytics,"Twitter Now Growing at a Staggering 1,382 Percent",bm95x,[deleted],,https://www.reddit.com/r/analytics/comments/bm95x/twitter_now_growing_at_a_staggering_1382_percent/,1.0,0.0,,en
1105839,2010-04-05 09:30:54,datasets,"MusicBrainz database - artists, releases, tracks, labels, incl. relationships",bmj4p,draicone,1212202444.0,https://www.reddit.com/r/datasets/comments/bmj4p/musicbrainz_database_artists_releases_tracks/,16.0,0.0,,en
1105840,2010-04-05 09:41:15,datasets,"The Guardian's data store - large collection of datasets on topics like banks, UK elections, government borrowing, primary school league tables...",bmj8u,draicone,1212202444.0,https://www.reddit.com/r/datasets/comments/bmj8u/the_guardians_data_store_large_collection_of/,4.0,0.0,,en
1105841,2010-04-05 10:24:11,datasets,"Lonely Planet Geo-related API [geocoding, place names, mapping]",bmjph,voltagex,1230861112.0,https://www.reddit.com/r/datasets/comments/bmjph/lonely_planet_georelated_api_geocoding_place/,3.0,0.0,,en
1105842,2010-04-05 10:39:48,datasets,"Real-Time Access to a lot of datasets through one API (over 200 datasets ranging from nextbus,twitter to amazon)",bmjvh,netaddict,1270398000.0,https://www.reddit.com/r/datasets/comments/bmjvh/realtime_access_to_a_lot_of_datasets_through_one/,5.0,1.0,"[YQL](http://developer.yahoo.com/yql/)

PROTIP: Try the console and press show tables on the right sidebar",en
1105843,2010-04-05 10:46:51,datasets,Publicly available and downloadable databases [repost from programming],bmjxh,voltagex,1230861112.0,https://www.reddit.com/r/datasets/comments/bmjxh/publicly_available_and_downloadable_databases/,2.0,0.0,,en
1105844,2010-04-05 11:21:01,datasets,StackOverflow trilogy sites data dump (Creative Commons),bmk7k,watt,1133758800.0,https://www.reddit.com/r/datasets/comments/bmk7k/stackoverflow_trilogy_sites_data_dump_creative/,16.0,0.0,,en
1105845,2010-04-05 11:50:12,datasets,LBNL/ICSI Enterprise Tracing Project (Packet capture samples),bmkh4,qwak,1253668442.0,https://www.reddit.com/r/datasets/comments/bmkh4/lbnlicsi_enterprise_tracing_project_packet/,1.0,1.0,,en
1105846,2010-04-05 12:22:25,datasets,Europeans Value Studies (EVS) Data and Downloads,bmkr5,[deleted],,https://www.reddit.com/r/datasets/comments/bmkr5/europeans_value_studies_evs_data_and_downloads/,2.0,0.0,,en
1105847,2010-04-05 12:24:26,datasets,"Denmark's roads, locations and districts (postal, tax, church, police, conscription, administrative, etc.)",bmkrt,spuur,1133845200.0,https://www.reddit.com/r/datasets/comments/bmkrt/denmarks_roads_locations_and_districts_postal_tax/,1.0,0.0,,en
1105848,2010-04-05 13:53:47,datasets,UC Irvine Machine Learning Repository,bmlnj,[deleted],,https://www.reddit.com/r/datasets/comments/bmlnj/uc_irvine_machine_learning_repository/,1.0,0.0,,en
1105849,2010-04-05 15:02:53,computervision,"Exciting Computer Vision papers from Kristen 
Grauman's UT-Austin Group",bmmco,jeanlucpikachu,1159634949.0,https://www.reddit.com/r/computervision/comments/bmmco/exciting_computer_vision_papers_from_kristen/,1.0,0.0,,en
1105850,2010-04-05 15:22:01,datasets,"Bureau of Labor &amp; Statistics has great data on inflation, prices, (un)employment, pay, benefits, &amp; many more",bmmkd,horseloverfat,1159282835.0,https://www.reddit.com/r/datasets/comments/bmmkd/bureau_of_labor_statistics_has_great_data_on/,2.0,0.0,,en
1105851,2010-04-05 16:11:54,datasets,Ask /r/datasets: good resource for city/town and zip data for states?,bmn4z,actionscripted,1159329915.0,https://www.reddit.com/r/datasets/comments/bmn4z/ask_rdatasets_good_resource_for_citytown_and_zip/,1.0,2.0,Doesn't need to be anything fancy: list of cities/towns and (if possible) ZIP codes. I've found millions of sites offering to sell this sort information but I cannot distinguish the reputable sites from the scam/crap sites.,en
1105852,2010-04-05 17:03:41,datasets,"All Wikimedia downloads: database dumps, static HTML and  DVD's",bmnt3,Harriv,1211720216.0,https://www.reddit.com/r/datasets/comments/bmnt3/all_wikimedia_downloads_database_dumps_static/,12.0,1.0,,en
1105853,2010-04-05 17:12:45,datasets,Amazon's AWS block storage devices can come pre-loaded with various public datasets.,bmnx6,brey,1199718055.0,https://www.reddit.com/r/datasets/comments/bmnx6/amazons_aws_block_storage_devices_can_come/,11.0,1.0,,en
1105854,2010-04-05 18:01:22,datasets,VDS Technologies - Map Data (Shapefiles),bmokj,Pocketcup,1239897570.0,https://www.reddit.com/r/datasets/comments/bmokj/vds_technologies_map_data_shapefiles/,4.0,0.0,,en
1105855,2010-04-05 18:03:08,datasets,UCSC Genome Browser - Want a copy of a human genome? Or a cat? How about a platypus?,bmole,sw17ch,1174750759.0,https://www.reddit.com/r/datasets/comments/bmole/ucsc_genome_browser_want_a_copy_of_a_human_genome/,11.0,0.0,,en
1105856,2010-04-05 18:12:00,datasets,"Comprehensive Knowledge Archive Network, CKAN",bmoqc,gtani7,1203126718.0,https://www.reddit.com/r/datasets/comments/bmoqc/comprehensive_knowledge_archive_network_ckan/,1.0,0.0,,en
1105857,2010-04-05 19:13:07,datasets,Cartographic Boundary Files - U.S. Census Bureau,bmpka,pponso1,1210640904.0,https://www.reddit.com/r/datasets/comments/bmpka/cartographic_boundary_files_us_census_bureau/,3.0,0.0,,en
1105858,2010-04-05 19:36:57,datasets,"US Census TIGER Files - Spatial data for roads, railroads, rivers, and lakes, as well as legal and statistical geographic areas in the US",bmpwn,[deleted],,https://www.reddit.com/r/datasets/comments/bmpwn/us_census_tiger_files_spatial_data_for_roads/,2.0,0.0,,en
1105859,2010-04-05 19:59:13,datasets,Top 1000 Stories on Reddit for the last 4 months,bmq8u,[deleted],,https://www.reddit.com/r/datasets/comments/bmq8u/top_1000_stories_on_reddit_for_the_last_4_months/,1.0,0.0,,en
1105860,2010-04-05 20:24:52,datasets,Lots of open datasets to play with at /r/opendata,bmqmu,mcantelon,1170643409.0,https://www.reddit.com/r/datasets/comments/bmqmu/lots_of_open_datasets_to_play_with_at_ropendata/,6.0,1.0,,en
1105861,2010-04-05 20:34:58,datasets,City of Toronto data set catalogue ,bmqse,wildmXranat,1211844321.0,https://www.reddit.com/r/datasets/comments/bmqse/city_of_toronto_data_set_catalogue/,1.0,0.0,,en
1105862,2010-04-05 21:07:32,datasets,"GeoCommons: Open Geo Data (KML,Shapefile, CSV)",bmr8n,DataMcDatarson,1270490589.0,https://www.reddit.com/r/datasets/comments/bmr8n/geocommons_open_geo_data_kmlshapefile_csv/,1.0,0.0,"If you're looking for GeoData, try checking here first. GeoCommons has also built and opensource geocoder (http://github.com/geocommons/geocoder) at GitHub to help everyone work with geodata. hope this is useful. ",en
1105863,2010-04-05 21:43:52,datasets,Movie Genre Dataset: Derived from Netflix prize dataset and scrape of Netflix genre data,bmrpo,[deleted],,https://www.reddit.com/r/datasets/comments/bmrpo/movie_genre_dataset_derived_from_netflix_prize/,3.0,0.0,,en
1105864,2010-04-05 22:29:12,datasets,Radish:  The Robotics Data Set Repository,bmsar,moyerma,1258550709.0,https://www.reddit.com/r/datasets/comments/bmsar/radish_the_robotics_data_set_repository/,2.0,0.0,,en
1105865,2010-04-06 00:39:32,datasets,Prosper.com: Peer-to-Peer Loans Data,bmtw2,loganfrederick,1252032115.0,https://www.reddit.com/r/datasets/comments/bmtw2/prospercom_peertopeer_loans_data/,4.0,0.0,,en
1105866,2010-04-06 02:42:46,datasets,"Business datasets (and others) in xls, csv, and xml 
formats",bmvaq,xtracare,1270510176.0,https://www.reddit.com/r/datasets/comments/bmvaq/business_datasets_and_others_in_xls_csv_and_xml/,2.0,0.0,,en
1105867,2010-04-06 03:59:31,datasets,Sunlight Labs: Design for America Contest - Visualize or Redesign Government data/forms,bmw31,invader,1178082059.0,https://www.reddit.com/r/datasets/comments/bmw31/sunlight_labs_design_for_america_contest/,8.0,0.0,,en
1105868,2010-04-06 07:00:40,datasets,Labeled Faces in the Wild Face Database ,bmy0q,dudehasgotnomercy,1267719369.0,https://www.reddit.com/r/datasets/comments/bmy0q/labeled_faces_in_the_wild_face_database/,5.0,0.0,,en
1105869,2010-04-06 07:48:43,computervision,Researchers enable a robot to fold towels,bmykk,iofthestorm,1199925415.0,https://www.reddit.com/r/computervision/comments/bmykk/researchers_enable_a_robot_to_fold_towels/,1.0,1.0,,en
1105870,2010-04-06 07:59:45,datasets,"GeoScience Australia data dumps [mapping, GIS ,geocoding, CC-BY]",bmypj,voltagex,1230861112.0,https://www.reddit.com/r/datasets/comments/bmypj/geoscience_australia_data_dumps_mapping_gis/,3.0,4.0,,en
1105871,2010-04-06 08:07:46,datasets,OpenStreetMap planet.osm: 8.2 GiB of city-level street data and other GIS features around the world,bmysy,piranha,1171844246.0,https://www.reddit.com/r/datasets/comments/bmysy/openstreetmap_planetosm_82_gib_of_citylevel/,20.0,0.0,,en
1105872,2010-04-06 16:48:22,datasets,UK General election 2010: the 10 key datasets,bn4o4,agconway,1228006618.0,https://www.reddit.com/r/datasets/comments/bn4o4/uk_general_election_2010_the_10_key_datasets/,7.0,0.0,,en
1105873,2010-04-06 18:55:28,MachineLearning,UK General election 2010: the 10 key datasets to help you decide,bn6lb,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/bn6lb/uk_general_election_2010_the_10_key_datasets_to/,3.0,0.0,,en
1105874,2010-04-06 20:56:57,MachineLearning,Lessons learned developing a practical large scale machine learning system (Google Research),bn8j9,cpdomina,1262293486.0,https://www.reddit.com/r/MachineLearning/comments/bn8j9/lessons_learned_developing_a_practical_large/,28.0,2.0,,en
1105875,2010-04-06 21:35:17,datasets,"US States ranked by Carbon Emission, size, population, and political conservativeness.",bn94g,citizenjr,1261499662.0,https://www.reddit.com/r/datasets/comments/bn94g/us_states_ranked_by_carbon_emission_size/,5.0,2.0,,en
1105876,2010-04-06 22:04:08,datasets,Statistics | Facebook,bn9j3,[deleted],,https://www.reddit.com/r/datasets/comments/bn9j3/statistics_facebook/,0.0,1.0,,en
1105877,2010-04-06 23:14:05,analytics,Google Search Funnels: The greatest assist specialist since Magic Johnson,bnaiw,dredman,1204832530.0,https://www.reddit.com/r/analytics/comments/bnaiw/google_search_funnels_the_greatest_assist/,1.0,1.0,,en
1105878,2010-04-07 03:44:35,datasets,National Vulnerability Database(s),bndoy,[deleted],,https://www.reddit.com/r/datasets/comments/bndoy/national_vulnerability_databases/,3.0,0.0,,en
1105879,2010-04-07 04:37:02,statistics,Help with SPSS coding,bne9r,[deleted],,https://www.reddit.com/r/statistics/comments/bne9r/help_with_spss_coding/,0.0,0.0,,en
1105880,2010-04-07 05:23:00,datasets,"Many Eyes, from IBM, helps make sense of all those data sets.",bnesn,[deleted],,https://www.reddit.com/r/datasets/comments/bnesn/many_eyes_from_ibm_helps_make_sense_of_all_those/,5.0,0.0,,en
1105881,2010-04-07 16:55:01,MachineLearning,TOWEL folding robot : The cutest robot on earth,bnn4u,[deleted],,https://www.reddit.com/r/MachineLearning/comments/bnn4u/towel_folding_robot_the_cutest_robot_on_earth/,28.0,4.0,,en
1105882,2010-04-07 17:27:14,computervision,OpenCV 2.1 has been released,bnnna,gijzelaerr,1227791829.0,https://www.reddit.com/r/computervision/comments/bnnna/opencv_21_has_been_released/,3.0,0.0,,en
1105883,2010-04-07 18:56:16,datasets,"600 datasets in Weka's ARFF format, different topics. Wih performance results of standard algorithms",bnp54,tunedit,1270655151.0,https://www.reddit.com/r/datasets/comments/bnp54/600_datasets_in_wekas_arff_format_different/,11.0,2.0,,en
1105884,2010-04-08 12:35:11,MachineLearning,IEEE ICDM'10 Contest: Call for Proposals,bo2d8,dzeina,1270719219.0,https://www.reddit.com/r/MachineLearning/comments/bo2d8/ieee_icdm10_contest_call_for_proposals/,4.0,1.0,,en
1105885,2010-04-08 13:09:34,analytics,How To Analyze Your Site Stats,bo2rt,blondishnet,1245452182.0,https://www.reddit.com/r/analytics/comments/bo2rt/how_to_analyze_your_site_stats/,1.0,0.0,,en
1105886,2010-04-08 15:47:30,datasets,"Hey /r/datasets. I'd really like to start working with some of the great datasets posted here, but have no clue where to start.",bo4pn,[deleted],,https://www.reddit.com/r/datasets/comments/bo4pn/hey_rdatasets_id_really_like_to_start_working/,13.0,15.0,,en
1105887,2010-04-08 22:10:35,statistics,"New Hiring Formula Values Math Pros -
Bay Area Employers Seek Statistical Experts Over Computer-Science Generalists",boamj,[deleted],,https://www.reddit.com/r/statistics/comments/boamj/new_hiring_formula_values_math_pros_bay_area/,1.0,1.0,,en
1105888,2010-04-08 23:05:18,analytics,"Tracking Multiple Websites &amp; Blogs in Google 
Analytics in a Single Profile",bobeg,lisbongirl,1183028516.0,https://www.reddit.com/r/analytics/comments/bobeg/tracking_multiple_websites_blogs_in_google/,7.0,0.0,,en
1105889,2010-04-08 23:45:03,statistics,Anyone figured out how to use R with multiple cores easily?,bobyr,prionattack,1257312771.0,https://www.reddit.com/r/statistics/comments/bobyr/anyone_figured_out_how_to_use_r_with_multiple/,11.0,11.0,"I'm decent with R, and I have some basic C++ experience, but multicore anything is beyond me at this point (and I have to be able to phase into it quickly...). Does anyone have any idea how to get R to utilize a dual or quad core processor?

I'm willing to learn some tricks and new functions, but I've had a hard time figuring out snow and similar things. I do a lot of simulation and iteration, which should be reasonable to make multi-threaded, but I'm not finding anything...",en
1105890,2010-04-09 09:18:23,datasets,OData SDK for PHP,boidj,voltagex,1230861112.0,https://www.reddit.com/r/datasets/comments/boidj/odata_sdk_for_php/,0.0,1.0,,en
1105891,2010-04-09 15:35:54,datasets,Guardian Datablog - Lots of UK data sets.,bomn4,zabouth1,1252680100.0,https://www.reddit.com/r/datasets/comments/bomn4/guardian_datablog_lots_of_uk_data_sets/,6.0,0.0,,en
1105892,2010-04-09 19:47:05,MachineLearning,An Overview of Data Mining Techniques,boqf9,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/boqf9/an_overview_of_data_mining_techniques/,34.0,0.0,,en
1105893,2010-04-10 06:03:37,statistics,Question: Dice Probability With Repeat Values,boxgf,[deleted],,https://www.reddit.com/r/statistics/comments/boxgf/question_dice_probability_with_repeat_values/,0.0,4.0,"Hello. I generally try to stay away from math and statistics but based on what I'm currently doing and some conversations I've had, I cannot get this concept out of my head.

Basically, I need an answer but I don't know enough to search for it without having to sift through a lot of unrelated material. I'm looking for some help from someone who deals in probability all the time in some capacity.

Anyway, the root problem is two decks of cards shuffled into one deck (termed a shoe). You have to draw 2 cards. What is the probability that both cards will be the same?

So, this is how I personally simplified it to try to figure this out. Let's say you have a 6 sided die with only 3 distinct values. Rolling the die once has equal probability of 1/3 even though the values repeat, it should still reduce down from 2/6 to 1/3. Anyway, let's say I take that die and black out one side with marker. The value is no longer available. Let's pretend I black out side 3. What is the new probability that I will roll a 3 since one's gone?

I can believe that no matter how many decks you shove in a shoe, it's always going to reduce to 1/52 because you draw one and there are only 52 distinct values. 

Any input here would be greatly appreciated. ",en
1105894,2010-04-10 09:45:55,statistics,Considering Graduate School in Statistics - Any advice?,bozf3,snoius,1265493835.0,https://www.reddit.com/r/statistics/comments/bozf3/considering_graduate_school_in_statistics_any/,0.0,1.0,"I'm currently a undergrad seeking a BA in Math with an emphasis in Statistics at the University of Nebraska. I'm about a year from graduation and the thought of grad school has been going around in my head for awhile. I have taken the following courses. 

MATH: Calculus Sequence - B average (3 sequential classes),
Differential Equations - A,
Matrix Theory - C+,
Modern/Higher Algebra - A,
Complex Variables - B,
Stochastic Processes - In progress.

STATS: Intro Statistics - A,
Intro Statistics II (ANOVA, Regression, Non-parametrics, design of experiments) - A,
Statistics for Engineers - B,
Distribution Theory - B,
Statistical Inference - B,
Experimental Design - In progress.

Any advice you can give me? Classes to take, things to do, not to do.....",en
1105895,2010-04-11 05:50:05,statistics,Fun with statistics (Christian Marks),bp9pk,fshahriar,1133758800.0,https://www.reddit.com/r/statistics/comments/bp9pk/fun_with_statistics_christian_marks/,9.0,4.0,,en
1105896,2010-04-12 06:15:07,statistics,use the Arithmetic average or not? I'm no expert,bpldy,[deleted],,https://www.reddit.com/r/statistics/comments/bpldy/use_the_arithmetic_average_or_not_im_no_expert/,0.0,1.0,,en
1105897,2010-04-12 15:03:09,MachineLearning,"Twitter distributed graph database
",bpr8c,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/bpr8c/twitter_distributed_graph_database/,9.0,0.0,,en
1105898,2010-04-12 16:08:01,MachineLearning,On-line competition platform where professors may launch a data mining contest for their students,bps24,tunedit,1270655151.0,https://www.reddit.com/r/MachineLearning/comments/bps24/online_competition_platform_where_professors_may/,5.0,2.0,,en
1105899,2010-04-12 18:33:59,MachineLearning,Part of Speech Tagging with NLTK Part 4 – Brill Tagger vs Classifier Taggers,bpu4j,japerk,1126238400.0,https://www.reddit.com/r/MachineLearning/comments/bpu4j/part_of_speech_tagging_with_nltk_part_4_brill/,7.0,0.0,,en
1105900,2010-04-13 00:18:44,MachineLearning,Three Sexy Skills of Data Geeks ,bpz2n,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/bpz2n/three_sexy_skills_of_data_geeks/,7.0,0.0,,en
1105901,2010-04-13 18:23:04,statistics,Perhaps it is time for peer-reviewed journals to include among their reviewers experts in graphical presentation alongside statisticians and epidemiologists.,bqbxs,BioGeek,1124683200.0,https://www.reddit.com/r/statistics/comments/bqbxs/perhaps_it_is_time_for_peerreviewed_journals_to/,3.0,0.0,,en
1105902,2010-04-13 21:03:21,statistics,Webinterface to R's ggplot2-library released,bqecz,mhermans,1169219262.0,https://www.reddit.com/r/statistics/comments/bqecz/webinterface_to_rs_ggplot2library_released/,15.0,0.0,,en
1105903,2010-04-14 00:05:56,statistics,Ask a Mathematician - Fundraising for Haiti. Colin will help solve your math problems in exchange for charitable donations...,bqgz6,Laser_Dragon,1250975470.0,https://www.reddit.com/r/statistics/comments/bqgz6/ask_a_mathematician_fundraising_for_haiti_colin/,3.0,0.0,,en
1105904,2010-04-15 03:02:18,datasets,Hey /r/datasets.  Any idea where to get a massive list of surnames?,br17c,gonzoisme,1242812605.0,https://www.reddit.com/r/datasets/comments/br17c/hey_rdatasets_any_idea_where_to_get_a_massive/,10.0,6.0,"I want a list of peoples surnames.  Any surname, any nationality, any level of polularity.  I have collected ~100,000, and I'm aiming for &gt;300,000... but the tail end of this is tricky.  Anyone know of any major lists/dbs?",en
1105905,2010-04-15 12:56:32,MachineLearning,Survey for a class at UNCG: Data Warehousing in the Cloud: Trend or Hype?,br7v3,[deleted],,https://www.reddit.com/r/MachineLearning/comments/br7v3/survey_for_a_class_at_uncg_data_warehousing_in/,2.0,0.0,,en
1105906,2010-04-15 19:15:47,statistics,"Jared Lander discusses the science, and statistics, of finding NYC best pizza",brd1w,agconway,1228006618.0,https://www.reddit.com/r/statistics/comments/brd1w/jared_lander_discusses_the_science_and_statistics/,3.0,4.0,,en
1105907,2010-04-15 21:16:10,MachineLearning,Machine Learning Contest: Predict the rate of traffic congestion based on previous observations,brf0u,cpdomina,1262293486.0,https://www.reddit.com/r/MachineLearning/comments/brf0u/machine_learning_contest_predict_the_rate_of/,13.0,0.0,,en
1105908,2010-04-16 02:54:16,statistics,"How Many Words Did Shakespeare Know? 66,534",brjhn,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/brjhn/how_many_words_did_shakespeare_know_66534/,11.0,3.0,,en
1105909,2010-04-16 06:59:57,datasets,Complex Network Resources,brm6s,psykocrime,1205139983.0,https://www.reddit.com/r/datasets/comments/brm6s/complex_network_resources/,5.0,0.0,,en
1105910,2010-04-16 10:43:02,datasets,Dutch zipcodes with geolocations,browz,mensink,1259495126.0,https://www.reddit.com/r/datasets/comments/browz/dutch_zipcodes_with_geolocations/,3.0,0.0,,en
1105911,2010-04-16 15:46:22,analytics,22 Absolutely Free Twitter Web Analytics Tools,brsir,kittuk,1249904483.0,https://www.reddit.com/r/analytics/comments/brsir/22_absolutely_free_twitter_web_analytics_tools/,2.0,1.0,,en
1105912,2010-04-16 17:44:04,statistics,"The Next Big Thing: SAS and SPSS!...wait, what?",brueu,agconway,1228006618.0,https://www.reddit.com/r/statistics/comments/brueu/the_next_big_thing_sas_and_spsswait_what/,18.0,7.0,,en
1105913,2010-04-16 21:57:23,statistics,How to analyze Likert type dependent variables,bryo3,[deleted],,https://www.reddit.com/r/statistics/comments/bryo3/how_to_analyze_likert_type_dependent_variables/,1.0,0.0,,en
1105914,2010-04-16 23:03:08,MachineLearning,Know machinelearning? Really camp? Have I got a competition for you.,brzk5,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/brzk5/know_machinelearning_really_camp_have_i_got_a/,17.0,0.0,,en
1105915,2010-04-17 05:23:18,statistics,Study for PhD Entrance Exams w/ Me in AZ this Summer?,bs3kl,patrick_____________,1256111336.0,https://www.reddit.com/r/statistics/comments/bs3kl/study_for_phd_entrance_exams_w_me_in_az_this/,0.0,2.0,"Hey Internet -

This is my first posting ever in the 2-3 year history of me reading reddit. Awesome.  

It's also my 2nd and 3rd post.  I am attempting to cross post to statistics and learnmath.  Wish me luck.

I'm going to be doing a PhD next year (in stats/machine learning), and I will be spending all summer in AZ (PHX area) relearning math, and trying to get myself up to a level where I could pass the top entrance exams. I don't need to take any for my program, but I want to spend my time learning some new things and getting in math-shape. I would really like to have a study-buddy. Anyone interested?
",en
1105916,2010-04-17 05:50:15,MachineLearning,Books Recommendations.,bs3uu,[deleted],,https://www.reddit.com/r/MachineLearning/comments/bs3uu/books_recommendations/,18.0,7.0,,en
1105917,2010-04-18 01:58:45,statistics,Question,bsg2j,madcapmag,1247346702.0,https://www.reddit.com/r/statistics/comments/bsg2j/question/,1.0,9.0,"I have a question about mean and variance. This is one of my homework questions and i just wanted to make sure I understand what is going on.

According to a survey, 60% of parents with children condone spanking as a regular form of punishment. Consider a random sample of 3 people with children, assume that x, the number in the sample that condone spanking, is a binomial random variable.

n=3 π=0.6

Using the binomial probability distribution-p(x) = C(n,x)(π^x)(1-π)^(n-x) I can come up with this table.
    x | p(x)
    0 |0.064
    1 |0.288
    2 |0.432
    3 |0.216

So, µ = nπ = 1.8

I can interpret that to mean that on average, 1.8 people out 3 people who have children will condone spanking.

σ^2 = nπ(1-π) = 3*0.6*0.4  = 0.72
σ = .85

(µ±σ) = (0.95, 2.65)
(µ±2σ) = (0.1, 3.5)

This means 95% of the population should fall between .1 and 3.5. But, because n=3 and we can not have 0.1 people, we can look at the range of 1 to 3. The actual population is 93.6%. This is close to the 95% that we were expecting to see.

Would this be the correct interpretation?

Thanks!",en
1105918,2010-04-18 10:40:48,statistics,Correlation scatter-plot matrix for ordered-categorical data,bsk8n,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/bsk8n/correlation_scatterplot_matrix_for/,5.0,9.0,,en
1105919,2010-04-19 12:50:01,statistics,Red-R: an dataflow programming interface for R,bszvb,mhermans,1169219262.0,https://www.reddit.com/r/statistics/comments/bszvb/redr_an_dataflow_programming_interface_for_r/,16.0,15.0,,en
1105920,2010-04-20 11:11:52,datasets,Historic Prices and Wages,bthbi,cavedave,1128052800.0,https://www.reddit.com/r/datasets/comments/bthbi/historic_prices_and_wages/,2.0,0.0,,en
1105921,2010-04-21 01:49:09,datasets,Top ~1000 /r/all stories for the last several weeks,bttjw,yellowbkpk,1180809196.0,https://www.reddit.com/r/datasets/comments/bttjw/top_1000_rall_stories_for_the_last_several_weeks/,16.0,3.0,,en
1105922,2010-04-21 12:32:31,statistics,Parallel Multicore Processing with R (on Windows),bu0t1,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/bu0t1/parallel_multicore_processing_with_r_on_windows/,9.0,2.0,,en
1105923,2010-04-21 16:20:23,MachineLearning,Initializing weights for Neural Network,bu3pf,[deleted],,https://www.reddit.com/r/MachineLearning/comments/bu3pf/initializing_weights_for_neural_network/,13.0,24.0,"Hello Redditers! :)
I'm doing some research on initializing weights and looking for algorithms or methods to do so. Can you please share algorithms/methods/frameworks. Thank you :)",en
1105924,2010-04-22 14:50:58,MachineLearning,2010 KDD Cup Competition - Educational Data Mining Challenge,bukra,cpdomina,1262293486.0,https://www.reddit.com/r/MachineLearning/comments/bukra/2010_kdd_cup_competition_educational_data_mining/,16.0,0.0,,en
1105925,2010-04-22 14:55:15,MachineLearning,Pascal2 Machine Learning Bootcamp - Marseille (France),bukte,cpdomina,1262293486.0,https://www.reddit.com/r/MachineLearning/comments/bukte/pascal2_machine_learning_bootcamp_marseille_france/,2.0,0.0,,en
1105926,2010-04-22 22:58:30,MachineLearning,"Data Mining book, with great reviews on Amazon. High correlation in ethnicity between authors and reviewers. ",bus9n,xamdam,1129780800.0,https://www.reddit.com/r/MachineLearning/comments/bus9n/data_mining_book_with_great_reviews_on_amazon/,0.0,0.0,,en
1105927,2010-04-22 23:31:24,datasets,Where can I find lists of...,busqa,spdub,1251245212.0,https://www.reddit.com/r/datasets/comments/busqa/where_can_i_find_lists_of/,4.0,4.0,"Nouns, Adjectives, and Verbs that have already been spread out into their own files.  I am interested in working on a computational linguistics project but my google-fu has failed me.  Any help is appreciated. Thanks.",en
1105928,2010-04-22 23:53:14,statistics,What distribution gives the correct mean and CI for time? ,but1o,[deleted],,https://www.reddit.com/r/statistics/comments/but1o/what_distribution_gives_the_correct_mean_and_ci/,0.0,0.0,,en
1105929,2010-04-23 11:46:49,datasets,Web as Corpus (lots of sorted keywords),bv0uv,voltagex,1230861112.0,https://www.reddit.com/r/datasets/comments/bv0uv/web_as_corpus_lots_of_sorted_keywords/,9.0,1.0,,en
1105930,2010-04-23 13:36:43,datasets,Eurostat - Lots of European Commission datasets ,bv227,zabouth1,1252680100.0,https://www.reddit.com/r/datasets/comments/bv227/eurostat_lots_of_european_commission_datasets/,7.0,1.0,,en
1105931,2010-04-23 18:19:30,artificial,Mind Creators,bv5wp,hyp3rVigi1ant,1259608259.0,https://www.reddit.com/r/artificial/comments/bv5wp/mind_creators/,1.0,0.0,,en
1105932,2010-04-24 01:49:19,statistics,Upgrading R on windows - another strategy (and the R code to do it),bvbty,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/bvbty/upgrading_r_on_windows_another_strategy_and_the_r/,3.0,0.0,,en
1105933,2010-04-24 05:26:28,statistics,"Selection Effects, Causality, and Bad Policy",bvdo9,AndrewKemendo,1190929741.0,https://www.reddit.com/r/statistics/comments/bvdo9/selection_effects_causality_and_bad_policy/,3.0,1.0,,en
1105934,2010-04-24 05:34:25,statistics,Completed SAS I Programming Essentials today--tips on expanding my skills before taking SAS II in July?,bvdqp,nipplicious,1268972133.0,https://www.reddit.com/r/statistics/comments/bvdqp/completed_sas_i_programming_essentials_todaytips/,1.0,4.0,,en
1105935,2010-04-24 14:48:01,statistics,Have you ever been given a dataset and you are expected to figure out what the previous guy did? I have a stupid question about Weighting,bvi5u,[deleted],,https://www.reddit.com/r/statistics/comments/bvi5u/have_you_ever_been_given_a_dataset_and_you_are/,3.0,10.0,"The guy before me had to ""leave"" abruptly (not sure on the whole story) and I am left to figure out one of his projects. I manage to find some of the old datasets that I will probably need for trending. But I have no details on the populations for these datasets and the data is weighted. I managed to figure out that it is was based on two demographic variables among a sea of poorly labeled numbers. But when I start to gathering my own data, I would like to use the same weighting as the previous guy so that there is consistency when trending. So here is the stupid question:

I don't know enough about statistics or weighting to know if there is only one possible way of calculating weights. Is there?  If there isn't, I guess I will have to go back to digging, looking for those population files...

I really don't want to use my own weights then have all the number jump by 20% and me left explaining why I am not wrong.",en
1105936,2010-04-25 00:04:27,statistics,"R question: ""fuzzy"" set intersection",bvn7d,austinap,1205165503.0,https://www.reddit.com/r/statistics/comments/bvn7d/r_question_fuzzy_set_intersection/,5.0,10.0,"I was wondering if anyone here could help me with a problem I have.  I have two sets of floating point values, and I want to find the intersection of the sets *within a tolerance*, e.g., find all items x in X where there is an item y in Y such that |x - y| &lt; a from some a &gt; 0. 

I can do this using sqldf and the query:

    SELECT DISTINCT value FROM X WHERE abs(X.value - Y.value) &lt; a;

This is a pretty ugly solution though, and I was wondering if anyone could guide me to a better way using built-in R commands.  I looked around the set operations in R and couldn't find any obvious way to accomplish this.  Thanks!",en
1105937,2010-04-25 10:57:33,statistics,R - Hundreds of R related articles from 70 bloggers,bvrwh,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/bvrwh/r_hundreds_of_r_related_articles_from_70_bloggers/,16.0,0.0,,en
1105938,2010-04-25 23:04:39,statistics,SAS v R: Getting help | Statistical Analysis Consulting,bvxqc,plf515,1271028320.0,https://www.reddit.com/r/statistics/comments/bvxqc/sas_v_r_getting_help_statistical_analysis/,0.0,0.0,,en
1105939,2010-04-26 01:21:50,statistics,SAS v. R: Ease of learning,bvyzp,[deleted],,https://www.reddit.com/r/statistics/comments/bvyzp/sas_v_r_ease_of_learning/,1.0,0.0,,en
1105940,2010-04-26 22:02:58,analytics,The Four Myths of Web Analytics Every Digital Marketer Must Know,bwd1w,strategy20,1272308469.0,https://www.reddit.com/r/analytics/comments/bwd1w/the_four_myths_of_web_analytics_every_digital/,3.0,0.0,,en
1105941,2010-04-27 02:46:53,statistics,ask r/statistics: is Za a function of a (a = alpha)?,bwgml,simplemathtome,1255306604.0,https://www.reddit.com/r/statistics/comments/bwgml/ask_rstatistics_is_za_a_function_of_a_a_alpha/,2.0,2.0,"stupid question I know, but I have this take home test, and I'm supposed to define n in terms of alpha, beta, and a few mu's (hypothesis testing).  I think I got the answer in terms of the critical z's (there are no numbers, it's purely symbolic.  I'm not looking for the answer, but if you were a professor, and I gave it in terms of Z(alpha) and Z(beta) instead of alpha and beta, would you dock points?",en
1105942,2010-04-27 11:57:21,MachineLearning,The best way to handle exceptions,bwn5o,[deleted],,https://www.reddit.com/r/MachineLearning/comments/bwn5o/the_best_way_to_handle_exceptions/,1.0,0.0,,en
1105943,2010-04-29 12:37:02,statistics,"Please tell me things about various moving 
averages so i can improve my generic Perl module 
for calculation of those",bxpov,Xenofur,1199397800.0,https://www.reddit.com/r/statistics/comments/bxpov/please_tell_me_things_about_various_moving/,0.0,3.0,"Hi statistics reddit,

I not looking for one or two specific data points, but more for generic input to give my mind some external stimuli to crunch on. As such, please bear with me if it gets rambly.

I'm currently working on a perl library to calculate averages on a given list of values. As a bit of explanation how i got to this: On my way to losing some weights I've written a web app to help me track things, which also creates graphs: http://i43.tinypic.com/9gdl3q.png

During the development process i strove to make the averages, specifically the red line in the top chart, fit the data points as close as possible.

I started out with a moving average, soon converted that to an exponentially smoothed moving average, then got annoyed by how that lagged behind the data constantly and first experimented with offsetting its display, and then in the last step made it not only take the data before each point into account, but also the data that came after it, resulting in what you currently see in the top red line in the linked chart.

After looking at that code i began cleaning it up a bit and i realized that I'd pretty much created a library to calculate generic moving averages with all sorts of properties. Currently i have this set of properties:
    * view window, input parameters to define on which data points in the input list
      averages should be calculated
    * look-around window size, how many other data points should be taken into
      account for the calculation of the average of each data point
    * look-around window offset, how much should the range of data points used to
      calculate an average for one data point moved forwards/backwards
    * alternatively: look-around window start/end (essentially window size and offset
      are used to calculate start and end of the window)
    * alternatively: balanced average with the look-around window centered on the
      data point and extended by a size in both directions
    * weighting method (currently only exponential)
    * weighting factor

Now the things I am pondering are these:
    * what's the official name for an input data point in this situation? (i'm german
      and have no formal statistics education)
    * what's the official name for a single data point involved in the calculation of
      an average for another data point?
    * what are the official names for various kinds of moving averages that can be
      generated with this kind of algorithm?
    * what other weighting methods exist?
    * am i missing other possible parameters?

In case anyone is inclined and knowledgable, here's the current perl code: http://pastebin.com/5h0gh9G4 (Not everything is implemented yet, I'm still thinking on some topics.)

Thanks in advance for any comments!
",en
1105944,2010-04-30 12:32:03,MachineLearning,Finance and machine learning master's thesis,by97n,markgraydk,1264534967.0,https://www.reddit.com/r/MachineLearning/comments/by97n/finance_and_machine_learning_masters_thesis/,20.0,16.0,"I'm doing a master's in economics and business administration and this fall I'll have to write my thesis. I'm the process of finding a research topic, and I would like your input. I would like to use machine learning techniques instead of typically used (time series) econometrics. My background in machine learning is lacking, I'm afraid, but I'm sure I can get up to speed during the process (I've only done one course in machine learning, but a few courses in econometrics). 

My idea is something along the line of forecasting and algorithmic trading/portfolio selection. I'm trying to set up a meeting with (hopefully) my future supervisor in a few weeks and thought that you guys might have just the right ideas for me. I'm currently trying to read up on relevant litterature, if you have any good sources for journal articles not to be missed, or even good textbooks, I'm all ears. 

Thanks!",en
1105945,2010-04-30 20:01:17,MachineLearning,"Data mining with WEKA, Part 1: Introduction and regression",byf76,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/byf76/data_mining_with_weka_part_1_introduction_and/,20.0,0.0,,en
1105946,2010-04-30 21:46:07,statistics,Tough question for you guys.  Let's say you have 901 coins that come out to exactly $100. What are the odds?,bygjp,werfnort,1261584199.0,https://www.reddit.com/r/statistics/comments/bygjp/tough_question_for_you_guys_lets_say_you_have_901/,2.0,10.0,"Background - my friend posted this on Facebook earlier.  ""I cashed in a jar of change today. There were 901 coins in the jar. The amount came out to 100.00. Exactly! What are the odds??? WHAT ARE THE ODDS??""

Naturally, that got me thinking.  What ARE the odds?  And then I started thinking about how to figure that out. And then my head hurt.",en
1105947,2010-04-30 22:29:16,statistics,Standard Error for means of unequal sample sizes?,byh4v,meows0r,1222229066.0,https://www.reddit.com/r/statistics/comments/byh4v/standard_error_for_means_of_unequal_sample_sizes/,1.0,18.0,"What N do we normalize the variance of the means by? I'm going with the conservative min(N), but has anyone done the work to show that using the harmonic/geometric/arithmetic mean of sample sizes is appropriate? Thanks /r/stats",en
1105948,2010-04-30 23:10:08,MachineLearning,Ask ML: Good resources for learning applied to time series data?,byhp7,pdovy,1199844333.0,https://www.reddit.com/r/MachineLearning/comments/byhp7/ask_ml_good_resources_for_learning_applied_to/,8.0,4.0,"Hi ML Reddit!  I'm looking for good resources related to machine learning with time series data.  In particular I'm looking for methods that given a set of examples could partition a set of time series data and assign labels to each partition.  Unsupervised methods would also be interesting.

I've been looking around online for something like this, but haven't had a whole lot of luck.  Most of the ML papers related to sequenced data I've found have been based around forecasting rather than classification.

Any pointers in the right direction would be appreciated!",en
1105949,2010-05-01 17:21:25,MachineLearning,Zulu - Automata learning from membership queries (Machine Learning competition),byqfo,cpdomina,1262293486.0,https://www.reddit.com/r/MachineLearning/comments/byqfo/zulu_automata_learning_from_membership_queries/,6.0,1.0,,en
1105950,2010-05-02 19:28:00,statistics,RKWard 0.5.3 is out (as of 30 April),bz33c,[deleted],,https://www.reddit.com/r/statistics/comments/bz33c/rkward_053_is_out_as_of_30_april/,5.0,1.0,,en
1105951,2010-05-02 22:31:27,analytics,Study: Online TV Viewers Willing To Watch More Ads,bz4qr,salvage,1184143405.0,https://www.reddit.com/r/analytics/comments/bz4qr/study_online_tv_viewers_willing_to_watch_more_ads/,4.0,1.0,,en
1105952,2010-05-03 03:43:12,datasets,General Social Survey data,bz799,chromaticburst,1252035508.0,https://www.reddit.com/r/datasets/comments/bz799/general_social_survey_data/,1.0,0.0,"I just read [this submission](http://www.reddit.com/r/science/comments/bz5hj/people_of_class_drink_alcohol/) which utilized the GSS. I've never heard of it before and think it's a pretty amazing corpus. [Official site](http://www.norc.org/GSS+Website/), [Link from article](http://sda.berkeley.edu/cgi-bin/hsda?harcsda+gss08nw)
",en
1105953,2010-05-04 16:31:26,artificial,"Two long video lecture series on AI, from IIT",bzx47,psykocrime,1205139983.0,https://www.reddit.com/r/artificial/comments/bzx47/two_long_video_lecture_series_on_ai_from_iit/,8.0,3.0,"Dug these links up to answer a question posted to [/r/programming](http://programming.reddit.com), but thought some people here might find them of use as well.


[Here](http://www.youtube.com/view_play_list?p=2F21126C45E3213F&amp;playnext=1&amp;playnext_from=PL&amp;v=1BRIjhX4JdU) you can find a 28 part course on AI from [IIT](http://www.iitkgp.ac.in/), taught by [P. Dasgupta](http://www.facweb.iitkgp.ernet.in/~pallab/).

And [a 40 part video series on AI, also from IIT, taught by Sudeshna Sarkar and Anupam Basu.](http://www.youtube.com/watch?v=fV2k2ivttL0&amp;feature=PlayList&amp;p=813E4962D5808FEA&amp;playnext_from=PL&amp;index=1&amp;playnext=2)

",en
1105954,2010-05-04 20:41:52,statistics,ask /r/statistics: Validating assumptions about population from sample studies,c0108,Dangling_Dawis,1267123977.0,https://www.reddit.com/r/statistics/comments/c0108/ask_rstatistics_validating_assumptions_about/,2.0,9.0,"I'm having a bit of a problem working out something, that probably has a simple solution.

Let's say I have 2 studies on a specific subject on my desk.  Both the studies asked the question ""How much are you willing to pay for item a?"".  For both studies, the same sample size was used (1000) chosen randomly from a population (100000, age group 20 - 30 etc.).  The time between the studies was 1 month.    

Study *a* concludes, that on average people are willing to pay 5$ for the item, with the standard deviance being 0.5$.    

Study *b* concludes, that on average people are willing to pay 6$ for the item, standard deviance being 1$.

I am aware that as sample size tends to population size, sample average approaches population average.  So I might be tempted to merge the two studies together.  However, if I am supposed to make assumptions about the population (with relation to the price value on item a) based on those two studies, how do I approach this problem?  

",en
1105955,2010-05-05 11:55:05,statistics,"Videos on Data Analysis with R: Introductory, Intermediate, and Advanced Resources",c09xn,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/c09xn/videos_on_data_analysis_with_r_introductory/,13.0,0.0,,en
1105956,2010-05-05 18:42:10,MachineLearning,The Fundamental Limits of Privacy For Social Networks ,c0emy,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/c0emy/the_fundamental_limits_of_privacy_for_social/,7.0,0.0,,en
1105957,2010-05-05 20:48:38,MachineLearning,"What is R? Intermediate and Advanced 
R. [Videos]",c0g5c,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/c0g5c/what_is_r_intermediate_and_advanced_r_videos/,9.0,0.0,,en
1105958,2010-05-06 20:02:09,statistics,R to MS-Word - example session on how to export R outputs into Word,c0o26,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/c0o26/r_to_msword_example_session_on_how_to_export_r/,8.0,5.0,,en
1105959,2010-05-07 18:31:41,statistics,Seeking assistance Making a repeating statement using Characters instead of Integers to control the looping,c1415,bbbeans,1202404177.0,https://www.reddit.com/r/statistics/comments/c1415/seeking_assistance_making_a_repeating_statement/,0.0,11.0,"Hello, I am working on a project in R and would greatly appreciate some assistance. 

  I have 13 variables that are all sequenced alphabetically. 

  Example: ""q01_a"", ""q01_b"", .....""q01_m"".   (they should all have underscores.....damn formatting)

  I have to make plots for all of them and I'd like to write it in a loop instead of having 13 lines that are all almost exactly the same. 

  Suggestions? 

  Thank you very much!",en
1105960,2010-05-07 19:31:14,statistics,What are the odds of drawing identical opening scrabble racks in back-to-back games?,c14xv,jacquizzl,1272987564.0,https://www.reddit.com/r/statistics/comments/c14xv/what_are_the_odds_of_drawing_identical_opening/,4.0,19.0,"As tournament scrabble player and hopeful stat undergrad this is a question Im really interested in.  The p of drawing any first rack is 1.0 and the p of drawing that specific rack a second time is the p of drawing those 7 tiles.  But when your first rack is unknown it becomes more difficult.  The way I've been trying to approach it is by trying to compute an ""average probability"" of drawing any first rack and then squaring that.  I know my language isn't technical and I might be very wrong in my logic. Ill discuss my approach more later, but I need to move out of my dorm right now.

scrabble tile distribution: http://boardgames.about.com/od/scrabble/a/tile_distribute.htm

Edit: Should've been more clear before but I was in a rush.  I am interested in drawing the same opening rack in consecutive games, so yes the tiles would be replenished after the 7th tile is drawn.  It seems like I was on the right track with my thinking, but as some of you have pointed out it would take an insane amount of time to calculate the actual value. ",en
1105961,2010-05-07 21:39:26,MachineLearning,Stanford video lectures on ML and NLP (in Silverlight! Why do they do that?),c16ay,[deleted],,https://www.reddit.com/r/MachineLearning/comments/c16ay/stanford_video_lectures_on_ml_and_nlp_in/,1.0,0.0,,en
1105962,2010-05-08 05:51:17,statistics,"Repost from /r/Technology: 689 Ted Talks Ranked in 
Order of Social Engagement (spreadsheet/stats/links) ",c1bbp,chewxy,1215510344.0,https://www.reddit.com/r/statistics/comments/c1bbp/repost_from_rtechnology_689_ted_talks_ranked_in/,5.0,1.0,,en
1105963,2010-05-08 06:02:56,datasets,"Repost from /r/Technology: 689 Ted Talks Ranked in 
Order of Social Engagement (spreadsheet/stats/links) ",c1bep,chewxy,1215510344.0,https://www.reddit.com/r/datasets/comments/c1bep/repost_from_rtechnology_689_ted_talks_ranked_in/,2.0,0.0,,en
1105964,2010-05-09 18:43:15,statistics,"Hans Rosling soon to present ""The Joy of Stats"" on the BBC.",c1s06,[deleted],,https://www.reddit.com/r/statistics/comments/c1s06/hans_rosling_soon_to_present_the_joy_of_stats_on/,17.0,2.0,,en
1105965,2010-05-09 23:33:33,statistics,Any recommendations for a book on mathematical statistics? ,c1umy,gabjuasfijwee,1260073537.0,https://www.reddit.com/r/statistics/comments/c1umy/any_recommendations_for_a_book_on_mathematical/,0.0,2.0,"I'm in a mathematical statistics series and we use the book Mathematical Statistics with Applications (Seventh Edition) by John E. Freund and it is abominable. It is quite vague in its explanations of just about everything. If anyone knows of any thorough books with good explanations, it would be very much appreciated.",en
1105966,2010-05-10 17:35:23,MachineLearning,Text Classification for Sentiment Analysis with a Naive Bayes Classifier,c252i,japerk,1126238400.0,https://www.reddit.com/r/MachineLearning/comments/c252i/text_classification_for_sentiment_analysis_with_a/,16.0,2.0,,en
1105967,2010-05-10 18:24:34,statistics,"""Teens surprisingly OK with texting during sex,"" notes Slate's news aggregator. This seemed like a good lead for a piece I've wanted to write for a while: just how much we should trust claims that 10% of people agree to claim X. In many cases, we probably should put little faith in those numbers.",c25qg,FieldofScience,1237390073.0,https://www.reddit.com/r/statistics/comments/c25qg/teens_surprisingly_ok_with_texting_during_sex/,15.0,0.0,,en
1105968,2010-05-10 23:09:55,analytics,"Leading online video platform Brightcove: Viewers are increasingly watching longer videos, length-watched grew by an average of 9.46% per month over the past six months... [TubeMogul Report]",c29pk,salvage,1184143405.0,https://www.reddit.com/r/analytics/comments/c29pk/leading_online_video_platform_brightcove_viewers/,3.0,0.0,,en
1105969,2010-05-11 00:18:22,statistics,R Command Abbreviations Explained,c2aiy,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/c2aiy/r_command_abbreviations_explained/,6.0,3.0,,en
1105970,2010-05-11 11:21:46,MachineLearning,A new series about companies that rely on machine learning techniques,c2hpj,databuff,1272414378.0,https://www.reddit.com/r/MachineLearning/comments/c2hpj/a_new_series_about_companies_that_rely_on_machine/,25.0,4.0,,en
1105971,2010-05-11 17:29:21,MachineLearning,"Data mining with WEKA, Part 2: Classification and clustering",c2m14,[deleted],,https://www.reddit.com/r/MachineLearning/comments/c2m14/data_mining_with_weka_part_2_classification_and/,22.0,3.0,,en
1105972,2010-05-11 20:34:28,MachineLearning,"Rank Aggregation in ""The Right Tool""",c2owr,DRMacIver,1183802175.0,https://www.reddit.com/r/MachineLearning/comments/c2owr/rank_aggregation_in_the_right_tool/,10.0,1.0,,en
1105973,2010-05-12 12:30:38,MachineLearning,"Do you have some interesting links, RSS feeds you can share? (ML, AI or anything interesting)",c30r1,resiros,1216493757.0,https://www.reddit.com/r/MachineLearning/comments/c30r1/do_you_have_some_interesting_links_rss_feeds_you/,1.0,1.0,The two I read are kurzweilai.net RSS feed and [Sciencedaily](http://www.sciencedaily.com/news/computers_math/) feed,en
1105974,2010-05-12 21:24:51,MachineLearning,"""Item Comparison"" vs ""Ranking"" in Collaborative Filtering",c380s,gabgoh,1252945089.0,https://www.reddit.com/r/MachineLearning/comments/c380s/item_comparison_vs_ranking_in_collaborative/,10.0,3.0,"Could anyone point me to a link which does some kind of semi-theoretical or theoretical analysis of ""Item Comparison"" filtering (e.g where the user is asked to compare two items, such as pictures and choose which one he/she prefers e.g [friendlyrank](http://friendlyrank.com/okstupid.html)) and ""Ranking"" (obvious). What is the former even called? Is one better than the other, and why? Discussion is welcome",en
1105975,2010-05-16 02:18:41,statistics,Can someone here explain the Neymann-Pearson Lemma to me?,c4lqq,gabjuasfijwee,1260073537.0,https://www.reddit.com/r/statistics/comments/c4lqq/can_someone_here_explain_the_neymannpearson_lemma/,1.0,3.0,"I understand that you set L0/L1 &lt; k, but then I don't really understand what that means and how you apply it.",en
1105976,2010-05-16 14:00:06,MachineLearning,"Data Dumps of ""The Right Tool"" now available - 60,000 answers and counting",c4qck,[deleted],,https://www.reddit.com/r/MachineLearning/comments/c4qck/data_dumps_of_the_right_tool_now_available_60000/,1.0,0.0,,en
1105977,2010-05-16 21:53:01,datasets,"Nightly data dumps from ""The Right Tool"" - survey on how programming languages are best described",c4u46,DRMacIver,1183802175.0,https://www.reddit.com/r/datasets/comments/c4u46/nightly_data_dumps_from_the_right_tool_survey_on/,8.0,1.0,,en
1105978,2010-05-17 12:09:59,MachineLearning,"AskML: How to determine the most confident 
instances?",c51zx,personanongrata,1200898594.0,https://www.reddit.com/r/MachineLearning/comments/c51zx/askml_how_to_determine_the_most_confident/,0.0,2.0,"As most of you know, confidence value for a model can be calculated with several techniques. For instance if you have an ensemble of decision tree classifiers, then you can find the confidence value by majority voting. 

Now, I have an ensemble of decision trees, i want to do sampling on the classified instances for active learning. But majority voting (in active learning this is called QBC) doesn't yield good results (actually it worsens). I want to try a different technique, probably smt like density based approaches will do OK. 

My main question is: 

Is it reasonable to think that the most confident classified example are the ones that are closest to the centroid of the cluster constituted by the classifier? If yes what is the mathematical reasoning behind that? If you have other suggestions please don't hesitate to share your ideas here.",en
1105979,2010-05-17 13:08:27,MachineLearning,Machinery sales now goes online,c52l7,devjohn1,1273554712.0,https://www.reddit.com/r/MachineLearning/comments/c52l7/machinery_sales_now_goes_online/,1.0,0.0,,en
1105980,2010-05-18 02:27:08,MachineLearning,Ask ML: I'm building a new desktop.  Recommendations?,c5cja,Troybatroy,1238986431.0,https://www.reddit.com/r/MachineLearning/comments/c5cja/ask_ml_im_building_a_new_desktop_recommendations/,0.0,3.0,"$1000 soft limit

My current desktop has fewer cores than my laptop, 1 and 2 respectively, so I'm overdue for an upgrade.

I don't need a new monitor or any of that stuff.  Other than doing my ML research I would be doing a little bit of home audio recording and tv watching on it... and of course redditting :)

I've got 3 Convential PCI cards that I need from my current setup (ATI Radeon HD 3650, M-Audio Delta 66, and a Haupaugge TV card).  Oh and I'd be dual booting Linux and Windows 7, i.e. already have a license.

Motherboard, chip set recommendations please.

4 core or 6?

I'm contemplating a smallish SSD to complement my current HDD.

I use R, so 16G RAM seems appropriate.",en
1105981,2010-05-18 04:09:03,datasets,[request] Dataset/anonymised email headers for SPF statistics,c5db9,voltagex,1230861112.0,https://www.reddit.com/r/datasets/comments/c5db9/request_datasetanonymised_email_headers_for_spf/,3.0,1.0,"This is pretty specialised, but is there anywhere I can get a whole lot of mail headers (anonymised, I'd hope) to gather some stats on SPF usage?

Microsoft did something like this with Hotmail in 2004, but I'm looking for something more recent.",en
1105982,2010-05-18 06:29:12,statistics,"Index of R tutorials (written by ""Curving Normality"")",c5em2,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/c5em2/index_of_r_tutorials_written_by_curving_normality/,12.0,0.0,,en
1105983,2010-05-18 15:37:30,statistics,"In case you didn't know, RKWard (gui front end for R) for windows is easy to install and works well",c5is5,jerryF,1212860987.0,https://www.reddit.com/r/statistics/comments/c5is5/in_case_you_didnt_know_rkward_gui_front_end_for_r/,7.0,0.0,,en
1105984,2010-05-19 07:37:20,datasets,Afrobarometer- Survey Data from African Countries,c5slk,loganfrederick,1252032115.0,https://www.reddit.com/r/datasets/comments/c5slk/afrobarometer_survey_data_from_african_countries/,1.0,0.0,,en
1105985,2010-05-19 17:31:55,statistics,The largest list of blogs who write about R,c5xhi,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/c5xhi/the_largest_list_of_blogs_who_write_about_r/,4.0,0.0,,en
1105986,2010-05-20 01:06:16,MachineLearning,Google Prediction API - Google Code,c62ym,[deleted],,https://www.reddit.com/r/MachineLearning/comments/c62ym/google_prediction_api_google_code/,45.0,6.0,,en
1105987,2010-05-20 10:33:15,statistics,Web-Friendly (Interactive) Visualizations in R (Prototype!),c67zo,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/c67zo/webfriendly_interactive_visualizations_in_r/,4.0,0.0,,en
1105988,2010-05-20 15:04:01,analytics,Blog Analytics: Basics of Analyzing Blog Traffic,c69ta,chokobaka,1248781670.0,https://www.reddit.com/r/analytics/comments/c69ta/blog_analytics_basics_of_analyzing_blog_traffic/,0.0,0.0,,en
1105989,2010-05-22 02:25:26,statistics,Help me please : is there an indicator of the amount of dispersion between two set of datas?,c6v1t,[deleted],,https://www.reddit.com/r/statistics/comments/c6v1t/help_me_please_is_there_an_indicator_of_the/,1.0,4.0,"Please don't laugh!
Well, i have two sets of datas. I just want to know if the dispersion that i have in the set one is the same as the dispersion that i have in set two. Why? Because for the set one actually i have a lot of different sets (it evolves with time), so i want to know the time where i'll have the same amount of dispersion in the first set that i have in the second. Well i hope you understand what i'm saying.",en
1105990,2010-05-22 14:22:09,statistics,Do blind people need R ?,c6zel,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/c6zel/do_blind_people_need_r/,0.0,2.0,"Hello dear reddit-ers. 

I just wrote a post trying to help make R more accessible to blind people:
http://www.r-statistics.com/2010/05/helping-the-blind-use-r-by-exporting-r-console-to-word/

But I am left wondering - is there a ""market"" for this?
How many blind people do you think would even have a need to know R ?",en
1105991,2010-05-22 18:44:30,statistics,A Tour through the Visualization Zoo,c7139,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/c7139/a_tour_through_the_visualization_zoo/,10.0,1.0,,en
1105992,2010-05-24 05:06:33,statistics,"Anyone want to check this result: 3.66x10^-6 probability of grandfather, father and son having same birthday? ",c7eac,AndrewKemendo,1190929741.0,https://www.reddit.com/r/statistics/comments/c7eac/anyone_want_to_check_this_result_366x106/,0.0,6.0,,en
1105993,2010-05-24 16:48:59,MachineLearning,Evaluating Stopwords and Bigram Collocations for Text Classification,c7jgg,japerk,1126238400.0,https://www.reddit.com/r/MachineLearning/comments/c7jgg/evaluating_stopwords_and_bigram_collocations_for/,9.0,0.0,,en
1105994,2010-05-25 12:13:00,statistics,Bad statistics: Map of where toursists flock,c7ul6,fxj,1169393085.0,https://www.reddit.com/r/statistics/comments/c7ul6/bad_statistics_map_of_where_toursists_flock/,7.0,3.0,,en
1105995,2010-05-26 02:28:45,datasets,Stanford Large Network Data Sets,c83hn,psykocrime,1205139983.0,https://www.reddit.com/r/datasets/comments/c83hn/stanford_large_network_data_sets/,1.0,0.0,,en
1105996,2010-05-26 03:46:22,statistics,Magic numbers: A meeting of mathemagical tricksters,c845k,AndrewKemendo,1190929741.0,https://www.reddit.com/r/statistics/comments/c845k/magic_numbers_a_meeting_of_mathemagical_tricksters/,2.0,0.0,,en
1105997,2010-05-26 07:09:28,artificial,Does anyone wish they were working on strong AI?  What's holding you back?,c85xi,aicurious,1274846105.0,https://www.reddit.com/r/artificial/comments/c85xi/does_anyone_wish_they_were_working_on_strong_ai/,27.0,33.0,"Strong AI has a romantic appeal and looks possible (or at least not obviously impossible), but no one seems to be working on it besides a few cranks.  Admittedly, there's pretty much no way to get paid to study strong AI, but has anyone here considered working on it on their own time?  What stopped you?  Too busy?  Lack of community?  Poor prospects of getting anywhere?",en
1105998,2010-05-27 04:26:54,analytics,Online video ads are more effective than TV because viewers are forced to watch them,c8i71,salvage,1184143405.0,https://www.reddit.com/r/analytics/comments/c8i71/online_video_ads_are_more_effective_than_tv/,2.0,1.0,,en
1105999,2010-05-27 13:39:54,artificial,Kurzweil’s Next Book: Creating An Artificial Mind,c8mjp,great-pumpkin,1212943450.0,https://www.reddit.com/r/artificial/comments/c8mjp/kurzweils_next_book_creating_an_artificial_mind/,1.0,0.0,,en
1106000,2010-05-27 14:41:44,statistics,How to run R on a supercomputer,c8n0l,fxj,1169393085.0,https://www.reddit.com/r/statistics/comments/c8n0l/how_to_run_r_on_a_supercomputer/,11.0,0.0,,en
1106001,2010-05-28 00:17:14,computervision,Image Segementation of E.coli ,c8tjg,Ells86,1263845670.0,https://www.reddit.com/r/computervision/comments/c8tjg/image_segementation_of_ecoli/,8.0,8.0,"I am a biologist and am in need of a bit of technical help.  I'm currently working on a project which requires us to automatically identify individual E. coli cells with high fidelity.  There are generally 200 images per 'movie' taken at 1 min. intervals.  We would like to analyze upwards of 100 movies.

I've tried the following to no avail (largely due to my inexperience):

* MatLab bwmorphs and the likes

* DIPimage w/in MatLab

* CellTracer (even if I were to tweak it just right, it still takes ~1hr per image)

The most success I've had has come via ImageJ (seen here: http://imgur.com/cW6wT.jpg )

1.) Enhance Contract  (0.4%)
2.) Unsharp Mask (40 - 0.9)
3.) Find Edges


The end goal of the analysis is to do one of the following:

* length of individuals cells

* area of individual cells

Here is the worst case scenario in terms of # of cells/low contrast: http://i.imgur.com/nJRL2.jpg


Any help would be immensely appreciated!",en
1106002,2010-05-28 10:03:54,statistics,Linking C in R on a Windows machine,c8ymn,[deleted],,https://www.reddit.com/r/statistics/comments/c8ymn/linking_c_in_r_on_a_windows_machine/,0.0,1.0,"Does anyone have experience in assembling an R package for Windows? I have .c files which need to be dyn loaded into my R functions. I've been trying to follow online examples using mingw with little success. The files link successfully on a Unix machine. 

If anyone might know a possible source of my problems, I would appreciate feedback.",en
1106003,2010-05-28 13:52:27,MachineLearning,"Autonomous quadrocopter flies through windows, straight into our hearts (video) -- Engadget",c907j,mikef22,1242682409.0,https://www.reddit.com/r/MachineLearning/comments/c907j/autonomous_quadrocopter_flies_through_windows/,48.0,6.0,,en
1106004,2010-05-29 22:46:15,statistics,"For computing beyond statistics, there is /r/numerical",c9g4r,dearsomething,1210808677.0,https://www.reddit.com/r/statistics/comments/c9g4r/for_computing_beyond_statistics_there_is/,7.0,3.0,,en
1106005,2010-05-29 22:51:42,MachineLearning,Come share some tips with us in /r/numerical (and help me grow it!).,c9g6i,dearsomething,1210808677.0,https://www.reddit.com/r/MachineLearning/comments/c9g6i/come_share_some_tips_with_us_in_rnumerical_and/,11.0,0.0,,en
1106006,2010-05-30 02:37:10,statistics,Question: comparing two estimates of random variable.,c9hpd,scott,1133067600.0,https://www.reddit.com/r/statistics/comments/c9hpd/question_comparing_two_estimates_of_random/,0.0,0.0,"Here's the situation: I'm estimating the mean of a random variable through sampling. We know the stdev of the RV pretty well, after all that converges pretty quickly. I have a published result that says that this RV has mean 55 with a 95% confidence interval of +- 20, the appropriate amount for the number of trials they did. 

I'm doing my own tests, and I want to know if I am simulating this random variable correctly. I'm still running the simulation, but at the current time, I predict a mean of 44.75 with about the same confidence interval. This means the standard deviation of each of our predictions is about 10.

How can I quantitatively measure the likelihood that I am measuring the same random variable? I can plot the two normal distributions, one from each of our estimates, and say ""hey, looks kinda close"". I can say ""eh it's only off by ten that's not too bad"", but what's a good rigorous method to compare the two? Or perhaps a rule of thumb?",en
1106007,2010-05-30 16:30:41,statistics,How Not To Run An A/B Test,c9m5n,[deleted],,https://www.reddit.com/r/statistics/comments/c9m5n/how_not_to_run_an_ab_test/,8.0,5.0,,en
1106008,2010-05-30 21:51:42,statistics,"RKWard on Mac: we are desparately in need of Mac users willing to help us by simply following our initial instructions, and telling us what goes wrong",c9og3,jerryF,1212860987.0,https://www.reddit.com/r/statistics/comments/c9og3/rkward_on_mac_we_are_desparately_in_need_of_mac/,6.0,0.0,,en
1106009,2010-05-30 22:20:23,statistics,Ask stats: Is there any sensible way to define sample entropy from a continuous distribution?,c9onq,moultano,1160099427.0,https://www.reddit.com/r/statistics/comments/c9onq/ask_stats_is_there_any_sensible_way_to_define/,0.0,10.0,"Hi all. I'm in the process of learning about various information-theory concepts, and I've come across something that Google isn't providing me any good answers to.

Suppose you have samples from a completely unknown real-valued distribution. If all of the samples have unique values, is there any sensible way to define the sample entropy of your observations? When using something like variance to measure spread, it's really useful to condition the observations on some other variable and then observe a decrease in the ""spread."" The definition of continuous entropy doesn't seem to permit this sort of thing though.

Any suggestions for further reading or just some keywords I could Google?",en
1106010,2010-05-31 13:52:14,statistics,Statisticians outperform Eurovision betting markets,c9v18,databuff,1272414378.0,https://www.reddit.com/r/statistics/comments/c9v18/statisticians_outperform_eurovision_betting/,2.0,2.0,,en
1106011,2010-05-31 17:10:59,statistics,Oracle Adds Support for Open-source R,c9wme,utcursch,1175517803.0,https://www.reddit.com/r/statistics/comments/c9wme/oracle_adds_support_for_opensource_r/,14.0,0.0,,en
1106012,2010-05-31 17:36:05,MachineLearning,Optimizing Discounts with Data Mining,c9wur,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/c9wur/optimizing_discounts_with_data_mining/,0.0,1.0,,en
1106013,2010-06-01 16:53:04,MachineLearning,A Shining City on a Spreadsheet,ca8oa,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/ca8oa/a_shining_city_on_a_spreadsheet/,0.0,0.0,,en
1106014,2010-06-01 16:53:10,datasets,A Shining City on a Spreadsheet,ca8od,cavedave,1128052800.0,https://www.reddit.com/r/datasets/comments/ca8od/a_shining_city_on_a_spreadsheet/,4.0,3.0,,en
1106015,2010-06-01 20:19:40,statistics,"National (US) Poll * June 1, 2010 * More U.S. Voters Want Arizona Style Immigration Law - Quinnipiac University  [Let's double check their stats...]",caawe,dearsomething,1210808677.0,https://www.reddit.com/r/statistics/comments/caawe/national_us_poll_june_1_2010_more_us_voters_want/,2.0,0.0,,en
1106016,2010-06-02 23:29:42,MachineLearning,"BBC statistics show, More or Less, is back",care7,databuff,1272414378.0,https://www.reddit.com/r/MachineLearning/comments/care7/bbc_statistics_show_more_or_less_is_back/,7.0,1.0,,en
1106017,2010-06-03 07:00:06,datasets,"Penn World Table, International Comparisons of Production, Income and Prices",cavuh,[deleted],,https://www.reddit.com/r/datasets/comments/cavuh/penn_world_table_international_comparisons_of/,2.0,0.0,,en
1106018,2010-06-03 13:59:41,MachineLearning,A soccer World Cup forecasting competition for statisticians ,caz6k,databuff,1272414378.0,https://www.reddit.com/r/MachineLearning/comments/caz6k/a_soccer_world_cup_forecasting_competition_for/,8.0,3.0,,en
1106019,2010-06-03 20:14:03,statistics,Does anyone have the 7th edition of John E. Freund's Mathematical Statistics that they would be willing to reference for me? I'm out of town and in deep shit...,cb3bq,prionattack,1257312771.0,https://www.reddit.com/r/statistics/comments/cb3bq/does_anyone_have_the_7th_edition_of_john_e/,0.0,6.0,,en
1106020,2010-06-04 00:44:19,MachineLearning,What is data science? - O'Reilly Radar,cb6ky,databuff,1272414378.0,https://www.reddit.com/r/MachineLearning/comments/cb6ky/what_is_data_science_oreilly_radar/,22.0,0.0,,en
1106021,2010-06-04 11:53:14,MachineLearning,Lecture 1 | Machine Learning (Stanford),cbch6,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/cbch6/lecture_1_machine_learning_stanford/,30.0,3.0,,en
1106022,2010-06-05 06:33:09,datasets,Request: In-store CCTV footage,cbnko,nicholaides,1157680987.0,https://www.reddit.com/r/datasets/comments/cbnko/request_instore_cctv_footage/,7.0,0.0,I'm looking for some samples of in-store CCTV footage. Anything more than 5 minutes would be ideal.,en
1106023,2010-06-05 21:01:15,statistics,A question on VWAP and TWAP,cbt9e,chaddubeebuzz,1275760301.0,https://www.reddit.com/r/statistics/comments/cbt9e/a_question_on_vwap_and_twap/,0.0,0.0,"Hi All,

This question is trading related but I think is actually grounded in basic stats. I've no formal training in Stats. Please help me.  

I have a basic question on VWAP and TWAP. I'm having some trouble getting to understand this intuitively.

1. I think VWAP helps traders benchmark their performance for a stock over a period of time, say the whole day. The VWAP price basically is the price at which the total day's trades would occur for the given notional value, if we were to have ONLY ONE TRADE.

So for 3 trades in a day (suppose)

2000@500
5000@501
1000@500

the vwap is 500.625. Fair enough.

Then why do traders use historical vwap? What is its use if VWAP should actually be a realtime calculation, so that one can execute trades in proportion to the total market volume, so as to get the vwap price?

2. TWAP as I understand is the average price of the execution over a specified time period. So why is this called 'Time Weighted' ? Isn't it misleading, because its finally just an average price, and has nothing to do with time as a weight?
  
Tks!



",en
1106024,2010-06-06 18:09:42,datasets,The 20 Newsgroups data set,cc0mm,antitheftdevice,1197662102.0,https://www.reddit.com/r/datasets/comments/cc0mm/the_20_newsgroups_data_set/,7.0,0.0,,en
1106025,2010-06-07 18:48:41,analytics,Prevent Good Metrics From Going Bad,ccd0k,theproductguy,1196970320.0,https://www.reddit.com/r/analytics/comments/ccd0k/prevent_good_metrics_from_going_bad/,2.0,1.0,,en
1106026,2010-06-07 22:08:28,MachineLearning,Major chains have been dropping like flies. Is it really easier to succumb to Chapter 11 than figure out how to do data mining?,ccfm9,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/ccfm9/major_chains_have_been_dropping_like_flies_is_it/,0.0,0.0,,en
1106027,2010-06-08 00:11:34,MachineLearning,Help needed: Spectral clustering (Malik &amp; Shi algorithm),cch62,dtelad11,1202659836.0,https://www.reddit.com/r/MachineLearning/comments/cch62/help_needed_spectral_clustering_malik_shi/,6.0,7.0,"(Melia and Shi algorithm, not Malik...)

Well, apparently a very good talk is not enough to reach a suitable implementation...

I have the following spectral clustering toy example:
http://www.sendspace.com/file/m545dl
(Apologies for the lousy website, it's the only one I could find)

My understanding is that after calculating P, the separation between the points should be found using the second eigenvector. However, the separation is in the forth eigenvector... Any redditors out there happen to understand what I'm doing wrong?",en
1106028,2010-06-08 05:09:30,MachineLearning,Ask ML: Complete newb here - help me understand the barriers of basic brain architecture simulation.,cck42,[deleted],,https://www.reddit.com/r/MachineLearning/comments/cck42/ask_ml_complete_newb_here_help_me_understand_the/,13.0,50.0,"So I've read through On Intelligence by Jeff Hawkins twice now (http://www.onintelligence.org/) and I have to say his proposed architecture on how the brain works seems somewhat trivial to accomplish in a language like C++.

First, my background. I did well in a couple of c++/java courses in college, but that was a few years back. It would take a bit of a refresher to do simple file i/o.

From what I can gather in Mr. Hawkins' book, however, it seems that it would be relatively simple to create an ""object"" that can pass on information to a higher level object. After a period of time, it can begin to recognize patterns (hmm, how it would recognize patterns would not be so trivial I imagine), and pass on those patterns to the higher level object. Then based on prediction of when those patterns occur, it could fire its response to the higher object before the pattern even finishes. Of course there would have to be a feedback channel, sending information ""down"" back to the beginning.

So I guess my question is, as someone with next to no AI experience, what are the barriers here? The basic structure of the brain (going only a few levels deep) seems somewhat ""easy"" to accomplish.

My apologies for my deep-seeded ignorance. :) Just a curious mind, I guess.

EDIT: Wow, thank you everyone for the awesome responses. I love a good discussion!",en
1106029,2010-06-08 17:41:26,statistics,R on iPhone/iPad ?,ccqta,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/ccqta/r_on_iphoneipad/,0.0,0.0,,en
1106030,2010-06-09 06:04:22,statistics,"Why Do I Have To Take Statistics?
 | Stats With Cats Blog",cczb0,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/cczb0/why_do_i_have_to_take_statistics_stats_with_cats/,0.0,0.0,,en
1106031,2010-06-09 20:07:25,datasets,If San Francisco Crime were Elevation,cd7hh,cavedave,1128052800.0,https://www.reddit.com/r/datasets/comments/cd7hh/if_san_francisco_crime_were_elevation/,10.0,0.0,,en
1106032,2010-06-09 22:02:49,statistics,how to estimate the min of a sample from a wacky distribution,cd8wy,jaberg,1276109107.0,https://www.reddit.com/r/statistics/comments/cd8wy/how_to_estimate_the_min_of_a_sample_from_a_wacky/,0.0,0.0,"I have run an experiment, that involves IID trials that come out with one score each.  What I really care about is the min over those scores.  Those scores have uncertainty, and I have good estimators for that.  But is there an unbiased estimator for the min?  What is my uncertainty in the minimum?  How should I think about this?",en
1106033,2010-06-09 23:26:56,analytics,Piwik: Real-time Open Source Web Analytics Software,cd9zi,shopvavavoom,1162074494.0,https://www.reddit.com/r/analytics/comments/cd9zi/piwik_realtime_open_source_web_analytics_software/,0.0,2.0,,en
1106034,2010-06-11 02:25:48,MachineLearning,AI That Picks Stocks Better Than the Pros,cdqnw,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/cdqnw/ai_that_picks_stocks_better_than_the_pros/,30.0,16.0,,en
1106035,2010-06-11 20:01:12,MachineLearning,Reproducible automated experiments in machine learning,ce06f,datt,1276003335.0,https://www.reddit.com/r/MachineLearning/comments/ce06f/reproducible_automated_experiments_in_machine/,11.0,0.0,,en
1106036,2010-06-14 00:13:26,computervision,Easy Introduction to Principal Components Analysis and Applications in Image Processing [PDF] ,celr1,0x2a,1240379297.0,https://www.reddit.com/r/computervision/comments/celr1/easy_introduction_to_principal_components/,6.0,2.0,,en
1106037,2010-06-14 05:10:10,statistics,Reality Statistics,cenv7,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/cenv7/reality_statistics/,0.0,2.0,,en
1106038,2010-06-14 07:27:32,statistics,Can someone explain to me central moments in English?,cep0i,grimtrigger,1250664652.0,https://www.reddit.com/r/statistics/comments/cep0i/can_someone_explain_to_me_central_moments_in/,3.0,3.0,"I know the formula, and that's really all I know.",en
1106039,2010-06-14 10:28:45,MachineLearning,Air Compressor For Home Enhancement,ceqbz,nationalweld,1273463008.0,https://www.reddit.com/r/MachineLearning/comments/ceqbz/air_compressor_for_home_enhancement/,0.0,0.0,,en
1106040,2010-06-14 17:07:36,statistics,Power Analysis with G*Power,cetjb,dearsomething,1210808677.0,https://www.reddit.com/r/statistics/comments/cetjb/power_analysis_with_gpower/,3.0,0.0,,en
1106041,2010-06-15 15:32:42,statistics,Regression Methods in Bio Statistics,cf6pm,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/cf6pm/regression_methods_in_bio_statistics/,8.0,0.0,,en
1106042,2010-06-15 18:16:24,analytics,"Facebook Hits 20 Million Video Uploads, 2 Billion Video Views Per Month",cf8pj,salvage,1184143405.0,https://www.reddit.com/r/analytics/comments/cf8pj/facebook_hits_20_million_video_uploads_2_billion/,3.0,0.0,,en
1106043,2010-06-15 20:29:32,MachineLearning,Hey MLreddit - is anyone attending ICML (International Conference on Machine Learning) next week? We can have a mini-meetup...,cfahv,urish,1221689900.0,https://www.reddit.com/r/MachineLearning/comments/cfahv/hey_mlreddit_is_anyone_attending_icml/,12.0,3.0,Plus World Cup viewings for the intersection of /r/MachineLearning and /r/soccer (or am I the only one???),en
1106044,2010-06-15 23:31:51,MachineLearning,Statistical Analysis and Visualization of the Drug War in Mexico,cfcrg,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/cfcrg/statistical_analysis_and_visualization_of_the/,5.0,4.0,,en
1106045,2010-06-16 00:32:15,MachineLearning,"AskML:  What's the standard terminology for this, or how would you describe it?  Integrating over time to combine the benefits of multiple separate low-probability features (details inside)",cfdgn,[deleted],,https://www.reddit.com/r/MachineLearning/comments/cfdgn/askml_whats_the_standard_terminology_for_this_or/,8.0,7.0,,en
1106046,2010-06-16 20:54:45,statistics,Social Scientist looking for good Stats Books,cfp45,Raatcharch,1266595457.0,https://www.reddit.com/r/statistics/comments/cfp45/social_scientist_looking_for_good_stats_books/,7.0,18.0,"I'm beginning a master's degree (on a PhD track) in Anthropology this fall, and I'd like to become a proficient statistician to aid in my research. I haven't taken a math class since 2002, and my previous statistical experience comes from a ""stats for dummies"" style anthropology class in my undergrad.

In my free time, I am hacking away at a basic Statistics textbook. Where should I look next? Does anyone have any suggestions for good books through which to teach myself advanced statistics?

EDIT: Thanks to everyone who has responded, your help is greatly appreciated!",en
1106047,2010-06-16 22:10:03,statistics,"can someone explain the kappa test to me, and to spice it up, do it with gangsta language?",cfq3y,NotAtTheTable,1268291295.0,https://www.reddit.com/r/statistics/comments/cfq3y/can_someone_explain_the_kappa_test_to_me_and_to/,0.0,8.0,"My boss asked me to figure out this test on SPSS and the internet, but I'm still not sure what I'm looking at, so I turn to reddit.",en
1106048,2010-06-16 23:23:45,MachineLearning,Eliminate Low Information Features to Improve Classifier Performance,cfqyo,japerk,1126238400.0,https://www.reddit.com/r/MachineLearning/comments/cfqyo/eliminate_low_information_features_to_improve/,8.0,0.0,,en
1106049,2010-06-17 00:12:17,MachineLearning,"IBM's new ""question-answering machine"" Watson",cfrhd,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/cfrhd/ibms_new_questionanswering_machine_watson/,26.0,3.0,,en
1106050,2010-06-17 08:36:59,statistics,How to calculate a running correlation coefficient?,cfw81,iampims,1229078494.0,https://www.reddit.com/r/statistics/comments/cfw81/how_to_calculate_a_running_correlation_coefficient/,6.0,8.0,"Hi reddit,

Would anyone of you be kind enough to explain me how to calculate a running correlation coefficient solely based on the running mean and running standard deviation for each dataset?

So far I haven't found a better solution than [this one](http://www.reddit.com/r/programming/comments/84o3g/is_there_a_way_to_compute_a_running_correlation/)

Given the expression of the correlation coefficient as:
&gt; the mean of the products of the standard scores

found [here](http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient#Definition) it seems that I have no other choice than storing the z-score for each element of the dataset and then recomputing r — or following the method described above.

Can someone enlighten me? Thx!",en
1106051,2010-06-17 11:51:01,MachineLearning,IEEE ICDM Contest: Road Traffic Prediction for Intelligent GPS Navigation. Want to form Team reddit?,cfxry,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/cfxry/ieee_icdm_contest_road_traffic_prediction_for/,3.0,1.0,,en
1106052,2010-06-17 16:23:10,MachineLearning,Blogs and tweets could predict the future,cg01j,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/cg01j/blogs_and_tweets_could_predict_the_future/,3.0,1.0,,en
1106053,2010-06-17 18:43:39,statistics,Support a new Q&amp;A website for Data-Analysis (based on the StackOverFlow engine): please join &amp; vote up!,cg1se,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/cg1se/support_a_new_qa_website_for_dataanalysis_based/,7.0,4.0,,en
1106054,2010-06-17 21:38:22,artificial,"IBM's AI-ish 'Watson' will compete, live, on Jeopardy this fall",cg43a,great-pumpkin,1212943450.0,https://www.reddit.com/r/artificial/comments/cg43a/ibms_aiish_watson_will_compete_live_on_jeopardy/,2.0,0.0,,en
1106055,2010-06-18 00:42:53,statistics,Another new grad student (in a pretty applied education program) here wanting to learn some statistics...advice on books for my situation?,cg68m,[deleted],,https://www.reddit.com/r/statistics/comments/cg68m/another_new_grad_student_in_a_pretty_applied/,3.0,9.0,,en
1106056,2010-06-18 10:41:27,analytics,The 1000 most-visited sites on the web [Google/DoubleClick],cgbjg,salvage,1184143405.0,https://www.reddit.com/r/analytics/comments/cgbjg/the_1000_mostvisited_sites_on_the_web/,5.0,0.0,,en
1106057,2010-06-18 15:30:40,MachineLearning,Discriminative vs generative models for classification,cgdmd,snippyhollow,1235325933.0,https://www.reddit.com/r/MachineLearning/comments/cgdmd/discriminative_vs_generative_models_for/,0.0,1.0,,en
1106058,2010-06-18 22:58:31,statistics,"Is there a ""normalizing"" transformation for standard deviation, when it equals zero given a small sample size?",cgj0g,quaternion,1252017693.0,https://www.reddit.com/r/statistics/comments/cgj0g/is_there_a_normalizing_transformation_for/,6.0,16.0,"I have 10 runs of a classifier for each of 18 subjects (I'm trying to classify some fMRI results... but that's immaterial).  I want to show that the classifier is more successful across all subjects in some Condition A than Condition B (it is!), but the classifier's performance is pretty unpredictable in Condition B, which is adding variance to the accuracy scores that is obscuring the effect showing ConditionA&gt;ConditionB.  I've tried all manner of transformations (arcsin-root, logit) but my p-value is just not quite low enough (p=.11) for the closed-minded folks that will be reading my paper.

Therefore, instead of analyzing just the accuracy of the classifier across Conditions A and B, I thought I'd want to analyze the effect size of the classifier's accuracy across Conditions A and B.  This can be done by using the t-statistics from a t-test of classifier accuracy in each condition against chance levels of classification for each condition.

However, in Condition A, the classifier kicks so much ass that it's at 100% accuracy 100% of the time for each subject; when I attempt to calculate the t-statistic I then need to divide by zero.  

The question: Is there a principled transformation I can use to make standard deviation nonzero, so that I can get t-statistics?  Just using an arbitrarily small value changes the t-statistics by an arbitrary amount, so I'm not comfortable with that.  I know there are similar transformations used in cases where an observer has a perfect hit rate (1 - 1/(2N)) for calculating dprime.  Is there something similar for st. dev?  Or should I give up on this and just build a three-level hierarchical model?

TL;DR: my standard deviation for one condition is truly, actually zero, but I need to calculate its t-statistic to compare it with another condition that is far less reliable.  Help!

EDIT: what about using a ""pooled variance"" estimate (i.e., the average variance across all conditions) for A?  This is at least a nonzero value, and it seems that such ""pooled variance"" techniques are sometimes used (although new to me).",en
1106059,2010-06-18 23:28:45,statistics,philosophy on counterfactuals - a crucial component in understanding statistics,cgjdd,[deleted],,https://www.reddit.com/r/statistics/comments/cgjdd/philosophy_on_counterfactuals_a_crucial_component/,0.0,0.0,,en
1106060,2010-06-19 06:13:24,analytics,"Google Notches Highest Search Share Ever at 66.4%, Shows ComScore",cgmpc,salvage,1184143405.0,https://www.reddit.com/r/analytics/comments/cgmpc/google_notches_highest_search_share_ever_at_664/,2.0,0.0,,en
1106061,2010-06-20 19:59:31,MachineLearning,"Practical Machine Learning course at Berkeley. Mike Jordan ""presents a broad overview of modern statistical machine learning from a practitioner's perspective""",ch1rp,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/ch1rp/practical_machine_learning_course_at_berkeley/,25.0,3.0,,en
1106062,2010-06-20 20:36:32,MachineLearning,Example of working with World Bank Data,ch23x,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/ch23x/example_of_working_with_world_bank_data/,12.0,0.0,,en
1106063,2010-06-20 20:38:38,MachineLearning,Algorithms for Massive Data [Princeton Class],ch24p,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/ch24p/algorithms_for_massive_data_princeton_class/,11.0,1.0,,en
1106064,2010-06-21 07:58:16,statistics,What should I use to compare 4 groups (IV) on a dichotomous outcome variable (DV). ,ch7i7,gluestickyum,1276817891.0,https://www.reddit.com/r/statistics/comments/ch7i7/what_should_i_use_to_compare_4_groups_iv_on_a/,1.0,7.0,,en
1106065,2010-06-21 16:52:44,analytics,How We Used Data to Win the Presidential Election [VIDEO],chbs7,salvage,1184143405.0,https://www.reddit.com/r/analytics/comments/chbs7/how_we_used_data_to_win_the_presidential_election/,4.0,0.0,,en
1106066,2010-06-22 02:27:55,MachineLearning,Microsoft Learning to Rank Datasets,chimx,cpdomina,1262293486.0,https://www.reddit.com/r/MachineLearning/comments/chimx/microsoft_learning_to_rank_datasets/,13.0,3.0,,en
1106067,2010-06-22 11:11:07,MachineLearning,2010 INFORMS Data Mining Contest 2010,chnfe,databuff,1272414378.0,https://www.reddit.com/r/MachineLearning/comments/chnfe/2010_informs_data_mining_contest_2010/,8.0,5.0,,en
1106068,2010-06-22 17:09:42,statistics,Is there a way to apply the ROUT outlier detection method to univariate data (and possibly do it in R)?,chqlk,[deleted],,https://www.reddit.com/r/statistics/comments/chqlk/is_there_a_way_to_apply_the_rout_outlier/,1.0,0.0,,en
1106069,2010-06-22 23:22:14,MachineLearning,Help start a Stack Exchange site for Machine Learning,chvg9,data_alchemist,1273725475.0,https://www.reddit.com/r/MachineLearning/comments/chvg9/help_start_a_stack_exchange_site_for_machine/,10.0,3.0,,en
1106070,2010-06-23 05:27:42,MachineLearning,Learning about Machine Learning,chz67,data_alchemist,1273725475.0,https://www.reddit.com/r/MachineLearning/comments/chz67/learning_about_machine_learning/,18.0,1.0,,en
1106071,2010-06-23 06:24:35,MachineLearning,Algorithms for Reinforcement Learning,chznj,data_alchemist,1273725475.0,https://www.reddit.com/r/MachineLearning/comments/chznj/algorithms_for_reinforcement_learning/,13.0,0.0,,en
1106072,2010-06-23 07:29:29,MachineLearning,Workshop on Algorithms for Modern Massive Data Sets (MMDS),ci08i,data_alchemist,1273725475.0,https://www.reddit.com/r/MachineLearning/comments/ci08i/workshop_on_algorithms_for_modern_massive_data/,0.0,0.0,,en
1106073,2010-06-24 19:00:42,MachineLearning,AskML: What's the difference between Independent Component Analysis and Principal Component Analysis?,cile6,chillage,1251759220.0,https://www.reddit.com/r/MachineLearning/comments/cile6/askml_whats_the_difference_between_independent/,16.0,10.0,"Both seem to take data down into lower dimensional space. Is there any practical difference between the two? Is one a more general case of the other?

[ICA](http://en.wikipedia.org/wiki/Independent_component_analysis)

[PCA](http://en.wikipedia.org/wiki/Principal_component_analysis)",en
1106074,2010-06-24 19:34:24,MachineLearning,"What are the top PhD programs in Statistical Learning (world wide, not necessarily US-centric)?",cilus,dmz,1137750403.0,https://www.reddit.com/r/MachineLearning/comments/cilus/what_are_the_top_phd_programs_in_statistical/,11.0,17.0,,en
1106075,2010-06-25 02:15:06,MachineLearning,Machine Learning Summer School 2009 (VideoLectures),ciqo1,cpdomina,1262293486.0,https://www.reddit.com/r/MachineLearning/comments/ciqo1/machine_learning_summer_school_2009_videolectures/,12.0,0.0,,en
1106076,2010-06-25 08:03:23,analytics,"Skype has more registered users than Facebook, over 160m more.",citq8,salvage,1184143405.0,https://www.reddit.com/r/analytics/comments/citq8/skype_has_more_registered_users_than_facebook/,13.0,2.0,,en
1106077,2010-06-25 13:07:07,analytics,Making Analytics Data Actionable Using Analytics API Tools,ciw2n,lilianMM,1260870834.0,https://www.reddit.com/r/analytics/comments/ciw2n/making_analytics_data_actionable_using_analytics/,1.0,0.0,,en
1106078,2010-06-25 20:01:55,MachineLearning,What are some challenging Bioinformatics problems that can be solved using a Machine Learning approach (and that haven't been beaten to death by researchers)?,cj05s,dmz,1137750403.0,https://www.reddit.com/r/MachineLearning/comments/cj05s/what_are_some_challenging_bioinformatics_problems/,12.0,7.0,,en
1106079,2010-06-25 23:27:01,statistics,Binary growth model convergence issues,cj2li,[deleted],,https://www.reddit.com/r/statistics/comments/cj2li/binary_growth_model_convergence_issues/,0.0,1.0,,en
1106080,2010-06-26 02:43:53,artificial,Kickstarter project to train Artificial intelligence to pass the Turing test,cj4du,[deleted],,https://www.reddit.com/r/artificial/comments/cj4du/kickstarter_project_to_train_artificial/,1.0,0.0,,en
1106081,2010-06-27 18:48:46,MachineLearning,"Minds, Machines, and Intelligence: A Conversation with Eric Horvitz (Vhennel9 video)",cjjk3,xamdam,1129780800.0,https://www.reddit.com/r/MachineLearning/comments/cjjk3/minds_machines_and_intelligence_a_conversation/,3.0,0.0,,en
1106082,2010-06-29 13:48:38,statistics,R-bloggers just passed the 2000 RSS readers mark!,ck3iq,[deleted],,https://www.reddit.com/r/statistics/comments/ck3iq/rbloggers_just_passed_the_2000_rss_readers_mark/,0.0,0.0,,en
1106083,2010-06-29 18:02:25,MachineLearning,PyBrain video with installation and use case tutorials -- shown at ICML MLOSS workshop.,ck61o,apollobp,1277823546.0,https://www.reddit.com/r/MachineLearning/comments/ck61o/pybrain_video_with_installation_and_use_case/,9.0,0.0,,en
1106084,2010-06-30 06:53:29,statistics,"Two tennis players play a tie breaker. As usual, to end the tie breaker one must be ahead by two games. Each serves well, and when serving has a 90% chance of winning. What is the probability the tie breaker goes 138 or more games?",ckepa,simpletype,1261668093.0,https://www.reddit.com/r/statistics/comments/ckepa/two_tennis_players_play_a_tie_breaker_as_usual_to/,8.0,16.0,,en
1106085,2010-07-01 02:36:52,statistics,How many dovetail shuffles suffice?,ckqcl,[deleted],,https://www.reddit.com/r/statistics/comments/ckqcl/how_many_dovetail_shuffles_suffice/,2.0,1.0,,en
1106086,2010-07-01 12:58:54,MachineLearning,Human Computation,ckvk9,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/ckvk9/human_computation/,10.0,3.0,,en
1106087,2010-07-01 14:20:40,MachineLearning,Stackoverflow for Machine Learning and Natural Language Processing,ckw5k,kzn,1162071679.0,https://www.reddit.com/r/MachineLearning/comments/ckw5k/stackoverflow_for_machine_learning_and_natural/,52.0,6.0,,en
1106088,2010-07-02 22:38:20,statistics,"This is probably a stupid question and it's just a coincidence, but is the 66% like it thing that seems to dominate most reddit submisions  something generic about the human population?",clf4a,[deleted],,https://www.reddit.com/r/statistics/comments/clf4a/this_is_probably_a_stupid_question_and_its_just_a/,1.0,2.0,,en
1106089,2010-07-02 22:55:49,statistics,Visualization of regression coefficients (in R),clf9z,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/clf9z/visualization_of_regression_coefficients_in_r/,8.0,11.0,,en
1106090,2010-07-03 01:05:34,MachineLearning,"Thoughts about MetaOptimize (the new ""Stackoverflow for Machine Learning"" website)",clgim,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/clgim/thoughts_about_metaoptimize_the_new_stackoverflow/,9.0,2.0,,en
1106091,2010-07-03 20:48:56,statistics,It’s All Greek | Stats With Cats Blog,clocx,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/clocx/its_all_greek_stats_with_cats_blog/,5.0,3.0,,en
1106092,2010-07-04 00:06:12,MachineLearning,"Nils Nilsson,  John McCarthy , Edward Feigenbaum  discuss Artificial Intelligence and Expert Systems. [Broadcast Date 1984]",clpt9,[deleted],,https://www.reddit.com/r/MachineLearning/comments/clpt9/nils_nilsson_john_mccarthy_edward_feigenbaum/,1.0,0.0,,en
1106093,2010-07-04 00:11:35,MachineLearning,"Glimpse of A.I. with John McCarthy , Nils Nilsson, Edward Feigenbaum [VID broadcast 1984]",clpud,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/clpud/glimpse_of_ai_with_john_mccarthy_nils_nilsson/,5.0,0.0,,en
1106094,2010-07-04 20:05:07,MachineLearning,Reading List for Stanford's AI's Qualifying Examination[1996],clwlo,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/clwlo/reading_list_for_stanfords_ais_qualifying/,6.0,1.0,,en
1106095,2010-07-04 21:29:22,MachineLearning,"How to get a PhD in AI. ""The Researcher's Bible""",clxai,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/clxai/how_to_get_a_phd_in_ai_the_researchers_bible/,33.0,3.0,,en
1106096,2010-07-04 22:42:35,MachineLearning,Proposed Novelty Based Learning Method ,clxv5,locster,1222291024.0,https://www.reddit.com/r/MachineLearning/comments/clxv5/proposed_novelty_based_learning_method/,0.0,0.0,,en
1106097,2010-07-04 23:32:44,MachineLearning,Detecting Spammers on Mechanical Turk,cly7y,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/cly7y/detecting_spammers_on_mechanical_turk/,2.0,0.0,,en
1106098,2010-07-05 10:10:41,MachineLearning,Classics of Statistics,cm29y,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/cm29y/classics_of_statistics/,11.0,1.0,,en
1106099,2010-07-05 12:29:24,MachineLearning,LCD Pinball Machine Can Play Any Table You Want,cm33k,Misyb,1272441990.0,https://www.reddit.com/r/MachineLearning/comments/cm33k/lcd_pinball_machine_can_play_any_table_you_want/,0.0,0.0,,en
1106100,2010-07-05 22:41:02,artificial,Musing on near-future AI applications,cm8bl,last_useful_man,1241273287.0,https://www.reddit.com/r/artificial/comments/cm8bl/musing_on_nearfuture_ai_applications/,1.0,0.0,,en
1106101,2010-07-06 00:57:09,MachineLearning,"Some of the near-term implications of AI: Autonomous Automobiles, Risk Analysis  and others.",cm9k0,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/cm9k0/some_of_the_nearterm_implications_of_ai/,19.0,1.0,,en
1106102,2010-07-06 06:31:36,MachineLearning,Judea Pearl's contributions to Causality [VID],cmc8g,[deleted],,https://www.reddit.com/r/MachineLearning/comments/cmc8g/judea_pearls_contributions_to_causality_vid/,1.0,0.0,,en
1106103,2010-07-06 10:50:38,statistics,For R users: New versions for ggplot2 (0.8.8) and plyr (1.0) were released today,cme8t,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/cme8t/for_r_users_new_versions_for_ggplot2_088_and_plyr/,7.0,0.0,,en
1106104,2010-07-06 18:14:15,analytics,Google Analytics Individual Qualification Exam Test Notes,cmi37,jensksorensen,1278420308.0,https://www.reddit.com/r/analytics/comments/cmi37/google_analytics_individual_qualification_exam/,1.0,1.0,,en
1106105,2010-07-06 22:55:42,statistics,How to Display Data Badly by Wainer (1984) [PDF],cmlsw,kanak,1155042601.0,https://www.reddit.com/r/statistics/comments/cmlsw/how_to_display_data_badly_by_wainer_1984_pdf/,14.0,1.0,,en
1106106,2010-07-07 00:36:23,statistics,How do I calculate the Standard Error of the Intercept in a Linear Regression with One X Variable?,cmmzb,discontinuity,1204666886.0,https://www.reddit.com/r/statistics/comments/cmmzb/how_do_i_calculate_the_standard_error_of_the/,3.0,6.0,,en
1106107,2010-07-07 19:48:17,statistics,Gallery of Data Visualization: The Best and Worst of Statistical Graphics ,cmxsk,kanak,1155042601.0,https://www.reddit.com/r/statistics/comments/cmxsk/gallery_of_data_visualization_the_best_and_worst/,7.0,1.0,,en
1106108,2010-07-08 00:42:41,statistics,"I'd like to better my grasp on statistics and data analysis.  Short of going back to school, where would I best start?",cn1iz,spencewah,1153945398.0,https://www.reddit.com/r/statistics/comments/cn1iz/id_like_to_better_my_grasp_on_statistics_and_data/,4.0,2.0,"I took an intro to stat course in college and that's pretty much the entirety of my exposure.  I'd love to hear about any online curriculum (wikiversity, academic earth, etc) that would get me started!",en
1106109,2010-07-08 16:46:08,MachineLearning,"Creating Friendly AI, The Analysis and Design of Benevolent Goal Architectures",cna8t,vorce,1263216793.0,https://www.reddit.com/r/MachineLearning/comments/cna8t/creating_friendly_ai_the_analysis_and_design_of/,9.0,5.0,,en
1106110,2010-07-08 18:07:47,statistics,Error calculation help,cnb7d,sewerbeing,1199483409.0,https://www.reddit.com/r/statistics/comments/cnb7d/error_calculation_help/,0.0,4.0,"I work in a physics lab and we have to report errors on the samples that we measure. I have a hopefully simple question on how to calculate this error.

We have two different measurements counts and current. We measure the counts from a sample for two seconds then its current and repeat. These two numbers are divided (counts/current) to provide a ratio. For each time we measure the sample we will have a few dozen of these ratios. My thought was if the count rate is high enough (more than a couple per second) we should be able to calculate a 95% confidence interval for the mean and report that as the error. Is this the appropriate way to do it? Also if there are few counts (less than 1 every 2 seconds) you can run into the case where several measurements have counts 0 and counts 1 leading to a split distribution rather than a nice normal one. How do we deal with that case? 

Thank you in advance and if more clarification is needed I would be happy to give it.",en
1106111,2010-07-09 17:50:22,statistics,Three Parameter Weibull Distribution?,cnpw9,rabidhummingbird,1255125460.0,https://www.reddit.com/r/statistics/comments/cnpw9/three_parameter_weibull_distribution/,5.0,6.0,"I'm working with life-time monitoring of electronic materials over the summer. Typically we've been using the 2 parameter Weibull distribution. It was suggested that I look into the three parameter Weibull distribution, but I have no idea what it is. Can anyone explain the difference between the 2 and the 3 parameter set, how to set-up/utilize the 3 parameter distribution, or possibly suggest articles or text books that would be helpful? Thanks!

Summary: I need to fit a set of ranked data to a three parameter Weibull distribution. Please help!",en
1106112,2010-07-10 23:56:08,MachineLearning,What are the most influential ideas we should tell other computer scientists about?,co5al,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/co5al/what_are_the_most_influential_ideas_we_should/,18.0,0.0,,en
1106113,2010-07-11 07:19:52,datasets,Twitter Dataset Scraping Tool - 140Kit,co8ar,tibbon,1202676354.0,https://www.reddit.com/r/datasets/comments/co8ar/twitter_dataset_scraping_tool_140kit/,0.0,0.0,,en
1106114,2010-07-11 12:11:08,MachineLearning,"Facebook, because of the interconnectedness of the data, didn't find any clustering scheme that worked in practice",co9v0,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/co9v0/facebook_because_of_the_interconnectedness_of_the/,16.0,5.0,,en
1106115,2010-07-11 14:44:57,analytics,Twitter Receives 800 Million Search Queries A Day,coair,salvage,1184143405.0,https://www.reddit.com/r/analytics/comments/coair/twitter_receives_800_million_search_queries_a_day/,6.0,0.0,,en
1106116,2010-07-11 17:20:58,statistics,"30 Samples. Standard, Suggestion, or Superstition ? | Stats With Cats Blog",cobbs,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/cobbs/30_samples_standard_suggestion_or_superstition/,6.0,0.0,,en
1106117,2010-07-11 22:55:29,MachineLearning,Why Google TV Could Destroy Nielsen’s Data [Data Mining Research],codzu,[deleted],,https://www.reddit.com/r/MachineLearning/comments/codzu/why_google_tv_could_destroy_nielsens_data_data/,1.0,0.0,,en
1106118,2010-07-13 01:49:18,MachineLearning,Starting off with OpenCV,cot8y,mathnathan,1277574833.0,https://www.reddit.com/r/MachineLearning/comments/cot8y/starting_off_with_opencv/,12.0,3.0,,en
1106119,2010-07-13 04:52:53,statistics,favorite essays &amp; papers discussing Bayesian vs frequentist methods?,cov2d,uiberto,1263701702.0,https://www.reddit.com/r/statistics/comments/cov2d/favorite_essays_papers_discussing_bayesian_vs/,5.0,5.0,"I have a background in computer science and biology. I've been implementing some MCMC-driven phylogenetic models and learning the statistics along the way. 

I'm getting a fair understanding of the points of contention between Bayesian and frequentist approaches. Unfortunately, most the colleagues I interact with have one strong viewpoint or another.

Do any of you have favorite essays or papers that disinterestedly explore the Bayesian vs frequentist feud?",en
1106120,2010-07-13 11:54:41,statistics,National census in 2011 could be last of its kind,coyyu,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/coyyu/national_census_in_2011_could_be_last_of_its_kind/,1.0,3.0,,en
1106121,2010-07-13 17:24:36,statistics,"Hierarchical Visualizations in R and the Javascript 
InfoVis Toolkit",cp1y3,[deleted],,https://www.reddit.com/r/statistics/comments/cp1y3/hierarchical_visualizations_in_r_and_the/,8.0,0.0,,en
1106122,2010-07-13 18:55:55,MachineLearning,Machine Learning Job Positions,cp375,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/cp375/machine_learning_job_positions/,22.0,7.0,,en
1106123,2010-07-13 19:28:01,statistics,Can someone explain gaussian process regression in GLM terms?,cp3n6,quaternion,1252017693.0,https://www.reddit.com/r/statistics/comments/cp3n6/can_someone_explain_gaussian_process_regression/,5.0,14.0,"I am not a Bayesian, and priors often confuse me.  I am looking at a paper that uses a gaussian process regression for classifying neuroimaging data, and wikipedia's explanations are entirely opaque to me.  Can someone provide the 10,000 foot view?",en
1106124,2010-07-13 22:55:00,statistics,True or false: every probability distribution over real values has a median.,cp6fw,[deleted],,https://www.reddit.com/r/statistics/comments/cp6fw/true_or_false_every_probability_distribution_over/,1.0,0.0,,en
1106125,2010-07-13 23:42:40,MachineLearning,Second Post - Basic Image Manipulation,cp74a,mathnathan,1277574833.0,https://www.reddit.com/r/MachineLearning/comments/cp74a/second_post_basic_image_manipulation/,2.0,0.0,,en
1106126,2010-07-14 11:37:29,MachineLearning,I'm looking to start a MSc that specialises in Machine Learning. Any advice?,cpdvl,[deleted],,https://www.reddit.com/r/MachineLearning/comments/cpdvl/im_looking_to_start_a_msc_that_specialises_in/,9.0,2.0,,en
1106127,2010-07-14 15:37:56,computervision,Basic Image Manipulation - OpenCV,cpfnx,mathnathan,1277574833.0,https://www.reddit.com/r/computervision/comments/cpfnx/basic_image_manipulation_opencv/,10.0,4.0,,en
1106128,2010-07-14 18:13:54,MachineLearning,Investment Firms Look to 'Artificial Intelligence' in Trade Decisions,cphms,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/cphms/investment_firms_look_to_artificial_intelligence/,19.0,15.0,,en
1106129,2010-07-14 21:05:49,statistics,Statistical Modeling: The Two Cultures [PDF],cpjy0,kanak,1155042601.0,https://www.reddit.com/r/statistics/comments/cpjy0/statistical_modeling_the_two_cultures_pdf/,13.0,2.0,,en
1106130,2010-07-15 13:22:53,statistics,Forecasting population counts,cptrj,[deleted],,https://www.reddit.com/r/statistics/comments/cptrj/forecasting_population_counts/,0.0,1.0,,en
1106131,2010-07-15 21:46:43,MachineLearning,Lecture Notes from MIT's Machine Learning Course,cpzqn,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/cpzqn/lecture_notes_from_mits_machine_learning_course/,27.0,0.0,,en
1106132,2010-07-16 00:12:53,statistics,I'm thinking about getting a M.S. is Stats. Where should I apply? Any other suggestions?,cq1po,dsabeti,1276901350.0,https://www.reddit.com/r/statistics/comments/cq1po/im_thinking_about_getting_a_ms_is_stats_where/,1.0,0.0,,en
1106133,2010-07-16 05:15:52,analytics,"Compete, Quantcast, Alexa and Nielson get reddit wrong, dhuh. These companies gauge totals by with tiny user groups from tool-bars and other browser extensions. ",cq4rx,salvage,1184143405.0,https://www.reddit.com/r/analytics/comments/cq4rx/compete_quantcast_alexa_and_nielson_get_reddit/,0.0,0.0,,en
1106134,2010-07-16 07:30:41,statistics,How to report the p-value or significance level for ANOVA of transformed data,cq5xg,ExperienceInfinity,1274152612.0,https://www.reddit.com/r/statistics/comments/cq5xg/how_to_report_the_pvalue_or_significance_level/,1.0,3.0,"I have a set of data, representing the task time for a three different groups or ""treatments"". In its raw form, the data is heavily skewed (because the task time has a lower limit); thus the normality assumption of common statistical analyses is violated. Applying a log-transform removes this skewness and makes the analysis ""valid"".

My question: when reporting the significance level (e.g. ""...p&lt;.05...""), do I need to un-transform this value in any way? I.e., is the p value of the transformed measurements the same as the ""p value"" of the raw data?

Thanks!",en
1106135,2010-07-16 10:08:44,statistics,Join the closed BETA of a new Statistical Analysis Q&amp;A website!,cq7ab,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/cq7ab/join_the_closed_beta_of_a_new_statistical/,6.0,3.0,,en
1106136,2010-07-17 03:40:06,MachineLearning,How the kernel trick allows SVMs to separate non-linearly separable classes.,cqhvh,aeacides,1151252716.0,https://www.reddit.com/r/MachineLearning/comments/cqhvh/how_the_kernel_trick_allows_svms_to_separate/,27.0,5.0,,en
1106137,2010-07-17 20:37:19,MachineLearning,Introduction to Statistical Machine Learning [VID],cqol3,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/cqol3/introduction_to_statistical_machine_learning_vid/,17.0,0.0,,en
1106138,2010-07-18 03:49:53,statistics,Purrfect Resolution | Stats With Cats Blog,cqrqm,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/cqrqm/purrfect_resolution_stats_with_cats_blog/,0.0,0.0,,en
1106139,2010-07-18 23:29:16,MachineLearning,State and future of AI- Marvin Minsky Video Lecture.,cqz7u,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/cqz7u/state_and_future_of_ai_marvin_minsky_video_lecture/,16.0,0.0,,en
1106140,2010-07-19 05:15:50,MachineLearning,Reading Data from Webcam - OpenCV,cr1xi,mathnathan,1277574833.0,https://www.reddit.com/r/MachineLearning/comments/cr1xi/reading_data_from_webcam_opencv/,10.0,1.0,,en
1106141,2010-07-19 06:13:20,computervision,Reading Data from a Webcam - OpenCV,cr2cx,mathnathan,1277574833.0,https://www.reddit.com/r/computervision/comments/cr2cx/reading_data_from_a_webcam_opencv/,1.0,0.0,,en
1106142,2010-07-19 19:01:11,statistics,"Berkeley undergraduate course: Concepts in Computing with Data. Covers R, SQL, XML, CGI. Contains Lecture Notes and Assignments. ",cr94l,kanak,1155042601.0,https://www.reddit.com/r/statistics/comments/cr94l/berkeley_undergraduate_course_concepts_in/,13.0,6.0,,en
1106143,2010-07-19 23:50:30,statistics,R in a Nutshell Review from Slashdot,crcrs,kanak,1155042601.0,https://www.reddit.com/r/statistics/comments/crcrs/r_in_a_nutshell_review_from_slashdot/,13.0,0.0,,en
1106144,2010-07-20 00:42:08,MachineLearning,Cluster with Clojure (agglomerative hierarchical clustering),crdcu,[deleted],,https://www.reddit.com/r/MachineLearning/comments/crdcu/cluster_with_clojure_agglomerative_hierarchical/,1.0,0.0,,en
1106145,2010-07-20 03:03:21,statistics,Good Stats comics?,cretk,schnifin,1262741107.0,https://www.reddit.com/r/statistics/comments/cretk/good_stats_comics/,0.0,0.0,"So I recently started a job as a Statistician after graduating. Since I don't have pictures of kids to put on my office door, I'm looking for a few good comics that have at least something to do with Statistics. Any ideas?",en
1106146,2010-07-20 19:06:36,analytics,HitWise Top 20 Sites &amp; Engines,crnz8,salvage,1184143405.0,https://www.reddit.com/r/analytics/comments/crnz8/hitwise_top_20_sites_engines/,2.0,1.0,,en
1106147,2010-07-21 02:21:09,MachineLearning,Need some definitions related to sparse signal representation.,crtgn,[deleted],,https://www.reddit.com/r/MachineLearning/comments/crtgn/need_some_definitions_related_to_sparse_signal/,3.0,6.0,,en
1106148,2010-07-21 17:46:00,MachineLearning,R in a Nutshell ebook $10 today (with code DDRNT),cs253,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/cs253/r_in_a_nutshell_ebook_10_today_with_code_ddrnt/,2.0,0.0,,en
1106149,2010-07-21 19:06:33,MachineLearning,How to win the KDD Cup Challenge with R and gbm,cs3ad,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/cs3ad/how_to_win_the_kdd_cup_challenge_with_r_and_gbm/,11.0,2.0,,en
1106150,2010-07-22 00:25:52,statistics,"Can someone explain whether these results are meaningful and if so, why?",cs7i8,cowinabadplace,1268208134.0,https://www.reddit.com/r/statistics/comments/cs7i8/can_someone_explain_whether_these_results_are/,2.0,13.0,"Hello, /r/statistics,

To start off with, [here's what I'm talking about](http://californiawatch.org/watchblog/role-personal-belief-vaccine-waivers-whooping-cough-mixed-bag). It claims that it is likely that regions of California with higher-than-average vaccination waivers also have a greater risk of epidemics. I just want to know if _this data_ shows these results.

I don't know very much about statistics and am likely to get it wrong if I were to attempt to learn and apply a complex technique on my own. Will any of you be able to tell me what technique to use to find out if the results that this text claims are significant or be able to tell me if they are significant?

Any help is appreciated. Thank you for taking the time to read.",en
1106151,2010-07-22 11:45:30,MachineLearning,Data mining challenges and real life projects,csdyv,databuff,1272414378.0,https://www.reddit.com/r/MachineLearning/comments/csdyv/data_mining_challenges_and_real_life_projects/,2.0,0.0,,en
1106152,2010-07-22 12:55:57,MachineLearning,Memphis uses predictive analytics to cut crime by 31%,cseh1,databuff,1272414378.0,https://www.reddit.com/r/MachineLearning/comments/cseh1/memphis_uses_predictive_analytics_to_cut_crime_by/,59.0,7.0,,en
1106153,2010-07-22 21:09:31,MachineLearning,A survey paper on Opinion Mining and Sentiment Analysis [PDF],csk3o,kanak,1155042601.0,https://www.reddit.com/r/MachineLearning/comments/csk3o/a_survey_paper_on_opinion_mining_and_sentiment/,9.0,0.0,,en
1106154,2010-07-22 22:30:33,statistics,A great go-to resource for power / sample size calculations,csl3y,[deleted],,https://www.reddit.com/r/statistics/comments/csl3y/a_great_goto_resource_for_power_sample_size/,2.0,4.0,,en
1106155,2010-07-22 22:35:41,statistics,New to r/statistics ... so everyone loves R?,csl67,slammaster,1262799182.0,https://www.reddit.com/r/statistics/comments/csl67/new_to_rstatistics_so_everyone_loves_r/,0.0,1.0,"So I just found /r/statistics, and I'm pleased to see that R seems to be the software of choice.  A couple of questions:

* what libraries has anyone produced?
* what R blogs does anyone write?

I'm particularly interested in the second, as I'd like to expand my horizons, particularly with visualizations and how to use ggplot2 and the other advanced plotting systems",en
1106156,2010-07-23 17:56:10,analytics,5 Steps to cleaning up your marketing campaigns with Old Spice Body Wash...,cswbn,SocialPMChick,1279824846.0,https://www.reddit.com/r/analytics/comments/cswbn/5_steps_to_cleaning_up_your_marketing_campaigns/,1.0,0.0,,en
1106157,2010-07-23 18:03:10,MachineLearning,A Robot that Flip Pancakes using Reinforcement Learning.,cswey,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/cswey/a_robot_that_flip_pancakes_using_reinforcement/,34.0,6.0,,en
1106158,2010-07-23 18:41:00,MachineLearning,We need Reddit commits to get AI stack exchange up and running (like stack overflow),csww1,Druzil,1194513566.0,https://www.reddit.com/r/MachineLearning/comments/csww1/we_need_reddit_commits_to_get_ai_stack_exchange/,1.0,1.0,,en
1106159,2010-07-23 19:15:14,MachineLearning,Is anyone finding SciLab useful for ML?,csxdm,xamdam,1129780800.0,https://www.reddit.com/r/MachineLearning/comments/csxdm/is_anyone_finding_scilab_useful_for_ml/,3.0,1.0,,en
1106160,2010-07-24 12:02:01,statistics,'Artificial Intelligence' Gains Fans Among Investors,ct6j6,datt,1276003335.0,https://www.reddit.com/r/statistics/comments/ct6j6/artificial_intelligence_gains_fans_among_investors/,9.0,2.0,,en
1106161,2010-07-25 07:52:14,MachineLearning,Dear /r/MachineLearning: I've started a new subreddit dedicated to papers in computer science. Please contribute.,ctenu,kanak,1155042601.0,https://www.reddit.com/r/MachineLearning/comments/ctenu/dear_rmachinelearning_ive_started_a_new_subreddit/,10.0,1.0,,en
1106162,2010-07-25 18:42:04,statistics,[Ask statistics]: What advice would you give to someone just starting to learn Bayesian statistics?,cti4r,TheLeaderIsGood,1250383289.0,https://www.reddit.com/r/statistics/comments/cti4r/ask_statistics_what_advice_would_you_give_to/,14.0,18.0,"I'm doing a stats course and we have finally come onto Bayesian statistics, so I'm wondering if there's a piece of advice you'd have loved to have heard when you started learning it. Maybe something to do with common pitfalls or logic traps but whatever you think. Thanks.

I did look in r/bayesian but there's less than 10 readers.",en
1106163,2010-07-25 22:13:32,statistics,Local R User Group Panel from useR! 2010 (video),ctjxb,agconway,1228006618.0,https://www.reddit.com/r/statistics/comments/ctjxb/local_r_user_group_panel_from_user_2010_video/,5.0,0.0,,en
1106164,2010-07-26 01:15:14,artificial,Air Force wants drones that can tell other planes' intent,ctlf3,secret_town,1279292087.0,https://www.reddit.com/r/artificial/comments/ctlf3/air_force_wants_drones_that_can_tell_other_planes/,0.0,0.0,,en
1106165,2010-07-26 01:57:18,MachineLearning,This file gives a quick demonstration of a few ML techniques that are available in R,ctlq0,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/ctlq0/this_file_gives_a_quick_demonstration_of_a_few_ml/,23.0,0.0,,en
1106166,2010-07-26 02:51:45,statistics,"[Ask Statistics] How do you estimate LATE when you have two treatments, both of which participants can drop out of",ctm2t,besttrousers,1198690709.0,https://www.reddit.com/r/statistics/comments/ctm2t/ask_statistics_how_do_you_estimate_late_when_you/,2.0,2.0,"The set up is that a sample is randomized, half of the population can opt in to one treatment, and half the population can opt into different treatment. What I want to do is compare the outcomes of the two treaments. Is the a good way to get the local average treatment effect? I thought I would be able to do it with instrumental variables, but all of the examples I've found are for when only one of the groups can opt out, and the control gets no intervention. Would finding the ITT and getting the Wald estimator work?",en
1106167,2010-07-26 03:34:07,statistics,I need a reference for a two-sample Z-test,ctmdg,[deleted],,https://www.reddit.com/r/statistics/comments/ctmdg/i_need_a_reference_for_a_twosample_ztest/,0.0,0.0,,en
1106168,2010-07-26 18:09:19,analytics,number of worldwide email users is projected to increase from over 1.4 billion in 2009 to almost 1.9 billion by 2013. [Radicati],cttv6,salvage,1184143405.0,https://www.reddit.com/r/analytics/comments/cttv6/number_of_worldwide_email_users_is_projected_to/,5.0,0.0,,en
1106169,2010-07-26 19:39:23,MachineLearning,Cloudera's Training Vids on Hadoop.,ctv2v,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/ctv2v/clouderas_training_vids_on_hadoop/,9.0,0.0,,en
1106170,2010-07-26 19:42:47,statistics,Samples and Potato Chips | Stats With Cats Blog,ctv4j,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/ctv4j/samples_and_potato_chips_stats_with_cats_blog/,0.0,1.0,,en
1106171,2010-07-26 21:39:28,MachineLearning,Question about notation in Vapnik's Statistical Learning Theory,ctwnp,[deleted],,https://www.reddit.com/r/MachineLearning/comments/ctwnp/question_about_notation_in_vapniks_statistical/,5.0,3.0,,en
1106172,2010-07-27 19:36:06,MachineLearning,Social Network Analysis for Telecoms,cua53,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/cua53/social_network_analysis_for_telecoms/,4.0,0.0,,en
1106173,2010-07-27 21:04:06,statistics,[ask r/statistics] Can someone help me understand the probability and expected values of dice rolls in the game Farkle?,cubbp,bsx,1159984483.0,https://www.reddit.com/r/statistics/comments/cubbp/ask_rstatistics_can_someone_help_me_understand/,0.0,2.0,"For those that don't know of the game, it is a dice game played with six D6.  You get points for 3 of a kind or more, a six dice straight (1-6), three pairs, ones and fives.  For more information, here is the [farkle wikipedia article](http://en.wikipedia.org/wiki/Farkle).

Here is what I have so far.  [google docs link](https://spreadsheets.google.com/ccc?key=0AgZx_o6n9KlddDBiSUwtSEV5S0xFakRJQmZmZnA4dFE&amp;hl=en&amp;authkey=CLrD-6YJ)

If you would like to help with the worksheet, give me a PM with your gmail address and I'll add you as an editor.",en
1106174,2010-07-27 21:34:32,statistics,I would like to get a Masters in Statistics please give me advice,cubq7,iterationx,1232815076.0,https://www.reddit.com/r/statistics/comments/cubq7/i_would_like_to_get_a_masters_in_statistics/,6.0,7.0,"I went to Univ of Wisconsin, Madison and graduated with a degree in Comp Sci in 2003.  There was plenty of required math for that degree, but I didn't get great grades.  I know I need to take the GRE.  Can you give me some generic advice, and give me an idea how hard it is to get accepted?  Ideally I would like to get a degree in Minnesota.",en
1106175,2010-07-28 13:11:25,MachineLearning,Spherical self-organizing map using efficient indexed geodesic data structure,cukzo,AndreasBWagner,1192582160.0,https://www.reddit.com/r/MachineLearning/comments/cukzo/spherical_selforganizing_map_using_efficient/,9.0,0.0,,en
1106176,2010-07-28 15:26:29,statistics,Advice for Students --- Especially Graduate Students in Statistics,cum31,kanak,1155042601.0,https://www.reddit.com/r/statistics/comments/cum31/advice_for_students_especially_graduate_students/,18.0,4.0,,en
1106177,2010-07-29 06:15:28,statistics,Blogging about R - presentation and audio from useR2010,cuww6,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/cuww6/blogging_about_r_presentation_and_audio_from/,0.0,0.0,,en
1106178,2010-07-29 18:37:17,statistics,Ask Reddit: What is the legal situation with analysing Wikileaks of classified military files?,cv3zn,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/cv3zn/ask_reddit_what_is_the_legal_situation_with/,4.0,1.0,,en
1106179,2010-07-29 19:13:45,datasets,Scraped Facebook directory - personal details for 100 million users,cv4iv,towel42,1255309414.0,https://www.reddit.com/r/datasets/comments/cv4iv/scraped_facebook_directory_personal_details_for/,6.0,1.0,,en
1106180,2010-07-30 00:35:42,MachineLearning,Introduction to Probability and Statistics Using R. Free Ebook,cv8um,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/cv8um/introduction_to_probability_and_statistics_using/,14.0,0.0,,en
1106181,2010-07-30 08:50:11,MachineLearning,Book Idea: A machine learning workbook with a step-by-step solution to the Nextflix Prize,cvdfs,jasonb,1145327805.0,https://www.reddit.com/r/MachineLearning/comments/cvdfs/book_idea_a_machine_learning_workbook_with_a/,26.0,4.0,,en
1106182,2010-07-30 09:22:42,MachineLearning,High-Frequency Programmers Revolt Over Pay,cvdr8,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/cvdr8/highfrequency_programmers_revolt_over_pay/,29.0,21.0,,en
1106183,2010-07-31 13:01:51,MachineLearning,Problem-Solving using Graph Traversals,cvsue,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/cvsue/problemsolving_using_graph_traversals/,15.0,0.0,,en
1106184,2010-07-31 13:52:47,MachineLearning,Twitter account of Machine learning reddit. In case you want to get tweeted new links.,cvt3t,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/cvt3t/twitter_account_of_machine_learning_reddit_in/,4.0,0.0,,en
1106185,2010-07-31 17:51:29,MachineLearning,Robotic arm learning how to flip pancakes,cvujy,[deleted],,https://www.reddit.com/r/MachineLearning/comments/cvujy/robotic_arm_learning_how_to_flip_pancakes/,0.0,0.0,,en
1106186,2010-07-31 23:23:37,statistics,Using Optmatch and RItools for Observational Studies,cvx55,mfredrickson,1208359648.0,https://www.reddit.com/r/statistics/comments/cvx55/using_optmatch_and_ritools_for_observational/,0.0,6.0,,en
1106187,2010-08-01 13:27:47,MachineLearning,What is PMML?,cw2g4,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/cw2g4/what_is_pmml/,9.0,0.0,,en
1106188,2010-08-01 13:53:45,MachineLearning,Interoperability of Machine Learning Models with PMML,cw2ko,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/cw2ko/interoperability_of_machine_learning_models_with/,4.0,1.0,,en
1106189,2010-08-01 18:04:24,statistics,Twitter Sparkline Generator using Unicode - Draws bar charts and line charts,cw3zj,[deleted],,https://www.reddit.com/r/statistics/comments/cw3zj/twitter_sparkline_generator_using_unicode_draws/,1.0,1.0,,en
1106190,2010-08-02 00:01:02,MachineLearning,Benford’s Law Tests for Wikileaks Data,cw66d,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/cw66d/benfords_law_tests_for_wikileaks_data/,28.0,7.0,,en
1106191,2010-08-02 04:54:08,statistics,Suggestion for question submission guideline on /r/statistics,cw8qi,[deleted],,https://www.reddit.com/r/statistics/comments/cw8qi/suggestion_for_question_submission_guideline_on/,11.0,6.0,"Hello all,

I've enjoyed reading the different educational links and discussions we've had on this subreddit. Our group is certainly diverse, with readers who are working in chemistry, business, and other academic / non-academic contexts. 

A few times, people have come here and tried to ask questions on analysis approaches or graphical representations or other things. These can be frustrating for us because they are full of jargon, shorthands, acronyms and have no clear, defined scientific question.

To start a point of reference, it might be good to outline a general structure for getting a representative quorum of our Reddit trained statisticians (or statistically trained Reddittors :) )

* Make a short, suggestive title. Phrase it in the infinitive so that filler words don't lead to ambiguous language. For instance instead of ""How do you get the difference in survival rates between control group and treatment group assigned tamoxifen"" just say ""Calculating difference in survival between two groups"". The rest can be explained in the body of the submission.

* Give a good solid paragraph of scientific background so that we can understand the problem. Define jargon. Use your Standard Academic Approach to Acronyms [SAAA], SAAA can help keep entries short and clear and teach us something too.

* Ask a clear well defined question. If it's not quantitative, we might bounce a few ideas off of you to get things rolling. If there are several things you want to compare, that's fine, but don't entangle them into a single question because it's just confusing. Different sentences for different ideas.

* Mention any approaches you are trying and why there might be a problem (e.g. the reviewer says they want a non-parametric test).

* Be open to different philosophies. Different people have had different training and backgrounds and will potentially recommend wildly different strategies for each problem.

* Try to use the correct language for your problem. We might have to retool your thinking because a conventional word can mean something completely different in statistics. Frequently, researchers ask how to test if two variables are correlated. By ""correlated"" they often mean something different from statistical correlation, the ratio of covariance to standard deviation of both variables, but want to know if any change in one variable effects a change in another variable.

* If there's a serious statistical precedent in your field, we need to know about it. For instance, in psychology, some scores for certain disorders are almost always analyzed using linear regression. If we didn't know that, we might suggest a proportional odds model or another cumulative link if the number of categories are small. We're obviously not gonna poke around 12-13 articles in the Journal of Psychiatric Research to find this out.

That's all I can think about for now. Sorry if it sounds pompous or absolutist. I'm willing to edit things down based on our consensus here.",en
1106192,2010-08-02 18:23:11,analytics,7 Game Changing Trends Driving by Marketing Analaytis,cwga6,SigmaMktg,1280761444.0,https://www.reddit.com/r/analytics/comments/cwga6/7_game_changing_trends_driving_by_marketing/,1.0,0.0,,en
1106193,2010-08-03 02:24:52,MachineLearning,Data Mining: a new weapon in the fight against Medicaid fraud,cwltf,databuff,1272414378.0,https://www.reddit.com/r/MachineLearning/comments/cwltf/data_mining_a_new_weapon_in_the_fight_against/,7.0,0.0,,en
1106194,2010-08-03 05:38:15,computervision,Color segmentation/thresholding technique question.,cwnw9,1276284,1225860559.0,https://www.reddit.com/r/computervision/comments/cwnw9/color_segmentationthresholding_technique_question/,3.0,7.0,"So, I'm relatively new to image processing and computer vision in general, but I'm eager to try. I'm by no means educated about any of this but I do find it interesting. My question is basically about what would be the best way (most accurate) to threshold a certain color in an image? 

I understand I have to train the system to recognize the color first. So far, I have taken images of a blue box, converted the pixels to YCbCr, generated a histogram, and computed the min max of each channel. I did this 10 or so times in different lighting to get better results. Then I use the ranges I get to threshold the pixels (except for Y channel) from an incoming video to get the color. The results have not been amazing, but it does work (it's pretty bad with shadows on the object). I also tried to do the same thing with the HSI color space.

I'm sure there are more efficient ways to do this, and I will probably experiment with [this](http://www.cs.cmu.edu/~mmv/papers/wirevision00.pdf) [PDF]. As I said, most of the stuff I'm doing is experimentation and some limited research. So, is there a specific color space I should be using for better results? Is gathering the min and max of a histogram a good way of finding the color ranges of the item? Really, any advice would be helpful.

I'm sure there are libraries like OpenCV that would make this simpler, but I want to try getting my hands dirty first. Also, sorry if this is not exactly the right place to post this, I'm unaware of another subreddit.

Thanks!",en
1106195,2010-08-03 13:21:19,MachineLearning,Move over Elo - a competition to create a new chess rating system,cws1p,databuff,1272414378.0,https://www.reddit.com/r/MachineLearning/comments/cws1p/move_over_elo_a_competition_to_create_a_new_chess/,7.0,3.0,,en
1106196,2010-08-03 16:39:18,statistics,"Careful Statistical Computing, part 1.",cwtyb,[deleted],,https://www.reddit.com/r/statistics/comments/cwtyb/careful_statistical_computing_part_1/,1.0,1.0,,en
1106197,2010-08-03 19:05:15,MachineLearning,Clustering techniques for outlier detection - Tutorial from ACM SIGKDD 2010 (PDF),cwvv4,stoplan,1272908585.0,https://www.reddit.com/r/MachineLearning/comments/cwvv4/clustering_techniques_for_outlier_detection/,20.0,2.0,,en
1106198,2010-08-03 19:43:20,statistics,Clustering techniques for outlier detection - Tutorial from KDD 2010 (PDF),cwwfg,stoplan,1272908585.0,https://www.reddit.com/r/statistics/comments/cwwfg/clustering_techniques_for_outlier_detection/,4.0,0.0,,en
1106199,2010-08-04 14:53:38,statistics,Please help me with selecting a sample set,cx8ki,cockmongler,1228401764.0,https://www.reddit.com/r/statistics/comments/cx8ki/please_help_me_with_selecting_a_sample_set/,0.0,6.0,"I'm currently stuck with a problem and have very little time to solve it, no it's not homework. I'm very rusty on stats so any help would be greatly appreciated.

I have a set of samples X of a continuous variable along with a subset of these samples Y. There is an upper limit on the number of possible samples each of these sets could contain (i.e. they are taken from a set of possible outcomes of finite size X' and Y' respectively) with |Y| &lt; |X| always, but the number of samples is always going to be much lower than the maximum. |X'| and |Y'| are known. I want to know if the expected value of a sample taken from Y' (E(Y')) is better calculated as E(Y) or E(X).

e.g. if Y = {1}, X = {0.5, 1.6}, |Y'| = 10 and |X'| = 100 is 1 or (0.5 + 1.6)/2 a better approximation of E(y in Y')?
",en
1106200,2010-08-04 15:52:50,MachineLearning,Online dating sites are a treasure trove for scientists,cx96q,[deleted],,https://www.reddit.com/r/MachineLearning/comments/cx96q/online_dating_sites_are_a_treasure_trove_for/,0.0,0.0,,en
1106201,2010-08-04 22:26:37,MachineLearning,Gamers beat algorithms at finding protein structures,cxehn,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/cxehn/gamers_beat_algorithms_at_finding_protein/,15.0,1.0,,en
1106202,2010-08-05 01:37:35,statistics,[Ask Statistics] Any good reads for understanding time series analysis?,cxgxa,brosephius,1202077709.0,https://www.reddit.com/r/statistics/comments/cxgxa/ask_statistics_any_good_reads_for_understanding/,10.0,10.0,"I've tried on many occasions to teach myself time series analysis, but end up losing focus due to equation/derivation overload. I have yet to find something that actually explained e.g. arima models in an intuitive way with real-world examples, which I think would help me grasp this stuff. Is there anything out there of this sort, or is this stuff just not easily simplified?",en
1106203,2010-08-05 12:35:39,MachineLearning,"Google CEO Schmidt: ""People Aren't Ready for the Technology Revolution""",cxna7,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/cxna7/google_ceo_schmidt_people_arent_ready_for_the/,36.0,10.0,,en
1106204,2010-08-05 22:17:16,statistics,Pitfalls of Using Standard Packages for Survey Data,cxtnx,[deleted],,https://www.reddit.com/r/statistics/comments/cxtnx/pitfalls_of_using_standard_packages_for_survey/,2.0,1.0,,en
1106205,2010-08-06 11:24:03,MachineLearning,Turing Test for Robot Stock Traders,cy1m1,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/cy1m1/turing_test_for_robot_stock_traders/,11.0,1.0,,en
1106206,2010-08-06 17:17:19,statistics,Twenty rules for good graphics,cy4qn,agconway,1228006618.0,https://www.reddit.com/r/statistics/comments/cy4qn/twenty_rules_for_good_graphics/,1.0,1.0,,en
1106207,2010-08-06 21:34:51,statistics,Statistics Jokes (please go and add your own :) ),cy85b,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/cy85b/statistics_jokes_please_go_and_add_your_own/,11.0,8.0,,en
1106208,2010-08-07 19:15:35,statistics,"Wikileaks geo. attack data by year and type, over Afghanistan map",cyi1x,[deleted],,https://www.reddit.com/r/statistics/comments/cyi1x/wikileaks_geo_attack_data_by_year_and_type_over/,0.0,0.0,,en
1106209,2010-08-07 22:51:58,MachineLearning,"Interested in ML for BCI (brain-computer interfaces)?  May create a subreddit, interest pending.",cyjv6,bciguy,1275791009.0,https://www.reddit.com/r/MachineLearning/comments/cyjv6/interested_in_ml_for_bci_braincomputer_interfaces/,0.0,1.0,,en
1106210,2010-08-08 22:20:50,statistics,The Zen of Modeling | Stats With Cats Blog,cyt90,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/cyt90/the_zen_of_modeling_stats_with_cats_blog/,1.0,0.0,,en
1106211,2010-08-09 00:30:04,MachineLearning,Readings on Online Machine Learning,cyue2,[deleted],,https://www.reddit.com/r/MachineLearning/comments/cyue2/readings_on_online_machine_learning/,0.0,2.0,,en
1106212,2010-08-09 00:59:45,MachineLearning,Any ideas on where to get training data?,cyumu,[deleted],,https://www.reddit.com/r/MachineLearning/comments/cyumu/any_ideas_on_where_to_get_training_data/,6.0,10.0,,en
1106213,2010-08-09 02:24:58,MachineLearning,Online Learning in Clojure,cyvck,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/cyvck/online_learning_in_clojure/,9.0,0.0,,en
1106214,2010-08-10 06:35:24,artificial,Book recommendation: good overview of AI?,czbvp,revocation,1244380960.0,https://www.reddit.com/r/artificial/comments/czbvp/book_recommendation_good_overview_of_ai/,9.0,10.0,"I recently saw a demonstration by Honda's Asimo. I was inspired. Since then I've read _Designing Sociable Robots_ and then thumbed through _AI: Structures and Strategies for Complex Problem Solving_ and _AI: A Modern Approach_. I wonder if you have any other recommendations that would allow me to wrap my head around the topics addressed in the field (I like books that don't require two hands to hold up! - e.g., Russell and Norvig). Many of the details are not unfamiliar to me as I've dabbled in statistical learning, graph theory, and several types of calculi for my research (and I program a lot)...",en
1106215,2010-08-10 14:31:29,MachineLearning,How Chris Raimondi won the Predict HIV Progression data mining competition,czg6s,databuff,1272414378.0,https://www.reddit.com/r/MachineLearning/comments/czg6s/how_chris_raimondi_won_the_predict_hiv/,13.0,2.0,,en
1106216,2010-08-10 19:34:55,statistics,Causes of US deaths overseas 2002-2009,czk6b,eurica,1260664508.0,https://www.reddit.com/r/statistics/comments/czk6b/causes_of_us_deaths_overseas_20022009/,4.0,4.0,,en
1106217,2010-08-11 18:30:12,MachineLearning,Personalized recommendations,czytd,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/czytd/personalized_recommendations/,6.0,0.0,,en
1106218,2010-08-11 18:53:58,statistics,What distribution does my data have?,czz43,jeanlucpikachu,1159634949.0,https://www.reddit.com/r/statistics/comments/czz43/what_distribution_does_my_data_have/,8.0,2.0,,en
1106219,2010-08-12 01:22:28,statistics,"[ask statistics] Has anyone performed data anonymization with k-anonymization, lattices or similar?",d049u,[deleted],,https://www.reddit.com/r/statistics/comments/d049u/ask_statistics_has_anyone_performed_data/,0.0,0.0,,en
1106220,2010-08-12 18:10:13,statistics,Rethinking my career (from econom[etr]ics to statistics),d0e06,crayolakitu,1275665938.0,https://www.reddit.com/r/statistics/comments/d0e06/rethinking_my_career_from_econometrics_to/,4.0,12.0,"Hi r/statistics,

I am finishing my PhD at a top 10 uni; unfortunately, the key thing that I learned over those years is that, well, I chose the wrong field. My PhD is the story of a man with a stats / econ background, who started a PhD in economics and slowly migrated to the most statisticsy corners of economics. I dislike economic research, am not too fond of economists in general; yet have a passion for data and estimation.

So I will be looking for a job pretty soon. I don't know what kind exactly. Could be anything from clinical trials to a postdoc in stats, or a statistician job in a private firm of some kind. I do have some jobs in mind that I would particularly like, but am flexible in my expectations.

Being based in London, I was thinking of going to [the RSS international conference](http://www.rss.org.uk/rss2010) to get a chance to meet a few statisticians and get their opinion on my CV; as well as get a better picture of what is hot in statistics than I can see from my economics vantage point. Do you guys think that's a silly idea? I basically know next to nobody doing stats in the UK; it is mostly a jump into the unknown. My undergrad diploma does qualify me as a statistician; and my PhD is very quantitative; so I got the skills, and what I don't have I am very motivated about acquiring.

What do you guys think? Any opinion is very, very welcome.",en
1106221,2010-08-12 18:57:29,statistics,"Statistical models for Stocks, commodoties, etc?",d0ema,shelmich,1276209122.0,https://www.reddit.com/r/statistics/comments/d0ema/statistical_models_for_stocks_commodoties_etc/,1.0,11.0,Has anyone here tried to develop a program to use statistics to model and predict the stock market? How did it turn out?,en
1106222,2010-08-13 03:48:21,statistics,A Taste of Reservoir Algorithms,d0kwb,indraniel,1214879371.0,https://www.reddit.com/r/statistics/comments/d0kwb/a_taste_of_reservoir_algorithms/,7.0,0.0,,en
1106223,2010-08-13 11:41:24,datasets,Rare Sharing of Data Leads to Progress on Alzheimer’s,d0p89,cavedave,1128052800.0,https://www.reddit.com/r/datasets/comments/d0p89/rare_sharing_of_data_leads_to_progress_on/,5.0,1.0,,en
1106224,2010-08-13 15:46:52,MachineLearning,Analysing spatial point patterns in 'R',d0r8c,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/d0r8c/analysing_spatial_point_patterns_in_r/,5.0,0.0,,en
1106225,2010-08-13 18:00:42,MachineLearning,Papers from ICML &amp; COLT 2010,d0sse,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/d0sse/papers_from_icml_colt_2010/,7.0,0.0,,en
1106226,2010-08-13 19:16:12,statistics,"World's Population, by latitude &amp; longitude",d0tv1,prionattack,1257312771.0,https://www.reddit.com/r/statistics/comments/d0tv1/worlds_population_by_latitude_longitude/,6.0,5.0,,en
1106227,2010-08-14 11:06:36,statistics,Statisticians of Reddit - is your job as boring as mine?,d12o8,out_of_ideas,1281771161.0,https://www.reddit.com/r/statistics/comments/d12o8/statisticians_of_reddit_is_your_job_as_boring_as/,18.0,26.0,"I got myself a Master in statistics about 5 years ago (and really enjoyed the coursework).  Since then I've worked a few different jobs, each of which was awful in its own special way.  This included a short stint in a bank, and a few years at internet / tech companies.  


The technical high point of my work-life has been building linear regression models and some control/test (or A/B test) analyses.  More commonly I find myself spending my days writing SQL queries, looking at descriptive statistics and studying trends of X (substitute with metric of interest) over time.  It has been somewhat mundane.


As time goes on, I find myself forgetting all the nifty things I had learned at school.  It has come to a point that, when I do see a  challenging problem, it makes me nervous.  I'm unsure of whether I still have the skills to tackle it.


So here are my questions for all you Reddit-reading statisticians out there:

Is this what most statistics jobs are like?
If not, where do you work?  And what kinds of things do you do on a daily basis?

If you have advice on how I could make my work more interesting, do let me know.  If not, I'd still welcome any comments you might have.  :-)",en
1106228,2010-08-14 18:54:31,MachineLearning,useR! 2010: Invited Lectures [VIDS],d15hy,Troybatroy,1238986431.0,https://www.reddit.com/r/MachineLearning/comments/d15hy/user_2010_invited_lectures_vids/,10.0,0.0,,en
1106229,2010-08-14 19:46:31,statistics,Why is square of deviations from mean preferred over the absolute deviation from mean? ,d15y3,[deleted],,https://www.reddit.com/r/statistics/comments/d15y3/why_is_square_of_deviations_from_mean_preferred/,1.0,1.0,"This question has puzzled me for 25 years, but this paper (off wikipedia) gave an answer: [Revisiting a 90-year-old debate: the advantages of the mean deviation](http://www.leeds.ac.uk/educol/documents/00003759.htm). 

I am curious to know how this paper has been received in the stat community. I learnt from it that the original reason (due to Fisher) for the squared approach to measuring dispersion is that it is more consistent while sampling (the deviation of the deviations is smaller when taking many samples) than with the absolute deviation approach. Why then stop at squares, why not cubes? Why not higher powers as a measure of dispersion?
",en
1106230,2010-08-15 15:12:11,computervision,Removing pedestrians from Google Street View images.,d1df2,[deleted],,https://www.reddit.com/r/computervision/comments/d1df2/removing_pedestrians_from_google_street_view/,7.0,3.0,,en
1106231,2010-08-15 19:15:43,statistics,Time Is On My Side | Stats With Cats Blog,d1f0v,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/d1f0v/time_is_on_my_side_stats_with_cats_blog/,1.0,0.0,,en
1106232,2010-08-16 05:30:05,statistics,Intro to Econometrics,d1k4d,Damark81,1274898720.0,https://www.reddit.com/r/statistics/comments/d1k4d/intro_to_econometrics/,7.0,2.0,"Hi reddits. I am working on a programming project that requires data analysis. Recently, I learned that the proper analysis technique for our data is data panel analysis, which is a part of econometrics. Given that my base is in computer engineering, I am completely in way over my head. Although I can code the simple statistic equation, I wish to be able to understand the technique/the field more clearly. Any suggestion on beginner reading for econometrics?",en
1106233,2010-08-16 09:08:58,statistics,Two surprising things about R,d1m1d,efrique,1219734906.0,https://www.reddit.com/r/statistics/comments/d1m1d/two_surprising_things_about_r/,12.0,7.0,,en
1106234,2010-08-16 18:44:17,MachineLearning,Suicide bombers do not buy life insurance,d1r2d,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/d1r2d/suicide_bombers_do_not_buy_life_insurance/,15.0,10.0,,en
1106235,2010-08-16 22:00:40,statistics,"ggplot2 plot builder is now on CRAN! (through Deducer, a GUI for R)",d1tof,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/d1tof/ggplot2_plot_builder_is_now_on_cran_through/,15.0,0.0,,en
1106236,2010-08-18 11:12:04,computervision,Talks from CVPR 2010,d2hun,[deleted],,https://www.reddit.com/r/computervision/comments/d2hun/talks_from_cvpr_2010/,10.0,0.0,,en
1106237,2010-08-18 12:11:18,statistics,Tourism forecasting competition - winner invited to publish in leading journal,d2ic0,databuff,1272414378.0,https://www.reddit.com/r/statistics/comments/d2ic0/tourism_forecasting_competition_winner_invited_to/,4.0,1.0,,en
1106238,2010-08-19 13:21:16,statistics,High-Level Interface Between R and Excel,d2yke,discontinuity,1204666886.0,https://www.reddit.com/r/statistics/comments/d2yke/highlevel_interface_between_r_and_excel/,11.0,3.0,,en
1106239,2010-08-19 16:27:35,statistics,"""Odds are, it's wrong: science fails to face the shortcomings of statistics"" (As seen in r/science and r/philosophyofscience)",d30e5,TheLeaderIsGood,1250383289.0,https://www.reddit.com/r/statistics/comments/d30e5/odds_are_its_wrong_science_fails_to_face_the/,8.0,7.0,,en
1106240,2010-08-19 21:27:29,MachineLearning,"reddit is hiring! [or, puzzle for job opportunity]",d34di,fbahr,1275413362.0,https://www.reddit.com/r/MachineLearning/comments/d34di/reddit_is_hiring_or_puzzle_for_job_opportunity/,0.0,1.0,,en
1106241,2010-08-22 19:08:14,statistics,The Five Pursuits You Meet in Statistics | Stats With Cats Blog,d43ia,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/d43ia/the_five_pursuits_you_meet_in_statistics_stats/,0.0,0.0,,en
1106242,2010-08-23 00:20:09,MachineLearning,The best blogs about data,d469t,[deleted],,https://www.reddit.com/r/MachineLearning/comments/d469t/the_best_blogs_about_data/,1.0,0.0,,en
1106243,2010-08-23 15:14:40,analytics,6 Applications of Predictive Analytics,d4dxt,DataInfoCom,1282565523.0,https://www.reddit.com/r/analytics/comments/d4dxt/6_applications_of_predictive_analytics/,1.0,0.0,,en
1106244,2010-08-23 20:59:30,statistics,Transforming data to fit a normal distribution,d4i58,curiouslystrongmints,1281424864.0,https://www.reddit.com/r/statistics/comments/d4i58/transforming_data_to_fit_a_normal_distribution/,8.0,19.0,"I have some biological count data and I have been asked to statistically analyse it.  For the sake of example, think of it as the number of ants found in different parts of a football pitch.

The biological count data is in an x by y grid, and the only two options I have been given are a Kruskal-Wallis non-parametric test or a two-way ANOVA.  The conclusions will be of the form ""yes, distance along the x-axis does affect the biological count"" etc.

None of the data fits a normal distribution (P&lt;0.005), and that's not at all surprising - there are lots of zeroes (which is characteristic of biological count data).  I have been trying to think logically about what transformations I could apply to transform the data into a normal distribution, allowing me to use a parametric test which (so I'm told) will allow me to make more conclusions.  

And this is where my brain hurts.

I think I'm comfortable with applying transformations to the data, then testing for normality.  But is it acceptable to eliminate some of the 'zero' data, and then perform a statistical test just on a certain portion of the data?  Using the ants-on-a-football-pitch example, it would be a matter of concluding ""in this particular quarter of the pitch where ants are found, the distance from the goal does make a difference to ant population""

Or by doing that am I chucking out real zero data?  This 'zero data' isn't just ""meh we didn't bother looking"", we actually observed that in these areas the biological count was zero, so it is data.

Maybe the answer is just ""stop butchering your data and just stick with a non-parametric test""...?

TL;DR Do you think it's acceptable to make statistical conclusions about a small portion of biological count data and ignore the rest?",en
1106245,2010-08-23 21:08:11,computervision,Beyond pixel-wise labeling: Blocks World Revisited,d4ia0,jeanlucpikachu,1159634949.0,https://www.reddit.com/r/computervision/comments/d4ia0/beyond_pixelwise_labeling_blocks_world_revisited/,6.0,0.0,,en
1106246,2010-08-24 03:16:39,statistics,An online p-value calculator/grapher that I made,d4mpz,[deleted],,https://www.reddit.com/r/statistics/comments/d4mpz/an_online_pvalue_calculatorgrapher_that_i_made/,13.0,7.0,,en
1106247,2010-08-24 11:45:51,MachineLearning,Bit.ly data scientist builds an email classifier to order emails by priority,d4rln,databuff,1272414378.0,https://www.reddit.com/r/MachineLearning/comments/d4rln/bitly_data_scientist_builds_an_email_classifier/,11.0,0.0,,en
1106248,2010-08-24 11:47:19,MachineLearning,How do we choose a mate? What data scientists are learning from online dating,d4rm9,databuff,1272414378.0,https://www.reddit.com/r/MachineLearning/comments/d4rm9/how_do_we_choose_a_mate_what_data_scientists_are/,20.0,1.0,,en
1106249,2010-08-24 14:58:31,statistics,Could a fellow Redditor please help me with this stat problem? ,d4t8u,squired,1248364214.0,https://www.reddit.com/r/statistics/comments/d4t8u/could_a_fellow_redditor_please_help_me_with_this/,1.0,17.0,"I apologize if this is the wrong sub-reddit to post this in. The following problem is a representative of a real world problem that I am attempting to solve. I am trying to flesh out an anti-scripting RTT and the combo's take a fair bit of effort to code. If I know roughly how long it would take to reverse-engineer/probe a set number of combo's, I'll have a better idea as to how many I'll need to build in. Thank you so much for any help that you can provide! 

________________________


You have a large bag.

Within that bag are an unknown number of unique items.

You may reach inside this bag, pull out one item, log that item, and then you must place it back into the bag.

**If there are 200 items in the bag, how many items must you pull to be 99% sure that you have pulled every item? **

________________________



*All my karma for some help!*
",en
1106250,2010-08-24 19:33:55,MachineLearning,Sparse matrix libraries: is everybody moving to Java?,d4weu,fierro999,1256319912.0,https://www.reddit.com/r/MachineLearning/comments/d4weu/sparse_matrix_libraries_is_everybody_moving_to/,10.0,19.0,"I'm looking for a free C/C++ sparse matrix library to use for a project. So far the best candidate I found is [SparseLib++](http://math.nist.gov/sparselib++/). However, their site says the project is in ""minimal maintenance"" mode. 

Then there are some Java projects like UJMP and Colt that look to actually be alive.

So, is the action in this area of scientific computing moving to Java, or am I overlooking some C++ project that is actually being maintained?
",en
1106251,2010-08-24 19:34:02,MachineLearning,"A listing of good, free machine learning textbooks…",d4wew,fbahr,1275413362.0,https://www.reddit.com/r/MachineLearning/comments/d4wew/a_listing_of_good_free_machine_learning_textbooks/,3.0,0.0,,en
1106252,2010-08-24 22:02:26,MachineLearning,Shane Legg - Machine Super Intelligence (incl. universal measure of intelligence),d4yif,dakk12,1185850891.0,https://www.reddit.com/r/MachineLearning/comments/d4yif/shane_legg_machine_super_intelligence_incl/,14.0,7.0,"I was at the singularity summit a couple weekends ago, and there was an interesting talk given by Shane Legg.  He just finished his thesis on Machine Super Intelligence, and he talked about a formal definition of intelligence.  Most of his stuff is about theoretic optimal models which are uncomputable, such as AIXI.  However he has been able to implement some things using monte carlo methods.

Here is his [thesis](http://www.vetta.org/documents/Machine_Super_Intelligence.pdf).  I'll post the video when it becomes available.",en
1106253,2010-08-24 23:48:44,MachineLearning,PCA: Better Java solution? Or is it time to switch to C?,d4zv0,[deleted],,https://www.reddit.com/r/MachineLearning/comments/d4zv0/pca_better_java_solution_or_is_it_time_to_switch/,0.0,0.0,"I have a matrix of approximately 300x7000 size. I am currently using colt as my matrix and linear alg library. Memory usage is atrocious (about 8gb) and after 8hrs my task times out during multiplication of the covariance matrix (of size 7000x7000). 

Am I just being daft? Is this an unreasonable size covariance matrix to calculate? Is there a faster method for PCA?",en
1106254,2010-08-25 03:20:50,statistics,Help with statistics final project and take a 4 question anonymous survey regarding distance and traditional education!,d524y,ivanmt42,1185306335.0,https://www.reddit.com/r/statistics/comments/d524y/help_with_statistics_final_project_and_take_a_4/,0.0,1.0,,en
1106255,2010-08-25 16:07:00,statistics,Difference in means of time-series data and assumptions of normality.,d59bi,curious_mind,1259710792.0,https://www.reddit.com/r/statistics/comments/d59bi/difference_in_means_of_timeseries_data_and/,2.0,5.0,"I'm currently working with some time series data and I want to establish confidence intervals on the difference.  In, this particular case, I'm comparing a measured temperature value with a modeled value.  

I'm just not sure what is the best way to carry out the comparison.  Currently, I'm just using a one sample confidence interval on the pairwise difference since this difference has a normal distribution.  Would it be better to use a two sample approach and assume either equal or unequal variance? Any insight would be greatly appreciated.  Thanks!


*Example:*


Lets say there is a weather station measuring the air temperature.  A model is run which creates a forecast of the maximum daily temperature 5 days from now.  I need to create a confidence interval on mean difference between the actual maximum temperature recorded by the weather station and what the model predicted it was going to be today.  Therefore, each observation will be comparing two values at the same point in time (5 days from now in this case), but there will 5 day lag between time that weather prediction was generated and the time that the actual measurement was recorded. 


In this case, I feel that the observations should be independent since the difference between the values at one point in time should not affect the difference between the values in the future (feel free to correct me!).

 
**Edit:** Grammar

**Edit2:** Addition of an example.  Hopefully it clarifies things a bit :)",en
1106256,2010-08-26 05:24:32,statistics,FiveThirtyEight: Nate Silver's Methodology - NYTimes.com,d5j2b,Troybatroy,1238986431.0,https://www.reddit.com/r/statistics/comments/d5j2b/fivethirtyeight_nate_silvers_methodology/,6.0,0.0,,en
1106257,2010-08-26 09:15:17,statistics,Distribution of the first million digits of √2's Decimal Expansion,d5lg2,Lors_Soren,1282243358.0,https://www.reddit.com/r/statistics/comments/d5lg2/distribution_of_the_first_million_digits_of_2s/,0.0,5.0,,en
1106258,2010-08-26 09:50:03,MachineLearning,Adventures in Data Land - Alex Smola's blog,d5lrn,kzn,1162071679.0,https://www.reddit.com/r/MachineLearning/comments/d5lrn/adventures_in_data_land_alex_smolas_blog/,0.0,0.0,,en
1106259,2010-08-27 19:32:09,statistics,In search of power-laws: WikiLeaks edition,d67d2,agconway,1228006618.0,https://www.reddit.com/r/statistics/comments/d67d2/in_search_of_powerlaws_wikileaks_edition/,4.0,1.0,,en
1106260,2010-08-28 02:21:17,statistics,"80% chance? Of my kids being hit while riding to 
school? It's more likely than you think!",d6d4t,[deleted],,https://www.reddit.com/r/statistics/comments/d6d4t/80_chance_of_my_kids_being_hit_while_riding_to/,6.0,3.0,,en
1106261,2010-08-28 04:38:46,MachineLearning,Boosted Decision Trees for Deep Learning,d6eku,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/d6eku/boosted_decision_trees_for_deep_learning/,12.0,1.0,,en
1106262,2010-08-28 04:42:27,MachineLearning,Two Surpising Things about R,d6emj,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/d6emj/two_surpising_things_about_r/,0.0,0.0,,en
1106263,2010-08-28 04:44:47,MachineLearning,Job opportunity for candidates to help in developing software and analyzing data using Bayesian methods.,d6eno,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/d6eno/job_opportunity_for_candidates_to_help_in/,3.0,0.0,,en
1106264,2010-08-28 04:45:49,MachineLearning,Speeding up parentheses in R,d6enx,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/d6enx/speeding_up_parentheses_in_r/,0.0,0.0,,en
1106265,2010-08-28 14:25:16,MachineLearning,Could data mining prevent another 60-mile traffic jam?,d6j21,databuff,1272414378.0,https://www.reddit.com/r/MachineLearning/comments/d6j21/could_data_mining_prevent_another_60mile_traffic/,7.0,1.0,,en
1106266,2010-08-29 06:47:15,statistics,The Right Tool for the Job | Stats With Cats Blog,d6r0g,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/d6r0g/the_right_tool_for_the_job_stats_with_cats_blog/,5.0,8.0,,en
1106267,2010-08-29 10:18:09,statistics,Statistical self-similarity in network traffic dynamics,d6sis,Lors_Soren,1282243358.0,https://www.reddit.com/r/statistics/comments/d6sis/statistical_selfsimilarity_in_network_traffic/,4.0,0.0,,en
1106268,2010-08-30 21:15:14,statistics,Need help leading a Statistics class,d7bwj,[deleted],,https://www.reddit.com/r/statistics/comments/d7bwj/need_help_leading_a_statistics_class/,4.0,4.0,"I am a long term substitute teacher for an AP Statistics class.

I have no background in Statistics.

Basically we're going through the textbook and figuring it out together. Any and all help would be appreciated.",en
1106269,2010-08-31 00:02:53,statistics,Negative Probability - does anybody understand this?,d7ek3,dsabeti,1276901350.0,https://www.reddit.com/r/statistics/comments/d7ek3/negative_probability_does_anybody_understand_this/,17.0,18.0,,en
1106270,2010-08-31 00:55:22,statistics,How to detect compare outliers,d7fbt,spgarbet,1191617059.0,https://www.reddit.com/r/statistics/comments/d7fbt/how_to_detect_compare_outliers/,3.0,7.0,"I have a scientist that came to me with the hypothesis that outliers will increase due to some underlying phenomenon. What is a good method to compare outliers in a series of datasets?

This paper, [Knight &amp; Wang 2009]( http://www.gmat.unsw.edu.au/snap/publications/knight&amp;wang2009a.pdf) says that the Danish method faired best in their tests. But it's a heuristic method! So would this be an acceptable method?",en
1106271,2010-08-31 22:38:33,statistics,Statistics help to a non-statistician regarding appropriate test to apply to data,d7uqj,[deleted],,https://www.reddit.com/r/statistics/comments/d7uqj/statistics_help_to_a_nonstatistician_regarding/,0.0,0.0,,en
1106272,2010-09-01 04:12:03,MachineLearning,Gmail launches the Priority Inbox - which orders emails by importance rather than chronology,d7z1e,databuff,1272414378.0,https://www.reddit.com/r/MachineLearning/comments/d7z1e/gmail_launches_the_priority_inbox_which_orders/,10.0,3.0,,en
1106273,2010-09-01 09:49:25,MachineLearning,Adioso releases the new version of its NLP flight search tool,d82sp,databuff,1272414378.0,https://www.reddit.com/r/MachineLearning/comments/d82sp/adioso_releases_the_new_version_of_its_nlp_flight/,3.0,0.0,,en
1106274,2010-09-01 17:57:11,data,Learn How To Recover Data from an Overwritten Hard Drive - Recover Your Overwritten Files,d87v1,scientificworld,1253015496.0,https://www.reddit.com/r/data/comments/d87v1/learn_how_to_recover_data_from_an_overwritten/,0.0,0.0,,en
1106275,2010-09-01 18:01:00,MachineLearning,Cluster Data by Making Faces,d87x0,fxj,1169393085.0,https://www.reddit.com/r/MachineLearning/comments/d87x0/cluster_data_by_making_faces/,1.0,0.0,,en
1106276,2010-09-01 18:07:44,MachineLearning,Syllabus for course on Data Science offered at RPI,d880y,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/d880y/syllabus_for_course_on_data_science_offered_at_rpi/,6.0,0.0,,en
1106277,2010-09-02 00:20:31,statistics,Desperate need of stats help for college.,d8e2k,[deleted],,https://www.reddit.com/r/statistics/comments/d8e2k/desperate_need_of_stats_help_for_college/,0.0,4.0,,en
1106278,2010-09-02 01:18:04,statistics,Parameter Estimation in AI/ROBOTICS,d8evo,beandipper,1282647848.0,https://www.reddit.com/r/statistics/comments/d8evo/parameter_estimation_in_airobotics/,3.0,5.0,"Would like to know how Estimation is used in AI, Robotics, Machine learning, etc. 
It's unclear to me what the distributions and parameters would be in these kind of scenarios. Please enlighten me :D",en
1106279,2010-09-02 04:09:59,artificial,More shameless advertising: University of Reddit - online AI introduction course with Python,d8gyo,[deleted],,https://www.reddit.com/r/artificial/comments/d8gyo/more_shameless_advertising_university_of_reddit/,17.0,0.0,,en
1106280,2010-09-02 11:56:31,MachineLearning,Barbecues - Outdoor Cooking Apparatus,d8ltz,remembering,1283410751.0,https://www.reddit.com/r/MachineLearning/comments/d8ltz/barbecues_outdoor_cooking_apparatus/,0.0,0.0,,en
1106281,2010-09-02 18:00:47,datasets,Social Network data sets,d8pjf,llimllib,1142360191.0,https://www.reddit.com/r/datasets/comments/d8pjf/social_network_data_sets/,13.0,0.0,,en
1106282,2010-09-03 00:55:43,statistics,Academy Award winners live longer than runners-up. Interview for med school on a rainy day and you won't get in. Class presidents die sooner. More people die in car crashes on election day. Who would've thunk that medical statistics would be so interesting? [r/science repost],d8vs6,Troybatroy,1238986431.0,https://www.reddit.com/r/statistics/comments/d8vs6/academy_award_winners_live_longer_than_runnersup/,9.0,0.0,,en
1106283,2010-09-03 03:48:16,MachineLearning,Bot Botany: K-Means and ggplot2,d8xtt,ezgraphs,1276218477.0,https://www.reddit.com/r/MachineLearning/comments/d8xtt/bot_botany_kmeans_and_ggplot2/,1.0,0.0,,en
1106284,2010-09-03 23:07:23,statistics,"Puzzle: Which of these are drawn from 𝓝(0,1) and which are drawn from AVG&lt; 𝓝(0,1) + 𝓝(-3,1) &gt; ?",d9ar3,Lors_Soren,1282243358.0,https://www.reddit.com/r/statistics/comments/d9ar3/puzzle_which_of_these_are_drawn_from_𝓝01_and/,0.0,0.0,,en
1106285,2010-09-05 05:04:59,artificial,Scientific study proving basically the exact thing I said a year ago on reddit.,d9ps6,ithkuil,1222549680.0,https://www.reddit.com/r/artificial/comments/d9ps6/scientific_study_proving_basically_the_exact/,0.0,4.0,"http://www.sciencedaily.com/releases/2010/09/100902121051.htm

&gt;This ancestral structure was likely a group of densely packed cells, which received and processed information about smell and directly controlled locomotion. It may have enabled our ancestors crawling over the sea floor to identify food sources, move towards them, and integrate previous experiences into some sort of learning.

Compare to my suggestion here:

http://www.reddit.com/r/artificial/comments/8uwcq/are_worms_intelligent/

&gt; This is me trying to start thinking about my own understanding of intelligence.
My idea is that maybe the basic capabilities of intelligent systems are less numerous and complex than most people think. Maybe its starts with a basic ability that even a worm or an insect has, and that just gets elaborated on and scaled up all the way to humans.

&gt; A worm has to do some things that are actually pretty smart when you compare them with the abilities of the average computer:

&gt; * integrate fairly arbitrary patterns of sensory information from its environment with a time dimension

&gt; * use this information to determine the actions that will most likely lead to a desired set of sensory patterns in the future

&gt; In other words, it has to, for example, **move in the direction of a scent to obtain food.**

&gt; My theory is that perhaps the worm must be intelligent in order to do this and that the difference between worm intelligence and human intelligence might just be a matter of something like being able to handle more complex sensory relationships over greater time spans and also having more built-in patterns for predicting the correct actions and integrating senses.
",en
1106286,2010-09-05 19:31:54,statistics,The Heart and Soul of Variance Control | Stats With Cats Blog,d9vgf,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/d9vgf/the_heart_and_soul_of_variance_control_stats_with/,5.0,0.0,,en
1106287,2010-09-06 02:46:52,datasets,London Bus &amp; Tube Timetable Dataset (XML),d9zht,voltagex,1230861112.0,https://www.reddit.com/r/datasets/comments/d9zht/london_bus_tube_timetable_dataset_xml/,7.0,0.0,,en
1106288,2010-09-06 19:57:10,MachineLearning,[Crosspost from r/proggramming:] Two Surpising Things about Rm by Radford Neal,da8uh,[deleted],,https://www.reddit.com/r/MachineLearning/comments/da8uh/crosspost_from_rproggramming_two_surpising_things/,0.0,1.0,,en
1106289,2010-09-07 16:21:59,computervision,Illumination for feature extraction. White LED's are not white?,dalol,townhall,1251896689.0,https://www.reddit.com/r/computervision/comments/dalol/illumination_for_feature_extraction_white_leds/,6.0,12.0,"Hi all, first time poster, long time lurker on this subreddit. I was wondering if anyone had any experience with object recognition in a controlled environment using artificial light sources. In my project, the exact color of the object is of prime importance. 
Somehow I missed the part in my upbringing where every kid is told that white LED's are really blue and yellow, so I build my ""controlled environment"" using UV and ""white"" LED's... with disappointing results.

I am used to working with natural and IR light. I suppose adding blue, green, yellow and red LED's to the mix could produce a better [spectrum](http://bmb.lcd.lu/science/230VAC_LED/led_spectra.jpg). Or adding a Halogen lamp?

How would you do it?
",en
1106290,2010-09-07 18:08:03,computervision,BMVC2010 Proceedings and 1 page abstract booklet,dan2q,CmdrSammo,,https://www.reddit.com/r/computervision/comments/dan2q/bmvc2010_proceedings_and_1_page_abstract_booklet/,4.0,1.0,,en
1106291,2010-09-08 04:46:55,MachineLearning,Anyone here update wikipedia's ML topics? What have you contributed to?,daw0b,bciguy,1275791009.0,https://www.reddit.com/r/MachineLearning/comments/daw0b/anyone_here_update_wikipedias_ml_topics_what_have/,7.0,11.0,And how could they be further improved?,en
1106292,2010-09-08 17:45:49,analytics,Five Analytics Metrics You Can't Afford To Miss ,db4c3,emmaseo,1282162534.0,https://www.reddit.com/r/analytics/comments/db4c3/five_analytics_metrics_you_cant_afford_to_miss/,1.0,0.0,,en
1106293,2010-09-09 00:40:20,statistics,"New to stats, easy question.",dbaue,[deleted],,https://www.reddit.com/r/statistics/comments/dbaue/new_to_stats_easy_question/,1.0,0.0,"Population is 1000 people. Sample is 100 people. I need at least 50% of the population to buy my ponies. If I ask the sample if they will buy my ponies, what is the probability that the sample result will match the population result? Or, what is the probability that the sample will report  less than 50% pony buyers while the population is actually greater than 50%?

How do I solve this?",en
1106294,2010-09-09 11:04:14,statistics,Tips for the R beginner (a 5 page overview),dbi3k,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/dbi3k/tips_for_the_r_beginner_a_5_page_overview/,22.0,2.0,,en
1106295,2010-09-10 00:00:11,statistics,How was the normal distribution discovered/invented?,dbrvi,fraggle1,1280751608.0,https://www.reddit.com/r/statistics/comments/dbrvi/how_was_the_normal_distribution_discoveredinvented/,13.0,12.0,"I consider myself a fairly educated and intelligent person, but I've never understood this in statistics ... 

In intro stats courses, you learn about normal distributions, and how they show up all the time in everyday life (i.e. if you measure the heights of a lot of people, they will be approximately normally distributed).  

But what's the history of this function?  I can think of three possibilities: 

* Scientists and/or mathematicians had a lot of data (of normally distributed variables like height) and noticed that when you plotted the frequencies, they tended to converge to a specific function.  
* Instead, they noticed patterns in the data (2/3 of the data fall within one sd of the mean, almost all fall within three sd of the mean, etc...) and wrote a mathematical equation that fit those parameters.  
* They already had this equation for some reason, then noticed that real-word data tended to follow it.

In the first two scenarios, the equation was designed to fit data, and in the third, the data was found to fit the equation.

In high school, I asked my stats teacher, and she didn't really give me an answer.
",en
1106296,2010-09-10 08:23:17,analytics,Mashable Feature Article on SeeVolution Real Time Analytics,dby7b,Quadw,1257465554.0,https://www.reddit.com/r/analytics/comments/dby7b/mashable_feature_article_on_seevolution_real_time/,1.0,0.0,,en
1106297,2010-09-11 17:09:46,statistics,R parallel processing backend for {plyr} with windows,dchqp,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/dchqp/r_parallel_processing_backend_for_plyr_with/,0.0,0.0,,en
1106298,2010-09-12 20:09:24,statistics,The Measure of a Measure | Stats With Cats Blog,dcut2,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/dcut2/the_measure_of_a_measure_stats_with_cats_blog/,4.0,0.0,,en
1106299,2010-09-13 05:33:15,statistics,Expectation after sampling and truncated gaussians,dd0m3,[deleted],,https://www.reddit.com/r/statistics/comments/dd0m3/expectation_after_sampling_and_truncated_gaussians/,0.0,0.0,,en
1106300,2010-09-13 15:14:35,MachineLearning,"Workshop on Machine Learning for Social Computing (MLSC2010), held in conjunction with NIPS 2010",dd5wj,fbahr,1275413362.0,https://www.reddit.com/r/MachineLearning/comments/dd5wj/workshop_on_machine_learning_for_social_computing/,3.0,0.0,,en
1106301,2010-09-13 19:17:33,MachineLearning,Manifold Assumption versus Margin Assumption,dd967,hapagolucky,1278138338.0,https://www.reddit.com/r/MachineLearning/comments/dd967/manifold_assumption_versus_margin_assumption/,10.0,1.0,,en
1106302,2010-09-13 19:26:44,statistics,The future of R - pessimistic thoughts by R founder Ross Ihaka,dd9bc,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/dd9bc/the_future_of_r_pessimistic_thoughts_by_r_founder/,37.0,11.0,,en
1106303,2010-09-13 20:21:24,MachineLearning,askMachineLearnit: Clueless leading a ML group,dda46,[deleted],,https://www.reddit.com/r/MachineLearning/comments/dda46/askmachinelearnit_clueless_leading_a_ml_group/,9.0,6.0,"Hi all,

A group of students were really interested in trying to learn something about machine learning. We're gonna go through some of the Tibs/Hastie book. Does anyone have advice to weigh in on what we should cover?",en
1106304,2010-09-14 07:13:22,datasets,California c-section rates by hospital type,ddipj,[deleted],,https://www.reddit.com/r/datasets/comments/ddipj/california_csection_rates_by_hospital_type/,4.0,0.0,,en
1106305,2010-09-14 07:59:24,analytics,"TubeMogul Q2 Report: Facebook and Twitter are growing faster than the major search engines in terms of video streams referred. If current trends continue, Facebook, growing at 48.3% per month, will be second only to Google within the year.",ddj6n,salvage,1184143405.0,https://www.reddit.com/r/analytics/comments/ddj6n/tubemogul_q2_report_facebook_and_twitter_are/,3.0,1.0,,en
1106306,2010-09-14 17:58:49,MachineLearning,"“simply start over and build something better” | Ross Ihaka, the father of R",ddpj3,Troybatroy,1238986431.0,https://www.reddit.com/r/MachineLearning/comments/ddpj3/simply_start_over_and_build_something_better_ross/,18.0,0.0,,en
1106307,2010-09-15 13:44:56,MachineLearning,A profile of those who compete in machine learning competitions,de4f6,databuff,1272414378.0,https://www.reddit.com/r/MachineLearning/comments/de4f6/a_profile_of_those_who_compete_in_machine/,18.0,1.0,,en
1106308,2010-09-15 16:53:47,MachineLearning,What is black and White and eats like a horse? Oh and uses machine learning?,de6fu,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/de6fu/what_is_black_and_white_and_eats_like_a_horse_oh/,11.0,0.0,,en
1106309,2010-09-15 21:20:30,statistics,Benchmarking GNU R: DirkE's view and a Ninja wishlist,dear7,[deleted],,https://www.reddit.com/r/statistics/comments/dear7/benchmarking_gnu_r_dirkes_view_and_a_ninja/,0.0,0.0,,en
1106310,2010-09-15 23:44:37,statistics,Need help with a simple (?) stats question that I have been unable to understand for many years,ded1t,[deleted],,https://www.reddit.com/r/statistics/comments/ded1t/need_help_with_a_simple_stats_question_that_i/,8.0,6.0,"I'm really bad with statistics, despite having taken some undergrad stats courses. I have spent time trying to research on my own, but always end up confusing myself even more.

Most likely I'm really stupid or my brain just cannot process stat concepts or maybe my question is wrong.

Here are two (related?) questions:

* 1) Out of a population of 1,276,645 there are 408,527 actions performed. Or, in other words, 32% of the population performs the action. Now a segment of the population is chosen which meets some criteria A: let's say 525,432. Out of this population, 131,358 perform this same action or 25%.

Are the actions performed by the entire population and the segment statistically significantly different? How do I calculate this?

* 2) I have two sets of impressions and clicks for two ad creatives being tested. A/B testing basically.

How do I calculate if the click through rates are statistically different?

Thanks for reading this",en
1106311,2010-09-16 15:37:43,statistics,Analytics and Journals,deo3f,[deleted],,https://www.reddit.com/r/statistics/comments/deo3f/analytics_and_journals/,0.0,0.0,,en
1106312,2010-09-16 23:12:39,MachineLearning,AskML: How to predict football results?,dev5t,Tafkas,1231818191.0,https://www.reddit.com/r/MachineLearning/comments/dev5t/askml_how_to_predict_football_results/,11.0,9.0,"A have a huge data set of European football results (various leagues and seasons). The data is comma separated and looks like this:

Date,HomeTeam,AwayTeam,FTHG,FTAG,FTR,HTHG,HTAG,HTR and many more.

I want to use this data to create a predictor for future games. Is there any resource where I can start looking into? Or if anybody has a clue how to start I would be happy if he could elaborate.",en
1106313,2010-09-17 17:13:51,datasets,Linked Data - Billion Triple Challenge 2010 Dataset,df7k3,stirfry,1159452197.0,https://www.reddit.com/r/datasets/comments/df7k3/linked_data_billion_triple_challenge_2010_dataset/,10.0,0.0,,en
1106314,2010-09-17 18:48:38,MachineLearning,Predict Obstructive Nephropathy? I can't even spell Nefropathy,df90h,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/df90h/predict_obstructive_nephropathy_i_cant_even_spell/,5.0,1.0,,en
1106315,2010-09-17 19:10:24,MachineLearning,AskML: How to predict an articles topic (from a set amount of choices) using the articles top keywords,df9dk,cjoudrey,1256561859.0,https://www.reddit.com/r/MachineLearning/comments/df9dk/askml_how_to_predict_an_articles_topic_from_a_set/,10.0,10.0,"I'm currently working on a hobby project that deals with machine learning. My goal is to detect the topic (i.e. Technology, Health, etc..) of a given article based on it's top keywords.

My plan was to manually attribute a topic for a given amount of articles then write a script which would automatically classify other articles based keyword score.

For instance if I have an article who's top keywords are: ""machine learning"", ""artificial intelligence"" and I manually set the topic to ""Technology"" then those keywords would get +1 for technology.

The problem is whenever a new keyword appears, I won't be able to classify the article automatically as the keyword will be unknown to the system.

Is there another way of doing this?

Thanks :)",en
1106316,2010-09-19 21:01:16,statistics,It’s All in the Technique | Stats With Cats Blog,dg0mx,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/dg0mx/its_all_in_the_technique_stats_with_cats_blog/,7.0,0.0,,en
1106317,2010-09-19 21:50:28,statistics,A question on counting methods.,dg13p,sir_lagalot,1274855145.0,https://www.reddit.com/r/statistics/comments/dg13p/a_question_on_counting_methods/,4.0,2.0,"Here is the question:  
A game is played where a dice is rolled until the number '3' appears. How many elements of the sample space correspond to the event that the 3 appears not later than the *k*th roll of the die.  
My sol'n: [;\sum_{n=0}^{k-1}5^n;]  
Textbook sol'n: [;\frac{5^k - 1}{4};]  
Both solutions produce the same results, but how do I arrive at the textbook solution?",en
1106318,2010-09-20 02:44:50,statistics,Question about experiment design: Do I need a Bonferroni correction?,dg450,oozie_nelson,1282067741.0,https://www.reddit.com/r/statistics/comments/dg450/question_about_experiment_design_do_i_need_a/,2.0,4.0,"Let's say I have 3 quantitative measurements from an experiment: call them A, B, and C. I also have 2 conditions: 1 and 2. I want to know how measurements A, B, and C differ between conditions. Crucially, I don't care if the measurements differ from each other within or between conditions; they are essentially unrelated. All I want to know is whether A is different in 1 and 2, B is different in 1 and 2, and C is different in 1 and 2. Is it acceptable to do 3 t-tests without Bonferroni correction (or some other means of reducing type-1 errors)? 

Thank you!",en
1106319,2010-09-20 13:04:58,statistics,Rattle Re-Introduced,dgaa3,alphakid,1284149221.0,https://www.reddit.com/r/statistics/comments/dgaa3/rattle_reintroduced/,2.0,0.0,,en
1106320,2010-09-20 16:21:22,statistics,JMP 9 releasing on Oct 12,dgcao,[deleted],,https://www.reddit.com/r/statistics/comments/dgcao/jmp_9_releasing_on_oct_12/,1.0,0.0,,en
1106321,2010-09-20 19:28:08,statistics,September Roundup by Revolution,dgf0r,[deleted],,https://www.reddit.com/r/statistics/comments/dgf0r/september_roundup_by_revolution/,0.0,0.0,,en
1106322,2010-09-20 20:40:54,statistics,R syntax highlighting for bloggers who use WordPress.com,dgg7h,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/dgg7h/r_syntax_highlighting_for_bloggers_who_use/,9.0,0.0,,en
1106323,2010-09-21 00:19:09,statistics,estimating the mean of a population from a sample of a sample,dgjmz,afithian,1257576079.0,https://www.reddit.com/r/statistics/comments/dgjmz/estimating_the_mean_of_a_population_from_a_sample/,5.0,10.0,"Hi /r/stats,

Let's say you take a sample S from a population of values. Then you estimate the mean and variance of S non-parametrically by bootstrapping S 1000 times. Is it possible to use this estimate of the mean and variance of S to estimate the mean and variance of the population?",en
1106324,2010-09-21 06:24:14,statistics,How do I teach myself statistics?,dgnxs,LiveBackwards,1179880213.0,https://www.reddit.com/r/statistics/comments/dgnxs/how_do_i_teach_myself_statistics/,10.0,13.0,"I'm in a required statistics course, and quite frankly, the teacher is dry, boring, and annoying. It doesn't help that the class is at 8 AM. I managed to get into a second year college course through AP credits from high school, but that was years ago now. My statistics is very rusty. The thing is, I'm really interested in learning statistics, but there must be a better way than attending this class at 8am and being both bored and annoyed. I need a refresher on introductory statistics and some solid reading on intermediate statistics. Reddit, can you point me towards good resources to learn statistics on my own?",en
1106325,2010-09-21 13:35:25,MachineLearning,"I am the founder of Kaggle, a platform for machine learning competitions. Happy to answer any questions",dgs8w,kaggle,1285064973.0,https://www.reddit.com/r/MachineLearning/comments/dgs8w/i_am_the_founder_of_kaggle_a_platform_for_machine/,40.0,31.0,"For those who haven't come across the platform before (http://kaggle.com), companies and researchers post their problems and have data scientists from all over the world compete to produce the most accurate models. 

Crowdsourcing data modelling is particularly effective because there are an infinite number of approaches that applied to any problem. By opening up a problem to a wide audience, the competition host quickly gets to the best that can be done given the inherent noise and richness of a dataset",en
1106326,2010-09-22 02:03:07,statistics,Excellent Regression Analysis Text for FREE online,dh2v0,sirsosay,1254514768.0,https://www.reddit.com/r/statistics/comments/dh2v0/excellent_regression_analysis_text_for_free_online/,14.0,0.0,,en
1106327,2010-09-22 06:57:02,MachineLearning,Regretting the dead (design of medical trials),dh6cw,secret_town,1279292087.0,https://www.reddit.com/r/MachineLearning/comments/dh6cw/regretting_the_dead_design_of_medical_trials/,11.0,0.0,,en
1106328,2010-09-22 16:18:13,data,The SMAQ stack for big data,dhbpu,macslocum,1149863955.0,https://www.reddit.com/r/data/comments/dhbpu/the_smaq_stack_for_big_data/,1.0,0.0,,en
1106329,2010-09-22 20:49:09,statistics,Top 10 Graphical User Interfaces in Statistical Software,dhfzg,[deleted],,https://www.reddit.com/r/statistics/comments/dhfzg/top_10_graphical_user_interfaces_in_statistical/,0.0,0.0,,en
1106330,2010-09-23 02:34:27,statistics,What math is necessary to understand statistical models?,dhl2c,punkideas,1282856760.0,https://www.reddit.com/r/statistics/comments/dhl2c/what_math_is_necessary_to_understand_statistical/,9.0,12.0,"I was wondering which mathematical classes/videos would be useful for understanding the underlying math of statistical models.  Specifically, I'm interested in the GLM, SEM, and bootstrapping.

The reason why is I'm planning on applying to quantitative psychology and I would like to expand my mathematics background in directions that would be helpful.  Also, good sources for math stats stuff would be appreciated.

EDIT:
I should have probably included what I do know.  I did take calculus *years* ago but really only remember basic derivatives.  I'm great with algebra, so that's not a problem.",en
1106331,2010-09-24 20:31:31,analytics,20onFive new episode. This week we interview Phil Sawyer on advertising effectiveness. Last week: Joselin Mane,dif1i,mnic001,1176491444.0,https://www.reddit.com/r/analytics/comments/dif1i/20onfive_new_episode_this_week_we_interview_phil/,1.0,0.0,,en
1106332,2010-09-25 23:57:40,MachineLearning,best resource to learn wavelet transforms?,diux0,bciguy,1275791009.0,https://www.reddit.com/r/MachineLearning/comments/diux0/best_resource_to_learn_wavelet_transforms/,14.0,8.0,Im studying EEG for my PhD. What is the best resource out there for wavelets?,en
1106333,2010-09-26 02:37:02,MachineLearning,A Taxonomy of Data Science,diwg1,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/diwg1/a_taxonomy_of_data_science/,18.0,3.0,,en
1106334,2010-09-26 09:41:12,statistics,Applied Statistics Project,dj02s,[deleted],,https://www.reddit.com/r/statistics/comments/dj02s/applied_statistics_project/,0.0,2.0,,en
1106335,2010-09-26 15:49:16,statistics,It was Professor Plot in the Diagram with a Graph | Stats With Cats Blog,dj28l,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/dj28l/it_was_professor_plot_in_the_diagram_with_a_graph/,2.0,2.0,,en
1106336,2010-09-26 18:00:00,statistics,Number of world's hungry falls below 1 Billion! Time to put 1 billion back into perspective,dj38l,jmsjoin,1161441718.0,https://www.reddit.com/r/statistics/comments/dj38l/number_of_worlds_hungry_falls_below_1_billion/,0.0,0.0,,en
1106337,2010-09-27 15:20:27,MachineLearning,How I did it: Lee Baker on winning the tourism forecasting competition,djgag,antgoldbloom,1285588580.0,https://www.reddit.com/r/MachineLearning/comments/djgag/how_i_did_it_lee_baker_on_winning_the_tourism/,0.0,0.0,,en
1106338,2010-09-27 15:33:20,statistics,How I did it: Lee Baker on winning the tourism forecasting competition,djgfo,antgoldbloom,1285588580.0,https://www.reddit.com/r/statistics/comments/djgfo/how_i_did_it_lee_baker_on_winning_the_tourism/,9.0,0.0,,en
1106339,2010-09-27 23:20:45,analytics,Track ROI with Google Analytics Goals &amp; Funnels,djnhc,[deleted],,https://www.reddit.com/r/analytics/comments/djnhc/track_roi_with_google_analytics_goals_funnels/,0.0,0.0,,en
1106340,2010-09-29 00:50:07,MachineLearning,Adaptive algorithm for non-stationary k-armed bandit?,dk7bn,cypherx,1141075032.0,https://www.reddit.com/r/MachineLearning/comments/dk7bn/adaptive_algorithm_for_nonstationary_karmed_bandit/,4.0,4.0,"Does anyone know of papers or tutorials which give a good algorithm for estimating expected reward (or at least probability of best reward) over k possible actions? The reward for each arm is (obviously) stochastic. The randomness is not stationary, it depends on some time-varying hidden variable. 

Thanks for any tips. ",en
1106341,2010-09-29 02:24:30,MachineLearning,Good Freely Available Textbooks on Machine Learning ,dk8hs,fregoli,1253518766.0,https://www.reddit.com/r/MachineLearning/comments/dk8hs/good_freely_available_textbooks_on_machine/,35.0,2.0,,en
1106342,2010-09-30 00:19:58,statistics,Approaching Statistics Take 3,dkp2w,tecratour,1231054623.0,https://www.reddit.com/r/statistics/comments/dkp2w/approaching_statistics_take_3/,2.0,8.0,"Hey r/Stats!

I'm a college guy who is now attempting to learn statistics for the 3rd time.  1st time was in h/s, dropped halfway through the year with a B-.  2nd time was a year and a half in a college stats course, ended with a B-.  Both times I felt like a monkey following orders.

Now I need to know stats for my major (economics) and I'm lost.  Everytime I look at stats I feel like its a big joke that I'm not apart of.  I can't seem to find any resources which approach it in ways I can relate.  I'm not a dumb guy, some things are intuitive and some aren't, stats being at the far end of un-relatable.  

Where can I get a breakdown of stats which is straightforward and easy to relate (IE- the connection of confidence intervals, z scores, t tests, null/alternative hypothesises(?), degrees of freedom, ect.)  We're also using ANOVA.

TL;DR- I'm a fucktard with statistics :P  Help me r/statistics, you're my only hope.",en
1106343,2010-09-30 08:23:30,artificial,A Neural net attempts to pilot a spacecraft through a tunnel,dkv5j,nhnifong,1278453901.0,https://www.reddit.com/r/artificial/comments/dkv5j/a_neural_net_attempts_to_pilot_a_spacecraft/,14.0,3.0,,en
1106344,2010-09-30 18:46:18,MachineLearning,The Data Science Venn Diagram,dl1s0,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/dl1s0/the_data_science_venn_diagram/,10.0,0.0,,en
1106345,2010-09-30 21:00:37,MachineLearning,"Algorithm Needed; $25,000 Reward [Hearst Challenge: Predict magazine sales at newsstands]",dl3yg,fbahr,1275413362.0,https://www.reddit.com/r/MachineLearning/comments/dl3yg/algorithm_needed_25000_reward_hearst_challenge/,13.0,0.0,,en
1106346,2010-10-01 03:38:30,statistics,Can anyone recommend a good book for Bayesian Inference?,dl9or,[deleted],,https://www.reddit.com/r/statistics/comments/dl9or/can_anyone_recommend_a_good_book_for_bayesian/,0.0,0.0,,en
1106347,2010-10-01 04:19:30,statistics,ManyEyes is an awesome visualizer for your data/statistics,dla6w,snipewiz,1256189534.0,https://www.reddit.com/r/statistics/comments/dla6w/manyeyes_is_an_awesome_visualizer_for_your/,8.0,0.0,,en
1106348,2010-10-01 06:28:53,computervision,A project I'm working on at my university - Object Recognition,dlbsh,Mad_Gouki,1191794182.0,https://www.reddit.com/r/computervision/comments/dlbsh/a_project_im_working_on_at_my_university_object/,11.0,7.0,,en
1106349,2010-10-01 14:19:29,MachineLearning,"Obsessed With Genes (Not Jeans), This Teen Analyzes Family DNA",dlg10,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/dlg10/obsessed_with_genes_not_jeans_this_teen_analyzes/,7.0,2.0,,en
1106350,2010-10-01 18:16:24,statistics,The Data Science Venn Diagram,dlivs,agconway,1228006618.0,https://www.reddit.com/r/statistics/comments/dlivs/the_data_science_venn_diagram/,14.0,0.0,,en
1106351,2010-10-01 20:50:18,computervision,Gamma Correction: Why It Is Important for Realtime 3D,dllb4,JeGX,1198848357.0,https://www.reddit.com/r/computervision/comments/dllb4/gamma_correction_why_it_is_important_for_realtime/,0.0,0.0,,en
1106352,2010-10-01 22:49:05,statistics,Best practices for managing a statistical analysis project ,dln33,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/dln33/best_practices_for_managing_a_statistical/,1.0,0.0,,en
1106353,2010-10-01 23:12:03,datasets,where can i found movies ticket sales data per city?,dlnf7,[deleted],,https://www.reddit.com/r/datasets/comments/dlnf7/where_can_i_found_movies_ticket_sales_data_per/,7.0,1.0,,en
1106354,2010-10-03 05:10:54,statistics,Any good book about the history of statistics?,dm2u6,ivanvladimir,1186706979.0,https://www.reddit.com/r/statistics/comments/dm2u6/any_good_book_about_the_history_of_statistics/,8.0,11.0,"Does any one can recommend a book about the history of statistics? I'm preparing a class, and I would like to give some insight about it during the class, and on the process I would like to learn about the developing of the field.

Thanks!",en
1106355,2010-10-03 20:20:09,statistics,Assuming the Worst | Stats With Cats Blog,dm9e9,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/dm9e9/assuming_the_worst_stats_with_cats_blog/,0.0,0.0,,en
1106356,2010-10-04 01:13:21,MachineLearning,How good is the Cross Entropy Method for large-scale optimization?,dmci2,cypherx,1141075032.0,https://www.reddit.com/r/MachineLearning/comments/dmci2/how_good_is_the_cross_entropy_method_for/,13.0,1.0,"Has anyone used the [Cross Entropy Method](http://en.wikipedia.org/wiki/Cross-entropy_method) to estimate the parameters of very large models? I'm curious how it performs when compared with more common techniques like stochastic gradient descent, coordinate descent, and algorithm-specific optimizations like SMO. Does it have the same wandering/slow convergence rate as evolutionary and annealing algorithms? ",en
1106357,2010-10-04 03:37:40,MachineLearning,How do you deal with grid search / cross validation data?,dme2x,anonygiraffe,1286150034.0,https://www.reddit.com/r/MachineLearning/comments/dme2x/how_do_you_deal_with_grid_search_cross_validation/,8.0,8.0,"I'm running some extensive tests on the effect of hyperparameters of a learning algorithm on a particular dataset. At each training epoch, I log various measures such as the current error. This is obviously generating a lot of data (roughly 500 training epochs per experiment, and there are over one thousand of those).

Some questions I am interested in answering are:

* Which hyperparameter combinations did the best overall, averaged over all cross validations?

* Did any of the individual experiments seem like they hadn't converged by the end of training? (""should I increase the maximum number of epochs?"")

* How much variation did the hyperparameters have across different folds?

* What's the general trend of a particular subset of hyperparameters if we fix the others?

All fairly basic questions. For now I have homebrewed scripts to answer most of these, but it would be nice to see what other people are using. The only other approach I tried was OpenOffice Calc but it didn't feel very intuitive and in the end didn't save me any time vs modifying scripts by hand.",en
1106358,2010-10-04 17:04:54,MachineLearning,So how many Redditors are doing the Google AI Challenge? What are your rankings at the moment?,dmm20,TheWalruss,1252475914.0,https://www.reddit.com/r/MachineLearning/comments/dmm20/so_how_many_redditors_are_doing_the_google_ai/,26.0,7.0,,en
1106359,2010-10-04 21:30:35,statistics,Reddit! Help me out with these stats problems! ,dmq0e,SCHOOLIMANGOOL,1286215718.0,https://www.reddit.com/r/statistics/comments/dmq0e/reddit_help_me_out_with_these_stats_problems/,0.0,2.0,"For the life of me. I don't know why I can't find the solution to these problems. 

Assume that 55% of the actual voters in election for state governor support the incumbent governor (p = 0.55). Calculate the probability of observing a sample proportion of voters 0.54 or less supporting the incumbent governor. 

You want to estimate the proportion of customers who are satisfied with their supermarket at level of confidence of .92, and within .025 of the true value.  How large of a sample is needed? 

You want to estimate the proportion of customers who are satisfied with their supermarket at level of confidence of .94, and within .025 of the true value. It has been estimated that p is between .60 and .85. How large of a sample is needed? 

Thanks for the help!",en
1106360,2010-10-05 08:02:41,artificial,"""Ask HN: So what's new in the world of A.I.?""",dmybw,secret_town,1279292087.0,https://www.reddit.com/r/artificial/comments/dmybw/ask_hn_so_whats_new_in_the_world_of_ai/,17.0,0.0,,en
1106361,2010-10-05 18:10:16,statistics,"Saw this in /r/math, thought /r/statistics would enjoy",dn4gf,seregygolovogo,1262112658.0,https://www.reddit.com/r/statistics/comments/dn4gf/saw_this_in_rmath_thought_rstatistics_would_enjoy/,24.0,2.0,,en
1106362,2010-10-06 02:53:25,statistics,"Hi Reddit, I have got a few questions for my assignment. I can't seem to solve 9 of these. I have no other way of getting help for these. I find them really tough. Please help me solve them. ",dnbv7,[deleted],,https://www.reddit.com/r/statistics/comments/dnbv7/hi_reddit_i_have_got_a_few_questions_for_my/,0.0,1.0,"1. Suppose S is uncountable. Show that it is impossible that P({s}) &gt; 0 for every s that belongs to S.

2. Suppose we choose a positive integer at random, according to some unknown probability distribution. Suppose we know that P({1, 2, 3, 4, 5}) = 0.3 and that P({4, 5, 6}) = 0.4 and that P({1}) = 0.1. What are the largest and smallest possible values of P({2})?

3. Suppose there are C people, each of whose birthdays (month and day only) are equally likely to fall on any of the 365 days of a normal year. Suppose C greater than or equal to 2.
a. What is the probability that all C people have the same exact birthday?
b. What is the probability that some pair of the C people have the same exact birthday?

4. Let A and B be events of positive probability. Prove that P(A|B) &gt; P(A) if and only if P(B|A) &gt; P(B).

5. Let P be some probability measure on sample space S = [0, 1].
a. Prove that we must have lim(n--&gt;1) P((0, 1/n)) = 0.
b. Show by example that we might have lim(n--&gt;1) P([0, 1/n)) &gt; 0.

6. Let X be a random variable.
a. Is it necessarily true that there is some real number c such that (X + c) is greater than or equal to 0?
b. Suppose the sample space S is finite. Then is it necessarily true that there is some real number c such
that (X + c) is greater than or equal to 0?

7. Suppose Alice flips three fair coins, and let X be the number of heads showing. Suppose Barbara flips five fair coins, and let Y be the number of heads showing. Let Z = X - Y . Compute P(Z = k) for every real number k.

8. Consider the function given by f(x) = (e^|x|)/2 for -infinity &lt; x &lt; +infinity. Prove that f is a density function.

9. Let X be a random variable, with cumulative distribution function Fx. Prove that P(X = a) = 0 if and only if the function Fx is continuous at a.",en
1106363,2010-10-06 06:26:51,statistics,A question regarding hypothesis testing and measurement errors.,dneci,heresubwaysandwich,1234841131.0,https://www.reddit.com/r/statistics/comments/dneci/a_question_regarding_hypothesis_testing_and/,1.0,2.0,"Perhaps one of you fine gentle-persons can help me with this..

Is it possible (or required) to include measurement errors when comparing two means of a measured variable for which the measurement error varies between each measurement?

e.g. I am measuring ""X"" with an instrument, and the result is a plot over time of ""X"" that changes predictably.  The instrument software fits the data to a known function, and extracts a number of variables from this best fit that I am interested in comparing among different experimental treatments.  However, there is noise in the data, resulting in sometimes large errors for the fitted variables.

It seems intuitive that these measurement errors should somehow be included in the hypothesis test.  Am I correct?  If so, what is the most appropriate way to compare the means?",en
1106364,2010-10-06 15:18:30,statistics,Break Our Steganographic System!,dnjfx,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/dnjfx/break_our_steganographic_system/,5.0,0.0,,en
1106365,2010-10-06 19:40:05,statistics,Need Ideas for Statistics Project,dnmzn,alwaystherookie,1280621068.0,https://www.reddit.com/r/statistics/comments/dnmzn/need_ideas_for_statistics_project/,2.0,11.0,"I really need ideas for a statistics experiment.  It needs to have two factors that have an effect on a response variable.  It has to be an experiment, examples of what it can't be are below.  The proposal for this is due today, but I have the rest of the semester to actually complete it.  I am drawing blanks on what to do though.  Thanks for any help.

Examples of projects that are NOT acceptable:

   1. Observational studies, surveys, opinion polls, etc.
   2. Anything to do with guns or alcohol.
   3. Anything trivial like dropping or rolling balls. Pure physics experiments are not interesting when you know the answer. (if you think you have a new wrinkle, make a proposal.)
   4. Direct comparison of sorting algorithms. (I'm getting tired of these; if you think you have a new wrinkle, make a proposal.)
   5. Time to boil different liquids (too boring, unless you have a novel twist).
   6. Gas mileage for different octanes (too hard to carry out, except on model planes or cars).
   7. Anything to do with popcorn (I'm tired of these,if you think you have a new wrinkle, make a proposal.))
",en
1106366,2010-10-06 23:32:33,statistics,Creating GUIs in R with gWidgets ,dnqor,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/dnqor/creating_guis_in_r_with_gwidgets/,9.0,4.0,,en
1106367,2010-10-07 15:38:18,statistics,"Cointegration in Panel tests on STATA, any ideas?",do18n,Enti_San,1271737857.0,https://www.reddit.com/r/statistics/comments/do18n/cointegration_in_panel_tests_on_stata_any_ideas/,4.0,1.0,I m working on my masters dissertation and will be using STATA for the empirical part. So are there any good books about recent works on Cointegration and Stationarity in Panel Data and their application on STATA?,en
1106368,2010-10-07 20:13:18,MachineLearning,Contest to build an R package recommendation engine,do56r,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/do56r/contest_to_build_an_r_package_recommendation/,19.0,0.0,,en
1106369,2010-10-07 20:14:12,statistics,Contest to build an R package recommendation engine,do57a,agconway,1228006618.0,https://www.reddit.com/r/statistics/comments/do57a/contest_to_build_an_r_package_recommendation/,11.0,0.0,,en
1106370,2010-10-07 22:04:51,analytics,"Just posted an interview with Judah Phillips, Host of Web Analytics Wednesdays (#WAW) in Boston. Any feedback?",do72y,mnic001,1176491444.0,https://www.reddit.com/r/analytics/comments/do72y/just_posted_an_interview_with_judah_phillips_host/,1.0,0.0,,en
1106371,2010-10-08 00:10:53,statistics,Help with a simple beginning Stats question?,do901,NTesla,1277807741.0,https://www.reddit.com/r/statistics/comments/do901/help_with_a_simple_beginning_stats_question/,4.0,11.0,"I understand how to use Standard Deviation in an analysis. My problem is with the SD equation itself. I guess what I'm asking is how is the SD equation derived? Why do we square the distances from the mean? What's the purpose of squaring? Why not use the actual values? If it's to get positive values, then why not just use absolute values? Do we average the variance to get a more even dispersion somehow?  
  
Also, Z-score is said to be however many SDs away from the mean. Then one unit-SD would be equal to the Z-score. If that's true, then why is the equation for Z-score (X-XBar)/SD? it should just be one SD unit. See why I'm confused?  
  
I can take all this at face value and just work with it; it's not a problem. But it's kind of like doing fractions: If I don't understand that 1/4 means 'one divided by 4' I can learn many procedures for
manipulating fractions, but I'll never understand what I'm really doing, and more importantly, I'll never know when to apply a certain manipulation. Am I being way off-the-wall with this?  
Thanks!",en
1106372,2010-10-08 02:29:12,statistics,Which two quantitative variables should I test for correlation?,doaxw,watermelondrea,1256264985.0,https://www.reddit.com/r/statistics/comments/doaxw/which_two_quantitative_variables_should_i_test/,0.0,2.0,"I'll be conducting a survey/investigation in the near future. I will choose ANY two quantitative variables to find from random people or things in order to find a possible correlation. My question to you is, which two variables should I compare? It would be best to yield the highest possible correlation from two seemingly unrelated variables, but I'll compensate on the unrelated to a degree.",en
1106373,2010-10-08 17:59:31,MachineLearning,Christopher Bishop's really good Introduction to Machine Learning and Bayesian Inference. [Video lecture],dokyb,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/dokyb/christopher_bishops_really_good_introduction_to/,27.0,3.0,,en
1106374,2010-10-08 19:05:45,statistics,Failed Charting,dolyl,jeanlucpikachu,1159634949.0,https://www.reddit.com/r/statistics/comments/dolyl/failed_charting/,6.0,5.0,,en
1106375,2010-10-08 23:29:10,MachineLearning,Some questions about computational intelligence and metaheuristic algorithms,dopzt,jasonb,1145327805.0,https://www.reddit.com/r/MachineLearning/comments/dopzt/some_questions_about_computational_intelligence/,6.0,0.0,,en
1106376,2010-10-09 23:23:19,statistics,Two-way ANOVA written as a linear regression model in matrix notations,dp32i,DTOWN_holler,1285445886.0,https://www.reddit.com/r/statistics/comments/dp32i/twoway_anova_written_as_a_linear_regression_model/,5.0,3.0,"Y(ijk) = mu +alpha(i) + beta(j) + error(ijk)

How the hello do I do this?  The question asks to simply write a two-way ANOVA model without interactions in matrix notations (with respect to the parameters mu, alpha(i), and beta(j)).

Can anyone point me in the direction of a text or references where I can learn how to do this myself?  I am in desperate need of a good linear/matrix algebra book.  Does anyone have any suggestions?  

Thank you in advance.",en
1106377,2010-10-10 16:37:42,MachineLearning,R package recommendation engine contest now live on Kaggle,dpayt,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/dpayt/r_package_recommendation_engine_contest_now_live/,11.0,0.0,,en
1106378,2010-10-11 03:01:06,MachineLearning,Can somebody help me understand entropy and its relationship to information gain?,dphcd,[deleted],,https://www.reddit.com/r/MachineLearning/comments/dphcd/can_somebody_help_me_understand_entropy_and_its/,11.0,6.0,,en
1106379,2010-10-11 20:34:31,statistics,Perspectives on Objectives | Stats With Cats Blog,dpsob,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/dpsob/perspectives_on_objectives_stats_with_cats_blog/,0.0,0.0,,en
1106380,2010-10-12 00:28:08,MachineLearning,Ask ML: Question about Brain Imaging and Reinforcement learning,dpw7s,ketor,1279434772.0,https://www.reddit.com/r/MachineLearning/comments/dpw7s/ask_ml_question_about_brain_imaging_and/,9.0,15.0,"Is there any research going on that takes inputs from brain imaging and decides whether the stimulus that elicited this reaction in the brain should be given a ""reward"" or ""punishment"" in the algorithm?

EDIT: 

to be more specific: 
Suppose there is a reinforcement learning algorithm that we want to display a certain stimulus X, let us assume it is a picture of some sort. it begins by showing stimuli to the subject. the subject's brain responds positively if it perceives the current stimulus to be close to the stimulus it wants(X) and negatively otherwise. This response in the brain is tapped using some sort of a brain mapping technique like EEG/fmRi etc. and the positive reaction is given as a ""reward"" to the reinforcement learning algorithm and negative reaction is given as a ""punishment"".",en
1106381,2010-10-12 07:30:14,statistics,Need help deciding how to analyze my data (about measurement variability of an instrument),dq1av,MountainX,1212608471.0,https://www.reddit.com/r/statistics/comments/dq1av/need_help_deciding_how_to_analyze_my_data_about/,1.0,9.0,"I am collecting data to compare two similar instruments for their consistency within a sequence of measurements. I want to identify which of the instruments can make a series of measurements with less variability.

In each sequence of measurements with each instrument, I try to take 12 measurements. Of these 12, several will be disqualified (e.g., because the instrument reports the measurement as unreliable). I'm left with a variable number of measurements in each sequence. It can range from about 2 to 12, with the typical count being 6. The measurements are reported as continuous numbers (with 4 or 5 significant digits).

I have repeated these measurement sequences many times already and I have several thousand total measurements.

What is the best statistic to describe the variability within a sequence of measurements? I've considered range, SD, variance, aad, mad, and a few others. I have no idea which will be best.

(In fact, I'm not really sure if a sequence of measurements represents a population or a sample. I'm tending to think of it as a population because my data contains every single entity that came into existence.)

Once I have that statistic, what is the best way to determine which of the two instruments has less variability in this test -- given that the order in which the measurements are taken seems to matter? One instrument ""goes first"" and the other ""goes second."" Without doing any analysis yet, it seems like the instrument that goes second tends to report less variability. I have randomized (by flipping a coin) the order in which the instruments are used.

**EDIT:** I only realized part way through the data collection that order might matter. So even though I am randomizing the order now, that was not true for the earlier data.

So I believe I have two nominal/ordinal independent variables (instruments A &amp; B, and ""goes first"" vs ""goes second""). The dependent variable is continuous. So these seems to fit an anova problem. If so, is there some easy to use &amp; free software that I can plug my data into?

Any more advice? Thanks",en
1106382,2010-10-12 15:38:05,statistics,Transformation of probability density function?,dq5u5,omegga,1221435247.0,https://www.reddit.com/r/statistics/comments/dq5u5/transformation_of_probability_density_function/,3.0,4.0,"Can someone explain the following formula? http://imgur.com/6lyAd

I'm stuck at the meaning of |dr/dy|. What does that mean? Differential?",en
1106383,2010-10-12 20:47:20,statistics,[Question] Linear regression residuals are not normal.  What’s to be done?,dqa8x,0pt1m1st1c,1286900606.0,https://www.reddit.com/r/statistics/comments/dqa8x/question_linear_regression_residuals_are_not/,4.0,17.0,"Fellow humans,

I’m trying to fit a linear regression model to some business data.  At first the results looked really exciting.  The R-square was approx 50%  (that isn’t too bad - the data is pretty noisy).  The independent variables showing up as significant were reasonable.  What’s more, the coefficients seemed to make sense directionally!

Then I ran a check for normality of errors and found the data to be quite non-normal.  The Jarque Bera, DW and Anderson Darling tests all had a small p-value (less than 0.005).  The QQ plot looked nothing like a straight line.  I tried a few transformation of the dependent variable – sqrt, log, inverse and some variations of these – but nothing seemed to help.

(1) Are there any other transformations / tricks I can try to make residuals normal?

(2) Assuming the normality assumption can not be made to hold, can I still interpret the regression coefficients in the usual way.  All I really want to do is understand the impact of the dependent variables on the response.  I’m not planning to use the equation to make predictions etc.

The second question is slightly more urgent I guess, because I'm really beginning to doubt if it is possible to transform this data into normal.  

If you have references / advice / similar experiences - that would be really great. Thanks!


**Edit :**

I'm going to go ahead and interpret the regression coefficients in the usual way because, as pfunkman noted, the estimates should be asymptotically normal for a large modeling population.  My sample is pretty large.  Should've mentioned this earlier.


I also want to add that I am really glad I posted this on Reddit.  I got much more out of it than just this one answer:

* I learnt that I've forgotten most of what I learned in school, even though it was just 5 years ago.  Time to pull out those books again.

* I learnt that am not alone in my statistical endeavors, even if I'm the only statistically educated person in my team  Help and suggestions are at hand when needed. :)


",en
1106384,2010-10-12 22:09:16,statistics,Sample size minimum for stratified finite populations,dqbhg,cypherignite,1264226526.0,https://www.reddit.com/r/statistics/comments/dqbhg/sample_size_minimum_for_stratified_finite/,4.0,3.0,"I have a question about sampling and I would appreciate any theoretical and applied help anyone can provide me with.

I have a survey that asks a yes/no question and I want to sample a portion of the population being looked at to determine the prevalence of this quality. The population is finite and has been stratified. I don’t have any estimates about the mean or variance of this statistic within the population as a whole or each stratum. I have no preexisting knowledge as this is the first time this has been looked at. Other than we think 90% will not have some trait and 10% will.

If I wanted to be 95% confident that the population mean within a stratum is plus/minus 5% of what I observe, how would I determine the minimum sample size necessary within each stratum?

Would something like this be applicable http://www.surveysystem.com/sample-size-formula.htm?

Using the notation on that page:
 SS=[(1.96^2)*(.9)*(1-.1)]/(.05^2)=138

And the new sample size with a finite population (just use a population of 100 as an example)

New SS=138/(1+(138-1)/100)=58

So I would need to sample 58 subjects from a population of 100.
Is this a legitimate method to use? Can I use this for each stratum to determine the minimum number to sample from each? What is the theoretical reason behind the above formula? And is this a meaningful way to design a sampling plan and talk about the results?
",en
1106385,2010-10-14 04:35:51,statistics,"Dear r/statistics, tell me about what you do",dqy79,ranksum,1271160610.0,https://www.reddit.com/r/statistics/comments/dqy79/dear_rstatistics_tell_me_about_what_you_do/,23.0,64.0,"I'm a little curious about the average reddit statistician. What do you do? How did you get involved in statistics? What's your educational background? What kind of software do you use? I know you've been waiting to spill the beans on your awesome data-analysis life, so do it! ",en
1106386,2010-10-14 05:05:54,statistics,Help! How to read the student_t table?,dqykd,Pinky008,1287021820.0,https://www.reddit.com/r/statistics/comments/dqykd/help_how_to_read_the_student_t_table/,1.0,3.0,"I'm having trouble with a stats assignment and need some help. I need to find the 95% confidence interval of the difference of two means for the following data: X1 = 14.21, SD1 = 8, n1 = 38, X2 = 17.13, SD2 = 8.49, n2 = 60. So far I've calculated SD(X1 - X2) = 1.72, X(X1 - X2) = -2.92, t(X1 - X2) = -1.7 and df = 29.16. I think those are the correct values but I'm having trouble reading the table and I'm not sure how to calculate the confidence interval. I know it needs to be in the form (X1 - X2) - e &lt; (X1 - X2) &lt; (X1 - X2) - e. I know how to do this for the normal distribution but I don't know how to find what e is for the student_t distribution and I can't seem find any examples, only theory and notation which isn't making sense to me. Could someone please provide me with an example or tell me how to read the t table or how to calculate e? Thanks.",en
1106387,2010-10-14 22:50:23,statistics,Continuous (Markov) matrices (?) and brane movement... kind of interesting.,draye,[deleted],,https://www.reddit.com/r/statistics/comments/draye/continuous_markov_matrices_and_brane_movement/,1.0,1.0,,en
1106388,2010-10-14 23:15:54,statistics,"""How I won the KDD Cup 2009 (using R)"" (~35 min video)",drb9k,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/drb9k/how_i_won_the_kdd_cup_2009_using_r_35_min_video/,1.0,0.0,,en
1106389,2010-10-15 01:38:29,statistics,how to analyze user behavior from an online game?,drdad,statsnewb,1287094307.0,https://www.reddit.com/r/statistics/comments/drdad/how_to_analyze_user_behavior_from_an_online_game/,7.0,9.0,"I'm a programmer with no background in stats. Lately I've been making reports with all sorts of data from our game but they're all pretty ""dumb"". I'm not really analyzing the data, I'm just aggregating it by various keys and presenting the results.

We track all kinds of things that the players do in the game and I want to figure out which of those things get players to spend money and I honestly have no clue where to begin.

I have hundreds of data points about each customer consisting of all the things they've done and their demographic info. I can easily massage this data into whatever format is best suited for analysis.

I can do this analysis in Python, Perl, R, C, whatever. I've never used R before but it doesn't seem difficult to figure out.


So, that is my situation. I've been scouring the internet for information about what I can do and so far here is what I've learned.


I've been scouring the internet and I have a little bit of an idea what I need to do but I'm not sure if it's right. It seems like I need to do multivariate analysis (http://en.wikipedia.org/wiki/Multivariate_analysis) since I have a lot of different variables.

I saw this page about multivariate statistics in R but it's got a lot going on and I have no clue which of those things I should try.
http://cran.r-project.org/web/views/Multivariate.html


Am I on the right track?

Is this stuff too complicated for someone with no background in stats to try and tackle?

I know that researchers at Universities have done this type of analysis, maybe I should reach out to them and see if they're interested in my data.

http://arstechnica.com/science/news/2009/02/aaas-60tb-of-behavioral-data-the-everquest-2-server-logs.ars
http://www.gamasutra.com/view/feature/2816/better_game_design_through_data_.php
http://blogs.parc.com/playon/",en
1106390,2010-10-15 06:18:05,statistics,"Hey, r/statistics, what was your FAVORITE statistics class in either undergrad or graduate school and why? i want to take that class!",drglh,gabjuasfijwee,1260073537.0,https://www.reddit.com/r/statistics/comments/drglh/hey_rstatistics_what_was_your_favorite_statistics/,7.0,24.0,,en
1106391,2010-10-15 11:50:32,statistics,The appendices removed from patients with Albanian names in six Greek hospitals were more than three times as likely to be perfectly healthy,drjvt,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/drjvt/the_appendices_removed_from_patients_with/,13.0,0.0,,en
1106392,2010-10-15 11:50:48,MachineLearning,The appendices removed from patients with Albanian names in six Greek hospitals were more than three times as likely to be perfectly healthy,drjvy,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/drjvy/the_appendices_removed_from_patients_with/,3.0,0.0,,en
1106393,2010-10-15 16:28:04,MachineLearning,Thoughts on data visualization at Dataists,drm9v,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/drm9v/thoughts_on_data_visualization_at_dataists/,19.0,1.0,,en
1106394,2010-10-16 05:37:22,datasets,Ask /r/datasets: new mover information?,drwos,ralphc,1148489729.0,https://www.reddit.com/r/datasets/comments/drwos/ask_rdatasets_new_mover_information/,4.0,0.0,"I'm looking for a way to collect new mover and/or house purchase information for 3 or 4 zipcodes near my church, for postcard sending purposes. With some searching I've seen that the post office guards this information and sells it to third parties for large amounts of money, which sells this service to regular folks. We're on a tight budget, and if possible I'd like to collect this information myself and I don't mind some data massaging and extraction, with trying one of the services as an option if it's not possible.",en
1106395,2010-10-17 01:07:24,MachineLearning,Limitations of Locality,ds6q5,danger_t,1249412021.0,https://www.reddit.com/r/MachineLearning/comments/ds6q5/limitations_of_locality/,9.0,0.0,,en
1106396,2010-10-17 20:30:37,statistics,"R-bloggers for other languages, wanna join? ",dsfrn,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/dsfrn/rbloggers_for_other_languages_wanna_join/,2.0,0.0,,en
1106397,2010-10-18 03:17:17,statistics,AskStats: Advice on going back to school for statistics,dsk4t,[deleted],,https://www.reddit.com/r/statistics/comments/dsk4t/askstats_advice_on_going_back_to_school_for/,1.0,6.0,,en
1106398,2010-10-18 13:05:02,statistics,[Question] How to show F-test for randomized block design reduces to paired t-test when number of treatments a=2,dsq5o,[deleted],,https://www.reddit.com/r/statistics/comments/dsq5o/question_how_to_show_ftest_for_randomized_block/,4.0,2.0,,en
1106399,2010-10-18 16:42:56,MachineLearning,Publications by googlers,dssf9,kzn,1162071679.0,https://www.reddit.com/r/MachineLearning/comments/dssf9/publications_by_googlers/,5.0,0.0,,en
1106400,2010-10-18 21:40:35,MachineLearning,Generating graphs of Twitter using R and Gephi (retweets and @-messages) ,dsxb3,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/dsxb3/generating_graphs_of_twitter_using_r_and_gephi/,13.0,2.0,,en
1106401,2010-10-18 21:48:00,statistics,Generating graphs of Twitter using R and Gephi (retweets and @-messages) ,dsxfv,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/dsxfv/generating_graphs_of_twitter_using_r_and_gephi/,1.0,0.0,,en
1106402,2010-10-19 10:40:11,statistics,Specific kind of graph in R/Matlab etc..,dt7s2,[deleted],,https://www.reddit.com/r/statistics/comments/dt7s2/specific_kind_of_graph_in_rmatlab_etc/,3.0,5.0,,en
1106403,2010-10-19 22:05:06,artificial,Could we replace a politician with an expert system?,dtgi9,[deleted],,https://www.reddit.com/r/artificial/comments/dtgi9/could_we_replace_a_politician_with_an_expert/,15.0,18.0,,en
1106404,2010-10-20 05:24:42,statistics,"Approach for ranking 100,000 runners",dtmt3,running_stats,1287536829.0,https://www.reddit.com/r/statistics/comments/dtmt3/approach_for_ranking_100000_runners/,9.0,7.0,"I'd like to get a little direction from the stats experts out there.  I am trying to rank a large population of runners.  Here is a summary of the data set

500,000 individual race results
100,000 different runners
300 different courses of varying difficulty and distance
Average of about 15 races per runner, with max of 50.

I was thinking about taking this approach (I am not a statistician, so this is basic)
Calculate a course conversion factor to normalize the performances.  The simple approach is to compare average times at the different courses to determine the factor.  This has flaws though.  The profile of runners who run one course is not always the same as those who run another.
Assuming I get course conversions, then calculate everyone's mile pace and rank them by that.  It seems I'd need to weight the most recent races more.
Any ideas are welcome.

Thanks,

Mike",en
1106405,2010-10-20 06:42:07,MachineLearning,Can someone help me understand boosting?,dtnw5,gct,1235022342.0,https://www.reddit.com/r/MachineLearning/comments/dtnw5/can_someone_help_me_understand_boosting/,13.0,16.0,"In particular AdaBoost, any papers accessible to an engineer but non-ML specialized one would be useful.  I'm particularly interested in how it can be used to combine many different weak classifiers into a stronger one.",en
1106406,2010-10-20 20:57:44,MachineLearning,A Survey of Genetics-based Machine Learning,dty7h,stoplan,1272908585.0,https://www.reddit.com/r/MachineLearning/comments/dty7h/a_survey_of_geneticsbased_machine_learning/,10.0,10.0,,en
1106407,2010-10-20 22:18:18,statistics,Hold on to your hats: it's World Statistics Day!,dtzna,GrumpySimon,1138450376.0,https://www.reddit.com/r/statistics/comments/dtzna/hold_on_to_your_hats_its_world_statistics_day/,2.0,0.0,,en
1106408,2010-10-21 00:24:10,statistics,[Question] looking for a 'menu' of the statistical methods that can be applied to different datasets and results they give,du1rp,projector,1269210852.0,https://www.reddit.com/r/statistics/comments/du1rp/question_looking_for_a_menu_of_the_statistical/,2.0,6.0,"I'm hungry to learn more about different stats, what they can be applied to and what they show. I'm a beginner, but I know that some methods are only applicable to ordinals, cardinals, categories etc., and that there are descriptive, inferential, exploratory and probably other types of stats.

I'd love some kind of 'menu', an overview of say 100 different stats, the kinds of data they work on and what they do. A guidebook for a beginner who's interested in statistical thinking, something with enough detail that I could actually apply and test out the methods to get a feel for them, but not so dense that I'd struggle to understand it.

Are there books or web pages like this you could recommend?",en
1106409,2010-10-21 23:11:12,statistics,Recommend some good SAS books?,dujps,nipplicious,1268972133.0,https://www.reddit.com/r/statistics/comments/dujps/recommend_some_good_sas_books/,6.0,12.0,"I work with large data sets and produce tables in Excel so I really just need SAS for the data crunching. I've taken SAS I and SAS II classes with the SAS Institute through work so I have a little experience with coding, but too much of the classes focused on things I won't encounter at my job (was geared toward financial/health databases I think). 

I will need to know some macros and haven't touched them yet. I have the basics of SQL and would love to learn more of that as well. Other than that, heavy on the basics and full of tricks and sample code.

Any recommendations/reviews would be appreciated, thanks!",en
1106410,2010-10-22 04:01:49,MachineLearning,Looking for feedback - A Short Simple Introduction to Information Theory,dunyt,moultano,1160099427.0,https://www.reddit.com/r/MachineLearning/comments/dunyt/looking_for_feedback_a_short_simple_introduction/,14.0,5.0,,en
1106411,2010-10-22 22:27:10,MachineLearning,How to avoid annoying a referee (of an academic article),dv0qv,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/dv0qv/how_to_avoid_annoying_a_referee_of_an_academic/,18.0,1.0,,en
1106412,2010-10-22 23:47:35,statistics,Half-life of a population of humans?,dv1ve,insincerity,1282325358.0,https://www.reddit.com/r/statistics/comments/dv1ve/halflife_of_a_population_of_humans/,0.0,9.0,"A question my son posed to me. Given a group of people all born at the same time, what would be the half-life of that group? Is it simply the average life expectancy of all the members of the group? I couldn't answer this one with certainty. Any help?",en
1106413,2010-10-23 07:45:47,statistics,Basic Errors To Avoid When Interpreting Survey Statistics,dv7fc,snipewiz,1256189534.0,https://www.reddit.com/r/statistics/comments/dv7fc/basic_errors_to_avoid_when_interpreting_survey/,4.0,0.0,,en
1106414,2010-10-23 17:21:07,MachineLearning,Publications by Googlers in Artificial Intelligence and Data Mining,dvbge,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/dvbge/publications_by_googlers_in_artificial/,30.0,1.0,,en
1106415,2010-10-24 14:35:03,MachineLearning,Can anyone point me at Jazz similar to this? Disclaimer: total jazz noob.,dvmh5,CmdrSammo,,https://www.reddit.com/r/MachineLearning/comments/dvmh5/can_anyone_point_me_at_jazz_similar_to_this/,3.0,7.0,,en
1106416,2010-10-24 17:03:32,computervision,Assistance Needed. Motion detection to serial port.,dvnfn,cynar,1262954747.0,https://www.reddit.com/r/computervision/comments/dvnfn/assistance_needed_motion_detection_to_serial_port/,3.0,4.0,"Hopefully you guys can help me out.

I'm trying to identify the point of maximum motion within a web-cam view and send it to a COM port on the computer (maybe with some idea of the magnitude of the motion).

Unfortunately, my programming skill is mostly with ICs and this is a little beyond me, at least in the time frame. I want to get it assembled and working for Halloween. 

Can anyone recommend a program that could do this?",en
1106417,2010-10-24 23:26:43,statistics,Tales of the Unprojected | Stats With Cats Blog,dvrbc,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/dvrbc/tales_of_the_unprojected_stats_with_cats_blog/,1.0,0.0,,en
1106418,2010-10-25 06:29:38,statistics,Why doesn't the variance of a best-fit line increase when the noise increases?,dvw4f,gobearsandchopin,1249171537.0,https://www.reddit.com/r/statistics/comments/dvw4f/why_doesnt_the_variance_of_a_bestfit_line/,6.0,10.0,"(xpost from math, not sure if it even went through the first time...)

This is something that has bothered me for a long time.

I generated some data points to represent a noisy line.  Basically:  y = m*x + b + gauss(0,noise_level)

Then I did a least squares fit.  I found that how close the fit parameters (m and b) are to the parent parameters definitely depends on the noise_level.  And also, chi-squared obviously depends on the noise level.

But when I look at the covariance matrix, it doesn't change at all when I vary the noise_level.

The reason this bothers me is that I want to know the uncertainy on the slope, which I would've *thought* should be the sqrt(covar[0][0]).  And then likewise the uncertainty on the intercept would be sqrt(covar[1][1]).

There is clearly something here I don't understand.",en
1106419,2010-10-25 22:31:18,statistics,"Probability of the game ""Set""... please help",dw80w,NaLaurethSulfate,1280988266.0,https://www.reddit.com/r/statistics/comments/dw80w/probability_of_the_game_set_please_help/,2.0,6.0,"So I am not very good at statistics, have taken it once in college, after very poor high school coverage, please correct my mistakes as something here must be wrong. This is not HW or anything like that just something for fun.

A quick introduction to the game set (http://www.setgame.com/set/index.html for more): 

There is a deck of 81 cards (3 ^ 4)
Each card contains some number of symbols
There are 4 variables per card:
*Shape: Oval, Squiggle, Diamond
*Fill: Solid, Empty, Hashed
*Color: Green, Red or Purple
*Number: 1, 2, 3

All the cards are unique, and every possible combination of the variables is included. 
A set is a group of 3 cards that are either the same or different in every variable. ie: cards which all have green hashed diamond, in quantities 1, 2 and 3 would be a set, because in the color, shape and fill they would be all the same and in number they would be all different.
	
The game play progresses as follows: 12 cards are turned face up on the table. Everyone looks for a set, and when someone finds they say ""set"" and remove those cards. New cards are turned up to replace them. If no set can be found 3 cards are added to the face up cards though when the first subsequent set is removed no cards are placed to replace them so that the number is usually 12, and returns to that if it grows.


**Now the question: For a given number of cards turned up on the table, what is the probability that a set exists in those cards?**

This is my answer so far, though it doesn't produce numbers that make sense to me, I think all the probabilities are too high:

for a number of cards n the probability is: ((nCr n 3) * (((nCr 81 2) / (nCr 3 2)) / (nCr 81 3))).

The thinking being that all possible sets in the deck are given by nCr 81 2 (since all combinations of 2 cards can make a set, they ""point"" to their third card), but their are three pairings of 2 cards per set, so we have to divide by three. The total number of potential sets is nCr 81 3, so the probability that any three cards are a set is the ratio of the total number of sets over the total number of ways to combine 3 cards. So to tie that to n cards we just find the number of ways to pick 3 cards from a given number. I must be making some mistake though because here is a list of the numbers that I get from 3 - 20:

[1.2658227848101266e-2,5.063291139240506e-2,0.12658227848101267,0.25316455696202533,0.4430379746835443,0.7088607594936709,1.0632911392405062,1.5189873417721518,2.088607594936709,2.7848101265822782,3.620253164556962,4.6075949367088604,5.7594936708860756,7.0886075949367084,8.60759493670886,10.329113924050633,12.265822784810126,14.430379746835444]

They go above one far too early, and what even is the interpretation of a probability that is greater than 1?

Please show me all of the things I am doing wrong here!

TLDR: In the game set what is the probability that there is a set in a given number of cards taken from the whole deck?


",en
1106420,2010-10-26 03:02:14,analytics,Real time Web analytics service Clicky updates interface… and gets an earful,dwc7p,siliconflorist,1221025353.0,https://www.reddit.com/r/analytics/comments/dwc7p/real_time_web_analytics_service_clicky_updates/,3.0,0.0,,en
1106421,2010-10-26 13:11:29,analytics,What Motivates Analytic Professionals?,dwjow,jaimefitzgerald,1197141466.0,https://www.reddit.com/r/analytics/comments/dwjow/what_motivates_analytic_professionals/,1.0,0.0,,en
1106422,2010-10-26 17:47:44,MachineLearning,Training Binary Text Classifiers with NLTK Trainer,dwn3j,japerk,1126238400.0,https://www.reddit.com/r/MachineLearning/comments/dwn3j/training_binary_text_classifiers_with_nltk_trainer/,21.0,0.0,,en
1106423,2010-10-26 20:38:46,statistics,"How do you find a p-value in SPSS for a ""Levene's Test""",dwq34,gbhall,1232529242.0,https://www.reddit.com/r/statistics/comments/dwq34/how_do_you_find_a_pvalue_in_spss_for_a_levenes/,0.0,0.0,"Data: http://cl.ly/2607470eb87993abe98b
Question with answer: http://cl.ly/8fe473ddded3761ff886

Please help, I have a test today!! I'm just stuck on this",en
1106424,2010-10-28 16:28:22,MachineLearning,"Descriptions of top solutions of the IEEE ICDM Contest: ""TomTom Traffic Prediction for Intelligent GPS Navigation"" provided by their designers are now available at TunedIT blog.",dxp2d,m3g4n3,1277388269.0,https://www.reddit.com/r/MachineLearning/comments/dxp2d/descriptions_of_top_solutions_of_the_ieee_icdm/,10.0,0.0,,en
1106425,2010-10-28 17:42:31,artificial,For those interested in how to achieve top results,dxq87,m3g4n3,1277388269.0,https://www.reddit.com/r/artificial/comments/dxq87/for_those_interested_in_how_to_achieve_top_results/,0.0,0.0,,en
1106426,2010-10-28 19:50:17,statistics,"Margin of error, and comparing proportions in the same sample",dxsf5,freakonometrics,1288274338.0,https://www.reddit.com/r/statistics/comments/dxsf5/margin_of_error_and_comparing_proportions_in_the/,4.0,0.0,,en
1106427,2010-10-29 18:42:04,analytics,Quantitative Analysis is Only Half The Story,dyadm,geniusofwi,1288292450.0,https://www.reddit.com/r/analytics/comments/dyadm/quantitative_analysis_is_only_half_the_story/,1.0,0.0,,en
1106428,2010-10-30 12:01:23,MachineLearning,"Natural Language Processing with Python #NLTK /by Bird, Klein, Loper (CC-BY-NC-ND/3.0 version)",dymyf,[deleted],,https://www.reddit.com/r/MachineLearning/comments/dymyf/natural_language_processing_with_python_nltk_by/,1.0,0.0,,en
1106429,2010-10-31 02:59:30,statistics,Help! Stats Question- Interpreting Skewed Data,dyvur,[deleted],,https://www.reddit.com/r/statistics/comments/dyvur/help_stats_question_interpreting_skewed_data/,0.0,4.0,,en
1106430,2010-10-31 17:46:25,statistics,looks like I failed at the reddit.com/rally trophy game... how many people were playing anyway? : crosspost from self.reddit.com,dz3ge,themathemagician,1284617118.0,https://www.reddit.com/r/statistics/comments/dz3ge/looks_like_i_failed_at_the_redditcomrally_trophy/,0.0,0.0,,en
1106431,2010-10-31 19:12:14,statistics,Statistics IRL at the Fear -&gt; Sanity Rally.,dz4ez,vstg005,1261220313.0,https://www.reddit.com/r/statistics/comments/dz4ez/statistics_irl_at_the_fear_sanity_rally/,35.0,2.0,,en
1106432,2010-10-31 23:50:23,statistics,Extracting information from a keyboard...,dz7nx,freakonometrics,1288274338.0,https://www.reddit.com/r/statistics/comments/dz7nx/extracting_information_from_a_keyboard/,1.0,0.0,,en
1106433,2010-11-01 06:23:30,MachineLearning,A Personal Perspective on Machine Learning,dzcde,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/dzcde/a_personal_perspective_on_machine_learning/,25.0,15.0,,en
1106434,2010-11-01 16:46:32,statistics,What's the correct test for answering this simple question?,dzii4,[deleted],,https://www.reddit.com/r/statistics/comments/dzii4/whats_the_correct_test_for_answering_this_simple/,0.0,18.0,,en
1106435,2010-11-02 09:02:29,MachineLearning,Google Workshop on Quantum Biology,dzxba,key95,1179214221.0,https://www.reddit.com/r/MachineLearning/comments/dzxba/google_workshop_on_quantum_biology/,0.0,0.0,,en
1106436,2010-11-03 08:12:50,statistics,Interesting statistics problem for you guys.,e0hhy,furiouszap,1217397081.0,https://www.reddit.com/r/statistics/comments/e0hhy/interesting_statistics_problem_for_you_guys/,2.0,6.0,"Long story short, I am completely stumped. Figured the infinite knowledge of reddit would be a good place to look.

I am wondering what the chances are that I will get at least 1 of each A B C D if I pull 7 letters out of a bag with an infinite amount of A's B's C's and D's in said bag.

At first I went ahead and said I have 4^7 = 16384 possible combinations but was thinking about it today and realized that is wrong as it doesn't matter what order they come out in. 

Now I am stumped and come to you. Thanks in advance.

ps. Post your math please.

",en
1106437,2010-11-03 13:38:16,MachineLearning,MetaOptimize/Q+A: Good Machine Learning Blogs ,e0kko,fbahr,1275413362.0,https://www.reddit.com/r/MachineLearning/comments/e0kko/metaoptimizeqa_good_machine_learning_blogs/,19.0,1.0,,en
1106438,2010-11-03 14:24:31,MachineLearning,"An old interview with the author of ""Collective Intelligence in Action"" on recommendation engines.",e0l38,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/e0l38/an_old_interview_with_the_author_of_collective/,5.0,0.0,,en
1106439,2010-11-03 15:24:14,MachineLearning,Criticism of Netflix's approach to Recommending movies.,e0lud,[deleted],,https://www.reddit.com/r/MachineLearning/comments/e0lud/criticism_of_netflixs_approach_to_recommending/,1.0,0.0,,en
1106440,2010-11-03 15:25:18,MachineLearning,Criticism of Netflix's approach to recommending movies.,e0luy,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/e0luy/criticism_of_netflixs_approach_to_recommending/,12.0,0.0,,en
1106441,2010-11-04 04:41:39,MachineLearning,Simple guide to support vector machines,e0yk6,[deleted],,https://www.reddit.com/r/MachineLearning/comments/e0yk6/simple_guide_to_support_vector_machines/,24.0,10.0,,en
1106442,2010-11-04 07:13:55,MachineLearning,"The graph of breakups over the course of the year, as culled from over 10,000 Facebook status updates by data-minded supersleuth David McCandless.",e10o1,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/e10o1/the_graph_of_breakups_over_the_course_of_the_year/,62.0,7.0,,en
1106443,2010-11-04 12:09:41,MachineLearning,Comparative analysis of two shared data sets: Mendeley Data vs. Netflix Data,e13da,fbahr,1275413362.0,https://www.reddit.com/r/MachineLearning/comments/e13da/comparative_analysis_of_two_shared_data_sets/,8.0,0.0,,en
1106444,2010-11-04 17:24:15,MachineLearning,[Need Help] Creating clusters of common features in a set of objects?,e176e,bluebluebl00,1288819079.0,https://www.reddit.com/r/MachineLearning/comments/e176e/need_help_creating_clusters_of_common_features_in/,5.0,11.0,"(crossposting from /r/math)

I have a dataset of about 15 features related to object. Here's a purely illustrative example of what the data looks like.
    
    Blue Eyes   | Black Hair | Tall  |......  | Number of objects with feature set
    Yes         |  Yes        | No   |......  | 150
    Yes         |  Yes        | Yes  |......  | 70
    Yes         |  No         | No    |......  | 200

I'd like to analyze these to get, maybe clusters of features that tend to occur together.

I tried some clustering algorithms on the dataset and got cluster tags for each object, but the method doesn't really tell me what the common set of features in each group are.

Any suggestions will be much appreciated! Thanks.",en
1106445,2010-11-05 14:22:37,data,How to Perform Hard Drive Recovery After Overwrite,e1nu8,scientificworld,1253015496.0,https://www.reddit.com/r/data/comments/e1nu8/how_to_perform_hard_drive_recovery_after_overwrite/,1.0,0.0,,en
1106446,2010-11-05 16:46:27,artificial,IEEE ICDM Contest Winning Algorithms - See the Descriptions ,e1p61,m3g4n3,1277388269.0,https://www.reddit.com/r/artificial/comments/e1p61/ieee_icdm_contest_winning_algorithms_see_the/,0.0,0.0,,en
1106447,2010-11-05 17:54:02,MachineLearning,IEEE ICDM Contest Winning Algorithms - See the Descriptions ,e1qd8,m3g4n3,1277388269.0,https://www.reddit.com/r/MachineLearning/comments/e1qd8/ieee_icdm_contest_winning_algorithms_see_the/,10.0,0.0,,en
1106448,2010-11-05 20:59:44,statistics,"Every time you fly, remember this....",e1tr4,expectingrain,1161312429.0,https://www.reddit.com/r/statistics/comments/e1tr4/every_time_you_fly_remember_this/,4.0,18.0,...you have a 50% chance of having a below average pilot :p,en
1106449,2010-11-06 03:44:17,MachineLearning,NLP CHALLENGE: FIND SEMANTICALLY RELATED TERMS OVER A LARGE VOCABULARY (&gt;1M)?,e1z7u,[deleted],,https://www.reddit.com/r/MachineLearning/comments/e1z7u/nlp_challenge_find_semantically_related_terms/,0.0,0.0,,en
1106450,2010-11-07 08:33:12,statistics,Ten Fatal Flaws in Data Analysis | Stats With Cats Blog,e2ekl,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/e2ekl/ten_fatal_flaws_in_data_analysis_stats_with_cats/,14.0,2.0,,en
1106451,2010-11-07 19:46:24,MachineLearning,"NELL – a computer system that, over time, is 
teaching itself to read and understand the web. (has 
a twitter feed of its results, even)",e2joo,camilop,1233528232.0,https://www.reddit.com/r/MachineLearning/comments/e2joo/nell_a_computer_system_that_over_time_is_teaching/,31.0,3.0,,en
1106452,2010-11-07 19:57:01,MachineLearning,Functions to use R as an image editor,e2jti,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/e2jti/functions_to_use_r_as_an_image_editor/,3.0,0.0,,en
1106453,2010-11-08 01:58:45,MachineLearning,Interesting AI Demos and Projects,e2o9f,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/e2o9f/interesting_ai_demos_and_projects/,9.0,0.0,,en
1106454,2010-11-08 09:56:00,computervision,Glyph tracking with MATLAB - help!,e2tyr,Robathome,1191247402.0,https://www.reddit.com/r/computervision/comments/e2tyr/glyph_tracking_with_matlab_help/,5.0,5.0,"Hey ComVisdit (?), I'm hoping to extract a little knowledge from the hivemind. I study legs and walking, and I want to be able to do it without a multi-million dollar vision system like [VICON](http://www.youtube.com/watch?v=2uDnW4AtFiE). I know that a webcam can [track a known glyph](http://www.youtube.com/watch?v=Zu2JBE36YvY) through space, and if there's a reference glyph included you can track orientation and position [pretty well](http://www.youtube.com/watch?v=JUgvGmNceuo) (note the ""glyphs"" on top of the robots).

I was wondering what kind of accuracy I'd get from a [1080p webcam](http://www.logitech.com/en-ca/webcam-communications/webcams/devices/6816) if I jacked the capture rate with with a purpose-built Linux box? I've had issues with consistent capture timing with Windows in the past, something about interrupts. 

I've had a lot of experience with MATLAB programming, but haven't really touched the vision blockset or image processing stuff... Does anyone know any good tutorials or guides to get me started on glyph tracking? Any help is greatly appreciated!",en
1106455,2010-11-08 10:59:40,MachineLearning,IJCNN 2011 Social Network Challenge [Develop a top-notch friend suggestion algorithm],e2ugv,fbahr,1275413362.0,https://www.reddit.com/r/MachineLearning/comments/e2ugv/ijcnn_2011_social_network_challenge_develop_a/,9.0,0.0,,en
1106456,2010-11-08 17:35:34,MachineLearning,A R wrapper for Google Prediction API ,e2y4z,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/e2y4z/a_r_wrapper_for_google_prediction_api/,9.0,0.0,,en
1106457,2010-11-08 20:10:25,statistics,A five part video series on text mining using RapidMiner,e30qw,[deleted],,https://www.reddit.com/r/statistics/comments/e30qw/a_five_part_video_series_on_text_mining_using/,11.0,0.0,,en
1106458,2010-11-08 22:53:35,statistics,Interesting Q &amp; A at /r/machinelearning with the guys from dataists,e33ic,kunjaan,1223255404.0,https://www.reddit.com/r/statistics/comments/e33ic/interesting_q_a_at_rmachinelearning_with_the_guys/,8.0,0.0,,en
1106459,2010-11-09 00:46:30,computervision,Looking for ARTag Rev2 SDK...,e35bq,Robathome,1191247402.0,https://www.reddit.com/r/computervision/comments/e35bq/looking_for_artag_rev2_sdk/,2.0,7.0,"I found out that the National Research Council of Canada funded an Augmented Reality program, which yielded a development kit called ARTag that was [proven to be superior](http://nparc.cisti-icist.nrc-cnrc.gc.ca/npsi/ctrl?action=rtdoc&amp;an=5763247&amp;article=0&amp;lang=en) to ARToolKit.

I want to get my hands on the SDK. Also, if there's a PDF of the ""Augmented Reality: A Practical Guide"" that was written to accompany the SDK, I would to get a copy of that too. I consider myself to be pretty good at the internets, but I can't seem to procure a copy of this out-of-print book (without paying for it, of course) or the SDK.

Can Reddit save the day?",en
1106460,2010-11-09 06:50:14,statistics,"new subreddit: Mathematical Psychology [includes Bayesian, NN, and stat models of mind]",e3arl,Lors_Soren,1282243358.0,https://www.reddit.com/r/statistics/comments/e3arl/new_subreddit_mathematical_psychology_includes/,9.0,0.0,,en
1106461,2010-11-09 07:50:11,MachineLearning,new subreddit: Mathematical Psychology (you know ML models are used all the time),e3bkq,Lors_Soren,1282243358.0,https://www.reddit.com/r/MachineLearning/comments/e3bkq/new_subreddit_mathematical_psychology_you_know_ml/,11.0,0.0,,en
1106462,2010-11-09 13:09:13,MachineLearning,Algo-Rhythm: Content Based Musical Search on YouTube,e3eqg,fregoli,1253518766.0,https://www.reddit.com/r/MachineLearning/comments/e3eqg/algorhythm_content_based_musical_search_on_youtube/,9.0,0.0,,en
1106463,2010-11-09 19:59:39,MachineLearning,A five part series on text mining with Rapidminer,e3ko3,[deleted],,https://www.reddit.com/r/MachineLearning/comments/e3ko3/a_five_part_series_on_text_mining_with_rapidminer/,17.0,1.0,,en
1106464,2010-11-09 21:47:41,MachineLearning,Anyone here attending NIPS 2010 this December?,e3mm7,loniousmonk,1288421034.0,https://www.reddit.com/r/MachineLearning/comments/e3mm7/anyone_here_attending_nips_2010_this_december/,14.0,8.0,"[Info](http://nips.cc/) about the conference.  It's in Vancouver, BC.  Post in the comments if you have a paper or poster accepted into the proceedings.  And it's not too late to register even if you didn't submit a paper.  Should be a good time. =)",en
1106465,2010-11-10 02:51:28,statistics,Don't be a Turkey,e3rm6,ezgraphs,1276218477.0,https://www.reddit.com/r/statistics/comments/e3rm6/dont_be_a_turkey/,5.0,1.0,,en
1106466,2010-11-10 15:32:31,MachineLearning,Machine Learning: A Love Story,e40g3,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/e40g3/machine_learning_a_love_story/,24.0,7.0,,en
1106467,2010-11-10 18:59:34,analytics,New WordPress Plugin from SeeVolution Available Today | Analytics News,e43vq,seevolution,1288998135.0,https://www.reddit.com/r/analytics/comments/e43vq/new_wordpress_plugin_from_seevolution_available/,1.0,1.0,,en
1106468,2010-11-11 00:19:16,statistics,Chris Fonnesbeck discusses Bayesian estimation with MCMC using PyMC [vid],e49o5,agconway,1228006618.0,https://www.reddit.com/r/statistics/comments/e49o5/chris_fonnesbeck_discusses_bayesian_estimation/,7.0,0.0,,en
1106469,2010-11-11 04:30:49,statistics,Any software recommendations?,e4dfz,Tiomaidh,1256863695.0,https://www.reddit.com/r/statistics/comments/e4dfz/any_software_recommendations/,3.0,12.0,"Hello,

I'm in AP Statistics this year, and am growing unsatisfied with my current way of doing homework--using Gnumeric and sometimes the TI-Nspire. I feel like there's got to be a better way out there--something that can do histograms and stemplots, and the five-number summary, and is normal distribution-aware, and so on and so forth--I'm sure there'll be more things that I haven't learned yet!.

My textbook uses Minitab, but that costs money. I'm a Linux user, and have heard about R, but I'm wondering if it isn't overkill. I know how to program, and am comfortable with bash and LaTeX, so I'm not completely opposed to something non-graphical but...is it worth it? Will it come in handy after I'm done with this course? (I'll be a computer science major, if that matters.) What about the GUI shells for it? It seems like there's a lot of them... Any other recommendations? Wikipedia has [dazzled me with choice](http://en.wikipedia.org/wiki/List_of_statistical_packages).

Sorry if this is kind of a lowbrow question, but I wasn't sure who else to ask besides reddit, and this seemed like the most appropriate subreddit.

Update: Looks like I'll be going with R.

Thanks in advance.",en
1106470,2010-11-11 06:01:40,MachineLearning,"Announcing Google Refine - tool for working with messy data sets, including cleaning up inconsistencies, transforming formats &amp; extending data from external web services or databases. ",e4eq0,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/e4eq0/announcing_google_refine_tool_for_working_with/,33.0,4.0,,en
1106471,2010-11-11 07:04:19,analytics,Great new analytical tool for FB advertising,e4fja,salvage,1184143405.0,https://www.reddit.com/r/analytics/comments/e4fja/great_new_analytical_tool_for_fb_advertising/,2.0,0.0,,en
1106472,2010-11-11 17:03:36,MachineLearning,Survey on Social Tagging [PDF],e4lto,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/e4lto/survey_on_social_tagging_pdf/,3.0,2.0,,en
1106473,2010-11-11 17:05:08,statistics,"Incanter: a Clojure-based, R-like platform for statistical computing and graphics.",e4lup,BioGeek,1124683200.0,https://www.reddit.com/r/statistics/comments/e4lup/incanter_a_clojurebased_rlike_platform_for/,11.0,3.0,,en
1106474,2010-11-11 20:00:58,statistics,"Google Refine - New Data Cleaning Software Released
 
 Project Hosting on Google Code",e4owi,gravity,1143916158.0,https://www.reddit.com/r/statistics/comments/e4owi/google_refine_new_data_cleaning_software_released/,1.0,1.0,,en
1106475,2010-11-11 23:11:09,MachineLearning,What XOR looks like to a trained artificial neural network (repost from r/PICS),e4sa5,ceilingpyro,1275551339.0,https://www.reddit.com/r/MachineLearning/comments/e4sa5/what_xor_looks_like_to_a_trained_artificial/,27.0,29.0,"[un-trained](http://imgur.com/Q6J7D.png) (weight matrix initialized to random numbers in [-1,1])

[trained](http://imgur.com/prYlg.png) (learning rate = 0.75, one iteration is one pass through the training data; in this case four input-output pairs)

**Context:** I just finished writing a pretty simple machine learning library targeted at recognizing features in images (right now the only learning algorithm in the lib is an ANN). As far as I know, the 'hello world' of multi-layer ANNs is learning XOR, since the problem of mapping the inputs to the outputs is not linearly separable for that data set.

To accomplish this, I set up a 3-layer ANN with 2 inputs, 3 hidden nodes, and 1 output (all non-output layers have bias nodes added automatically). Activation was accomplished using the generic sigmoid function. It took much less than a tenth of a second for the ANN to learn the data.

I was suddenly intrigued by what the output of the neural net would look like over its full trained domain, so dumped the numbers and generated the graph using **QtiPlot** (probably the best open-source data analysis &amp; plotting software I've ever seen). Result = fucking awesome and elegant, so I thought I'd share.

Edit: Note: the first graph is labeled incorrectly.",en
1106476,2010-11-12 00:33:40,MachineLearning,AskML: For a Fast Euclidean Distance Algorithm,e4tlz,[deleted],,https://www.reddit.com/r/MachineLearning/comments/e4tlz/askml_for_a_fast_euclidean_distance_algorithm/,6.0,22.0,,en
1106477,2010-11-12 14:38:02,MachineLearning,New white paper from Numenta/Jeff Hawkins on Hierarchical Temporal Memory algorithms,e53is,[deleted],,https://www.reddit.com/r/MachineLearning/comments/e53is/new_white_paper_from_numentajeff_hawkins_on/,23.0,5.0,,en
1106478,2010-11-13 05:39:55,statistics,Normality assumptions and you; a.k.a. why you should think differently about many popular statistical tests,e5g93,[deleted],,https://www.reddit.com/r/statistics/comments/e5g93/normality_assumptions_and_you_aka_why_you_should/,10.0,8.0,,en
1106479,2010-11-13 08:08:19,MachineLearning,road marking machine suppliers,e5hu1,tianxiafox,1289628404.0,https://www.reddit.com/r/MachineLearning/comments/e5hu1/road_marking_machine_suppliers/,0.0,1.0,,en
1106480,2010-11-13 18:31:08,MachineLearning,R-bloggers.com/lang - R content in your language,e5mla,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/e5mla/rbloggerscomlang_r_content_in_your_language/,0.0,0.0,,en
1106481,2010-11-13 20:50:54,statistics,Anyone familiar with count timeseries?,e5o8j,pressed,1192748700.0,https://www.reddit.com/r/statistics/comments/e5o8j/anyone_familiar_with_count_timeseries/,0.0,2.0,"Here's my problem: I have a month-long timeseries of count data. Counts are typically 1-5 but there are many zeroes and many missing data. So periods of high signal are represented by a density of counts rather than an increase of their magnitude.

I want to correlate this data with data of a lower frequency, i.e. average it. I feel like the arithmetic mean and standard deviation do not adequately represent my data. What would a statistician do?

[I realize that I could simply add the original counts, but I would like to use the original data to estimate an uncertainty]

edit: Any book recommendations are welcome",en
1106482,2010-11-13 21:26:50,MachineLearning,Google Faculty Summit 2009: Statistical Machine Translation by Google's lead for machine translation [VID],e5oo7,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/e5oo7/google_faculty_summit_2009_statistical_machine/,3.0,0.0,,en
1106483,2010-11-14 03:22:44,statistics,Does anyone have suggestions for how to respecify a bimodally distributed variable for a multiple linear regression model? ,e5si1,[deleted],,https://www.reddit.com/r/statistics/comments/e5si1/does_anyone_have_suggestions_for_how_to_respecify/,4.0,2.0,,en
1106484,2010-11-14 10:09:14,statistics,R-bloggers.com/lang - R content in your language,e5w1j,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/e5w1j/rbloggerscomlang_r_content_in_your_language/,9.0,0.0,,en
1106485,2010-11-14 18:31:17,statistics,You Can Lead a Boss to Data but You Can't Make Him Think | Stats With Cats Blog,e5yk0,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/e5yk0/you_can_lead_a_boss_to_data_but_you_cant_make_him/,11.0,0.0,,en
1106486,2010-11-14 19:06:30,MachineLearning,Music Information Retrieval. Proposal of new lyrics recognition method.,e5z0d,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/e5z0d/music_information_retrieval_proposal_of_new/,7.0,0.0,,en
1106487,2010-11-15 16:39:41,statistics,What is the test statistic in a Wilcoxon rank-sum test telling me?,e6dxn,sausagefeet,1262786895.0,https://www.reddit.com/r/statistics/comments/e6dxn/what_is_the_test_statistic_in_a_wilcoxon_ranksum/,6.0,9.0,"Disclaimer, this question is related to my homework however the question is not my homework.  If you feel my question is inappropriate I apologize.

I'm doing some work that involves calculating the test statistic (using software) for a bunch of samples of data and then comparing them and it dawned on me that I don't really understand what the test statistic is telling me.  I've done some poking around the internet for a clue but most resources I found simply went over calculating the test statistic and then looking up its p-value.  Is the test statistic itself actually useful in comparing to other test statistics or is it just a way to get to a p-value?

Thank you
",en
1106488,2010-11-15 19:42:14,computervision,Road Detection (with Hough Transform),e6h56,MachineVision,1284977835.0,https://www.reddit.com/r/computervision/comments/e6h56/road_detection_with_hough_transform/,10.0,16.0,"I'm developing an application that can detect and track structured roads (motorways/freeways). I'm new to computervision but I have it working fairly well. Here's what I'm doing:

1) Convert image to grayscale
2) Canny Edge detection
3) Split image into left and right halfs
4) Apply Hough transform on each half.
5) Average/fuse lines that are near to each other. Only allow lines of that are between certain angles (no need to draw horizontal lines as they'll never be part of any road).
6) Find the intersection of resultant lines from the left and right half. This will be the detected vanishing point.
7) Draw lines along the detected road markers to the vanishing point.

What I was hoping for was some pointers on how to proceed further. I still have to get video working, but it seems work fairly well. Here are some examples:

http://i115.photobucket.com/albums/n282/PsychedelicBreakfast/Pic3.jpg
http://i115.photobucket.com/albums/n282/PsychedelicBreakfast/Pic4.jpg

Before I started splitting the image and averaging/fusing lines together, I could not get the application to detect the road in the 2nd picture. So I do feel that it works alright but I would like to improve it even further.

I'm also lost when it comes to curves. How can I ensure that the application can detect turns as well?",en
1106489,2010-11-16 01:59:46,statistics,What is better SPSS or SAS,e6nto,sac2hayward,1289865006.0,https://www.reddit.com/r/statistics/comments/e6nto/what_is_better_spss_or_sas/,12.0,59.0,"Im looking to put some money in the software, just wondering what is more widely used in industry?",en
1106490,2010-11-16 10:36:16,MachineLearning,Natural Language Processing for the Working Programmer (in Haskell),e6v15,[deleted],,https://www.reddit.com/r/MachineLearning/comments/e6v15/natural_language_processing_for_the_working/,0.0,0.0,,en
1106491,2010-11-16 12:48:20,MachineLearning,Ask ML. What effect does sampling have on model accuracy?,e6w9x,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/e6w9x/ask_ml_what_effect_does_sampling_have_on_model/,18.0,9.0,,en
1106492,2010-11-16 16:21:33,statistics,Why Psychologists Must Change the Way They Analyze Their Data: The Case of Psi (pdf),e6yks,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/e6yks/why_psychologists_must_change_the_way_they/,20.0,7.0,,en
1106493,2010-11-16 21:54:43,MachineLearning,Using network analysis and LDA to identify communities in academic co-authorship network,e74is,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/e74is/using_network_analysis_and_lda_to_identify/,0.0,1.0,,en
1106494,2010-11-17 03:04:47,MachineLearning,SwarmWiki - A resource for agent- and individual-based modelers and the home of Swarm,e79go,psykocrime,1205139983.0,https://www.reddit.com/r/MachineLearning/comments/e79go/swarmwiki_a_resource_for_agent_and/,7.0,0.0,,en
1106495,2010-11-17 05:43:56,statistics,Question on normalising data by a novice,e7bvu,Emanresu2009,1255307969.0,https://www.reddit.com/r/statistics/comments/e7bvu/question_on_normalising_data_by_a_novice/,7.0,6.0,"Hi guys,

I've only ever taken one class in statistics in my undergrad and now I've been given what is in essence a statistical exercise which while not overly complex I'm having trouble conceptualising the answer.

So I have some 30 data points (I know its a very small sample but these points are very hard to get) what we've seen is that there are several outliers (after graphing) we're trying to work out what causes the changes. So I've gone about collecting related information (7 other variables )from different systems where we think there might be some relationship. I've asked excel for a regression check and it's told me that the p-value is only greater than 0.5 in 3 of the 7 cases. 

I've interperted that only 3 variables have a statistically significant correlation. 

So now the thought is great these 3 variables have some connection but how can I ""normalise"" the other variables for these variables. 

Stats has never been my strong point but I'm willing to learn so please take it slow.

thanks in advance ",en
1106496,2010-11-17 07:25:31,MachineLearning,Couple of Research Papers in Collaborative Filtering.,e7dd2,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/e7dd2/couple_of_research_papers_in_collaborative/,8.0,0.0,,en
1106497,2010-11-17 09:37:51,MachineLearning,"""The humanities and social sciences are the emerging domains for using high-performance computers""",e7f2s,feverdream,1276316399.0,https://www.reddit.com/r/MachineLearning/comments/e7f2s/the_humanities_and_social_sciences_are_the/,9.0,0.0,,en
1106498,2010-11-17 16:01:31,MachineLearning,Detexify Explained,e7iby,[deleted],,https://www.reddit.com/r/MachineLearning/comments/e7iby/detexify_explained/,19.0,0.0,,en
1106499,2010-11-17 16:39:58,MachineLearning,"TunedIT and SIAM SDM'11 Contest Started.
1,000$ to win for Prediction of Chemical Properties. ",e7ivr,m3g4n3,1277388269.0,https://www.reddit.com/r/MachineLearning/comments/e7ivr/tunedit_and_siam_sdm11_contest_started_1000_to/,1.0,0.0,,en
1106500,2010-11-17 19:30:04,MachineLearning,Learning comment vote counts on reddit,e7lu3,iamapipebomb,1253338464.0,https://www.reddit.com/r/MachineLearning/comments/e7lu3/learning_comment_vote_counts_on_reddit/,6.0,7.0,"I've chosen this as the topic of my project in applied machine learning. It seems fitting to post it here. I've just started, but anything created is (and will be) available in github. http://github.com/iamapipebomb/aml_reddit

I'm more than welcome to any thoughts or, even better, papers on similar endeavors. ",en
1106501,2010-11-17 20:14:17,MachineLearning,How to deal with features that are both categorical and numerical?,e7mnq,WiIIis,1198688482.0,https://www.reddit.com/r/MachineLearning/comments/e7mnq/how_to_deal_with_features_that_are_both/,5.0,27.0,"Hey all,

I am constructing features for doing some ML. Some of these features are part categorical and part numerical.  For instance, take the shortest path from node A to node B on a graph.  There are two basic ""categories"" of this feature (finite vs infinite), but if you look within the finite paths, there is a natural numerical ordering:

[Inf Inf 2 4 5 2 Inf 3 5 8...]


How do you handle such a feature in a classification/clustering scenario? I can just use it as an ""all or none"" categorical feature, but that fails to exploit the feature values in the ""all"" group.  I could use some kind of two-stage heuristic approach, but I'm not sure if there are better methods out there.

Thanks for your input!  ",en
1106502,2010-11-18 05:51:08,statistics,Two-Dimensional Correlated Random Walk [Pics &amp; Code],e7wgw,topheroly,1245562759.0,https://www.reddit.com/r/statistics/comments/e7wgw/twodimensional_correlated_random_walk_pics_code/,2.0,4.0,"I was playing around with a one-dimensional correlation and I thought it would be more fun to see what happens when you go to two-dimensions and the correlation is with a multinomial, instead of a binomial.  Here are the results.


[R Code to generate similar plots like this](http://pastie.org/1307504)

___________________________________________________________

[NE Biased Correlation](http://topheroly.imgur.com/correlated_twodimensional_random_walk_captures/RI0bc)


[W Biased Correlation](http://topheroly.imgur.com/correlated_twodimensional_random_walk_captures/DoIVq)


For Comparison, here are two completely random walks:


[Completely Random Walk 1](http://topheroly.imgur.com/correlated_twodimensional_random_walk_captures/121Vy)


[Completely Random Walk 2](http://topheroly.imgur.com/correlated_twodimensional_random_walk_captures/JVr42)


Note: Since I was just playing around with some code, the function is not optimized for efficiency and does not include fringe starting cases.


Edit:  I forgot to mention, I coded the boundaries as Completely Absorbing and left a block of code in comments for a pair of reflecting boundaries.",en
1106503,2010-11-18 07:44:58,statistics,Why Rob Tibshirani is a statistician,e7y5c,kanak,1155042601.0,https://www.reddit.com/r/statistics/comments/e7y5c/why_rob_tibshirani_is_a_statistician/,0.0,0.0,,en
1106504,2010-11-18 12:04:45,statistics,Know of any R blogs in your own language (assuming it's not English)?,e80zz,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/e80zz/know_of_any_r_blogs_in_your_own_language_assuming/,3.0,2.0,"I am looking for non-English bloggers, writing about R.  Here is my reason why:  http://www.r-bloggers.com/r-bloggers-now-in-your-language/

Any suggestions?",en
1106505,2010-11-18 23:49:20,statistics,Can someone make sense of this for me?,e8bjq,hfmurdoc,1253914216.0,https://www.reddit.com/r/statistics/comments/e8bjq/can_someone_make_sense_of_this_for_me/,9.0,18.0,"This is in the exercise list for my Probability and Statistics class. I don't need help solving it, I just can't wrap my head around the result... So it goes like this:

There's a 0.005 probability that someone has cancer of a given type.

There's a test for it and, given the event ""+"" for positive test and the event ""C"" for someone having cancer:

P(+|C) = 0.99 and P(not+|notC) = 0.95. I'm supposed to calculate P(C|+) (which is 0.0905. Solution:)

    P(C|+) = P(C and +)/P(+) = P(+|C)P(C)/P(+)
    P(+) = P(+|C)P(C) + P(P|notC)P(notC) = 0.0547
    =&gt; P(C|+) = 0.0905

    The result given in the solutions of the exercise is the same as the one I get.

Which means that even though the test has a 99% chance of finding cancer if there is cancer to be found, and only 5% chance of ""finding"" cancer where there is none, there is only ~9% chance of your having cancer if the test is positive. What the hell? I don't understand why it would be so low...


PS.: There is the chance that both me and the solutions are wrong though...

EDIT: this: was this.",en
1106506,2010-11-19 00:49:04,MachineLearning,What are some techniques for identifying unusual changes in a time series?,e8ckl,combobob,1286912897.0,https://www.reddit.com/r/MachineLearning/comments/e8ckl/what_are_some_techniques_for_identifying_unusual/,17.0,21.0,"for example, if I'm logging CPU usage of an app, and I want an alert when it spikes, but isn't a blip that comes right back down. I know there are several parameters I need to define (like how much of a move is worth noting), but I'm interesting in learning about techniques in this area, examples, etc., if anyone can point me to some reading material or suggest related subjects to study (I assume signal processing?)",en
1106507,2010-11-19 03:38:32,MachineLearning,How to learn time series with categorical data?,e8f05,Doormat88,1279713187.0,https://www.reddit.com/r/MachineLearning/comments/e8f05/how_to_learn_time_series_with_categorical_data/,6.0,6.0,"A time series with continuous data or discrete data with a euclidean distance is suitable for classical signal processing techniques. But I have a dataset where the values are categorical and the distance metric is binary (either the same, or different). For example, letters from an alphabet. What techniques are there to learn such a time series?",en
1106508,2010-11-20 14:20:35,statistics,What do you use to produce aesthetically pleasing graphs in R?,e9399,trolleshwar,1277825888.0,https://www.reddit.com/r/statistics/comments/e9399/what_do_you_use_to_produce_aesthetically_pleasing/,14.0,15.0,"Beginner here. The lines in the default graphs are not smooth. The fonts look terrible, as if they are out of a 1990s graphics package.

I googled a bit and found out about Cairo and ggplot2, but would like to know more.

What do you use to create smooth, aesthetically pleasing graphs in R, which can be used to impress people other than math geeks and statisticians?

Is there any way of changing ""color schemes"" of graphs quickly?

Not trolling. Serious question from a beginner.",en
1106509,2010-11-20 14:45:07,MachineLearning,Hello /MachineLearning. I am hoping to get a Masters(and perhaps a PhD) in Machine Learning and was wondering if you can answer some questions.,e93dw,braclayrab,1158972013.0,https://www.reddit.com/r/MachineLearning/comments/e93dw/hello_machinelearning_i_am_hoping_to_get_a/,20.0,48.0,"First, what is seen as the difference between Machine Learning and Artificial Intelligence in academia? Has AI fallen into disrepute? 

Second, I believe that studying the brain is necessary in order to learn how to build AI systems. If a professor studies ""Neural Networks"", is that the same thing? I am interested in studying the entire brain(and nervous system). How far would that be from ""neural networks""?

Thirdly, does anyone have any information about ETH in Zurich? I am determined to go there, please send me a PM.

Edit:
Thanks for all the info everyone!",en
1106510,2010-11-20 17:54:26,MachineLearning,AskML: Internships in in the industry for ML (NY area),e94sw,notyetcsgrad,1290267834.0,https://www.reddit.com/r/MachineLearning/comments/e94sw/askml_internships_in_in_the_industry_for_ml_ny/,4.0,3.0,"I am grad student near NY, and many companies are coming forward for internships. Can anyone recommend a place looking where one can work on/experiment with machine learning techniques?

It's not something I have experience with (I've had 3 years in ASP.net), but it is something I want to learn.",en
1106511,2010-11-21 05:18:58,statistics,Learning to use STATA (have never used any statistical software),e9cmi,statastata,1290307813.0,https://www.reddit.com/r/statistics/comments/e9cmi/learning_to_use_stata_have_never_used_any/,0.0,4.0,"So I majored in International Relations and Development and I've noticed that a lot of jobs in my field want candidates that have some data analysis skills, i.e. ability to perform descriptive and econometric analysis of cross-section and panel data.

I've used excel quite a bit to build time series, tables and graphs, but STATA seems a lot more challenging, especially since I'm not that familiar with statistics in general.

So how hard is it to learn to use this software to perform, what I would like to believe, are relatively simple types analyses? 

thanks in advance for all your help!",en
1106512,2010-11-21 06:09:22,statistics,How to go about finding a relationship between racers starting positions and finishing positions.,e9d5j,f4m1n3,1290015129.0,https://www.reddit.com/r/statistics/comments/e9d5j/how_to_go_about_finding_a_relationship_between/,4.0,13.0,"This question is regarding a report I have to do for a class.  I tried posting in the homework help subreddits but never received an answers so now I appeal to you.

I'm trying to do a test to show a  correlation or lack of correlation between starting and finishing positions.  My data comes from here:  http://www.amstat.org/publications/jse/datasets/nascard.dat

Originally I had done a Chi-Sq test for independence.  I had created categories as such:

    proc format;
    value startposgroup          low-10 = '0-10'
    			11-20 = '11-20'
                21-30 = '21-30'
    			31-high = '31+';

    /*categorize finish position*/
    proc format;
    value finishposgroup          low-10 = '0-10'
    				11-20 = '11-20'
    				21-30 = '21-30'
     				31-high = '31+';

 but then I realized that starting and finishing positions were ordinal data therefore the Chi-Sq test would probably not be appropriate.  Does my grouping of the start/finish positions allow me to run a Chi-Sq or must I use Spearman's Rank Correlation?",en
1106513,2010-11-21 21:49:56,statistics,Fifty Ways to Fix your Data | Stats With Cats Blog,e9krc,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/e9krc/fifty_ways_to_fix_your_data_stats_with_cats_blog/,0.0,0.0,,en
1106514,2010-11-22 03:46:34,MachineLearning,Half decent tool for importing &amp; manipulating spatial data from many many formats,e9pec,defrost,1193799184.0,https://www.reddit.com/r/MachineLearning/comments/e9pec/half_decent_tool_for_importing_manipulating/,3.0,1.0,,en
1106515,2010-11-22 08:12:16,MachineLearning,Animating a function's 3D wireframe plots using R / ImageMagick,e9sw6,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/e9sw6/animating_a_functions_3d_wireframe_plots_using_r/,7.0,1.0,,en
1106516,2010-11-22 19:36:45,MachineLearning,GA Tech “dialect topic modeling” system learns to read medical documents with varying levels of technical detail,ea19p,stoplan,1272908585.0,https://www.reddit.com/r/MachineLearning/comments/ea19p/ga_tech_dialect_topic_modeling_system_learns_to/,14.0,1.0,,en
1106517,2010-11-23 09:15:49,statistics,Repost from r/science: Rejecting Precognition using Bayesian Statistics,eae97,[deleted],,https://www.reddit.com/r/statistics/comments/eae97/repost_from_rscience_rejecting_precognition_using/,14.0,4.0,,en
1106518,2010-11-23 21:22:25,statistics,Need a good SPC reference,eaog7,jbrandon,1226956931.0,https://www.reddit.com/r/statistics/comments/eaog7/need_a_good_spc_reference/,4.0,4.0,I just began working in a chip fab and have been asked to introduce SPC to the assembly area I'm working in. Can anyone recommend a good text on SPC? I'm just getting introduced to it.,en
1106519,2010-11-23 23:32:04,artificial,Stanford Parser Preprocessing Techniques,eaqun,imneal,1260042947.0,https://www.reddit.com/r/artificial/comments/eaqun/stanford_parser_preprocessing_techniques/,1.0,3.0,"Are any of you aware of preprocessing techniques for the stanford parser? I would like to preprocess sentences such that they are better parsed by the dependency parser.  Any links, books, papers? 

Thanks! 
",en
1106520,2010-11-24 00:13:40,MachineLearning,"Theano 0.3.0 has been released. Theano is an optimizing compiler for evaluating math expressions on CPUs and GPUs. Developed by the LISA machine learning lab at the University of Montreal, it is particularly suited to gradient-based machine learning. (xpost from /r/Python)",earlq,dwf,1180746219.0,https://www.reddit.com/r/MachineLearning/comments/earlq/theano_030_has_been_released_theano_is_an/,12.0,0.0,,en
1106521,2010-11-24 01:05:25,MachineLearning,"$10,000 on offer for a competition to build an algorithm that predicts commute times on a Sydney freeway",easib,antgoldbloom,1285588580.0,https://www.reddit.com/r/MachineLearning/comments/easib/10000_on_offer_for_a_competition_to_build_an/,32.0,13.0,,en
1106522,2010-11-24 03:15:17,MachineLearning,Suggested books or online courses for machine learning?,eaudv,PeoriaJohnson,1254792524.0,https://www.reddit.com/r/MachineLearning/comments/eaudv/suggested_books_or_online_courses_for_machine/,9.0,8.0,"I have a solid mathematical background, and I'm ready to hop into the field.  What textbook do you suggest?  Have you seen worthwhile online courses?  

In short, how did you learn the subject?",en
1106523,2010-11-24 13:49:03,MachineLearning,Jeff Hawkins talking about recent advances in Hierarchical Temporal Memory algorithms -- 12th November 2010,eb273,[deleted],,https://www.reddit.com/r/MachineLearning/comments/eb273/jeff_hawkins_talking_about_recent_advances_in/,18.0,1.0,,en
1106524,2010-11-24 18:43:39,analytics,How to Monitor Promotional Campaigns with Google Analytics,eb66l,craigbuckler,1231584052.0,https://www.reddit.com/r/analytics/comments/eb66l/how_to_monitor_promotional_campaigns_with_google/,1.0,0.0,,en
1106525,2010-11-25 20:40:08,artificial,Reminder: Support Creation of the Artificial Intelligence Q&amp;A area on Stack Exchange,ebq6g,psykocrime,1205139983.0,https://www.reddit.com/r/artificial/comments/ebq6g/reminder_support_creation_of_the_artificial/,9.0,1.0,,en
1106526,2010-11-26 20:19:00,MachineLearning,New generalization of convolutional networks gets better results on CIFAR dataset,ec555,[deleted],,https://www.reddit.com/r/MachineLearning/comments/ec555/new_generalization_of_convolutional_networks_gets/,8.0,1.0,,en
1106527,2010-11-27 06:35:41,MachineLearning,"Just you you know, Machine Learning Journal from SpringerLink has free complete access until December 31",ecchp,DrRobotnic,1274939093.0,https://www.reddit.com/r/MachineLearning/comments/ecchp/just_you_you_know_machine_learning_journal_from/,17.0,11.0,,en
1106528,2010-11-28 18:05:24,statistics,Secrets of Good Correlations | Stats With Cats Blog,ecvtj,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/ecvtj/secrets_of_good_correlations_stats_with_cats_blog/,8.0,5.0,,en
1106529,2010-11-29 13:51:09,analytics,Twitter-lytics? Micro blogging giant announces launch of its own analytics platform,eda7c,mattwalkerseo,1291031341.0,https://www.reddit.com/r/analytics/comments/eda7c/twitterlytics_micro_blogging_giant_announces/,1.0,0.0,,en
1106530,2010-11-29 14:29:00,MachineLearning,Wall Street Firm Uses Algorithms to Make Sports Betting Like Stock Trading,edaj3,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/edaj3/wall_street_firm_uses_algorithms_to_make_sports/,20.0,4.0,,en
1106531,2010-11-29 19:02:31,statistics,SAS-X: an online (unofficial) journal about the SAS(R) software - written by bloggers,edea0,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/edea0/sasx_an_online_unofficial_journal_about_the_sasr/,5.0,2.0,,en
1106532,2010-11-30 08:33:09,statistics,How do you compare two time series?,edrsx,[deleted],,https://www.reddit.com/r/statistics/comments/edrsx/how_do_you_compare_two_time_series/,1.0,14.0,"In my line of work I very often receive time series data (like network logs) which I can transmogrify into an array of form {timestamp,  count of interesting events} or {timestamp, measurement of interesting value)

I'm interested in finding ways to compare two time series of this sort, but I'm not sure where to start.

In practical terms, I want to be able to take network logs from two different enclaves, and then figure out the probability that the difference between the two data sets is due to random chance.  I have a pretty good idea already of how to decompose the time series into (e.g.) trend features and cyclical features, I have a functional understanding of moving averages, etc.

What I don't grok is how I can do something like an ANOVA or t-test to be able to say ""The difference between these two time series is statistically significant.""

Anyone want to clue me in?  Links are definitely welcome, I will read just about anything.  Thanks!",en
1106533,2010-11-30 15:58:38,MachineLearning,"Hi /r/ML, I need some help with detecting changes in dynamical data! ",edwcj,BombFire,1274860634.0,https://www.reddit.com/r/MachineLearning/comments/edwcj/hi_rml_i_need_some_help_with_detecting_changes_in/,6.0,15.0,"I'm a researcher charged with developing algorithms that can detect changes in a mineralogical process. The process is in closed loop (under expert system control) and as such, there exists a high degree of dependence (cross correlation) between some of the input variables. Moreover, all of the time series show strong serial/auto correlation structure. Some questions:

* 1) Has anyone had success with this type of problem (change point detection on non i.i.d. data)? If so, any guidance will be much appreciated.

* 2) The serial correlation means the data is non i.i.d., don't statistical learning theory assume the training data is i.i.d.? Should I not therefore be posting this on [/r/DSP](http://www.reddit.com/r/DSP/)?
",en
1106534,2010-11-30 17:56:39,MachineLearning,Riders on a swarm,edy2w,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/edy2w/riders_on_a_swarm/,11.0,0.0,,en
1106535,2010-11-30 21:11:05,analytics,Optimizing with SeeVolution Website Analytics for Google Places | Analytics News,ee1iz,seevolution,1288998135.0,https://www.reddit.com/r/analytics/comments/ee1iz/optimizing_with_seevolution_website_analytics_for/,1.0,1.0,,en
1106536,2010-12-01 07:46:16,statistics,Please help with stats homework!!,eec78,bokonamama,1290840912.0,https://www.reddit.com/r/statistics/comments/eec78/please_help_with_stats_homework/,0.0,6.0,"So here's the question:

In 2004, the Internal Revenue Service received 312,226,042 individual tax returns. Of these, 12,757,005 reported an adjusted gross income of at least $100,000, and 240,128 reported at least $1 million. If you know that a randomly chosen return shows an income of $100,000 or more, what is the conditional probability that the income is at least $1 million?

So here's what I did but I have NO idea if it is correct and I need to understand this stuff for the final! :(

P(A) = an income of at least $100,000

So P(A) = 12,757,005 / 312,226,042

P(A) = .041

P(B) = an income of at least $1 million

So P(B) = 240,128 / 312,226,042

P(B) = .00077

SO here's what I did - of the .041 or 4.1% who have an income of at least $100,000, .00077 or .07% make $1 million. So, .07% OF the 4.1% is:

.00077/.041 = .018 or 1.8%

Is this right................?
",en
1106537,2010-12-01 17:03:35,MachineLearning,5 of the Best Free Data Mining Software,eeiup,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/eeiup/5_of_the_best_free_data_mining_software/,18.0,9.0,,en
1106538,2010-12-01 23:09:39,statistics,Some help with measurement uncertainties required,eeppd,Madmonkey91,1291236978.0,https://www.reddit.com/r/statistics/comments/eeppd/some_help_with_measurement_uncertainties_required/,2.0,3.0,"I am a 2nd year mechanical engineering student doing an internship at an aircraft maintenance firm. Currently I am tasked with calculating uncertainty values derived from calibration tests, so that they can be recorded on the test certificates. 

I am doing this by first using the calibration certificates provided by labs using reference standards, reading the standard uncertainty value off it, and then combining that with standard uncertainties derived from in house testing. The in house testing procedure is as follows: 

1) Take 3 measurements

2) Derive mean and standard deviation values

3) Using student's t-distribution with 2 DoF, k=4.3 

http://en.wikipedia.org/wiki/Student%27s_t-distribution

I have an excel spreadsheet to calculate the combined expanded uncertainty with inputs for the standard uncertainty from the reference standards, in house testing as well as for the k value/DoF, with sheets for each individual instrument being calibrated.

Unfortunately I have not had much experience in this field in the past and as such am running into several problems with my understanding of the subject matter and thus getting stuck. For example, the calibration takes place over a range of values. Is it necessary to calculate the uncertainty at every test point? Because that seems way too time consuming for the person carrying out the calibration ( even with the use of my excel spreadsheet). Is there some way of calculating a total uncertainty for a device?

EDIT: Just realised hadn't specified formulae used for calculating combined expanded undertainty. Total degreres of freedom are calculated using the Welch-Satterthwaite formula and the total combined standard uncertainty is calculated by adding all the input standard uncertainties in quadrature. Then the coverage factor is derived and multiplied to the combined standard uncertainty give the combined expanded uncertainty",en
1106539,2010-12-03 00:08:43,statistics,"Hans Rosling's 200 Countries, 200 Years, 4 Minutes - The Joy of Stats",ef98r,higherpower,1289835583.0,https://www.reddit.com/r/statistics/comments/ef98r/hans_roslings_200_countries_200_years_4_minutes/,11.0,1.0,,en
1106540,2010-12-03 00:26:53,statistics,Several Statistics Supplications,ef9kf,[deleted],,https://www.reddit.com/r/statistics/comments/ef9kf/several_statistics_supplications/,5.0,8.0,"It seems like you folks get a lot of these requests around here, but the almighty internet is beginning to fail me and the rudimentary statistics taught in my introductory university course have left me ill prepared for the problems i'm dealing with. So if somebody would be as kind as to point me in the right direction here i would really appreciate it.

Basically I'm doing some independent research involving glacial lakes and have a few (many) questions as to how to best analyze the data. I have divided my lake sample population (n=356) into 4 discrete categories based on mapping parameters and wish to look for differences between each of these categories for several standard water chemistry/lake morphology parameters.

The way I decided to do this was using ANOVA and then a Tukey-Kramer test to look for a significant difference between the mean values of the different variables for each of the groups.

**[1]** Do i need to adjust the p-value from 0.05? I read that in multiple comparisons you sometimes need to use and adjusted p-value, but sometimes you don't, and can't find consistent recommendations either way. 

**[2]** Much of my data is non-normal, and/or the variances across groups are not equal. I am using data transformations (log x, sqrt x, etc) in order to produce approx. normal frequency distributions for the ANOVA (how sensitive is ANOVA to departures from normality and equal variance)? However, I have a few variables that remain intractable ([example](http://i.imgur.com/Lf9cp.png)). Are non-parametric tests like Kruskal-Wallis suitable substitutes for ANOVA, and which would work best?

**[2.5]** Is it possible to do post-hoc analysis like the Tukey-Kramer test after a non-parametric test? 

**[2.75]** What's the difference between doing 6 t-Tests and doing an ANOVA+Tukey test? Would i need to adjust p-values for the t-Tests?

**[2.99999...]** After doing a data-transformation, I have to back-transform my results, or so I have heard. When, procedure-wise, do you do the back-transform? Should I report the mean (and other statistics) of the untransformed data, or the statistics of the back-transformed data?

**[3]** How much does sample size matter? I can't really do anything about changing the size of my samples; the number of lakes in each category is 171, 139, 34, and 12. This is a rough reflection of how common each lake type actually is, but it also makes inferring things very difficult if i only have 12 lakes of one type, right?

I know this is kind of a lot but all my googling and reading isn't helping anymore. I just feel like i'm in rather over my head here. If someone can help it would be very appreciated :)

Thanks!",en
1106541,2010-12-03 03:07:12,MachineLearning,"Some Internet-Use Tracking Firms to Reveal What They Know -- a group of online tracking rivals are building a service that lets consumers see what information those companies know about them.

",efc4s,Wonnk13,1287022683.0,https://www.reddit.com/r/MachineLearning/comments/efc4s/some_internetuse_tracking_firms_to_reveal_what/,17.0,0.0,,en
1106542,2010-12-03 04:36:16,statistics,Suggestions for getting started with R?,efdcp,chirpychirp,1277776610.0,https://www.reddit.com/r/statistics/comments/efdcp/suggestions_for_getting_started_with_r/,22.0,11.0,"I want to start using R and I'm a little overwhelmed by the program. I started messing around with SAS and things were going well (I have a very limited knowledge of coding), running simple tests and graphs etc. but I'd like to be fluent in R so I can have the capabilities of working at home. 

There are tons of websites, books and tutorials out there. Do you folks suggest something to read over another? Tips for starting out? What you wish you knew then that you do now? 

I have data! Help me play with it!",en
1106543,2010-12-03 05:53:37,computervision,Anyone attend ISVC?,efej3,zwa,1205753869.0,https://www.reddit.com/r/computervision/comments/efej3/anyone_attend_isvc/,4.0,0.0,"Stuck in the airport due to snow related delays, so thought I'd ask. Favourite paper was probably the ""Curve Filter Transform"", mostly because it relates closely to what I'm doing (curvilinear feature extaction), but with a more principled basis. Any papers stand out for you?

Proceedings here:
http://www.springerlink.com/content/978-3-642-17288-5/contents/
http://www.springerlink.com/content/978-3-642-17273-1/contents/
http://www.springerlink.com/content/978-3-642-17276-2/contents/",en
1106544,2010-12-03 23:33:50,statistics,X-post from /r/math: Question taking confidence into account when calculating entropy,eftp2,the_merk,1286900499.0,https://www.reddit.com/r/statistics/comments/eftp2/xpost_from_rmath_question_taking_confidence_into/,2.0,0.0,,en
1106545,2010-12-05 02:49:38,data,Linked Open Data star scheme by example,egc7o,sebastien_vigneau,1261332598.0,https://www.reddit.com/r/data/comments/egc7o/linked_open_data_star_scheme_by_example/,1.0,0.0,,en
1106546,2010-12-05 03:26:52,MachineLearning,Generating samples from Gaussian,egco4,humble_human,1271426509.0,https://www.reddit.com/r/MachineLearning/comments/egco4/generating_samples_from_gaussian/,7.0,11.0,"Hi,

I am kinda new to this field so this might be a really stupid question. But the thing is that I have thought really hard about this and haven't found any mention of it anywhere. 

Given a mean and variance how do you generate data points from that univariate gaussian? And given a mean vector and covariance matrix how do you generate points from a multivariate gaussian?",en
1106547,2010-12-05 13:03:55,statistics,Many Paths Lead to Models | Stats With Cats Blog,egidc,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/egidc/many_paths_lead_to_models_stats_with_cats_blog/,0.0,0.0,,en
1106548,2010-12-05 17:40:24,MachineLearning,Genetic optimization for Trading Strategies using Rapidminer and R,egkgt,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/egkgt/genetic_optimization_for_trading_strategies_using/,27.0,4.0,,en
1106549,2010-12-05 23:11:04,statistics,"The ""President of Madagascar"" comics as a learning tool?",egoqn,[deleted],,https://www.reddit.com/r/statistics/comments/egoqn/the_president_of_madagascar_comics_as_a_learning/,0.0,0.0,,en
1106550,2010-12-06 05:46:32,computervision,OpenCV now available with support for GPU processing,eguae,conniption,1275789317.0,https://www.reddit.com/r/computervision/comments/eguae/opencv_now_available_with_support_for_gpu/,14.0,1.0,,en
1106551,2010-12-06 16:31:24,MachineLearning,Reasoning About a Highly Connected World,eh1dx,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/eh1dx/reasoning_about_a_highly_connected_world/,23.0,1.0,,en
1106552,2010-12-07 10:16:08,statistics,Need help for college paper!,ehjnj,[deleted],,https://www.reddit.com/r/statistics/comments/ehjnj/need_help_for_college_paper/,0.0,3.0,,en
1106553,2010-12-07 13:11:38,statistics,Is this statistically significant? (not homework),ehljf,logantauranga,1212045370.0,https://www.reddit.com/r/statistics/comments/ehljf/is_this_statistically_significant_not_homework/,4.0,10.0,"On my last diet, there were 25 weeks in which I averaged between 6000-7000 kJ (1434-1673 kcal) daily. I lost weight on 19 of these weeks.

Is this statistically significant, and how confident could I be of losing weight given a similar week in the future?",en
1106554,2010-12-07 16:49:58,MachineLearning,$3 Million Health Prize Challenges Innovators to Improve Health Through Analytics,ehoah,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/ehoah/3_million_health_prize_challenges_innovators_to/,1.0,1.0,,en
1106555,2010-12-07 17:11:10,statistics,Pretty please help me! Final is tomorrow! Stuck on this one problem....? ,ehon6,bokonamama,1290840912.0,https://www.reddit.com/r/statistics/comments/ehon6/pretty_please_help_me_final_is_tomorrow_stuck_on/,0.0,4.0,"6)	The American Red Cross says that about 45% of the U.S. population has Type O blood, 40% has Type A, 11% has Type B, and the rest are Type AB.

iv.	At least one person is Type B?

This was the only one I couldn't figure out how to do....? 
",en
1106556,2010-12-07 21:09:00,statistics,I Need a Link to Any Professional Journal That Has a Hypothesis Test in it.  Please Help Me Find One.,eht8l,showbreadrules,1264768737.0,https://www.reddit.com/r/statistics/comments/eht8l/i_need_a_link_to_any_professional_journal_that/,0.0,1.0,"I need to write a 1-2 page report showing the test and how it relates to the research.  This is for a basic statistics class, but the journal can be about any subject.",en
1106557,2010-12-08 00:09:06,statistics,How much should I charge?,ehwu3,xxgambinoxx,,https://www.reddit.com/r/statistics/comments/ehwu3/how_much_should_i_charge/,9.0,15.0,"So I've been asked to do some very simple analysis for a resident at a hospital. Essentially, a couple a few survival analyzes(KM) and get the results in a publishable format. He has told me he has the funding to pay me but I'm not sure how much to charge. 

The data set is fairly clean and only took a few hours to get into a usable format. I'd say I could get the whole thing ready to go in a couple days max.

So Reddit, How much should I charge?",en
1106558,2010-12-08 00:20:00,MachineLearning,"Wikileaks has 1,000,000 supporters on facebook --rapidly gaining more.",ehx10,[deleted],,https://www.reddit.com/r/MachineLearning/comments/ehx10/wikileaks_has_1000000_supporters_on_facebook/,0.0,3.0,,en
1106559,2010-12-08 08:26:57,statistics,Am I receiving a proper math/statistics education?,ei4ui,sirsosay,1254514768.0,https://www.reddit.com/r/statistics/comments/ei4ui/am_i_receiving_a_proper_mathstatistics_education/,9.0,20.0,"Here's an overview of what I've done:

Math:
Calc 2-3,
Diffy Q,
Linear Algebra,
Discrete

Stats:
Intro Stats,
Elem. Probability Theory,
Elem. Statistics Theory,
Sampling Techniques,
Multiple Linear Regression

...and a programming course

I'm asking because I'm done with my math requirements, and I'm planning on graduating.  The thing is, I feel like I'm lukewarm in my education.  Like I got a taste of math and a taste of statistics, and now they're just going to hand me a degree without me being very specialized in either. (I'm a math major with an emphasis in statistics).

When I come to this reddit,  more often than I would like the topics go straight over my head as if I have no idea what in the hell stats really is.  I'm much more comfortable in the math reddit, but even there, a lot of it is wooshing over my head as well.

Plain and simple: I feel like a phony.  If I go for my masters at a prestigious school after I graduate, am I going to get my butt kicked?  Why do I feel my degree should have been more difficult?

Also, another thing that has always bothered me.  I took most of my math classes my freshman and sophomore year.  I haven't taken Calc 3 or diffyQ in almost 3 years, and I sure as hell don't remember a lot of it.  Will this affect my ability to be successful in graduate school?

Thanks for any advice.
",en
1106560,2010-12-08 11:30:07,MachineLearning,A Mind Made from Memristors (/r/cogsci repost),ei73d,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/ei73d/a_mind_made_from_memristors_rcogsci_repost/,0.0,1.0,,en
1106561,2010-12-08 16:09:23,MachineLearning,"Five data blogs you should read (Data geekery, visualization and journalism)",eia89,fbahr,1275413362.0,https://www.reddit.com/r/MachineLearning/comments/eia89/five_data_blogs_you_should_read_data_geekery/,32.0,0.0,,en
1106562,2010-12-08 22:40:30,MachineLearning,"Mining of Massive Datasets: Rajaraman, Ullman (PDF, draft text)",eihvx,gtani7,1203126718.0,https://www.reddit.com/r/MachineLearning/comments/eihvx/mining_of_massive_datasets_rajaraman_ullman_pdf/,16.0,1.0,,en
1106563,2010-12-09 01:04:33,MachineLearning,Dear reddit: help with mapping a community,eikjt,orangepotion,1246804916.0,https://www.reddit.com/r/MachineLearning/comments/eikjt/dear_reddit_help_with_mapping_a_community/,3.0,8.0,"I am mapping a social media community to find their links, their clusters and their structure, using R and Ruby. What community do you suggest? I have thought of flickr, twitter and foursquare, but I am open to suggestions.",en
1106564,2010-12-09 14:24:33,MachineLearning,"/r/datasets/ - Datasets for Data Mining, Analytics and Knowledge Discovery",eiw2w,fbahr,1275413362.0,https://www.reddit.com/r/MachineLearning/comments/eiw2w/rdatasets_datasets_for_data_mining_analytics_and/,19.0,1.0,,en
1106565,2010-12-09 22:53:04,statistics,"Please Reddit/r/statistics, do you know where to find hsbdataB.sav? ",ej4zn,y2kerick,1284996208.0,https://www.reddit.com/r/statistics/comments/ej4zn/please_redditrstatistics_do_you_know_where_to/,5.0,6.0,"I live in a third world country (Bolivia, probably you've never heard of it) so it's hard for me to apply to a credit card, that's why I can't buy the book ""SPSS for intermediate statistics"" which comes whit the file I need (hsbdataB.sav) 
Do you know where I can find it?
 
PS I already looked for it in the internet (e.g. torrents)

PS2 The book uses some other files like college student data.sav, product data.sav, mixedMANOVAdata.sav etc. It would be terrific if you tell where I can get them, too.

PS3 I've mentioned the credit card situation because the only way I'm able to buy this book is through Amazon",en
1106566,2010-12-10 00:41:48,datasets,SNAP: Network datasets: 476 million Twitter tweets,ej70h,orangepotion,1246804916.0,https://www.reddit.com/r/datasets/comments/ej70h/snap_network_datasets_476_million_twitter_tweets/,11.0,4.0,,en
1106567,2010-12-10 03:22:52,datasets,Facebook and Last.fm datasets,ej9ow,pubmixful,1291943990.0,https://www.reddit.com/r/datasets/comments/ej9ow/facebook_and_lastfm_datasets/,19.0,2.0,,en
1106568,2010-12-10 17:08:42,MachineLearning,R client library for the Google Prediction API,ejkcr,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/ejkcr/r_client_library_for_the_google_prediction_api/,16.0,5.0,,en
1106569,2010-12-11 07:27:28,MachineLearning,Shane Legg - Universal Measures of Intelligence [video 41:36],ejy5s,dakk12,1185850891.0,https://www.reddit.com/r/MachineLearning/comments/ejy5s/shane_legg_universal_measures_of_intelligence/,13.0,8.0,,en
1106570,2010-12-11 08:50:05,statistics,Model with multiple factor predictors in R,ejz3c,valen089,1288238268.0,https://www.reddit.com/r/statistics/comments/ejz3c/model_with_multiple_factor_predictors_in_r/,5.0,4.0,"I'm using R to do an analysis on a linear model with three categorical predictors and two continuous predictors.  The problem I'm having is that I unsure of how to determine how well the terms predict the response given that I have multiple categorical terms.  Basically this is what I've been able to do so far:

    &gt; model = pod(Y ~ X1 + X2, group=X3)
    &gt; anova(model)

Unfortunately this does not include all of the terms I'm trying to test :-(",en
1106571,2010-12-11 18:03:24,analytics,Preview of SeeVolution Real-Time Mouse Movement Heatmap Analytics! | Analytics News,ek3rm,seevolution,1288998135.0,https://www.reddit.com/r/analytics/comments/ek3rm/preview_of_seevolution_realtime_mouse_movement/,1.0,1.0,,en
1106572,2010-12-12 05:02:25,statistics,Could someone explain how to approach this task? (factory makes bottles of 1L capacity which is acceptable to vary by 30 ML ).,ekcbd,cadddr,1292122646.0,https://www.reddit.com/r/statistics/comments/ekcbd/could_someone_explain_how_to_approach_this_task/,0.0,3.0,"Factory makes bottles of 1L capacity which is acceptable to vary by 30 ML (970-1030 ML total).
In a batch of 100 bottles, what is the probability that all the bottles will have less than 101.8 L total capacity.

I tried to compute expected value of 101.8 but nothing works since I don't have the standard deviation here.

Any help welcome.",en
1106573,2010-12-12 17:50:34,statistics,Show variable-'neighbour' correlation,ekipz,[deleted],,https://www.reddit.com/r/statistics/comments/ekipz/show_variableneighbour_correlation/,1.0,0.0,,en
1106574,2010-12-12 19:02:40,datasets,Help me enter data to build a College Football Bowl Game model!  I'm halfway there!,ekjl1,[deleted],,https://www.reddit.com/r/datasets/comments/ekjl1/help_me_enter_data_to_build_a_college_football/,0.0,0.0,"This is a good website to get the data from (where I've been getting it from): http://www.cfbstats.com/


Here's the spreadsheet: https://spreadsheets.google.com/ccc?key=tphrNSOKkgVHG1qgfoNddPw&amp;hl=en#gid=0",en
1106575,2010-12-12 20:29:06,statistics,The Seeds of a Model | Stats With Cats Blog,ekkrr,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/ekkrr/the_seeds_of_a_model_stats_with_cats_blog/,3.0,1.0,,en
1106576,2010-12-12 22:23:26,MachineLearning,"I am now a Masters of Computer Science and Information Systems! Here is my thesis: ""Identifying Individuals Using Facial Dynamics with Active Appearance-based Hidden Markov Models""",ekmgd,[deleted],,https://www.reddit.com/r/MachineLearning/comments/ekmgd/i_am_now_a_masters_of_computer_science_and/,24.0,2.0,,en
1106577,2010-12-13 06:38:47,MachineLearning,R with Vim on Mac OS X;  Computational Mathematics,ekts4,Wonnk13,1287022683.0,https://www.reddit.com/r/MachineLearning/comments/ekts4/r_with_vim_on_mac_os_x_computational_mathematics/,7.0,3.0,,en
1106578,2010-12-13 11:43:08,MachineLearning,"Melbourne Uni is offering $5,000 for an algorithm to predict the outcome of grant applications",ekxt1,antgoldbloom,1285588580.0,https://www.reddit.com/r/MachineLearning/comments/ekxt1/melbourne_uni_is_offering_5000_for_an_algorithm/,30.0,4.0,,en
1106579,2010-12-13 12:39:42,statistics,Room for one more in statistical consulting industry?  Tips for a prospective freelancer?,ekybf,[deleted],,https://www.reddit.com/r/statistics/comments/ekybf/room_for_one_more_in_statistical_consulting/,7.0,5.0,,en
1106580,2010-12-13 23:58:19,statistics,F-tests and large degrees of freedom?,el8uz,[deleted],,https://www.reddit.com/r/statistics/comments/el8uz/ftests_and_large_degrees_of_freedom/,5.0,10.0,"So I'm an astrophysicist (not a statistician) working on a large data set, and I'm trying to find the optimal order of a fit to some data. I've fit two dimensional models (using Chi-Squared minimization) to the data, but since the data (by visual inspection on a 3D plot) appears to have a larger dependence on one of the variables over the other, I thought I would do some F-tests to compare fits of various orders to determine which fit is ""optimal.""

My problem, though, is that the data set is quite large... and hence I have a large number of degrees of freedom (i.e. Fitting four thousand data points with a 21-parameter model, or a 5th order polynomial in two variables). So, I'm not sure about how to go about computing the probability for finding an f-value larger than the one that I have, given that the formula for the probability includes the Beta function... and I'd have to compute that Beta function for arguments on the order of 1000.

Does anyone have any idea on alternate methods for testing the benefit of additional terms when large degrees of freedom are involved?",en
1106581,2010-12-14 00:06:57,statistics,Chi square problem. Help?,el90r,Yorkatron,1289335874.0,https://www.reddit.com/r/statistics/comments/el90r/chi_square_problem_help/,0.0,0.0,"I have a problem I was working on and just didn't really get it, I was wondering if anyone could help me out?

A survey finds that 56 females prefer men who are much taller than them, and 50 females prefer men only slightly taller than themselves. With alpha =.05, what can you conclude about the preference of females?",en
1106582,2010-12-14 02:44:33,MachineLearning,"Genetic Algorithm for ""Hello World"". In JavaScript.",elbqx,[deleted],,https://www.reddit.com/r/MachineLearning/comments/elbqx/genetic_algorithm_for_hello_world_in_javascript/,60.0,15.0,,en
1106583,2010-12-14 08:07:25,MachineLearning,Alan Mislove - You Are Who You Know: Inferring User Profiles in Online Social Networks [video 19:25],elh34,dakk12,1185850891.0,https://www.reddit.com/r/MachineLearning/comments/elh34/alan_mislove_you_are_who_you_know_inferring_user/,10.0,1.0,,en
1106584,2010-12-14 16:04:40,statistics,Florence Nightingale: The compassionate statistician,elmn4,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/elmn4/florence_nightingale_the_compassionate/,9.0,1.0,,en
1106585,2010-12-14 16:45:52,MachineLearning,"ICML 2011, The 28th International Conference on Machine Learning",elnbw,orangepotion,1246804916.0,https://www.reddit.com/r/MachineLearning/comments/elnbw/icml_2011_the_28th_international_conference_on/,17.0,1.0,,en
1106586,2010-12-15 06:21:31,statistics,"I worked my way through Yudkowsky's ""An Intuitive Explanation of Bayes' Theorem,"" and I think I'm starting to get it.  Can anyone tell me if I'm on the right the track?",em11w,tree_or_up,1263601594.0,https://www.reddit.com/r/statistics/comments/em11w/i_worked_my_way_through_yudkowskys_an_intuitive/,11.0,8.0,"[Yodkowsky's explanation](http://yudkowsky.net/rational/bayes)

Please go easy on me if I've got this totally wrong.  My math/stats background isn't strong, but I'm finding certain aspects of statistics really fascinating, and I really want to learn more.

Let's suppose that:

25% of all internet users have acted like a troll at least once.
20% of people who've acted like trolls have reddit accounts.
40% of people who have reddit accounts have never trolled.

What's the likelihood that someone with a reddit account has also trolled at least once?

Okay, let's assume we have 100 people total.  So 25 of them have trolled, and 75 haven't.  5 of those who have trolled (20% of the 25) also have reddit accounts.  Of the people who haven't trolled, 30 have reddit accounts (40% of 75).

So, the total number of people with reddit accounts is 35 (5 redditors who have trolled + 30 redditors who haven't).  Of those 35, 5 have trolled -- which means that there's approximately a 14% chance (5/35) that any given redditor has trolled at least once.

Did I get that right?",en
1106587,2010-12-15 10:57:54,analytics,Looking for advanced career opportunities?,em4qv,iqrconsulting,1288244989.0,https://www.reddit.com/r/analytics/comments/em4qv/looking_for_advanced_career_opportunities/,1.0,0.0,,en
1106588,2010-12-16 10:58:36,statistics,Hans Rosling's Chart is SERIOUSLY MISLEADING,emqlj,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/emqlj/hans_roslings_chart_is_seriously_misleading/,0.0,1.0,,en
1106589,2010-12-17 12:06:49,MachineLearning,Essentials of Metaheuristics,enc9t,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/enc9t/essentials_of_metaheuristics/,27.0,4.0,,en
1106590,2010-12-17 13:11:44,statistics,Quick question: How to carry out hypothesis testing in the Neyman-Pearson framework when I want to show that there's no significant difference.,encup,CommentSense,1292583891.0,https://www.reddit.com/r/statistics/comments/encup/quick_question_how_to_carry_out_hypothesis/,5.0,14.0,"This came up when consulting a client who wanted to show that using two separate machines to measure outcomes is a not an issue, i.e. they produce the same results. He wanted to prove that these machines' outcomes are not statistically different. Using

Ho: mu1 = mu2 vs. Ha: mu1 != mu2

a p-value &gt; 0.05 (or some other critical value) could mean that 1) they are indeed not significantly different or 2) not enough power.

So how do I systematically show it's (1)? I know this is not a problem in the Bayesian framework but I want to do this using a frequentist approach. 

Much thanks.",en
1106591,2010-12-17 18:16:56,MachineLearning,Redditors: Let me know your preferred data mining software,engmb,[deleted],,https://www.reddit.com/r/MachineLearning/comments/engmb/redditors_let_me_know_your_preferred_data_mining/,18.0,24.0,"For data prediction (not statistical analysis) which one is your favourite and preferred tool: Weka vs R vs IBM SPSS.

Or do you have any other personal favourite?",en
1106592,2010-12-18 14:33:08,statistics,Why is the mode such a neglected statistical parameter?,enw1v,CommentSense,1292583891.0,https://www.reddit.com/r/statistics/comments/enw1v/why_is_the_mode_such_a_neglected_statistical/,1.0,17.0,"It is after all the most likely value of a distribution. Yet, we focus so much on the mean and sometimes the median. But if your population is skewed and sample size is not large, wouldn't asking what is the most probable outcome be the right question?

It seems that for continuous distributions we tend to ignore this notion for the sake of mathematical simplicity -- mode is just not pretty to deal with. But if I'm a gambling man then my money is on the mode. Casinos on the other hand have the law of large numbers on their side and they can bet on the expected value. Just a thought.",en
1106593,2010-12-18 23:14:22,statistics,Mobile phone masts linked to sharp rise in births - Or why correlation is not causation,eo1ns,key95,1179214221.0,https://www.reddit.com/r/statistics/comments/eo1ns/mobile_phone_masts_linked_to_sharp_rise_in_births/,6.0,3.0,,en
1106594,2010-12-19 18:45:39,statistics,You're Off to Be a Wizard | Stats With Cats Blog,eocpy,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/eocpy/youre_off_to_be_a_wizard_stats_with_cats_blog/,5.0,0.0,,en
1106595,2010-12-20 09:46:24,MachineLearning,Tell /r/ML: The Artificial Intelligence StackExchange is now in Public Beta,eonvl,[deleted],,https://www.reddit.com/r/MachineLearning/comments/eonvl/tell_rml_the_artificial_intelligence/,1.0,0.0,,en
1106596,2010-12-20 10:02:15,MachineLearning,Tell /r/machinelearning:  The Artificial Intelligence community on StackExchange is now in public beta.,eoo2a,psykocrime,1205139983.0,https://www.reddit.com/r/MachineLearning/comments/eoo2a/tell_rmachinelearning_the_artificial_intelligence/,28.0,17.0,,en
1106597,2010-12-20 13:34:31,statistics,Testing for non-normal distributions,eoq0o,theroc1217,1287993825.0,https://www.reddit.com/r/statistics/comments/eoq0o/testing_for_nonnormal_distributions/,1.0,6.0,"I'm working with some game date from Halo:Reach for my senior project. Awesome? I know. Anyways, one thing I want to do is test weather certain weapons have a bias. That is, do the kills-per-weapon-per-player-per-game follow a single distribution, or a sum of conditional distributions. That is, if I take my date sets as all the kills from one player with one weapon in one game. For example, I could get many date points from one game. 

One idea I had was to normalize the distributions, then rank them, e.g. assigning each point a value according to it's rank. My hypothesis is that biased weapons are not good, unless utilized in a certain manner. For example, the shotgun would have one distribution of with a low average, being the times it was used out in the open, and one with a high average, for when one was camping with or such. Then check the probabilities of these ranks being farther from the center, e.g. the standard deviations of the ranks belonging to one weapon. If they were normalized, one would expect them to all be approximately the same. But if the ranks are clumped near the edges, they will have ranks farther from the center. Does this make sense? I am working with non-parametric statistics, so I've got to make my own tests and charts. ",en
1106598,2010-12-20 17:07:26,statistics,Quick question! Relationship between two variables,eosbq,SammyGreen,1238596057.0,https://www.reddit.com/r/statistics/comments/eosbq/quick_question_relationship_between_two_variables/,5.0,7.0,"Hey all, hope someone can help me as this is driving me nuts. I am investigating carbon sequestration under different land use classes and want to compare soil pH with the amount of carbon at given depths. I have data that looks like this:

                  pH                  C%
Site 1:
      5 cm     4.84             1.58%
    15 cm     4.24             0.09%
    30 cm     3.87             0.08%
    50 cm     3.84             0.08%

Site 2:
      5 cm     6.39             3.77%
    15 cm     6.35             2.46%
    30 cm     6.08             1.44%
    50 cm     6.12             1.24%

Is there a method where I can find the relationship, if any, between pH levels and the % of carbon? I'm a pretty basic user of sigmaplot and stats isnt my strong point but I figured if there was anyone who could figure this out it'd be the collective minds over in this subreddit :)

And if this is the wrong subreddit to ask questions, instead of downvoting this could you please point me in the direction of subreddit where people might be willing to help? Thanks!

**Edit**: Great, thanks for all the help! Definitely points me in the right direction!",en
1106599,2010-12-20 18:49:24,MachineLearning,Organize a professional Machine Learning Challenge for Your Friends,eou0r,m3g4n3,1277388269.0,https://www.reddit.com/r/MachineLearning/comments/eou0r/organize_a_professional_machine_learning/,10.0,0.0,,en
1106600,2010-12-21 04:01:12,statistics,Where r/statistics and r/fffffffuuuuuuuuuuuu intersect,ep32m,loniousmonk,1288421034.0,https://www.reddit.com/r/statistics/comments/ep32m/where_rstatistics_and_rfffffffuuuuuuuuuuuu/,0.0,7.0,,en
1106601,2010-12-21 21:23:43,statistics,"The 2010 resident population of the United States is 308,745,538. ",eph6r,scottcmu,1242066220.0,https://www.reddit.com/r/statistics/comments/eph6r/the_2010_resident_population_of_the_united_states/,6.0,1.0,,en
1106602,2010-12-21 22:07:20,statistics,Questions for learning R,ephy6,amirightfolks,1280970578.0,https://www.reddit.com/r/statistics/comments/ephy6/questions_for_learning_r/,13.0,18.0,"I am currently pursuing a masters in statistics and want to learn some programming languages. In my introductory courses we haven't really used programs yet and I would like to learn R because it seems to be one of the more popular statistical languages. Any advice on going about doing this, such as websites or books that could be helpful?


Also, I keep hearing that the great thing about R is that the user can make changes. Can somebody explain what exactly this means?",en
1106603,2010-12-22 01:17:33,MachineLearning,Machine Learning for Human Memorization,epl5a,danger_t,1249412021.0,https://www.reddit.com/r/MachineLearning/comments/epl5a/machine_learning_for_human_memorization/,12.0,0.0,,en
1106604,2010-12-26 21:01:21,statistics,The Santa Claus Strategy | Stats With Cats Blog,erpet,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/erpet/the_santa_claus_strategy_stats_with_cats_blog/,3.0,1.0,,en
1106605,2010-12-27 02:55:27,statistics,What is a good way to learn the basics of random forests?,ert99,[deleted],,https://www.reddit.com/r/statistics/comments/ert99/what_is_a_good_way_to_learn_the_basics_of_random/,7.0,6.0,,en
1106606,2010-12-27 07:27:15,statistics,What test to perform on users with two indicator variables?,erwsd,PUSHERXIII,1287848278.0,https://www.reddit.com/r/statistics/comments/erwsd/what_test_to_perform_on_users_with_two_indicator/,0.0,0.0,"I have data that's organised as such

http://i.imgur.com/KKCnC.png

N=100ish users and any combination of 0's or 1's to denote if they dislike/like subject X and or subject Y. I want to test that if there is a strong enough correlation between users that like both subject X and subject Y then I can confidently suggest that any new user that likes subject X will also like subject Y and vice versa.

Not sure if this is touching on the area of VSMs or if it can be figured out more easily. Thanks for your time.",en
1106607,2010-12-28 03:58:05,statistics,Why the other lines always seem to move faster than yours,esbq8,jaybol,1191132779.0,https://www.reddit.com/r/statistics/comments/esbq8/why_the_other_lines_always_seem_to_move_faster/,1.0,0.0,,en
1106608,2010-12-28 20:02:53,MachineLearning,The Joy of Stats (from Gapminder so this should stay up),esn1d,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/esn1d/the_joy_of_stats_from_gapminder_so_this_should/,29.0,2.0,,en
1106609,2010-12-29 08:46:44,statistics,Some questions about confidence intervals and Likert data,esymi,[deleted],,https://www.reddit.com/r/statistics/comments/esymi/some_questions_about_confidence_intervals_and/,6.0,6.0,"Hello, /r/statistics, please help me out! I've been charged with analyzing the results of several surveys with Likert data. I've been winging it for the past few months going on the stats books we have in the office and what I can get off the internet. Needless to say, statistician I am not. I have some time now before the new batch of surveys go out and I want to check and refine my methodology with the experts before I go through it again. Specifically, I have a couple questions for which I haven't found an answer:

1) Most of the survey questions warrant a finite population correction for the standard error of the mean, but I have the problem of not always knowing with certainty which underlying population should represent the total population N. Here's why: 

All respondents answer the question ""Are you A or B?"" before answering a series of Likert scale questions, which are different depending on whether the respondent selected A or B. Suppose I'm looking at the subset of respondents who answered ""A"". For the finite population correction for the standard error of the mean for the ""A"" Likert data, is it kosher to use the proportion of the known total population (from which all respondents are drawn) who answered A in the sample as N? 

To illustrate, say I have a sample of 100 respondents from a finite population of 1000. 50 of the respondents answer A, and so answer a bunch of questions based on A. For these questions that only respondents who selected ""A"" answer, can I use 500 as the underlying N in the finite population correction? The problem is that the proportion who answered A is only a sample and so the true value is unknown...I thought a conservative approach would be to use the proportion of the population that is the upper 95% CI for those that answered ""A"", but I settled on using the exact proportion from the sample.

2) Regarding measuring the error of the Likert data--what should I do? Most of what I've read seems to indicate that reporting a mean and standard error is misleading since the data is ordinal. So...how would I compute a standard error and confidence interval for the median?

3) I've been using tests that assume that the data is normally distributed when it definitely is not...the Likert questions are on a scale of 1 to 5 and the most common responses are 1 or 5--most people have strong opinions one way or the other. Does this completely invalidate my confidence intervals for the mean of the responses? Is there a relatively easy way to correct for this?


I just re-read my post and realized it sounds like gibberish, but I don't know how else to phrase it. Let me know where you need clarifications....

Thanks for reading if you got this far.",en
1106610,2010-12-30 19:27:22,statistics,"Rosling's ""Joy of Statistics"" (long version)",etq4t,Bayesbayer,1268860759.0,https://www.reddit.com/r/statistics/comments/etq4t/roslings_joy_of_statistics_long_version/,0.0,0.0,,en
1106611,2010-12-31 01:42:59,statistics,Measuring catastrophic risk,etwh8,jambarama,1130472000.0,https://www.reddit.com/r/statistics/comments/etwh8/measuring_catastrophic_risk/,4.0,1.0,,en
1106612,2010-12-31 14:33:24,computervision,why I should be hacking with a kinect,eu581,[deleted],,https://www.reddit.com/r/computervision/comments/eu581/why_i_should_be_hacking_with_a_kinect/,0.0,0.0,,en
1106613,2010-12-31 20:58:20,statistics,How should I analyze this data.,eu9iq,gone_to_plaid,1275878118.0,https://www.reddit.com/r/statistics/comments/eu9iq/how_should_i_analyze_this_data/,0.0,1.0,"I am trying to compare two sets of data to see if the differences in the data is significant.  The data consists of student's scores on a question for a final exam for two classes that were taught using different methods.  The average of one class is 20 points higher than the other on the same question, but the sample sizes are small.  

What kind of test should I use to see get some idea if the difference is significant.  ",en
1106614,2010-12-31 21:06:53,statistics,Why are variances additive?,eu9ne,discontinuity,1204666886.0,https://www.reddit.com/r/statistics/comments/eu9ne/why_are_variances_additive/,8.0,7.0,"I have a question regarding something I read recently.  The text uses linear regression heavily, but assumes the reader is familiar with the underlying theory.  What I'm trying to understand is the following quote, ""In analyzing risk, it is convenient to use variances rather than standard deviations because the variances of uncorrelated variables are additive.""  I almost understand what's being alluded too, but I can't quite grasp it. Any takers?",en
1106615,2011-01-01 14:32:40,MachineLearning,my list of cool machine learning books,euivi,BioGeek,1124683200.0,https://www.reddit.com/r/MachineLearning/comments/euivi/my_list_of_cool_machine_learning_books/,38.0,8.0,,en
1106616,2011-01-01 19:07:31,statistics,"R-bloggers in 2010: Top 14 R posts, site statistics and invitation for sponsors | R-statistics blog",eukvb,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/eukvb/rbloggers_in_2010_top_14_r_posts_site_statistics/,18.0,0.0,,en
1106617,2011-01-02 02:44:09,statistics,Delete rows from R data frame,euq6a,orangepotion,1246804916.0,https://www.reddit.com/r/statistics/comments/euq6a/delete_rows_from_r_data_frame/,2.0,1.0,,en
1106618,2011-01-02 06:30:10,statistics,SAS Training's Greatest Hits for 2010,eusr9,jambarama,1130472000.0,https://www.reddit.com/r/statistics/comments/eusr9/sas_trainings_greatest_hits_for_2010/,1.0,0.0,,en
1106619,2011-01-02 06:54:37,statistics,Probability and Statistics cheat sheet — The Endeavour,eut24,jeromyanglim,1293934956.0,https://www.reddit.com/r/statistics/comments/eut24/probability_and_statistics_cheat_sheet_the/,15.0,2.0,,en
1106620,2011-01-02 07:31:24,MachineLearning,"Videos on Data Analysis with R: Introductory, Intermediate, and Advanced Resources",eutho,jeromyanglim,1293934956.0,https://www.reddit.com/r/MachineLearning/comments/eutho/videos_on_data_analysis_with_r_introductory/,19.0,1.0,,en
1106621,2011-01-02 09:20:38,MachineLearning,"Ask ML:  Detecting variable, recurring signals",euuqv,[deleted],,https://www.reddit.com/r/MachineLearning/comments/euuqv/ask_ml_detecting_variable_recurring_signals/,19.0,12.0,"I'm looking for references on this subject.  It's similar to commonly-used methods to integrate weak classifiers into a strong system, except using a rolling time-window to take advantage of signal repetition.  

Background:  I'm building an algorithm to detect a repetitive but highly-variable acoustic signal - so variable that each time it occurs (approx. once per second), I only have about a 25% chance of detecting it.  But by using a long time window, say 100 seconds, I give myself 100 detection opportunities and might detect the signal 25 separate times.  Then I can roll this window forward through time, processing the data real-time as it's received, and if I have more than, say, 15 detections in the window, I can declare that the signal's been detected with high confidence.  

It's a two-detection-threshold system.  I have one threshold for my instantaneous detector, which can get about 25% Probability of Correct Classification (Pcc) with fairly low Probability of False Alarm (Pfa).  Then I have a separate threshold, 15 detections in the time window, which operates on the aggregate instantaneous detection output.  Both of these thresholds need to be determined ahead of time from training data.  

What is this type of analysis even called?  Has anyone else used anything similar?  Where can I find details on the theoretical underpinnings of this type of approach?  I'm tentatively calling this approach taking advantage of signal recurrence by using ""local context"", and it's clearly a powerful tool to improve system performance, but I am totally lacking on footnotes.  

Any thoughts, suggestions or references would be deeply appreciated.  Thanks in advance.  ",en
1106622,2011-01-02 15:18:02,statistics,Live Long and Publish | Stats With Cats Blog,eux9v,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/eux9v/live_long_and_publish_stats_with_cats_blog/,0.0,0.0,,en
1106623,2011-01-03 08:50:39,statistics,"Getting Started with Sweave: R, LaTeX, Eclipse, StatET, &amp; TeXlipse",evan9,[deleted],,https://www.reddit.com/r/statistics/comments/evan9/getting_started_with_sweave_r_latex_eclipse/,0.0,4.0,,en
1106624,2011-01-03 10:32:13,statistics,List of favorited data analysis cartoons on Stats.SE,evbsd,jeromyanglim,1293934956.0,https://www.reddit.com/r/statistics/comments/evbsd/list_of_favorited_data_analysis_cartoons_on/,0.0,0.0,,en
1106625,2011-01-03 11:46:14,MachineLearning,Unit Testing in R: The Bare Minimum,evchv,mycatharsis,1294046118.0,https://www.reddit.com/r/MachineLearning/comments/evchv/unit_testing_in_r_the_bare_minimum/,12.0,2.0,,en
1106626,2011-01-03 14:27:44,MachineLearning,What are your thoughts on Google prediction API?,evdxb,[deleted],,https://www.reddit.com/r/MachineLearning/comments/evdxb/what_are_your_thoughts_on_google_prediction_api/,13.0,18.0,"I find the entire training process too opaque. (Perhaps because I haven't looked at the advanced features?)
What's the training algorithm they use? Is it neural networking or SVM or whatever?

Also, given the already innumerable open source alternatives like R, RapidMiner &amp; Weka I don't see any reason (except CPU horsepower) why I should start using this API.",en
1106627,2011-01-03 18:38:47,statistics,"Help me /r/statistics, you're my only hope!",evh31,Congruence,1276174673.0,https://www.reddit.com/r/statistics/comments/evh31/help_me_rstatistics_youre_my_only_hope/,2.0,17.0,"So ... I figured I could do some statistics-slinging with my A-level math from high school. Suffice to say, I am getting pummeled ... hard.

I am doing my first ""real"" data-collection, and I had a pretty good idea of what I was going for. Only problem, I cannot, for the life of me, figure out what manipulations to run to get a p value I can use for anything. I've tried ttest, ranksum, ANOVA and all that jazz, but none of it seems to intuitively make sense. I've gotta the calibration curve up and it is beautiful and seems to corroborate my hypothesis - but it's that damn p value I feel I need, and which continuously eludes me.

Brief explanation: I'm trying to do stuff about overconfidence/calibration, so I got individuals and pairs, respectively to answer five questions with two possible answers, and then to assess the probability of their own correctness.

So, I have data for the following variables: group (0 for individual, 1 for group), pct (assessed probability), gpct (assessed probability rounded to nearest ten), outcome (0 for wrong, 100 for right - but can change it back to 1/0, but figured this would be better for comparing means ... or something).

Can anyone give me any pointers whatsoever? I am more than willing to figure this stuff out myself, I just seem to have hit a dead end. I was considering something about mean pct minus mean outcome for each of the groups, but that would eliminate the number of observations and make me unable to run ttests and ANOVAs? My brain is mush..

A sincere plea for help,
and cookies and milk (laboriously drawn in paint!) for whoever tries!",en
1106628,2011-01-03 21:01:43,statistics,NHL PS3 stats - serious rivalry with my homeboy,evjof,randude,1286197010.0,https://www.reddit.com/r/statistics/comments/evjof/nhl_ps3_stats_serious_rivalry_with_my_homeboy/,0.0,1.0,"My buddy and i have played over 650 games on playstation NHL series - I've created a spreadsheet tracking every conceivable stat that he and i can come up with - if anyone is both a hockey fan and a stat geek, please take a look and see if you have any other stat ideas for us to add into the sheet. Some of the categories we have now include:

*Home wins being outshot
*Away wins being outshot
*Home wins with less ToA (time of attack)
*Away wins with less ToA
*Home wins outshot &amp; less ToA
*Away wins outshot &amp; less ToA

*Home wins leading after 2 periods
*Home wins trailing after 2 periods
*Home wins tied after 2 periods
*Away wins leading after 2 periods
*Away wins trailing after 2 periods
*Away wins tied after 2 periods
*Wins by 1 goal leading after 2 periods
*Wins by 2 goals leading after 2 periods
*Wins by 3 goals leading after 2 periods
*Wins by 4 goals leading after 2 periods
*Wins by 5 or more goals leading after 2 periods
*Wins by 1 goal trailing after 2 periods
*Wins by 2 goals trailing after 2 periods
*Wins by 3 goals trailing after 2 periods
*Wins by 4 goals trailing after 2 periods
*Wins by 5 or more goals trailing after 2 periods
*Wins by 1 goal tied after 2 periods
*Wins by 2 goals tied after 2 periods
*Wins by 3 goals tied after 2 periods
*Wins by 4 goals tied after 2 periods
*Wins by 5 or more goals tied after 2 periods

*Wins leading by 1 goal after 2 periods
*Wins leading by 2 goals after 2 periods
*Wins leading by 3 goals after 2 periods
*Wins leading by 4 goals after 2 periods
*Wins leading by 5 or more goals after 2 periods
*Wins trailing by 1 goal after 2 periods
*Wins trailing by 2 goals after 2 periods
*Wins trailing by 3 goals after 2 periods
*Wins trailing by 4 goals after 2 periods
*Wins trailing by 5 or more goals after 2 periods

*Games won scoring 1 goal
*Games won scoring 2 goals
*Games won scoring 3 goals
*Games won scoring 4 goals
*Games won scoring 5 goals
*Games won scoring 6 or more goals

See the ""index"" tab for an explanation of each tab - each version/year of the NHL series is on it's own tab - also the sheet has functions that you need office 2k7 or above to see.

Much appreciated to the community for any help/ideas.

 ",en
1106629,2011-01-04 08:12:49,datasets,DAE have data for operating systems exploited by botnets?,evur9,[deleted],,https://www.reddit.com/r/datasets/comments/evur9/dae_have_data_for_operating_systems_exploited_by/,6.0,0.0,"I'd like to see the distribution among Linux, OS X, BSD, and versions of M-Windows.",en
1106630,2011-01-04 08:30:23,MachineLearning,Ask ML: Document ranking with user ratings?,evv06,eggbrain,1236193390.0,https://www.reddit.com/r/MachineLearning/comments/evv06/ask_ml_document_ranking_with_user_ratings/,10.0,7.0,"I've had a fun idea for awhile (not for profit, just for my own entertainment), but I keep running into barriers (as I am still pretty new at Machine Learning), and figured I'd ask the knowledgeable minds of Reddit.

Here's the gist of it: I have a database of scholarly articles from a wide range of categories that I've archived from the web, stored into an sqlite3 database. It stores the title of the scholarly article, the author, the general category (science/medicine/psychology/technology/etc) and the entire text of the article (~600 words on average). What I want to do is present the user with a list of these articles, and have them rate each one 1-5 stars. The hope is that over time, the system will learn which articles the user likes, and which articles the user hates, and give the user a predicted value of stars next to new articles (very similar to how Netflix shows you how much it thinks you will like a movie). The hope would be that it factored in everything, the author, the title, the categories, and the full text, to get an idea of what kind of things you liked, and also know how much weight to give each one of those factors.

The problem I'm running into is that I really don't know where to go next. I looked into document ranking algorithms, but [Wikipedia doesn't exactly tell you which algorithms are 'best', or, at the very least, most widely used.](http://en.wikipedia.org/wiki/Learning_to_rank#Approaches). I also thought from reading [O'Reilly's Programming Collective Intelligence](http://oreilly.com/catalog/9780596529321) that I could use a Pearson correlation/Euclidean distance score to solve the problem, but that seems to only be relevant if you have multiple users to compare to, which, if I was just starting my website out, would not be possible. Plus, this seems like it wouldn't factor in all the variables needed. 

Basically, I don't know what I am doing, but I do have a good idea of what I want to do, and I figure someone could at least point me in the right direction. What type of algorithm is best suited for this type of system? Neural Network? Bayesian classifier? Are there some good books besides ""Programming Collective Intelligence"" that provide a good knowledge base of machine learning and computer programming? Is it even possible to compare so many features (title, author, category, and ESPECIALLY 600 words of the article) and get a decent ranking?",en
1106631,2011-01-04 14:43:43,artificial,"MIT Pokerbots Competition - A computerized poker tournament, where teams have 1 month to program a completely autonomous ""pokerbot"" to compete against other teams",evynn,fbahr,1275413362.0,https://www.reddit.com/r/artificial/comments/evynn/mit_pokerbots_competition_a_computerized_poker/,9.0,3.0,,en
1106632,2011-01-04 14:45:41,MachineLearning,"MIT Pokerbots Competition - A computerized poker tournament, where teams have 1 month to program a completely autonomous ""pokerbot"" to compete against other teams",evyog,fbahr,1275413362.0,https://www.reddit.com/r/MachineLearning/comments/evyog/mit_pokerbots_competition_a_computerized_poker/,7.0,2.0,,en
1106633,2011-01-04 15:26:28,datasets,Datasets for development economists,evz48,cavedave,1128052800.0,https://www.reddit.com/r/datasets/comments/evz48/datasets_for_development_economists/,7.0,0.0,,en
1106634,2011-01-04 17:51:39,MachineLearning,Microsoft Research Speller Challenge - Develop a speller that generates the most plausible spelling alternatives for a search query,ew19d,fbahr,1275413362.0,https://www.reddit.com/r/MachineLearning/comments/ew19d/microsoft_research_speller_challenge_develop_a/,10.0,22.0,,en
1106635,2011-01-05 02:36:52,statistics,Question about which test to use,ewao4,thrower00,1294187364.0,https://www.reddit.com/r/statistics/comments/ewao4/question_about_which_test_to_use/,2.0,6.0,"Hi, I have a question I was hoping someone could answer. Forgive me, it's been a few years since my last statistics class.


I collected a variety of different salary reports for a job. Using these multiple reports, I'd like to calculate a value that describes the likelihood that a given salary belongs to a certain job.



For instance, take this simple example:
Job X has 3 salary reports:


         10,000 - 20,000

         12,000 - 19,000

         11,000 - 23,000



If I have a salary of 13,000 - Is there a test that can tell me the likelihood that the person with this salary has a job X?",en
1106636,2011-01-05 19:11:21,MachineLearning,crossposted from r/funny [image],ewopu,[deleted],,https://www.reddit.com/r/MachineLearning/comments/ewopu/crossposted_from_rfunny_image/,25.0,2.0,,en
1106637,2011-01-05 20:58:54,MachineLearning,Hierarchical Classification,ewqxo,japerk,1126238400.0,https://www.reddit.com/r/MachineLearning/comments/ewqxo/hierarchical_classification/,0.0,0.0,,en
1106638,2011-01-05 21:11:22,datasets,"/r/datasets, does anyone here know of a medium/large dataset that contains birthday in addition to any other type of data?",ewr78,gigamosh57,1234321967.0,https://www.reddit.com/r/datasets/comments/ewr78/rdatasets_does_anyone_here_know_of_a_mediumlarge/,5.0,9.0,I want to show a friend of mine any statistical evidence I can find that supports/debunks the idea that your birth month/sign has an impact on your choices (astrology).,en
1106639,2011-01-05 21:56:29,statistics,TV Documentary The Joy of Stats w/ Hans Rosling: Now Available Online,ews5g,snipewiz,1256189534.0,https://www.reddit.com/r/statistics/comments/ews5g/tv_documentary_the_joy_of_stats_w_hans_rosling/,10.0,0.0,,en
1106640,2011-01-05 23:39:32,statistics,"Choosing the correct Statistical Test in SAS, Stata and SPSS",ewu7e,teh_winnar,1288240554.0,https://www.reddit.com/r/statistics/comments/ewu7e/choosing_the_correct_statistical_test_in_sas/,7.0,14.0,,en
1106641,2011-01-06 01:45:33,MachineLearning,Single Layer Networks in Unsupervised Feature Learning: The Deep Learning Killer,ewwl1,cmmdevries,1238291391.0,https://www.reddit.com/r/MachineLearning/comments/ewwl1/single_layer_networks_in_unsupervised_feature/,19.0,1.0,,en
1106642,2011-01-06 04:43:13,MachineLearning,Video and slides for talk by Drew Conway at NYC Machine Learning on modeling network growth using graph motifs,ewzo6,mycatharsis,1294046118.0,https://www.reddit.com/r/MachineLearning/comments/ewzo6/video_and_slides_for_talk_by_drew_conway_at_nyc/,4.0,0.0,,en
1106643,2011-01-06 05:24:55,datasets,"Transatlantic Slave Trade database released.  All information gathered from public sources is public domain, and any ""imputed"" data is released under Creative Commons license.  276 variables captured for 34,948 voyages.",ex0ec,Quintote,1269890504.0,https://www.reddit.com/r/datasets/comments/ex0ec/transatlantic_slave_trade_database_released_all/,16.0,3.0,,en
1106644,2011-01-06 08:30:35,statistics,"Seattle's skin tight, rubber, black, and golden suited superhero",ex3ko,[deleted],,https://www.reddit.com/r/statistics/comments/ex3ko/seattles_skin_tight_rubber_black_and_golden/,0.0,2.0,,en
1106645,2011-01-06 12:43:19,MachineLearning,You are not Paranoid and clustering told them that,ex6ha,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/ex6ha/you_are_not_paranoid_and_clustering_told_them_that/,9.0,0.0,,en
1106646,2011-01-06 19:15:04,MachineLearning,The Learning Behind Gmail Priority Inbox (pdf),exc77,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/exc77/the_learning_behind_gmail_priority_inbox_pdf/,46.0,6.0,,en
1106647,2011-01-07 04:10:38,statistics,Random math problem.,exmoq,Dr_Jackson,1291066431.0,https://www.reddit.com/r/statistics/comments/exmoq/random_math_problem/,1.0,18.0,"Here’s a problem I came up with but cannot solve:

There are two options, option A and option B. If option A is chosen, a random number between 1 and 10 is chosen. If option B is chosen then and even number between 1 and 10 is picked. Whether A or B is picked is also completely random. 50-50 chance.

The question is what are the odds that an odd number will be picked and what are the odds that an even number will be picked? What are the odds that a specific odd number will be picked and what are the odds that a specific even number will be picked? How about two odd numbers in a row or two even numbers in a row? Two specific odd numbers in a row or two specific even numbers in a row? Or just two specific numbers in a row? (Clarification on that last one: There are different odds of picking two odd numbers in a row then even ones. But that's the point. Two numbers in a row taking into account that that the first number could be even or odd)

The only way I've been able to find answers was to make a program and simulate 1,000,000 iterations and count the results. Here is as far as I got: the odds of picking a specific odd number was 1 in 20. Even: 3 in 20 and two numbers in a row regardless of parity: 1 in 8. I understand the first one because for a specific odd number to get picked, option A must be chosen (1/2). Then there are ten numbers available to get picked (1/10). 1/2 * 1/10 is 1/20. I can't quite figure out the 3/20 result and I'm not even going to try to figure out the 1/8 outcome!

So any ideas? If this doesn’t belong in r/statistics then sorry!",en
1106648,2011-01-07 06:42:24,datasets,Google Refine looks like an awesome tool for playing these the datasets from this subreddits,exp9p,chime,1134104400.0,https://www.reddit.com/r/datasets/comments/exp9p/google_refine_looks_like_an_awesome_tool_for/,14.0,1.0,,en
1106649,2011-01-07 17:06:08,MachineLearning,Ask ML: Newbie looking for key sources,exx5k,[deleted],,https://www.reddit.com/r/MachineLearning/comments/exx5k/ask_ml_newbie_looking_for_key_sources/,9.0,11.0,"I have a background in CS and some exposure to a variety of neural networks such as SOM, mixture networks, k-means, neural gas, LVQ, etc.

I'd like to learn more about clustering and classification. Can you point me at either good reference books or, preferably, great research papers?

Sorry for the open-ended question. I'm trying to learn :)

Thanks!

**Update:** Thank you, folks! Lots of great suggestions :D",en
1106650,2011-01-07 21:27:12,datasets,Five data blogs to have on your radar,ey284,kev097,1286915567.0,https://www.reddit.com/r/datasets/comments/ey284/five_data_blogs_to_have_on_your_radar/,15.0,0.0,,en
1106651,2011-01-07 22:25:25,MachineLearning,Machine Learning scared my potential employer,ey3fp,Mangalaiii,1284102384.0,https://www.reddit.com/r/MachineLearning/comments/ey3fp/machine_learning_scared_my_potential_employer/,38.0,32.0,"I interviewed with a major marketing company who asked me to improve their public-facing website's ability to predict ""hot"" stories based on the tweets containing the story URL.

I got all excited in the interview and started telling the manager there how I could use machine learning algorithms to automatically predict which stories would be successful based on the rate of tweets about the story.

When I started describing what machine learning was, the manager interviewing me reacted as if I was describing some magical voodoo. He said things like ""that won't scale"" even though I was only describing running an SVM on a small amount of training data.

They just want to predict simple stuff, but the more I try to clarify what machine learning is, the more skeptical the manager is getting. I think he thinks I'm trying to sell him some sort of snake oil.

Has anyone else ever experienced this where an employer is scared off because they don't understand what machine learning/information retrieval is and what it can do for them?",en
1106652,2011-01-07 23:02:12,statistics,SilveR - an online statistics application built in Silverlight and R,ey45k,robalexclark,1208418186.0,https://www.reddit.com/r/statistics/comments/ey45k/silver_an_online_statistics_application_built_in/,0.0,0.0,,en
1106653,2011-01-08 07:28:41,statistics,I want to learn R by doing a problem.  Suggestions?,eybyy,ImHalfAwake,1282980063.0,https://www.reddit.com/r/statistics/comments/eybyy/i_want_to_learn_r_by_doing_a_problem_suggestions/,9.0,22.0,"I've been slowly reading Verzani's SimpleR book, but I feel like I'd learn at a faster pace if I can just work on a real problem using R.  Right now, I'm looking into building a simple linear multiple regression model (thinking about into the NBA and what data(rebounds, steals, turnovers, age) correlates with team wins).  I've already downloaded R, anyone have any suggestions or advice?",en
1106654,2011-01-09 08:44:33,MachineLearning,"""Several CiteSeerx services may not be available indefinitely""?",eyupj,yaroslavvb,1174946940.0,https://www.reddit.com/r/MachineLearning/comments/eyupj/several_citeseerx_services_may_not_be_available/,1.0,0.0,"This announcement is now on all Citeseer search pages. I searched and couldn't find any more information, any idea what this means?",en
1106655,2011-01-10 17:35:01,statistics,Gelman's Bayesian stats booked banned in China,ezk7g,agconway,1228006618.0,https://www.reddit.com/r/statistics/comments/ezk7g/gelmans_bayesian_stats_booked_banned_in_china/,28.0,5.0,,en
1106656,2011-01-10 20:31:52,statistics,I've finally set up an RSS reader and I'm looking for some good stats blogs to add.  Any suggestions other than R-Bloggers?,eznta,daledinkler,1199898909.0,https://www.reddit.com/r/statistics/comments/eznta/ive_finally_set_up_an_rss_reader_and_im_looking/,3.0,4.0,"I've done some searching, but Google is no help when your search string is some variant of 'Statistics rss feeds'.  Unless you want stats on rss feeds.",en
1106657,2011-01-11 14:23:00,artificial,The eyes hold they key to crossing the uncanny valley,f06et,AndrewKemendo,1190929741.0,https://www.reddit.com/r/artificial/comments/f06et/the_eyes_hold_they_key_to_crossing_the_uncanny/,4.0,0.0,,en
1106658,2011-01-11 18:52:18,MachineLearning,OpenData + R + Google = Easy Maps ,f0arr,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/f0arr/opendata_r_google_easy_maps/,21.0,3.0,,en
1106659,2011-01-11 22:57:08,statistics,"Statistical model used in Okcupid ""Mathematics of Beauty"" post",f0g2h,inquilinekea,1274654696.0,https://www.reddit.com/r/statistics/comments/f0g2h/statistical_model_used_in_okcupid_mathematics_of/,7.0,10.0,"The post is here:
http://blog.okcupid.com/index.php/the-mathematics-of-beauty/

My question is this: Is there an objective way to generate the weights of the model .4*m1+...+.9*m5 + k, where the weights are .4, ..., .9?",en
1106660,2011-01-12 03:46:46,statistics,Consulting: payment vs authorship,f0lp3,[deleted],,https://www.reddit.com/r/statistics/comments/f0lp3/consulting_payment_vs_authorship/,8.0,6.0,"I'm a graduate student with a strong statistics background, so my skills are occasionally needed by colleagues, and I'm planning to pursue statistical consulting more vocationally after graduation. Some of my collaborators have offered co-authorship when I contribute a lot of value to the analysis, and since I'm not taking payment at the moment, it's a no-brainer. 

But it leads me to wonder: when I'm consulting for pay later, is it typical/appropriate to also be listed as an author or in the acknowledgements? It would obviously be a huge help for attracting new clients, but I'm not sure what the ethical implications are for me or the clients. On one hand, payment should be sufficient without them needing to dilute the authorship list. On the other hand, is excluding me from authorship violating the authors-are-responsible-for-content ethic?

Or something else I've missed entirely...?",en
1106661,2011-01-12 16:07:16,statistics,"SPSS help: I know what I want, don't know how to get it",f0vzh,Iregretthisusername,1291910053.0,https://www.reddit.com/r/statistics/comments/f0vzh/spss_help_i_know_what_i_want_dont_know_how_to_get/,2.0,11.0,"I recently posted this in biology -&gt; http://www.reddit.com/r/biology/comments/et43e/statistics_for_comparing_growth_between_cultures/

I'm now trying to do a repeated measures ANOVA in SPSS but I'm finding it very confusing. Does anyone know how I should be organising my data?

I'm looking at the differences in growth rates between each strain across 4 separate light levels, I assume I would have to do 4 separate ANOVAs? I'm not expecting any significance, but I'm being marked on my ability to analyse rather than the results themselves.

I've been using Dytham's 'Choosing and using statistics; a biologist's guide' (page 107-108 for anyone who has it), but I can't make sense of the worked example.

If someone could talk me through transferring and formatting my data from Excel to SPSS, that would be much appreciated.
",en
1106662,2011-01-12 20:16:20,statistics,Occam's razor and Bayes' theorem,f10rj,[deleted],,https://www.reddit.com/r/statistics/comments/f10rj/occams_razor_and_bayes_theorem/,1.0,0.0,,en
1106663,2011-01-12 22:23:40,MachineLearning,Supreme Court To Rule On ‘Data Mining’ Of Prescription Drug Records,f13iv,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/f13iv/supreme_court_to_rule_on_data_mining_of/,8.0,0.0,,en
1106664,2011-01-13 11:12:31,statistics,What are the odds of being able to spell a given word using x randomly chosen Scrabble tiles?,f1glj,Zulban,1250049294.0,https://www.reddit.com/r/statistics/comments/f1glj/what_are_the_odds_of_being_able_to_spell_a_given/,5.0,3.0,"I'm not so much interested in the specific answer, but more how to calculate such a thing. Is it easy, is it hard?

I don't know how to handle things like the blank tile (which can be any letter you want) or words with two of the same letter (like the word ""letter"", which has two ""e""s and two ""t""s). [Tile distribution](http://en.wikipedia.org/wiki/Scrabble_letter_distributions#English).

I'd even appreciate links to Wikipedia articles so long as the specific theory applies to this case.",en
1106665,2011-01-13 14:28:24,MachineLearning,"need the book
Applied Multivariate Statistical Analysis, 6th Ed. (ISBN 9780131877153) by Richard A. Johnson and Dean W. Wichern ",f1ikv,qwsazxerfdcv,1287517243.0,https://www.reddit.com/r/MachineLearning/comments/f1ikv/need_the_book_applied_multivariate_statistical/,0.0,1.0,"hi 
I need this book , and the amazon price is a bit out of my reach. 
Is there anyone willing to part with theirs ? Thanks for the help",en
1106666,2011-01-13 17:59:22,MachineLearning,Music Information Retrieval competition,f1lyx,bart_blaszczyk,1294929530.0,https://www.reddit.com/r/MachineLearning/comments/f1lyx/music_information_retrieval_competition/,1.0,0.0,,en
1106667,2011-01-13 22:40:24,MachineLearning,IBM Supercomputer Watson owns humans at Jeopardy [video].,f1s0n,skydivingdutch,1175480446.0,https://www.reddit.com/r/MachineLearning/comments/f1s0n/ibm_supercomputer_watson_owns_humans_at_jeopardy/,35.0,14.0,,en
1106668,2011-01-13 23:39:25,statistics,A statistics question from an intro class I took a few years ago.,f1t9x,[deleted],,https://www.reddit.com/r/statistics/comments/f1t9x/a_statistics_question_from_an_intro_class_i_took/,2.0,2.0,"A few years ago while I was a freshman at college, I took a basic intro stats course.  We did simple things such as T-tests, standard deviations, confidence intervals, stuff that any basic course would cover.  For some reason, a problem that came up in the course just popped back into my head, and I'm curious as to what the correct response to it is.

The question was describing a pregnant woman who wrote into a Dear Abbey, and is as follows:

    Dear Abby,

    You wrote in your column that a woman is pregnant for 266 days. Who said so? I carried my baby for ten months and five days, and there is no doubt about it because I know the exact date my baby was conceived. My husband is in the Navy and it couldn't possibly have been conceived any other time because I saw him only once for an hour, and I didn't see him again until the day before the baby was born.

    I don't drink or run around, and there is no way this baby isn't his, so please print a retraction about the 266-day carrying time because otherwise I am in a lot of trouble.

    San Diego Reader"".

She gave us an alpha value and a standard deviation and asked us simply if San Diego Reader was telling the truth or not.  Using the data, it could be found that the length of the pregnancy gave a z-score greater than the z-critical, so the answer to the problem was that she was not telling the truth.

However, the answer I wrote was that although the z-score was outside the z-critical value, the tests were not necessarily conclusive because I didn't think that she constituted a random sample.  Sure, there may be a 1% chance (or whatever it was, don't remember exactly, but less than alpha) of a woman chosen at random having a pregnancy the same length as hers was, but I said that you couldn't take a model that assumes your samples are random and apply it to someone who wrote in claiming to have a certain pregnancy length.  I wrote that any analysis concluded using this model would be invalid because the assumption of a random sample is not followed, so no conclusive statement can be made.

I am wrong in my thinking here?  I'm just curious, more than anything else.",en
1106669,2011-01-13 23:49:19,statistics,When does one use a 1 tailed or 2 tailed test?,f1thj,vureal,1294345692.0,https://www.reddit.com/r/statistics/comments/f1thj/when_does_one_use_a_1_tailed_or_2_tailed_test/,8.0,36.0,Okay this may seem superficially trivial but as the textbook reason is basically: domain knowledge and priors. But we know the probability of rejected H0 is higher if you have reason to believe in the 1 tailed test than if you were just agnostic and used 2 tailed. Doesn't that imply that you are much more likely to find what you want to find if you go out looking for it? So I guess the question really is when do you ever use a 1-tailed test.,en
1106670,2011-01-14 11:01:30,statistics,"Lies, damn lies, and statistics - Unleashed (ABC)",f24n2,mycatharsis,1294046118.0,https://www.reddit.com/r/statistics/comments/f24n2/lies_damn_lies_and_statistics_unleashed_abc/,7.0,1.0,,en
1106671,2011-01-14 14:45:25,MachineLearning,How I did it: Will Cukierski on finishing second in the IJCNN Social Network Challenge,f26to,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/f26to/how_i_did_it_will_cukierski_on_finishing_second/,11.0,10.0,,en
1106672,2011-01-14 18:27:59,MachineLearning,Winning the UCSD Data Mining Contest 2009,f2add,chuckbot,1267696238.0,https://www.reddit.com/r/MachineLearning/comments/f2add/winning_the_ucsd_data_mining_contest_2009/,13.0,0.0,,en
1106673,2011-01-14 21:51:01,statistics,Tutorials for regression analysis in R?,f2emn,austinap,1205165503.0,https://www.reddit.com/r/statistics/comments/f2emn/tutorials_for_regression_analysis_in_r/,14.0,7.0,"Hey everyone, I'm looking for some online tutorials (including datasets?) for doing a regression analysis in R.  Any suggestions?  I haven't been able to pull up anything particularly useful through google searching.  Thanks!",en
1106674,2011-01-15 00:12:42,MachineLearning,Where to study ML in Europe?,f2hk1,vadimkantorov,1295042998.0,https://www.reddit.com/r/MachineLearning/comments/f2hk1/where_to_study_ml_in_europe/,3.0,17.0,"If in Europe, what are the best MSc programs for studying ML?
Cheers!",en
1106675,2011-01-15 00:35:06,statistics,Two variables side by side in a SPSS bar graph?,f2hz1,Fogge,1280495119.0,https://www.reddit.com/r/statistics/comments/f2hz1/two_variables_side_by_side_in_a_spss_bar_graph/,4.0,8.0,"I've made a survey study and I was looking to condense the number of graphs I have in my report by displaying the different answer options (currently labels for variables) on the X axis and count/percentage or whatever on the Y axis, and have different colored bars for different statements (effectively putting two different bar graphs into the same graph to save space).

My data is entered with the statements being variables and the answers ranging from 0 to 9 with the same labels for the same answers across all variables. I want the graph to look something like this: http://www.nzssds.org.nz/system/files/SPSS+graph+2-1.png but with different statements (IE variables) instead of the age groups used in that particular graph. Is this at all doable? All I've managed so far is to split, for example, all the ""4"" answers for a certain statement into separate bars indicating what each case answered to a different statement which isn't very helpful or illustrating at all, I really don't need to show that six of the people answering ""4"" also answered ""4"" to that other statement ಠ_ಠ.

I am using version 19 for what it's worth.

Edit: Ninjaedit.",en
1106676,2011-01-15 04:10:12,MachineLearning,The 1st International Workshop on Mobile Social Networks (MSN 2011),f2lei,orangepotion,1246804916.0,https://www.reddit.com/r/MachineLearning/comments/f2lei/the_1st_international_workshop_on_mobile_social/,3.0,0.0,,en
1106677,2011-01-15 17:58:27,artificial,A chatterbot built using a neural networks approach (reservoir computing),f2u71,nadie854,1295106489.0,https://www.reddit.com/r/artificial/comments/f2u71/a_chatterbot_built_using_a_neural_networks/,5.0,7.0,"Reserbot is a chatterbot, but this bot is using an approach based on echo state neural networks. It tries to learn natural languages learning just from imitation.
This project is looking for collaborators, comments, ideas, etc.

URL: https://github.com/neuromancer/reserbot

Thanks!",en
1106678,2011-01-15 18:30:25,MachineLearning,Call for Papers/Abstracts: IJCNN,f2un4,dearsomething,1210808677.0,https://www.reddit.com/r/MachineLearning/comments/f2un4/call_for_papersabstracts_ijcnn/,8.0,2.0,[Submissions now open](http://www.ijcnn2011.org/index.php).,en
1106679,2011-01-16 03:03:15,artificial,"NPR Covers the Singularity – As The Biggest Threat 
to Humanity!",f31ww,AndrewKemendo,1190929741.0,https://www.reddit.com/r/artificial/comments/f31ww/npr_covers_the_singularity_as_the_biggest_threat/,3.0,0.0,,en
1106680,2011-01-16 03:40:39,statistics,"What do employers mean when they list ""proficiency in SAS"" as a job requirement?",f32e5,BirthDeath,1295140816.0,https://www.reddit.com/r/statistics/comments/f32e5/what_do_employers_mean_when_they_list_proficiency/,5.0,21.0,"I am a statistics graduate student and am applying for summer internships, many of which require proficiency in SAS.  

This strikes me as a bit strange since my department almost exclusively uses R or (for computationally intensive tasks) lower level languages like FORTRAN or C.  

I had a license for SAS from when I worked on an empirical project in a different department and I used it extensively to create libraries to clean data and extract relevant variables.  I typically would then convert the data set to *.csv format and read it into R.  I rarely performed any extensive analysis within SAS; I found the interface to be a bit clunky.

Now, since my license expired, I do just about everything in R and my SAS skills have gotten a bit rusty.  I am a bit worried that my interviewers will ask me how to perform specific operations within SAS that I will not be able to answer.  

I can access SAS from my school's network, so I was going to try to brush up on it over the next few weeks.   Is there anything specific reference that I should go over, preferably a reference for people familiar with R who are converting to SAS (I know that the reverse is much more common)?

Finally, what is the role of SAS in the private sector (insurance/finance)?  Do they just use it to clean large data sets or do they use it to perform all of their analysis? 
  ",en
1106681,2011-01-16 04:23:22,statistics,Repeated-measures Regression?,f32z0,[deleted],,https://www.reddit.com/r/statistics/comments/f32z0/repeatedmeasures_regression/,1.0,0.0,"I have several beakers sitting on my lab bench. Four are filled with ethanol, four are filled with maple syrup, and four are filled with water. All beakers are heated to 75^o C and then allowed to cool. Every minute for 30 minutes, I measure the temperature of every beaker.

I'd like to test two hypotheses:

1. The temperature curves over time for each of the three substances are sufficiently described by an exponential decay curve: 

        temp = e^(-time/b)

    In other words, a natural log transformation of the temperature data is a simple linear relationship with time: 

        ln(temp) = beta*time, where beta = -1/b

2. The estimated beta (or the inverse rate of decay) for each substance is significantly different.

We used to do this analysis by computing a simple linear regression through the pooled data of each substance and comparing the estimated betas (which have very narrow confidence intervals, because it turns out that physics works). But now I know better: the longitudinal data are *not* independent and violate regression assumptions. I've seen a few very complex time series analyses that I could use instead, but the problem is so simple that I really, really want a GLM solution. I think repeated measures ANOVA might be the right way to go, but I'm not sure, nor can I find a good source for how to do it.

Any ideas? Is repeated measures ANOVA the right way to go? If so, where can I find a good step-by-step for how to get the estimated betas out of an ANOVA in SPSS or R?",en
1106682,2011-01-16 08:36:55,MachineLearning,Deanonymization used to win the IJCNN Social Network Challenge,f360y,antgoldbloom,1285588580.0,https://www.reddit.com/r/MachineLearning/comments/f360y/deanonymization_used_to_win_the_ijcnn_social/,22.0,2.0,,en
1106683,2011-01-16 21:03:08,statistics,Your one year odds of dying in a car accident are 1/6500. Your one year odds of dying on your birthday are 1/365. Doesn't this mean that you're more likely to die on your birthday than to die in a car accident?,f3dc0,teabagcity,1282490254.0,https://www.reddit.com/r/statistics/comments/f3dc0/your_one_year_odds_of_dying_in_a_car_accident_are/,0.0,15.0,"I could use a little help because I always seem to miss a step while doing statistical evaluations. 

Also, if I am correct in understanding that you are more likely to die on your birthday than in a car accident with one year odds, does this hold true if you are talking about a lifetime?",en
1106684,2011-01-16 22:04:04,statistics,Ockham’s Spatula | Stats With Cats Blog,f3e7f,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/f3e7f/ockhams_spatula_stats_with_cats_blog/,5.0,0.0,,en
1106685,2011-01-17 02:54:28,statistics,"What if errors aren't N(0,sig^2)? Are linear regression estimates robust?",f3igz,24601G,1239333050.0,https://www.reddit.com/r/statistics/comments/f3igz/what_if_errors_arent_n0sig2_are_linear_regression/,6.0,17.0,"I have some data for which I know what the theoretical relationship is: exponential decay. I can transform that into a linear relationship (natural log transform), and I've done a simple linear regression with the transformed data to test how good the theory is. The R^2 values tell me that this exponential decay model is very useful (&gt;98% variance), but technically I'm failing the assumption about normally distributed errors. My residuals (though small) are increasing (maybe quadraticly) with the predictor. ([residuals plot here](http://imgur.com/ucn5H))

What estimates of the linear regression are invalidated by this violated assumption? Can I still use my estimated beta, beta confidence interval, R^2 ?

edit: here's a plot of my [data and fit](http://imgur.com/LRGiR).",en
1106686,2011-01-17 13:28:42,MachineLearning,Feature order problem in paired element dataset,f3qv8,Froost,1239307369.0,https://www.reddit.com/r/MachineLearning/comments/f3qv8/feature_order_problem_in_paired_element_dataset/,8.0,8.0,"Hey everyone,

I have a little problem and was curious if there was any research about the best way to handle such data.

So, I have a data table, each row has multiple features/class etc. But my rows are created from pairings; that is, instead of ""row1"", ""row2"" I have ""pair_1-2"",""pair_1-3"", ""pair_2-3"" etc. Basically my table is a NxN matrix of M-feature vectors.

In my feature vector I have some features pertaining to the pairing in question (kind of similarity/dissimilarity measures), those are OK. 

However I also have features from the elements that create the pairing as well. So, I'll have features such as ""SizeOf-1"", ""SizeOf-2"", the size value of the first and second element in the pairing. Those are problematic; because their order does not matter, they should be completely interchangeable; I can switch the values of size1 and size2 for randomly selected rows and it will still mean the same thing (there is no ""first"" or ""second"" in the pairing). But for classification purposes they are separate features, thus their order is of some importance to the learning algorithm. 

Thus, while the actual model is ""If any of those elements in the pair have size &gt; X"", the classifier gives two separate branches, and those branches will have less support than the actual rule (due to the fact that in some rows the first one will be &gt;X while in some it will be the second). Those features create such minor problems, increase the complexity of the model unnecessarily (and probably decreasing the accuracy to some extent) etc. 

So, how can I handle such values more efficiently? I thought about duplicating each row to have both orderings (at least the support of each rule becomes the actual one), but forcing the duplicated rows to be together in either training and test set is problematic in 3rd party server code, plus the duplication also creates some other problems in another step of our algorithm. 

Any help will be appreciated. Thanks.

",en
1106687,2011-01-17 18:22:31,MachineLearning,A chatterbot built using a neural networks approach (reservoir computing),f3uvz,nadie854,1295106489.0,https://www.reddit.com/r/MachineLearning/comments/f3uvz/a_chatterbot_built_using_a_neural_networks/,7.0,13.0,"Reserbot is a chatterbot, but this bot is using an approach based on echo state neural networks. It tries to learn natural languages learning just from imitation. This project is looking for collaborators, comments, ideas, etc.

URL: https://github.com/neuromancer/reserbot

Thanks!",en
1106688,2011-01-17 19:51:25,MachineLearning,Statistical podcast: Random and Pseudorandom ,f3wk2,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/f3wk2/statistical_podcast_random_and_pseudorandom/,5.0,1.0,,en
1106689,2011-01-17 19:51:51,statistics,Statistical podcast: Random and Pseudorandom,f3wki,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/f3wki/statistical_podcast_random_and_pseudorandom/,4.0,0.0,,en
1106690,2011-01-17 21:50:51,statistics,An Introduction to Probability and Random Processes [PDF],f3yx3,Ulvund,1177090152.0,https://www.reddit.com/r/statistics/comments/f3yx3/an_introduction_to_probability_and_random/,11.0,2.0,,en
1106691,2011-01-18 01:54:39,statistics,Question about publishing SPSS Figures.,f43k2,xxgambinoxx,,https://www.reddit.com/r/statistics/comments/f43k2/question_about_publishing_spss_figures/,3.0,11.0,"Well I know that I should be using R, but I haven't gotten around to learning it yet. What is the best way to create publishable figures in SPSS? 

For instance I have a series of survival curves but they may not be the best resolution. Should I export them in Tiff or EPS etc?",en
1106692,2011-01-18 04:38:21,MachineLearning,How to analyze Craigslist's entire history? Jeremy Zawodny is taking suggestions.,f46c5,BioGeek,1124683200.0,https://www.reddit.com/r/MachineLearning/comments/f46c5/how_to_analyze_craigslists_entire_history_jeremy/,9.0,1.0,,en
1106693,2011-01-18 13:24:41,MachineLearning,CS359G at Stanford: Graph Partitioning and Expanders,f4djk,gtani,1136005200.0,https://www.reddit.com/r/MachineLearning/comments/f4djk/cs359g_at_stanford_graph_partitioning_and/,3.0,0.0,,en
1106694,2011-01-18 16:33:35,MachineLearning,Statistics in the Real World: The Search for the Scorpion,f4fwr,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/f4fwr/statistics_in_the_real_world_the_search_for_the/,23.0,0.0,,en
1106695,2011-01-18 19:18:04,MachineLearning,Confused as to how to approach a problem.,f4j1k,leondz,1165669130.0,https://www.reddit.com/r/MachineLearning/comments/f4j1k/confused_as_to_how_to_approach_a_problem/,1.0,4.0,"Hi.

My problem is this. I have a set of nodes. I also have a handful of links. Links and nodes are each describable in terms of features. Each link will connect exactly two nodes, in a specific order. I would like to use machine learning to determine, given a specific link, which nodes should be connected by that link. I have a whole bunch of gold standard data. How can I approach this problem?

For context, nodes in this example are named entities in a text, and the links are relations between them.",en
1106696,2011-01-18 22:43:24,statistics,Correct test for analysing difference between two regressions?,f4nfe,Iregretthisusername,1291910053.0,https://www.reddit.com/r/statistics/comments/f4nfe/correct_test_for_analysing_difference_between_two/,8.0,14.0,"I used a Mann-Whitney U test, as the data failed a Levene's Equality of Variance test.

I was told this was the incorrect method and GLM should have been applied instead. Not being familiar with GLM, can anyone explain why (and if) the Mann-Whitney U test was inappropriate?

See [graph](http://i.imgur.com/puwZZ.png) of my data (n = 72) 

edit: Forgive me for asking any silly questions, but my knowledge of statistics is pretty much 'beginner.' But I ask these questions to learn, so bear with me. Also, my program of choice is SPSS.",en
1106697,2011-01-19 09:37:41,statistics,Applying stats to web analytics data,f4zas,[deleted],,https://www.reddit.com/r/statistics/comments/f4zas/applying_stats_to_web_analytics_data/,1.0,0.0,,en
1106698,2011-01-19 09:53:18,statistics,How to apply stats to web analytics data?,f4zhs,Aristaios,1288244745.0,https://www.reddit.com/r/statistics/comments/f4zhs/how_to_apply_stats_to_web_analytics_data/,6.0,7.0,"I've taken some basic stats but it's not clear what statistical tests, if any, I can apply to test web statistics.  

For example, if you have visits (or visitors) to a site (or webpage) in two different months how would you test is traffic in month A is significantly higher (or lower) than in month B.  

A T-test maybe, but if so how would I calculate the standard deviation?  Some sort of binary tests, but even that doesn't seem to be appropriate?  Or maybe there's nothing that can be done with this sort of data?

Suggestions welcome",en
1106699,2011-01-19 11:45:30,analytics,Program Details - Data Analytics Course,f50s5,iqrconsulting,1288244989.0,https://www.reddit.com/r/analytics/comments/f50s5/program_details_data_analytics_course/,0.0,0.0,Here is the program details of the data analytics certification course at the Academy for Decision Science Ahmedabad.,en
1106700,2011-01-19 20:16:02,statistics,Normalisation Question,f58w7,gordonj,1282789455.0,https://www.reddit.com/r/statistics/comments/f58w7/normalisation_question/,3.0,4.0,"I have a question about how I should normalise 2 histograms which are made up of sets of clustered families of different sizes (based on a similarity measure between the elements in the set). Set 1 contains ~33000 elements clustered in ~6000 families and Set 2 contains ~1000 elements in ~400 families. I have made histograms of frequency vs. family size (i.e. the number of times a family of size X is found in the dataset) for each set. What I would like to do is normalise them in such a way so that I can take the ratio of a given family size from each set and come out with a meaningful measure of the relative over/under-abundance of families of that size in one set relative to the other. I initially thought that I should divide each family size column by the total number of elements in its set, but after a little thought, I'm not so sure it's correct. Any thoughts or suggestions would be very welcome. I hope this post makes some sense to people and is not inappropriate for this subreddit (I have translated it so as to remove biological nomenclature and make it more general), Thanks Reddit.",en
1106701,2011-01-19 20:49:57,statistics,Combining the measures recall and precision,f59m8,sabertoothedhedgehog,1293632592.0,https://www.reddit.com/r/statistics/comments/f59m8/combining_the_measures_recall_and_precision/,3.0,6.0,"Hi Reddit,

This is my first post, so I hope I do follow the Reddiquette. Please be kind.

I am currently writing my Master's thesis and stumbling upon a mathematical/statistical problem. Maybe you can give me a hint.

I am dealing with a scenario where items have to be labelled by multiple people. My (uncorrelated) labels can only be selected from a large given set of labels. There are only a few relevant labels compared to thousands of labels that are irrelevant for any given item.

Given one single item and the labelling decision of all the people I asked I would like to estimate the probability for *each* label that it should be assigned (= that it is relevant to the item). But I do not know the true labels and I only have a historical labelling performance for each labeller.

So what I need is statistical inference, I assume. Either Bayesian or Maximum Likelihood without prior. Please correct me if I'm wrong.

In the domain of Information Retrieval (Computer Science) there are two interesting measures that could be used: [recall and precision](http://en.wikipedia.org/wiki/Precision_and_recall). They are converse conditional probabilities that I would like to use as labeller-specific performance measures.
In our case that would be:

* recall = conditional probability that a label is assigned (classified as relevant) given that the label is in fact relevant
* precision = conditional probability of label being relevant given that it has been assigned (classified as relevant)

Is there way to use these two measures (of each of the labellers) and combine them in some sort of Maximum Likelihood Estimation so that I obtain a ""probability of relevance"" for each label?

One toy example:

* I have an image with some depicted scenery.
* I have a set of standard labels = {tree, car, house, fish, calculator}
* I ask two people to label the image using the standard set of labels.
* One guy return {tree, fish}; he had a recall of 0.6 and precision of 0.6 in the past.
* The other guy returns {car, fish}; he had the same recall and precision in the past.

What is the probability that ""tree"" is correct? What about ""fish"" and ""car""?

Thank you so much for any comment or hint!

PS.: Would be cool to acknowledge a redditor in my thesis ;-)",en
1106702,2011-01-19 21:52:28,analytics,potential job in web analytics... need to analyze some data.  what are they looking for?,f5az7,therewontberiots,1278672342.0,https://www.reddit.com/r/analytics/comments/f5az7/potential_job_in_web_analytics_need_to_analyze/,2.0,4.0,"i decided grad school (physics) was not for me and i am branching out into the job market.  a web analytics place is interested in me (and i'm interested in any kind of data analysis).  ""The exercise is to use a comparison of three or more months of data to prepare a 5 to 10 slide PowerPoint presentation of any significant information about site visitors, what they are doing, how they arrive at our site that we could use to improve site performance as an acquisition source.""  he said i should 'tell a story'.  this is a field i am unfamiliar with so i'm looking for any basic tips, common pitfalls, and expectations.  thanks.  (i am quite familiar with data analysis in general)",en
1106703,2011-01-20 00:08:08,MachineLearning,Ford launches a challenge: build a classifier to determine whether or not drivers are alert,f5dzs,antgoldbloom,1285588580.0,https://www.reddit.com/r/MachineLearning/comments/f5dzs/ford_launches_a_challenge_build_a_classifier_to/,27.0,8.0,,en
1106704,2011-01-20 06:21:39,MachineLearning,Get ready for the 2011 March Madness Predictive Analytics Challenge,f5kze,danger_t,1249412021.0,https://www.reddit.com/r/MachineLearning/comments/f5kze/get_ready_for_the_2011_march_madness_predictive/,6.0,3.0,,en
1106705,2011-01-20 09:10:17,statistics,Stats on a Business Database,f5nr9,evilmaus,1278545501.0,https://www.reddit.com/r/statistics/comments/f5nr9/stats_on_a_business_database/,5.0,10.0,"I work as a web developer for a bookstore that sells textbooks in partnership with universities.  It's a small IT shop with a relaxed culture.  As such, I have complete access to the business databases.  Our web apps are written in PHP, which I'm finding to be somewhat poor on any math higher than trig.  I'm semi-comfortable with Python and have been playing around wtih SciPy, as well as hitting the books on probability and statistics pretty hard lately.  

What I'd like to know is if anyone has any suggestions for fun or interesting ways that I can analyze the data in there, particularly in ways that would actually be meaningful and insightful to the business.  Whereas it'd be nice to get some useful insights to pass up the line and potentially wow my superiors, the main motivation is to actually LEARN this stuff, such as how to extract insights from a pile of data, so I can apply it later on in life.",en
1106706,2011-01-20 20:41:29,statistics,40 Fascinating Blogs for the Ultimate Statistics Geek ,f5yd5,soupydreck,1252986260.0,https://www.reddit.com/r/statistics/comments/f5yd5/40_fascinating_blogs_for_the_ultimate_statistics/,27.0,3.0,,en
1106707,2011-01-21 00:42:39,MachineLearning,Heritage Health Prize,f63hh,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/f63hh/heritage_health_prize/,10.0,2.0,,en
1106708,2011-01-21 07:46:05,artificial,You've just got to love science. The video of these quadrotor robots is way cool. Watch them build a building by themselves.,f6b0m,b_ohare,1281763217.0,https://www.reddit.com/r/artificial/comments/f6b0m/youve_just_got_to_love_science_the_video_of_these/,1.0,1.0,,en
1106709,2011-01-21 15:09:38,statistics,Open source and online graphing; very pretty,f6g7d,basscadet,1187963763.0,https://www.reddit.com/r/statistics/comments/f6g7d/open_source_and_online_graphing_very_pretty/,7.0,0.0,,en
1106710,2011-01-21 17:03:25,statistics,How to check if something behaves like a Wiener process?,f6hyw,Ulvund,1177090152.0,https://www.reddit.com/r/statistics/comments/f6hyw/how_to_check_if_something_behaves_like_a_wiener/,9.0,5.0,"Imagine there are 30 brownian motion-like processes. How could you check if it has the properties of a Wiener process? (Came up with the problem myself for the record :)

I have come up with a few ideas:

* relative changes should be normally distributed for large amounts of data

* all the ""end values"" of the processes should tend towards a normal distribution (for large numbers of processes)

* No large auto-correlation between relative changes (when the trend is removed)

Does any of you have any additional ideas ?",en
1106711,2011-01-21 20:37:34,MachineLearning,Final Jeopardy: Can a Machine Think?,f6m9x,[deleted],,https://www.reddit.com/r/MachineLearning/comments/f6m9x/final_jeopardy_can_a_machine_think/,2.0,1.0,,en
1106712,2011-01-23 02:37:40,statistics,3 Reasons Why Researchers Hate Focus Groups #MRX,f7ac1,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/f7ac1/3_reasons_why_researchers_hate_focus_groups_mrx/,2.0,0.0,,en
1106713,2011-01-23 02:41:38,statistics,A Pie Chart of my Favourite bars…,f7aec,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/f7aec/a_pie_chart_of_my_favourite_bars/,0.0,0.0,,en
1106714,2011-01-23 05:31:59,statistics,Presenting Results to a non-technical audience,f7crb,BirthDeath,1295140816.0,https://www.reddit.com/r/statistics/comments/f7crb/presenting_results_to_a_nontechnical_audience/,4.0,8.0,"  I engage in a fair amount of interdisciplinary work (mostly in the social sciences) and I find that many ""non-technical"" people just gloss over my analysis and look solely at the conclusion.  

Can anyone recommend any resources or techniques to better communicate my methodology to those with limited technical backgrounds? ",en
1106715,2011-01-23 23:28:11,computervision,Show Reddit: I made a web app for marking up objects in images.,f7okc,egonSchiele,1222879479.0,https://www.reddit.com/r/computervision/comments/f7okc/show_reddit_i_made_a_web_app_for_marking_up/,5.0,5.0,,en
1106716,2011-01-23 23:28:18,statistics,Anyone know where i can get a copy of minitab 12.1?,f7okj,jezmaster,1268341962.0,https://www.reddit.com/r/statistics/comments/f7okj/anyone_know_where_i_can_get_a_copy_of_minitab_121/,0.0,9.0,,en
1106717,2011-01-24 10:49:45,MachineLearning,The Social Media Analytics Research Toolkit - Now With Sentiment Analysis!,f7yui,[deleted],,https://www.reddit.com/r/MachineLearning/comments/f7yui/the_social_media_analytics_research_toolkit_now/,2.0,0.0,,en
1106718,2011-01-24 19:30:40,statistics,"Hey guys, I don't know if you do this but I would really like some help choosing a statistical analysis method for a project I'm working on (for school).",f865m,amoebacorn,1266099431.0,https://www.reddit.com/r/statistics/comments/f865m/hey_guys_i_dont_know_if_you_do_this_but_i_would/,4.0,9.0,"So I have this science class where we have to do our own projects and then submit them to a science fair. The ""teacher"" said we have to explain to her what type of statistical analysis we're going to use on our data, and I know diddly-squat about statistics. She won't help either.

Basically for my project I have two tanks with a few snails in each. One tank will have a little bit of oil, and the other won't (the control). I'm going to measure the rate at which each tank of snails eats the algae lining the walls of the tank. If you need more specifics just ask, I would really appreciate some help.",en
1106719,2011-01-24 21:35:42,MachineLearning,Resource on optimization techniques in R?,f88u6,[deleted],,https://www.reddit.com/r/MachineLearning/comments/f88u6/resource_on_optimization_techniques_in_r/,6.0,3.0,"Hello all,

I'm trying to implement a missing data algorithm in R and am a bit confused connecting their documentation with other resources out there.

Specifically, how does nlm do minimization? Is this a quasi-newton algorithm? Is it even considered first order (since you can receive the Hessian matrix as output).

Anyway, it's obvious that I'm confused, so any comments or documentation could be useful.",en
1106720,2011-01-25 05:26:59,statistics,Here's an example of Stata's high quality graphics system,f8hue,[deleted],,https://www.reddit.com/r/statistics/comments/f8hue/heres_an_example_of_statas_high_quality_graphics/,16.0,8.0,,en
1106721,2011-01-25 12:35:28,statistics,How do you explain reproducible research to clients? - Social data blog,f8n1q,mycatharsis,1294046118.0,https://www.reddit.com/r/statistics/comments/f8n1q/how_do_you_explain_reproducible_research_to/,4.0,1.0,,en
1106722,2011-01-25 14:22:02,MachineLearning,Free E-Book: Clever Algorithms: Nature-Inspired Programming Recipes (/r/csbooks crosspost),f8o1j,fbahr,1275413362.0,https://www.reddit.com/r/MachineLearning/comments/f8o1j/free_ebook_clever_algorithms_natureinspired/,14.0,3.0,,en
1106723,2011-01-25 14:39:39,MachineLearning,The Quest for Artificial Intelligence ,f8o7r,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/f8o7r/the_quest_for_artificial_intelligence/,26.0,4.0,,en
1106724,2011-01-25 17:14:48,MachineLearning,Review of “R Graphs Cookbook” by Hrishi Mittal,f8ptz,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/f8ptz/review_of_r_graphs_cookbook_by_hrishi_mittal/,2.0,0.0,,en
1106725,2011-01-25 18:23:07,MachineLearning,Can police really predict crime before it happens?,f8r6s,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/f8r6s/can_police_really_predict_crime_before_it_happens/,13.0,8.0,,en
1106726,2011-01-25 21:36:15,analytics,Analytics trend fail.,f8vmb,splatt9,1290475784.0,https://www.reddit.com/r/analytics/comments/f8vmb/analytics_trend_fail/,1.0,0.0,,en
1106727,2011-01-26 18:43:06,MachineLearning,Project Kipling Data Journalism Tools Release 0.2.0 - Sentiment Analysis,f9fmh,[deleted],,https://www.reddit.com/r/MachineLearning/comments/f9fmh/project_kipling_data_journalism_tools_release_020/,8.0,0.0,,en
1106728,2011-01-26 19:09:48,statistics,askstat: Pretty R code for blogs,f9g74,[deleted],,https://www.reddit.com/r/statistics/comments/f9g74/askstat_pretty_r_code_for_blogs/,8.0,3.0,"Hi all,

I'm interested in blogging on some of my work in statistics and R so I can contribute to everyone's awesome and meaningful discussions. I've noticed that people are able to embed pretty R code, colored and delimited into HTML. Can anyone shed light on how they do this?
",en
1106729,2011-01-26 23:59:24,MachineLearning,"New subreddit for bigdata! Also anyone going to the Strata conference in Santa Clara, CA from Feb. 1-3?",f9m1v,ohsnaaap,1270647929.0,https://www.reddit.com/r/MachineLearning/comments/f9m1v/new_subreddit_for_bigdata_also_anyone_going_to/,14.0,0.0,,en
1106730,2011-01-27 03:40:33,statistics,ggobi - really cool free open source visualization tool - i think i might start using this while building models.  ,f9q8c,[deleted],,https://www.reddit.com/r/statistics/comments/f9q8c/ggobi_really_cool_free_open_source_visualization/,6.0,1.0,,en
1106731,2011-01-27 06:50:20,MachineLearning,Conduct Social Media Sentiment Analysis Research with SMART@zmeb,f9ttd,[deleted],,https://www.reddit.com/r/MachineLearning/comments/f9ttd/conduct_social_media_sentiment_analysis_research/,1.0,0.0,,en
1106732,2011-01-27 10:03:39,statistics,AskStat:  Dice rolls and probability,f9wxz,na85,1254906711.0,https://www.reddit.com/r/statistics/comments/f9wxz/askstat_dice_rolls_and_probability/,0.0,9.0,"Hi folks, I have a quick question.

If I roll n dice of m sides each, what is the expected value/average outcome?

Thanks",en
1106733,2011-01-27 18:02:59,statistics,is there a chart tool for the web that can do an interactive scatter plot with labels (not coordinates) ?,fa37t,bucetacabeluda,1296144103.0,https://www.reddit.com/r/statistics/comments/fa37t/is_there_a_chart_tool_for_the_web_that_can_do_an/,3.0,6.0,"see,
something like google's 

http://code.google.com/apis/visualization/documentation/gallery/scatterchart.html

only shows the coordinates and/or the axis label name,
but let's say each dot had a name, like john, mary, etc...

how do i make it show up as well.

obviously it doesnt have to be google charts,

im looking for ANY tool that can post neat looking interactive charts on the web.

thank you
 
",en
1106734,2011-01-27 18:08:44,MachineLearning,How to label all the outliers in a boxplot (using R),fa3cd,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/fa3cd/how_to_label_all_the_outliers_in_a_boxplot_using_r/,0.0,0.0,,en
1106735,2011-01-27 18:09:01,statistics,How to label all the outliers in a boxplot (using R),fa3cn,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/fa3cn/how_to_label_all_the_outliers_in_a_boxplot_using_r/,4.0,0.0,,en
1106736,2011-01-27 22:27:07,statistics,Trying to determine the proper statistical test to use in this one experiment along with proper statistical analysis,fa957,sab3r,1256320656.0,https://www.reddit.com/r/statistics/comments/fa957/trying_to_determine_the_proper_statistical_test/,4.0,10.0,"Lets say you have a test tube and in this test tube you have some ants (lets say 20 ants). In the absence of any stimuli gradient, the ants are random. Lets say you put a repulsive, volatile chemical repellent at one end of the tube and then the ants run away from it. So lets say initially, you put let the volatile chemical repellent diffuse for a certain amount of time and then you put the ants into the tube at the repellent end. You count the number of ants in each third of the tube for 30 minutes. So you make a graph of the percent of ants in the repellent end as a function of time and you want to see if one ant is behaving differently from another type of ant and determine statistical significance etc. What test would one use?

Additionally, if you were to do the above on one kind of ant and your graph had curves that went all over the place (inconsistent results), is it appropriate or not appropriate to simply average the results? Thanks.",en
1106737,2011-01-27 23:53:35,analytics,Speech Analysis | In Obama's Words | The Washington Post,fab07,[deleted],,https://www.reddit.com/r/analytics/comments/fab07/speech_analysis_in_obamas_words_the_washington/,1.0,0.0,,en
1106738,2011-01-28 00:50:23,MachineLearning,Is a $3 million health-care prize for innovation the answer to Obama’s call?,fac7x,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/fac7x/is_a_3_million_healthcare_prize_for_innovation/,5.0,1.0,,en
1106739,2011-01-28 04:54:48,statistics,Does anyone have a dataset for US voting rates by state?,fagsy,shadowofthe,1273624437.0,https://www.reddit.com/r/statistics/comments/fagsy/does_anyone_have_a_dataset_for_us_voting_rates_by/,3.0,4.0,Am working in stata and before I go out and make one I thought I would ask if anyone here had or could find one?,en
1106740,2011-01-28 15:18:20,MachineLearning,The Genius of Alan Turing (in development feature length documentary),fapg3,incandescant_seraph,1296044764.0,https://www.reddit.com/r/MachineLearning/comments/fapg3/the_genius_of_alan_turing_in_development_feature/,32.0,2.0,,en
1106741,2011-01-28 16:54:58,MachineLearning,Machine Learning and GsoC 2011,far1g,mfalcon,1264563760.0,https://www.reddit.com/r/MachineLearning/comments/far1g/machine_learning_and_gsoc_2011/,2.0,7.0,"Recently I began to read a book called ""Programing Collective Intelligence"" and it's really interesting to me. I decided I want to get into the ML world and I think the GsoC 2011 would be a great way to help me do this.

So I'm looking for ML open source projects that are likely to participate in the program in order to know them and make a ""pre-selection"" of the ones I'd like to participate. I'm an intermediate Python programmer so it'd be great if the project uses this language but I know Java is very popular in ML projects.
",en
1106742,2011-01-28 22:45:28,statistics,I have a question for you all about bootstrapping and/or Bayesian analysis that I need some help on.,fay8b,daledinkler,1199898909.0,https://www.reddit.com/r/statistics/comments/fay8b/i_have_a_question_for_you_all_about_bootstrapping/,2.0,4.0,"Okay, I have a set of data that has uncertainty along both the x and y axis.

The y-axis uncertainty is not normally distributed, and doesn't follow any sort of standard distribution (it is calibrated 14C data), it is sometimes unimodal, sometimes bimodal, it all depends.

The x-axis uncertainty is the product of a function f(z) with a measurable standard error where z itself is the product of a function g(z) that has multinomial error (I model it with Dierdelicht error).

Maybe this makes no sense.  I don't know much about Bayesian stats, but I'm slowly learning.  My question is this (I am using R):  If I bootstrap everything, say, for 1000 repetitions of g(z) I calculate f(x) and for each f(x) I produce f(x)+rnorm(100,SE) where SE is the estimated error of f(x).

Then I have 100000 values for f(x).

So, um, what the hell do I have?

I want to generate probability fields along the x and y axes in the end, but at this point I'm kind of lost.  I feel like this is really screaming for Bayesian analysis, but that bootstrapping is probably not completely wrong.  Any pointers?",en
1106743,2011-01-29 00:22:10,MachineLearning,What are the good stackexchange-style Q&amp;A machine learning sites out there?,fb05p,rm999,1175510936.0,https://www.reddit.com/r/MachineLearning/comments/fb05p/what_are_the_good_stackexchangestyle_qa_machine/,8.0,11.0,"I have been a member of metaoptimize since it began, and it has been disappointing. The site is heavily geared towards NLP, which is fine but there is so much more to machine learning. I also find it to be too theoretical; I think most of the active members there are graduate students. Also, there isn't much activity there. 

I have been checking out stockoverflow's [stats site](http://stats.stackexchange.com/) which seems promising so far. But stats, while overlapping a huge amount with machine learning, can be slightly different. For example, questions on deep belief networks seem rare there, even though it is a moderately active area of research in the machine learning community.

Are there any other sites I am missing?",en
1106744,2011-01-29 02:02:10,statistics,"Help! I am extremely sick, and my statistics homework is due at midnight.",fb20k,lenny00,1254773042.0,https://www.reddit.com/r/statistics/comments/fb20k/help_i_am_extremely_sick_and_my_statistics/,0.0,0.0,"The chapter is about histograms and relative frequencies. I just woke up from a nap yesterday around noon, my head is pounding and i need to be close to a toilet(for two obvious reasons). Anyone want to take a stab at it so I can suffer without adding math to the equation?

It's all online. ",en
1106745,2011-01-29 05:59:43,MachineLearning,$3 million machine learning prize : develop a predictive algorithm which identifies patients who will be admitted to the hospital within six months,fb5oy,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/fb5oy/3_million_machine_learning_prize_develop_a/,25.0,21.0,,en
1106746,2011-01-29 06:54:48,statistics,Four ways of viewing statistics,fb6gm,mycatharsis,1294046118.0,https://www.reddit.com/r/statistics/comments/fb6gm/four_ways_of_viewing_statistics/,12.0,3.0,,en
1106747,2011-01-29 20:01:04,MachineLearning,Amazon to launch recommender test bench,fbekp,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/fbekp/amazon_to_launch_recommender_test_bench/,10.0,4.0,,en
1106748,2011-01-30 01:34:05,statistics,SOFA Statistics is open-source software with an emphasis on ease of use and learning as you go. [video],fbjoo,robotrebellion,,https://www.reddit.com/r/statistics/comments/fbjoo/sofa_statistics_is_opensource_software_with_an/,1.0,1.0,,en
1106749,2011-01-30 05:21:44,statistics,Why would my slope be the same for my pearson correlation coefficient for a given linear relationship (not standardized!) and for another its not equal? ,fbmsg,[deleted],,https://www.reddit.com/r/statistics/comments/fbmsg/why_would_my_slope_be_the_same_for_my_pearson/,2.0,2.0,i think that says it all- needless to say i am lost. ,en
1106750,2011-01-30 08:22:14,statistics,E squared VS R squared,fbp0z,[deleted],,https://www.reddit.com/r/statistics/comments/fbp0z/e_squared_vs_r_squared/,3.0,2.0,"why is eta- squared (correlation ratio) so much larger than r squared for certain equations? I understand that e squared is used for non-linear relationships and R squared for linear relations- however, when i calculate both why would one b larger than the other (they are calculated the same way! just with different information- so what in that information makes one larger than the other)",en
1106751,2011-01-30 22:29:36,statistics,Statistics Problem - Homework Help?,fbyf7,mprivitt,1289780617.0,https://www.reddit.com/r/statistics/comments/fbyf7/statistics_problem_homework_help/,0.0,2.0,"""Sixty percent of the students at a certain school wear neither a ring nor a necklace.  Twenty percent wear a ring and 30 percent wear a necklace.  If one of the students is chosen randomly, what is the probability that this student is wearing

(a) a ring or a necklace?
(b) a ring and a necklace?",en
1106752,2011-01-30 22:45:52,statistics,Question for R,fbyp4,amirightfolks,1280970578.0,https://www.reddit.com/r/statistics/comments/fbyp4/question_for_r/,10.0,8.0,"I need to do an assignment where I make a sample of size 200 of a random exponential variable with lamda=1/15. So, I know I can do this by just writing `rexp(200,1/15)`, but I have to do this 100 times. Basically, I will have 100 different samples of size 200. Can I possibly do this using a for loop and and save the results in an array instead of constantly rewriting it every time? 

If somebody could help me out with the details it would be much appreciated. ",en
1106753,2011-01-31 01:03:28,statistics,DAE hate the semicolon because of SAS?,fc0zu,DullHypothesis,1294192834.0,https://www.reddit.com/r/statistics/comments/fc0zu/dae_hate_the_semicolon_because_of_sas/,5.0,6.0,"Every time I program something, I miss a semicolon. I do this for work EVERY DAY, and I still cannot in all of my ability, remember all the semicolons. It makes me feel like an idiot. 

Add on top of this that I submit my programs with JCL, because I work in the year 1982.",en
1106754,2011-01-31 02:13:02,statistics,Limits of Confusion | Stats With Cats Blog,fc23s,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/fc23s/limits_of_confusion_stats_with_cats_blog/,0.0,0.0,,en
1106755,2011-01-31 06:39:09,computervision,[kickstarter] an open source cat face recognition login system. I'm look for backers. ,fc6e5,econnerd,1210048095.0,https://www.reddit.com/r/computervision/comments/fc6e5/kickstarter_an_open_source_cat_face_recognition/,3.0,7.0,,en
1106756,2011-01-31 13:21:27,MachineLearning,"Jeopardy, IBM, and Wolfram",fcbpa,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/fcbpa/jeopardy_ibm_and_wolfram/,19.0,14.0,,en
1106757,2011-01-31 18:55:38,MachineLearning,"Free E-Book: A Field Guide to Genetic Programming (by Riccardo Poli, William B. Langdon, Nicholas F. McPhee - 2008)",fcgs2,fbahr,1275413362.0,https://www.reddit.com/r/MachineLearning/comments/fcgs2/free_ebook_a_field_guide_to_genetic_programming/,27.0,6.0,,en
1106758,2011-01-31 20:31:51,statistics,Follow-up R question,fcitz,amirightfolks,1280970578.0,https://www.reddit.com/r/statistics/comments/fcitz/followup_r_question/,7.0,12.0,"Thanks for the help everyone. I used `y&lt;matrix(rexp(20000,1/15),nrow=100)` to make the 100 different exponential samples. The next step I want to do will involve bootstrapping. For each sample, I want to resample 200 times and save the mean in a matrix. So far this is what I have:

`bootstrap&lt;-matrix(nrow=100,ncol=200)`

`for(i in 1:200) bootstrap[1,i]&lt;-mean(sample(y[1,],replace=T))`

So this works somewhat fine, but it would mean I have to do this 100 times and type in 2,3,4,5,etc. I know there must be an easier way to go about this. Any advice?",en
1106759,2011-02-01 01:32:57,MachineLearning,An Machine Learning system that can hear what you're typing ,fcph1,[deleted],,https://www.reddit.com/r/MachineLearning/comments/fcph1/an_machine_learning_system_that_can_hear_what/,15.0,7.0,,en
1106760,2011-02-01 08:20:32,analytics,The Asymptote of Data Quality,fcx53,FreshOutOfGeekistan,1286976143.0,https://www.reddit.com/r/analytics/comments/fcx53/the_asymptote_of_data_quality/,1.0,0.0,,en
1106761,2011-02-01 19:26:42,MachineLearning,"Free E-Book: Bayesian Reasoning and Machine Learning (by David Barber. CUP, 2011 - Draft) (/r/csbooks crosspost)",fd6we,fbahr,1275413362.0,https://www.reddit.com/r/MachineLearning/comments/fd6we/free_ebook_bayesian_reasoning_and_machine/,11.0,1.0,,en
1106762,2011-02-01 19:29:46,MachineLearning,The Matrix Cookbook: A free mathematical desktop reference on matrices,fd6za,fbahr,1275413362.0,https://www.reddit.com/r/MachineLearning/comments/fd6za/the_matrix_cookbook_a_free_mathematical_desktop/,36.0,1.0,,en
1106763,2011-02-01 22:23:13,datasets,"Free, Public Data Sets",fdb4a,orangepotion,1246804916.0,https://www.reddit.com/r/datasets/comments/fdb4a/free_public_data_sets/,14.0,0.0,,en
1106764,2011-02-01 22:26:45,statistics,Cracking the Scratch Lottery Code,fdb7b,orangepotion,1246804916.0,https://www.reddit.com/r/statistics/comments/fdb7b/cracking_the_scratch_lottery_code/,24.0,12.0,,en
1106765,2011-02-02 00:29:43,statistics,"The Matrix Cookbook - A summary of properties of and calculations with matrices, including applications to multivariate probability distributions (xpost from MachineLearning)",fde1t,dY_dX,1272517452.0,https://www.reddit.com/r/statistics/comments/fde1t/the_matrix_cookbook_a_summary_of_properties_of/,13.0,2.0,,en
1106766,2011-02-02 00:58:31,statistics,AskStatistics: HELP. I'm stats-impaired and trying to finish my Thesis.,fdeom,[deleted],,https://www.reddit.com/r/statistics/comments/fdeom/askstatistics_help_im_statsimpaired_and_trying_to/,2.0,9.0,"Hello, r/Statistics. I'm miserable and throwing myself on your mercy; I hope someone out there can help.

I'm a linguist finishing my MA thesis and it turns out what I've basically produced is a huge spreadsheet of data (who knew?). I need to demonstrate significance with my results and every time I think I have a handle on what I need to do I get confused again. My data looks something like this:

    ID      Group     Age     Race     Gender    Symptomatic?   Feature1    Feature2
    1        1        19-35   Cauc     M         Y              0           4
    2        1        36-55   Hisp     M         N              0           0
    3        1        56-75   Cauc     M         Y              3           1
    4        1        36-55   Cauc     M         Y              1           2
    5        1        36-55   Hisp     M         N              0           0
    6        1        56-75   Hisp     F         Y              5           0
    7        2        56-75   AfAm     F         N              0           0
    8        2        19-35   Hisp     M         N              0           0
    9        2        19-35   Cauc     M         Y              7           4
    10       2        36-55   Cauc     F         N              0           0

A bit of info:

* This is a big simplification. Group ""1"" actually has ~150 participants, and ~50 in group 2, with a much longer list of ""features"" for each. ""Symptomatic Y/N"" is determined by whether any of the features contain a numeric value.
* So basically I think I have categorical data for ""symptomatic"" and interval data for each feature - right? The features are frequency counts, like how many times the individual says a certain word.
* What I THINK I know about this is that I would do some sort of Chi-square to compare #s of symptomatic individuals in each group, age range, race, gender, etc. Yes? And then is it an ANOVA for the features? I need to know whether the frequency of Feature1 in Group 1 vs. Group 2 is significantly different, etc.
* I have SPSS 19. I bought [Discovering Statistics With SPSS](http://www.amazon.com/Discovering-Statistics-Introducing-Statistical-Methods/dp/1847879071/ref=sr_1_1?ie=UTF8&amp;qid=1296600991&amp;sr=8-1) and a few other statistics books but everything seems to be making it worse. I'm not an idiot; this was actually a pretty intense study. I just can't seem to wrap my head around what tests are appropriate here.

I HOPE SOMEONE CAN HELP! I can provide more information if I didn't give enough. Reddit will totally get a thank you in my thesis acknowledgements, heh.",en
1106767,2011-02-02 01:25:37,statistics,Good text for regression modelling?,fdf5p,phllystyl,1290640589.0,https://www.reddit.com/r/statistics/comments/fdf5p/good_text_for_regression_modelling/,3.0,5.0,"Greetings r/statistics,

I am currently working towards a masters in epi, and am currently taking biostatistics II.  We are learning multivariate linear and logistics regression model creation, testing etc; and have found the hand outs in class to be a bit inadequate at times.  Does anyone have a text that they really liked for learning this material? I hope to use this information in clinical research, and we use STATA in my program, if that steers anyone in one direction or another.  

Thanks for your help!",en
1106768,2011-02-02 16:32:59,statistics,Groundhog day 2011: how well can groundhogs predict the weather?,fdt7a,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/fdt7a/groundhog_day_2011_how_well_can_groundhogs/,7.0,1.0,,en
1106769,2011-02-02 21:37:34,analytics,Web Analytics and Metrics: China Discussion,fdzx1,kjshea,1294881955.0,https://www.reddit.com/r/analytics/comments/fdzx1/web_analytics_and_metrics_china_discussion/,1.0,0.0,,en
1106770,2011-02-02 22:54:58,computervision,Mikolajczyk Feature detection help,fe1o1,gatman02,1282134360.0,https://www.reddit.com/r/computervision/comments/fe1o1/mikolajczyk_feature_detection_help/,2.0,2.0,"I know this is a bit of a shot in the dark, but has anyone else here worked with the feature detectors and evaluation work flow created by K. Mikolajczyk and company? I would like to do an evaluation of his methods, as well as others, on a specific type of data set. The websites www.featurespace.org and http://www.robots.ox.ac.uk/~vgg/research/affine/detectors.html binaries are provided, but no source (there is source for the evaluation, but not for the detectors). 

I'm running into trouble replicating his results on his own data set. Digging in a bit deeper, I've found that there are 3 sets of binaries that have evolved over time: h_affine, extract_features, compute_descriptors. All of these binaries can perform harris-affine feature detection, however, all give different results given the same thresholds.

Anyway, if you've had some experience with these binaries please let me know and I can go into the problem in a bit more detail.

Thanks",en
1106771,2011-02-03 06:22:14,datasets,The CAIDA Anonymized 2011 Internet Traces Dataset,fea0m,llimllib,1142360191.0,https://www.reddit.com/r/datasets/comments/fea0m/the_caida_anonymized_2011_internet_traces_dataset/,3.0,0.0,,en
1106772,2011-02-03 06:29:57,MachineLearning,Strata2011 Conference Videos on YouTube,fea60,mycatharsis,1294046118.0,https://www.reddit.com/r/MachineLearning/comments/fea60/strata2011_conference_videos_on_youtube/,12.0,0.0,,en
1106773,2011-02-03 07:37:44,datasets,"/r/datasets, does anyone here have a copy of the Edinburgh Twitter Dataset or the Stanford SNAP Twitter Dataset for download, for academic purposes?",febek,3con0mist,1261199713.0,https://www.reddit.com/r/datasets/comments/febek/rdatasets_does_anyone_here_have_a_copy_of_the/,8.0,3.0,"Hi /r/datasets! 

I was just wondering if anyone here has a copy of the **Edinburgh Twitter Corpus** or **Stanford SNAP Twitter Dataset** for download, for the purposes of an academic research project I am currently working on...

The Edinburgh Dataset was previously available at http://demeter.inf.ed.ac.uk/ --&gt; [http://demeter.inf.ed.ac.uk/datasets/twitter/twitterStream-20091110-20100201-v0.1.1.gz](http://demeter.inf.ed.ac.uk/datasets/twitter/twitterStream-20091110-20100201-v0.1.1.gz), and the SNAP dataset at [http://snap.stanford.edu/data/twitter7.html](http://snap.stanford.edu/data/twitter7.html), but now both seem to have been taken offline at Twitter's request - although plenty of academic papers have used the sets and have already been published.

So I was wondering if anyone here has a copy they downloaded a while ago lying around that they wouldn't mind sharing.

Thank you for reading! :)
Some amazing data on this community, what a gem!! ",en
1106774,2011-02-03 14:35:07,datasets,"Looking for continuously updated sports datasets (Football, soccer, basketball, ..)",fegmm,Ulvund,1177090152.0,https://www.reddit.com/r/datasets/comments/fegmm/looking_for_continuously_updated_sports_datasets/,6.0,8.0,Anyone have any pointers?,en
1106775,2011-02-03 19:42:14,MachineLearning,"Mallet - Java-based package for NLP, document classification, clustering, topic modeling, information extraction, and other ML applications to text.",fembb,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/fembb/mallet_javabased_package_for_nlp_document/,13.0,2.0,,en
1106776,2011-02-03 21:11:22,statistics,Request for research help...,feoea,TheGerminator,1296758338.0,https://www.reddit.com/r/statistics/comments/feoea/request_for_research_help/,3.0,9.0,"I am looking at the effect of certain environmental contaminants on the germination and growth of seeds, and would love to get your help and feedback on the statistics for this project. I've got Hays' ""Statistics"" from the intermediate-level stats class that I took in college, and I have a decent grasp on how to perform various statistical tests, but I want to make sure that I'm using the correct tests for this project!

So, the nitty gritty: The seeds will be germinated in water containing a target contaminant. For each contaminant, I will test six different concentrations, spanning several orders of magnitude. For each concentration of each contaminant, I will test 20 seeds of each plant species. To be clear, a given group of seeds will only be exposed to water with one contaminant at one concentration. Control samples will be the same, just with no contaminant added. After some predetermined number of days, I will count how many seeds germinate or don't (binary yes/no data) and measure the length of the roots and stems on the germinated seeds (continuous data with a low end at 0, for ungerminated seeds). 

And my questions:

- Should I use a t-test to look at the differences between number of seeds germinated at different concentrations?

- In preliminary tests, I used one-way ANOVAs to look at the difference in root or stem lengths of a single species exposed to a single contaminant, where the contaminant concentration is the variable. I used the Tukey test to see which pairs of concentrations caused significant differences in the plants' growth. Is this an acceptable approach? Is it Ok to include the ungerminated seeds in this data set, with a value of 0 for their root and stem lengths, or would it be preferred to count only the germinated seeds in the tests comparing root and stem lengths?

- If I wanted to look at, say, whether a given contaminant impacts the growth of plant species A more than species B, could I do that using a two-way ANOVA, with concentration and species as the two variables?

- How could I compare the effects of contaminant A and contaminant B on a given species?

I'm open to any suggestions and comments that you have, and would be happy to provide any clarification that's needed. I really appreciate any help that you can give! Many thanks!",en
1106777,2011-02-03 21:18:46,statistics,Answer to a tedious R problem,feokc,[deleted],,https://www.reddit.com/r/statistics/comments/feokc/answer_to_a_tedious_r_problem/,1.0,8.0,"Has anyone contrived a clever way to take element wise means of lists of matrices?

For instance, suppose you have the following simulation:

    simOut &lt;- replicate(1000, {
    x &lt;- rnorm(10)
    y &lt;- rpois(10, lambda=exp(x))
    f &lt;- lm(y~x)
    list(coef(f), vcov(f))
    })

The output is a list of signed dimension and each element is a matrix. I would like element-wise means of those things without averaging all the matrix components together.

Thoughts?",en
1106778,2011-02-04 06:16:40,datasets,"Twitter Data Set: 9,281,007 tweets across 135,825 users, free for non-commercial research purposes,  Creative Commons License",fezdi,3con0mist,1261199713.0,https://www.reddit.com/r/datasets/comments/fezdi/twitter_data_set_9281007_tweets_across_135825/,34.0,15.0,,en
1106779,2011-02-04 19:34:34,MachineLearning,Strataconf announcement for the $3 million Heritage Health Prize,ffax2,antgoldbloom,1285588580.0,https://www.reddit.com/r/MachineLearning/comments/ffax2/strataconf_announcement_for_the_3_million/,14.0,2.0,,en
1106780,2011-02-05 01:08:38,statistics,What is this distribution called?,ffhpa,ron_leflore,1288901761.0,https://www.reddit.com/r/statistics/comments/ffhpa/what_is_this_distribution_called/,16.0,9.0,,en
1106781,2011-02-06 02:41:38,statistics,Inference Proportions,fg1io,[deleted],,https://www.reddit.com/r/statistics/comments/fg1io/inference_proportions/,1.0,6.0,"Ho: Pw-Po=O
Ha:Pw-Po&gt;0

Pw is death due to contaminated water         16/414
Po ///  // / / / / /  / Without Contm water      3/228


I need to provide how significant is the evidence.. 

But problem is when I use Z statistics = Pw-Po/SEdp   conditions are Success and fail to be at least 5 which contradict with second sample Po 3/228 



any advice what to do ...


",en
1106782,2011-02-06 08:51:17,statistics,The 60th most popular website in the US,fg6g0,[deleted],,https://www.reddit.com/r/statistics/comments/fg6g0/the_60th_most_popular_website_in_the_us/,1.0,0.0,,en
1106783,2011-02-06 23:05:20,statistics,Dealing with Dilemmas | Stats With Cats Blog,fgg3l,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/fgg3l/dealing_with_dilemmas_stats_with_cats_blog/,3.0,0.0,,en
1106784,2011-02-07 11:38:06,datasets,"13 thousand data sets, 100 million time series, 600 million facts - DataMarket blog",fgrqs,user24,,https://www.reddit.com/r/datasets/comments/fgrqs/13_thousand_data_sets_100_million_time_series_600/,20.0,1.0,,en
1106785,2011-02-07 16:05:35,datasets,Collegiate Facebook Data,fgunw,orangepotion,1246804916.0,https://www.reddit.com/r/datasets/comments/fgunw/collegiate_facebook_data/,15.0,0.0,,en
1106786,2011-02-07 21:30:32,datasets,Where to find good data sets,fh14v,larrydag,1257172302.0,https://www.reddit.com/r/datasets/comments/fh14v/where_to_find_good_data_sets/,0.0,0.0,,en
1106787,2011-02-08 17:56:47,statistics,Update on data for the March Madness prediction contest,fhk3a,WillieWampum,1249176349.0,https://www.reddit.com/r/statistics/comments/fhk3a/update_on_data_for_the_march_madness_prediction/,5.0,1.0,,en
1106788,2011-02-08 18:07:58,MachineLearning,Update on data for the March Madness prediction contest,fhkcj,WillieWampum,1249176349.0,https://www.reddit.com/r/MachineLearning/comments/fhkcj/update_on_data_for_the_march_madness_prediction/,6.0,0.0,,en
1106789,2011-02-08 21:21:19,MachineLearning,How to Use Spatial Statistics to Crack a Scratch-Off Game ,fhons,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/fhons/how_to_use_spatial_statistics_to_crack_a/,7.0,3.0,,en
1106790,2011-02-09 01:57:31,datasets,Getthedata.org: A Stack-Overflow-style site for data,fht99,kev097,1286915567.0,https://www.reddit.com/r/datasets/comments/fht99/getthedataorg_a_stackoverflowstyle_site_for_data/,23.0,0.0,,en
1106791,2011-02-09 12:51:44,analytics,What is the need for Data Analytics?,fhxw6,iqrconsulting,1288244989.0,https://www.reddit.com/r/analytics/comments/fhxw6/what_is_the_need_for_data_analytics/,1.0,0.0,Data Analytics provides an insight on the interpretation of data upon which a company can base their strategic plans and operational policies.,en
1106792,2011-02-10 03:07:17,statistics,Determining sample size/# of samples needed for avg,fif0p,fatswaller,1297298774.0,https://www.reddit.com/r/statistics/comments/fif0p/determining_sample_size_of_samples_needed_for_avg/,4.0,7.0,"Hopefully this makes sense. I think this is something simple but I'm having a hard time wrapping my head around what I need to do. 

I have extremely large pool of samples that need to measured, most of which are likely superfulous. In order to reduce the processing time, I want to determine how many samples I need to measure to get reasonably close to the average (within 95% CI?). They are a continuous variable, for example, the duration of a particular behavior. 

For my finite variables, a type of behavior, I took one group and comprehensively plotted every data point for an individual on a scatter plot and made a log curve to see how many data points it took to pick up 95% of the variation. I want to do something similar but with a continuous variable so that I can make comparisons with other individuals.

Is there some way to legitimize the amount of samples I need to cut down my work? Any input on this is appreciated. ",en
1106793,2011-02-10 07:58:39,datasets,Creating n-grams from the Twitter dataset posted recently. Everyone on Twitter is on their way to something.,fikrq,chime,1134104400.0,https://www.reddit.com/r/datasets/comments/fikrq/creating_ngrams_from_the_twitter_dataset_posted/,17.0,4.0,,en
1106794,2011-02-10 09:33:56,statistics,Comparing slopes of distributions,fim9z,Aristaios,1288244745.0,https://www.reddit.com/r/statistics/comments/fim9z/comparing_slopes_of_distributions/,0.0,1.0,"This is maybe not strictly speaking a statistics question, but more data analysis (and falls into the category of ""Things I should know but don't"")

Let's say I have two series of data, say population by year, and I'm graphed that data and then fitted it with a least-squares line (simple Excel linear trendline or whatever).  So I can look at the slopes of those two trendlines, but I can't really compare them (say country 1 started at 100 million and country 2 started at 1 million).  

So, how can I compare the two slopes, or rather, how can I compare their rates of growth over the period?  I guess I could compare the change between the first year and last year in both distributions and compare the %s, but I'm thinking there must be something else I'm missing?",en
1106795,2011-02-10 10:38:44,MachineLearning,"High-Performance Neural Networks for Visual Object Classification (Top performance on Cifar10, Mnist, Norb w/o unsupervised but lots of GPU)",fin4q,visionlessvisionary,1255005680.0,https://www.reddit.com/r/MachineLearning/comments/fin4q/highperformance_neural_networks_for_visual_object/,28.0,6.0,,en
1106796,2011-02-10 16:50:28,statistics,When is *interactive* data visualization useful to use?,firmk,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/firmk/when_is_interactive_data_visualization_useful_to/,5.0,6.0,"Hello all,

While preparing for a talk I will give soon,  I recently started digging into two major (Free) tools for interactive data visualization: [GGobi][1] and [mondrian][2] - both offer a great range of capabilities (even if they're a bit buggy).

I wish to ask for your help in articulating (both to myself, and for my future audience) **When is it helpful to use interactive plots? Either for data exploration (for ourselves) and data presentation (for a ""client"")?**

For when explaining the data to a client, I can see the value of animation for:

- Using ""identify/linking/brushing"" for seeing which data point in the graph is what.
- Presenting a sensitivity analysis of the data (e.g: ""if we remove this point, here is what we will get)
- Showing the effect of different groups in the data (e.g: ""let's look at our graphs for males and now for the females"")
- Showing the effect of time (or age, or in general, offering another dimension to the presentation)

For when exploring the data ourselves, I can see the value of identify/linking/brushing when exploring an outlier in a dataset we are working on.

But other then these two examples, I am not sure what other practical use these techniques offer.  Especially for our own data exploration!

It could be argued that the interactive part is good for exploring (For example) a different behavior of different groups/clusters in the data.  But when (in practice) I approached such situation, what I tended to do was to run the relevant statistical procedures (and post-hoc tests) - and what I found to be significant I would then plot with colors clearly dividing the data to the relevant groups.  From what I've seen, this is a safer approach then ""wondering around"" the data (which could easily lead to data dredging (were the scope of the multiple comparison needed for correction is not even clear).

I'd be very happy to read your experience/thoughts on this matter.

(this question can be a wiki - although it is not subjective and a well thought-out answer will gladly win my ""answer"" mark :) )



  [1]: http://www.ggobi.org/
  [2]: http://rosuda.org/mondrian/",en
1106797,2011-02-10 16:50:36,MachineLearning,When is *interactive* data visualization useful to use?,firmo,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/firmo/when_is_interactive_data_visualization_useful_to/,16.0,4.0,"Hello all,

While preparing for a talk I will give soon,  I recently started digging into two major (Free) tools for interactive data visualization: [GGobi][1] and [mondrian][2] - both offer a great range of capabilities (even if they're a bit buggy).

I wish to ask for your help in articulating (both to myself, and for my future audience) **When is it helpful to use interactive plots? Either for data exploration (for ourselves) and data presentation (for a ""client"")?**

For when explaining the data to a client, I can see the value of animation for:

- Using ""identify/linking/brushing"" for seeing which data point in the graph is what.
- Presenting a sensitivity analysis of the data (e.g: ""if we remove this point, here is what we will get)
- Showing the effect of different groups in the data (e.g: ""let's look at our graphs for males and now for the females"")
- Showing the effect of time (or age, or in general, offering another dimension to the presentation)

For when exploring the data ourselves, I can see the value of identify/linking/brushing when exploring an outlier in a dataset we are working on.

But other then these two examples, I am not sure what other practical use these techniques offer.  Especially for our own data exploration!

It could be argued that the interactive part is good for exploring (For example) a different behavior of different groups/clusters in the data.  But when (in practice) I approached such situation, what I tended to do was to run the relevant statistical procedures (and post-hoc tests) - and what I found to be significant I would then plot with colors clearly dividing the data to the relevant groups.  From what I've seen, this is a safer approach then ""wondering around"" the data (which could easily lead to data dredging (were the scope of the multiple comparison needed for correction is not even clear).

I'd be very happy to read your experience/thoughts on this matter.

(this question can be a wiki - although it is not subjective and a well thought-out answer will gladly win my ""answer"" mark :) )



  [1]: http://www.ggobi.org/
  [2]: http://rosuda.org/mondrian/",en
1106798,2011-02-10 20:34:25,statistics,"Free Text on ""Exploratory Factor Analysis"" by Ledyard Tucker and Robert MacCallum",fiwrd,fbahr,1275413362.0,https://www.reddit.com/r/statistics/comments/fiwrd/free_text_on_exploratory_factor_analysis_by/,9.0,0.0,,en
1106799,2011-02-11 01:54:09,statistics,USA Today attempts to find a causal link between low income and obesity in women.,fj3tt,B_Master,1289011096.0,https://www.reddit.com/r/statistics/comments/fj3tt/usa_today_attempts_to_find_a_causal_link_between/,1.0,6.0,,en
1106800,2011-02-11 03:56:06,statistics,Downloading an external file in R,fj606,topheroly,1245562759.0,https://www.reddit.com/r/statistics/comments/fj606/downloading_an_external_file_in_r/,3.0,4.0,"Hello all, I'm trying to download a file on a server had has HTTP authentication in R. I've tried a few things so far with no avail.
    
    address = ""http://username:password@webaddress.com/textfile.txt:8080""
    download.file(address, ""Testing.txt"", method = ""wget"")

I don't know what is going wrong with this code, but if anyone can suggest something better/easier let me know.

Output from above code:

    --2011-02-10 19:46:43--  http://username:*password*@webaddress.com/stats.txt:8080/
    Resolving webaddress.com... IPADDRESSHERE
    Connecting to webaddress.com|IPADDRESSHERE|:80... connected.
    HTTP request sent, awaiting response... 401 Authorization Required
    Reusing existing connection to webaddress.com:80.
    HTTP request sent, awaiting response... 401 Authorization Required
    Authorization failed.
    Warning message:
    In download.file(address, ""Testing.txt"", method = ""wget"") :
      download had nonzero exit status

",en
1106801,2011-02-11 08:04:10,statistics,weighting with r,fjabk,[deleted],,https://www.reddit.com/r/statistics/comments/fjabk/weighting_with_r/,1.0,0.0,:),en
1106802,2011-02-11 23:23:48,artificial,Building Watson - A Brief Overview of the DeepQA Project,fjpdk,aaronmyster,1271955996.0,https://www.reddit.com/r/artificial/comments/fjpdk/building_watson_a_brief_overview_of_the_deepqa/,13.0,1.0,,en
1106803,2011-02-12 00:17:09,statistics,Ensuring the Data-Rich Future of the Social Sciences [pdf],fjqg8,thegabeman,1239690590.0,https://www.reddit.com/r/statistics/comments/fjqg8/ensuring_the_datarich_future_of_the_social/,9.0,1.0,,en
1106804,2011-02-12 01:11:31,datasets,The Million Song Dataset (freely-available collection of audio features and metadata for a million contemporary popular music tracks),fjrgx,las3rjock,1186221020.0,https://www.reddit.com/r/datasets/comments/fjrgx/the_million_song_dataset_freelyavailable/,26.0,0.0,,en
1106805,2011-02-12 17:19:22,MachineLearning,Can Machine Learning be Secure?[PDF],fk2ts,personanongrata,1200898594.0,https://www.reddit.com/r/MachineLearning/comments/fk2ts/can_machine_learning_be_securepdf/,11.0,0.0,,en
1106806,2011-02-12 18:35:50,statistics,Where to publish your next (stat) paper,fk3qp,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/fk3qp/where_to_publish_your_next_stat_paper/,6.0,3.0,,en
1106807,2011-02-12 18:36:24,MachineLearning,Where to publish your next (ML) paper,fk3qy,[deleted],,https://www.reddit.com/r/MachineLearning/comments/fk3qy/where_to_publish_your_next_ml_paper/,0.0,2.0,,en
1106808,2011-02-13 01:39:49,MachineLearning,biologist looking for advice on pros and cons of bayesian networks,fka75,giror,1269318363.0,https://www.reddit.com/r/MachineLearning/comments/fka75/biologist_looking_for_advice_on_pros_and_cons_of/,21.0,20.0,"I am wondering about the pros and cons of learning bayesian networks. In the past ten years they have come up over and over in the computational biology literature, but I keep hearing that they are more over-fitted than useful. 


I am asking because I am wondering if using bayesian networks is worth it in my situation:

I have 2000 samples each with a 1000 discrete features that I want to use to predict 1 continuous variable. The hope is that I could build up a bayesian network that would characterize a small portion of my 1000 features as important and to identify logic gates between the important features. 

I thought about doing this as an alternative to doing regressions, because the dimensionality of the models limits me to testing interactions of two features at a time and prohibits me from finding higher order interactions. 

What I am not sure of is: 

(1) do people currently learn the networks using MCMC or GA or is there something better?

(2) what are the usual criticisms of Bayes Nets?

(3) I think I should test the significance of the bayes net using the likelihood ratio test with the naive bayes as the null hypothesis, but should this be done for different parts of the network or only for the whole thing?

thanks in advance for any feedback.

**edit: **

**thanks for the feedback everyone, especially the backwards moose, esoom! **

**I have some more things to mull over and will likely be back after I have tried several of the things that were suggested.
**",en
1106809,2011-02-13 09:37:57,MachineLearning,"Screw valentines day, I'm looking forward to the big showdown of IBM's Watson against jeopardy champions (Feb 14-16). This could be a milestone on par with Deep Blue defeating Gary Kasparov!",fkgkz,recalcitrantid,1238114113.0,https://www.reddit.com/r/MachineLearning/comments/fkgkz/screw_valentines_day_im_looking_forward_to_the/,6.0,6.0,,en
1106810,2011-02-13 10:31:40,MachineLearning,"Dumped on by data scientists: Neil Saunder's discusses pros and cons of the term ""data scientist""",fkh2z,[deleted],,https://www.reddit.com/r/MachineLearning/comments/fkh2z/dumped_on_by_data_scientists_neil_saunders/,1.0,0.0,,en
1106811,2011-02-13 17:34:57,statistics,What do you guys do for a living?,fkkmk,mcdougan,1201712621.0,https://www.reddit.com/r/statistics/comments/fkkmk/what_do_you_guys_do_for_a_living/,14.0,30.0,"For those of you who use stats on a regular basis for your career, what is your day like?",en
1106812,2011-02-13 20:07:21,statistics,I'm wondering if there are any good introductory statistics books from the perspective of numerical analysis,fkmt6,Ulvund,1177090152.0,https://www.reddit.com/r/statistics/comments/fkmt6/im_wondering_if_there_are_any_good_introductory/,6.0,1.0,,en
1106813,2011-02-13 21:43:37,statistics,I just won/lost AMAZINGLY at a drinking game! Can anybody do the math for me? (xpost from r/beer),fkoe1,EmmCeeAdams,1285071896.0,https://www.reddit.com/r/statistics/comments/fkoe1/i_just_wonlost_amazingly_at_a_drinking_game_can/,0.0,4.0,"I've played two rounds of a drinking game called ""Kings and Bloods"" ( here are the rules http://www.webtender.com/handbook/games/kingsandblood.game) and I would like to know the statistics on this... I pulled two of four kings the first round and ALL FOUR KINGS the second. What is the probability of this happening?",en
1106814,2011-02-13 23:20:01,statistics,Short question about comparing means.,fkq18,BBQ_SPARE_RIB_MOON,1296010319.0,https://www.reddit.com/r/statistics/comments/fkq18/short_question_about_comparing_means/,1.0,20.0,"I am comparing the mean age of two populations by an Ind. Samples T-Test. However, I have the data for one of the population and only the mean for the second. Can I still do a comparison? Can I assume that the populations have the same distribution? If so how would I got about this in SPSS?
",en
1106815,2011-02-14 06:03:11,datasets,Free wireless / GPS data (among others) for research or experiment use,fkwqu,ejel,1289355471.0,https://www.reddit.com/r/datasets/comments/fkwqu/free_wireless_gps_data_among_others_for_research/,9.0,0.0,,en
1106816,2011-02-14 08:08:17,MachineLearning,The discovery of structural form,fkyqn,danger_t,1249412021.0,https://www.reddit.com/r/MachineLearning/comments/fkyqn/the_discovery_of_structural_form/,9.0,2.0,,en
1106817,2011-02-14 15:38:43,MachineLearning,GPU-accelerated LIBSVM,fl47u,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/fl47u/gpuaccelerated_libsvm/,20.0,2.0,,en
1106818,2011-02-14 17:13:24,statistics,March Madness Prediction Contest: Data!,fl5fz,wtf_ftw,1240271200.0,https://www.reddit.com/r/statistics/comments/fl5fz/march_madness_prediction_contest_data/,5.0,0.0,,en
1106819,2011-02-14 19:34:51,MachineLearning,Statisfactions: The Sounds of Data and Whimsy,fl84j,[deleted],,https://www.reddit.com/r/MachineLearning/comments/fl84j/statisfactions_the_sounds_of_data_and_whimsy/,0.0,0.0,,en
1106820,2011-02-14 21:09:11,statistics,In need of a good text on Statistical Inference!!!,flacj,dschmerl,1258438778.0,https://www.reddit.com/r/statistics/comments/flacj/in_need_of_a_good_text_on_statistical_inference/,7.0,11.0,"Does anyone know of a helpful/understandable book on statistical inference (sufficiency, data reduction, convergence, etc)? I'm in a class on it now and my professor is terrible, so I want to try and read up on it and figure it out on my own.  If anyone has any suggestions, that would be great.  Thanks so much!",en
1106821,2011-02-15 01:07:51,statistics,Anyone have any good links for learning Bayesian statistics that do NOT assume a working knowledge of calculus?,flfux,jjrs,1170375384.0,https://www.reddit.com/r/statistics/comments/flfux/anyone_have_any_good_links_for_learning_bayesian/,10.0,20.0,"Liberal arts major doing a course in Bayesian computing with WINBUGS and getting my ass kicked. Understand the basic concepts, but now need to learn stuff like calculating posterior probabilities and learn difference between gamma and beta distributions. All the texts I've seen just cheerfully show the math as if its all self-evident, and seem to assume knowledge of beta distributions (what are values a and b? how and why do they interact that way?)

Anyone have any advice? I understand frequentist statistics reasonably well, so I think I can do it given the right guide. But there doesn't seem to be anything out there for genuine non-math-major beginners.  ",en
1106822,2011-02-15 04:37:32,datasets,"HealthData.gov launches Feb 15 -- a consolidated inventory of all federal data sets with over 2,000 national/state/county health metrics. Salivate, health care data junkies.",flk42,CitizenJosh,1204742087.0,https://www.reddit.com/r/datasets/comments/flk42/healthdatagov_launches_feb_15_a_consolidated/,22.0,3.0,,en
1106823,2011-02-15 09:54:43,MachineLearning,The process behind Watson's face and voice,flphc,ENOTTY,1238700510.0,https://www.reddit.com/r/MachineLearning/comments/flphc/the_process_behind_watsons_face_and_voice/,12.0,0.0,,en
1106824,2011-02-15 13:27:41,MachineLearning,Human vs Machine IBM Challenge Day 1,flrpp,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/flrpp/human_vs_machine_ibm_challenge_day_1/,13.0,4.0,,en
1106825,2011-02-15 14:47:10,MachineLearning,Gaussian Processes (regression case) outputs one dimensional data only?,flslu,[deleted],,https://www.reddit.com/r/MachineLearning/comments/flslu/gaussian_processes_regression_case_outputs_one/,0.0,1.0,Anyone knows about this?,en
1106826,2011-02-16 00:00:20,statistics,Any insight on quasi-newton (BFGS) methods for logistic regression?,fm3xt,BirthDeath,1295140816.0,https://www.reddit.com/r/statistics/comments/fm3xt/any_insight_on_quasinewton_bfgs_methods_for/,4.0,17.0,"I am trying to solve a logistic regression problem using a quasi-newton method (i.e. approximating the hessian matrix using the score function) but I cannot find a good reference or even the exact construction of the approximated matrix.

I already solved the problem using a standard Newton-Raphson algorithm, so I have the score function, which I know is correct. I think I am supposed to implement the BFGS algorithm , but I am not sure how to account for stepsize or how to perform a line-search.",en
1106827,2011-02-16 19:17:56,MachineLearning,IBM Watson's wagering strategies ,fmku4,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/fmku4/ibm_watsons_wagering_strategies/,29.0,8.0,,en
1106828,2011-02-17 06:40:39,statistics,Men know where the clitoris is better than women?,fmz7n,msalstrom,1296883784.0,https://www.reddit.com/r/statistics/comments/fmz7n/men_know_where_the_clitoris_is_better_than_women/,0.0,13.0,,en
1106829,2011-02-17 08:15:15,statistics,"Geotagging Visualization of Philly - light on the data, but a great visualization nonetheless",fn11p,[deleted],,https://www.reddit.com/r/statistics/comments/fn11p/geotagging_visualization_of_philly_light_on_the/,2.0,1.0,,en
1106830,2011-02-17 11:56:11,datasets,Google opens up Public Data Explorer to anyone,fn3x2,cavedave,1128052800.0,https://www.reddit.com/r/datasets/comments/fn3x2/google_opens_up_public_data_explorer_to_anyone/,16.0,0.0,,en
1106831,2011-02-17 12:30:54,MachineLearning,Is Google's Public Data Explorer the first step toward a universal data format?,fn4bc,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/fn4bc/is_googles_public_data_explorer_the_first_step/,12.0,0.0,,en
1106832,2011-02-17 15:02:54,statistics,"Icelandic online petition about Icesave, to be delivered to the President after Gallup has verified it. ",fn627,[deleted],,https://www.reddit.com/r/statistics/comments/fn627/icelandic_online_petition_about_icesave_to_be/,0.0,1.0,"Ok, it works like this in Iceland, everybody has a Kennitala (like a social security number but these are not private, anybody can find out anothers persons Kennitala via a national registry)

The president of Iceland is about to get around 37.000 signatures from an online petition to not sign the law (which parliament has already  passed) about Icesave (not important for this discussion, but here if you want to know http://en.wikipedia.org/wiki/Icesave_dispute).

edit: voting population of Iceland is probably around 220.000

So basically anybody can go in and enter any kennitala and that will be registered as a signature agains the law.  Before they hand it to him they will give it to Gallup in Iceland to verify the signatures.

How is this done ? How do they verify or check the signatures if there is no means to know who entered the ""kennitala"" in the online petition?

Do they make phonecalls to a sample ?  How many checks would they have to make to make it statistically significant ? 

tl;dr How do does one go about verifying an online petition - the signatures on the petition are hidden.",en
1106833,2011-02-17 16:18:59,statistics,How OkCupid Demystifies Dating With Big Data,fn7ba,orangepotion,1246804916.0,https://www.reddit.com/r/statistics/comments/fn7ba/how_okcupid_demystifies_dating_with_big_data/,8.0,1.0,,en
1106834,2011-02-17 18:31:51,statistics,I have a model building problem.  Infer latent states according to probability profiles,fna2k,quaternion,1252017693.0,https://www.reddit.com/r/statistics/comments/fna2k/i_have_a_model_building_problem_infer_latent/,2.0,11.0,"I have a group of subjects performing a simple task: they have to say whether the current stimulus matches the one seen three trials previously.  We predict that subjects perform this task by using an internal counting scheme - they essentially give each stimulus a ""rank order"" value that is either 1, 2 or 3 (so the 4th trial gets a value of ""1"").  They then use that to retrieve the last item with the same count, which allows them to match items over time.

I want to infer what the internal value is of that ""count"" based on behavior.

A simple approach is to just assume perfect counting - that is, the first, fourth, seventh, etc trials get a value of 1, the second, fifth, eighth etc trials get a value of 2, and the third, sixth, ninth, etc trials get a value of 3.  The problem here is that subjects will occasionally forget to ""reset"" the count to 1 after a trial of rank order 3, and I really want to be able to catch that.

Thus, I would like to use a more sophisticated approach: I want to estimate the internal count value.  I have a few phenomena that might help (these predictions are based on the way numbers are represented in the brain):

A) When the internal count is ""3,"" subjects should be more likely to make mistakes when the current stimulus matches the one presented 1 or 4 trials previously.

B) When the internal count is ""2,"" subjects should be more likely to make mistakes when the current stimulus matches the one presented 2 or 5 trials previously

C) When the internal count should be ""1,"" if subjects fail to detect a match between the current stimulus and the one presented 3 trials previously, they have probably forgotten to reset the count.

If I can produce a model that estimates the internal count value in some principled fashion, then I will test that model by examining how neural activity covaries with the estimated count value (we predict a very particular region of the brain to correlate with a particular pattern to these count values) and with ""resets"" of the count value (again, we predict a very different part of the brain will be correlated with resets).  I already have this data; I just need to build the model!

Any suggestions on how to build the model for inferring the latent ""count"" values?  The only approach I've come up with is to start with the simple scheme (assuming perfect counting) and then iteratively change count values until phenomena A-C are maximized, but I worry about overfitting in this case (the end result may look nothing like a count).",en
1106835,2011-02-18 03:58:14,MachineLearning,"DataMarket.com Launches with 100 Million Open 
Data Time Series [crosspost from /r/opendata]",fnlt1,lars_,1197145867.0,https://www.reddit.com/r/MachineLearning/comments/fnlt1/datamarketcom_launches_with_100_million_open_data/,22.0,1.0,,en
1106836,2011-02-18 04:36:00,MachineLearning,Celebrating Watson as an innovation,fnmhn,[deleted],,https://www.reddit.com/r/MachineLearning/comments/fnmhn/celebrating_watson_as_an_innovation/,1.0,1.0,,en
1106837,2011-02-18 06:21:43,datasets,"Has anyone put the Jeopardy! questions into a structured dataset, available for download?",fnofj,Quintote,1269890504.0,https://www.reddit.com/r/datasets/comments/fnofj/has_anyone_put_the_jeopardy_questions_into_a/,20.0,11.0,"I'll parse it if it comes to it (all questions are available on [j-archive.com](http://www.j-archive.com/)), but thought I'd check to see if someone had already done the leg work.  

I'm of course inspired by IBM's Jeopardy appearance, but not enough to needlessly replicate work that someone's likely already done.  And, oh yes, I'm chronically lazy. :-)",en
1106838,2011-02-18 14:57:44,MachineLearning,Key themes on data from Strata Conference ,fnuyn,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/fnuyn/key_themes_on_data_from_strata_conference/,4.0,1.0,,en
1106839,2011-02-19 01:19:43,statistics,"In a project proposal, what would classify as forecasting problem in Economic Forecasting?",fo786,[deleted],,https://www.reddit.com/r/statistics/comments/fo786/in_a_project_proposal_what_would_classify_as/,1.0,0.0,If possible please provide examples.,en
1106840,2011-02-19 01:41:09,statistics,"Scott Lynch's ""Introduction to Applied Bayesian Statistics and Estimation for Social Scientists"" (Springer, 2007) as PDF from the author's website; see also: http://www.princeton.edu/~slynch/index_files/Page781.htm",fo7lj,fbahr,1275413362.0,https://www.reddit.com/r/statistics/comments/fo7lj/scott_lynchs_introduction_to_applied_bayesian/,28.0,2.0,,en
1106841,2011-02-20 23:27:14,statistics,"Is there a commonly accepted name for the ratio of false positives to false negatives? I could swear that I learned this at one point in school, but I can't find any authoritative reference to it.",fp5dl,mjk1093,1156005148.0,https://www.reddit.com/r/statistics/comments/fp5dl/is_there_a_commonly_accepted_name_for_the_ratio/,10.0,9.0,,en
1106842,2011-02-20 23:59:37,statistics,"Draft of a new book by Terence Tao, ""Topics in random matrix theory"", available for download",fp5wp,fbahr,1275413362.0,https://www.reddit.com/r/statistics/comments/fp5wp/draft_of_a_new_book_by_terence_tao_topics_in/,6.0,0.0,,en
1106843,2011-02-21 00:01:04,MachineLearning,"Draft of a new book by Terence Tao, ""Topics in random matrix theory"", available for download",fp5xe,fbahr,1275413362.0,https://www.reddit.com/r/MachineLearning/comments/fp5xe/draft_of_a_new_book_by_terence_tao_topics_in/,14.0,3.0,,en
1106844,2011-02-21 12:15:39,statistics,OpenBUGS scripting question,fph81,Phaedrus85,1292406495.0,https://www.reddit.com/r/statistics/comments/fph81/openbugs_scripting_question/,2.0,10.0,"I would like to save some of my simulation results (namely summary statistics for my parameters) to text files using scripts, but can't find any commands for it in the OpenBugs documentation. Did I miss something obvious? If it doesn't have that functionality, does anyone know a good resource for writing new scripts?",en
1106845,2011-02-21 17:42:24,statistics,Can someone recommend a good medical statistics book please?,fplho,spotted_dick,1261951746.0,https://www.reddit.com/r/statistics/comments/fplho/can_someone_recommend_a_good_medical_statistics/,2.0,3.0,"I don't really need to know how to do the calculations, but I need to learn how to interpret the medical literature in peer-reviewed journals. Most textbooks I have seen are so full of formulae, it just gives me a headache.",en
1106846,2011-02-21 19:16:54,MachineLearning,R versus Matlab in Mathematical Psychology,fpnh4,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/fpnh4/r_versus_matlab_in_mathematical_psychology/,6.0,2.0,,en
1106847,2011-02-22 05:30:52,statistics,"Using R for Introductory Statistics, hypergeometric distribution",fq0lq,christopherbare,1291844617.0,https://www.reddit.com/r/statistics/comments/fq0lq/using_r_for_introductory_statistics/,2.0,0.0,,en
1106848,2011-02-22 15:29:04,statistics,Advice: Consider getting an Graduate Certificate in Applied Statistics. Thoughts?,fq97d,chaoticneutral,1258587986.0,https://www.reddit.com/r/statistics/comments/fq97d/advice_consider_getting_an_graduate_certificate/,6.0,9.0,"I have my bachelors in Psychology and currently work in opinion research. I have a pretty good grasp of basic stats and research methodology and I am really good at SPSS (I'm sorry, I use to do some SAS but it is what my company uses). At my job, I'm currently straddling this awkward line between Quantitative and Qualitative research. I'd really like to just stick with the quantitative side of things, but I feel I do not have the formal training/credentials to be an authoritative source of information on the subject. This is where more schooling comes in to play.

Does anyone have any opinions on Graduate certificates? Outside what they teach me, are they respected enough to help me find a stats orientated job? Or are these graduate certificates equivalent to getting a piece of paper from a diploma mill?

Currently look at these programs which are offered online (Local colleges don't offer anything):

* [Stanford](http://scpd.stanford.edu/public/category/courseCategoryCertificateProfile.do?method=load&amp;certificateId=1209602#searchResults)

* [Texas A&amp;M](http://www.stat.tamu.edu/dist/content_link.php?page=Certificates)

* [Penn State](http://www.worldcampus.psu.edu/AppliedStatisticsCertificate.shtml)



Thanks in advance for any insights!!!",en
1106849,2011-02-22 22:40:40,statistics,Does a coefficient of determination (R^2) of .9998 correspond to a .5% error?,fqix5,jasond33r,1246983989.0,https://www.reddit.com/r/statistics/comments/fqix5/does_a_coefficient_of_determination_r2_of_9998/,8.0,11.0,"I'm really sorry to ask be asking you guys but I don't really know who else to ask.

I am creating a report that includes a linear regression of some data and I am being told that:

""As a rough rule of thumb, if you need an accuracy of about
0.5%, you need an R^2 of 0.9998; if a 1% error is good
enough, an R^2 of 0.997 will do; and if a 5% error is
acceptable, an R^2 of 0.97 will do.""

But from everything I read about coefficient of determination is that the value directly corresponds to the percentage of explained variation in the data set so an R^2 of .9998 would equal 99.98% explained variation.  Am I just not understanding something about the request of ""error"" and ""accuracy"" or am I right to be confused by their instructions?   I would really appreciate any clarification you could provide.

Edit: [Here](http://courses.chem.indiana.edu/c117/documents/C117S2011Experiment4AccuracyLab4pp.pd) is a link to the instructions themselves that contain the statement about R^2 value and accuracy.  The context is for establishing a calibration curve to test phosphate levels in water based on two parameters, light absorptivity and phosphate concentration.  This is a directly linear relationship if knowing that helps.  As for what they mean by accuracy and error in this context, your guess is as good as mine.  ",en
1106850,2011-02-23 01:06:31,datasets,"Open Call for Visualization Exhibition that references the following phenomena: stellar evolution, star structure, star properties, nuclear fusion, spectrum luminosity and/or chemical composition of a star.",fqm7o,kabr,1221124243.0,https://www.reddit.com/r/datasets/comments/fqm7o/open_call_for_visualization_exhibition_that/,8.0,0.0,,en
1106851,2011-02-23 08:14:31,statistics,Purpose of Moment Generating Functions,fquww,lastever,1283784880.0,https://www.reddit.com/r/statistics/comments/fquww/purpose_of_moment_generating_functions/,2.0,8.0,"I can transform my PDFs to moment generating functions when asked to on problems. But what is the purpose of moment generating functions. 

edit1: thank you for helping me understand MGFs.",en
1106852,2011-02-23 17:11:37,statistics,"I have a lot of experimental data, and I'm looking for some advice on analysing it.",fr2gv,Haiku_attack,1211866928.0,https://www.reddit.com/r/statistics/comments/fr2gv/i_have_a_lot_of_experimental_data_and_im_looking/,4.0,13.0," Hi,
 First off, I should mention that I'm fairly out of practice with my stats. I'm using PASW (SPSS) and I'm just about done exporting data from some experiments. 
 
 Basically, I have quite a few variables I want to examine and I was wondering if you any of juicy brains would give me some advice on it. I'm mostly trying to figure out the best way to collapse my data and input it. It's an eye-tracking study. 
 Here's the breakdown:
34 participants, each of whom view 20 slides of text in alphabets they are not familiar with. There are two possible formats for the text, spaced or unspaced, so they all see every slide but only in one format. 
 The data I have is as follows:

Nominal values:
Position of the character (I could use this as ordinal, but no need)
Type of character (one of three alphabets)

Scale:
Total look time for each character (one value per participant)

 I can collapse on the participant level, by taking an average of every characters looktime for each character. 
 However, I'm also interested in collapsing at the character level. 
I can also divide the characters into those with any positive looktime, and those with 0. This would give me a binary choice of ""looked at"" or ""not looked at"".

*Edit*

Here's a sample of my data. 
https://spreadsheets.google.com/ccc?key=0Aokc1xWuO9E1dDNDcWJiMEFiUkdDa0t1WGg1V3VXNVE&amp;hl=en&amp;authkey=CKTbmK4C#
 The participant data is on sheet 1, and the data about each lookzone is on sheet two. Essentially I'm looking for a good way to combine the two. 




",en
1106853,2011-02-23 20:49:03,analytics,"Hi, /r/analytics. I have a question about Google Analytics that no one on the forum seems able or willing to answer. Any help is most appreciated.",fr7o3,fake_again,1284398362.0,https://www.reddit.com/r/analytics/comments/fr7o3/hi_ranalytics_i_have_a_question_about_google/,4.0,3.0,,en
1106854,2011-02-23 21:38:40,statistics,Why are all p-values are equally likely if we sample from the null distribution?,fr8ve,rottenborough,1257706820.0,https://www.reddit.com/r/statistics/comments/fr8ve/why_are_all_pvalues_are_equally_likely_if_we/,0.0,1.0,"I feel like this should be trivial, but I just don't see it somehow. Is it not more likely to get the expected value of the null distribution as suppose to, say, the critical value under alpha=.05, if we sample from the null distribution?",en
1106855,2011-02-24 04:10:39,statistics,[HELP] I have a great idea but I need help with the math/stats/???,frhi8,epicerr,1247869589.0,https://www.reddit.com/r/statistics/comments/frhi8/help_i_have_a_great_idea_but_i_need_help_with_the/,0.0,5.0,"Hey all,

This is my first time posting a math/stats based question. Its been years since I have formally studied math or needed to done anything complicated. This concludes my disclaimer. Any help would be great!

I work in the legal industry. In my job I take people's medical records and misc. expenses from accidents and attempt to calculate what their claims are worth. I also need to be able to reasonably predict what insurance companies are going to think about our claim and what they will offer. 

There are two aspects to this that are interesting. 1) There are definitive values (Values are specific for each case, but will not change. E.g. Medical bills, lost wages, travel expense, medication expenses.) 2) What I call ""Subjective"" factors: Positives and negatives. 

Positive subjective factors add value to the claim but the value that they add can vary. E.g. Intensity of pain after the accident, Intensity of pain during treatment, speed of vehicle at impact, witnesses, statements made by at-fault driver, etc etc (close to 50 or 60)

Negative subjective factors reduce the value of the claim but the value that it reduces can vary. E.g. How long person waited to see doctors, number of missed appointments, lack of damage to vehicle, etc etc.

I have some old cases where I have all the information (definitive values as well as all the subjective factors) as well as what the insurance companies have done. I am hoping that I am ""reverse"" engineer a ""formula"".

Is this even feasible? 
Is this super complicated math?
Advice or ideas?


Any constructive advice would be greatly appreciated...",en
1106856,2011-02-24 06:14:02,datasets,Looking for stock market data (15 minute delayed),frk5y,RandyPonce,1213736107.0,https://www.reddit.com/r/datasets/comments/frk5y/looking_for_stock_market_data_15_minute_delayed/,11.0,8.0,"I'm looking for an inexpensive place to retrieve stock market data via an API. Data doesn't have to be real time, but 15 minute delayed would be nice. If this is impossible, how about just closing prices?",en
1106857,2011-02-24 08:31:30,statistics,"Advanced Data Analysis Subject with PDF Notes, R code, and more",frmqr,mycatharsis,1294046118.0,https://www.reddit.com/r/statistics/comments/frmqr/advanced_data_analysis_subject_with_pdf_notes_r/,30.0,3.0,,en
1106858,2011-02-24 11:52:39,MachineLearning,"How to build your own ""Watson Jr."" in your basement",frpjb,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/frpjb/how_to_build_your_own_watson_jr_in_your_basement/,36.0,0.0,,en
1106859,2011-02-24 20:58:16,MachineLearning,How To Get Experience Working With Large Datasets,frz7d,orangepotion,1246804916.0,https://www.reddit.com/r/MachineLearning/comments/frz7d/how_to_get_experience_working_with_large_datasets/,10.0,0.0,,en
1106860,2011-02-24 23:57:29,MachineLearning,Book review: 25 Recipes for Getting Started with R,fs3ip,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/fs3ip/book_review_25_recipes_for_getting_started_with_r/,0.0,0.0,,en
1106861,2011-02-25 12:14:38,MachineLearning,Model of attentional blink: A super neat perceptual phenomenon with a temporal consideration,fsgp7,WayNoWay,1288323043.0,https://www.reddit.com/r/MachineLearning/comments/fsgp7/model_of_attentional_blink_a_super_neat/,8.0,0.0,,en
1106862,2011-02-25 23:18:35,artificial,"How to build your own ""Watson Jr."" in your basement",fsswb,aaronmyster,1271955996.0,https://www.reddit.com/r/artificial/comments/fsswb/how_to_build_your_own_watson_jr_in_your_basement/,18.0,4.0,,en
1106863,2011-02-26 00:48:15,statistics,What's r/statistics opinion of Bayesian data analysis vs. Null-hypothesis significance testing?,fsup8,[deleted],,https://www.reddit.com/r/statistics/comments/fsup8/whats_rstatistics_opinion_of_bayesian_data/,15.0,64.0,"It just seems that everyone agrees that bayesian data analysis is mature enough to become the *way* to analyze data and forget all the defects that NHST has. However, most literature seems to keep using NHST. What's r/statistics opinion?",en
1106864,2011-02-26 23:24:54,datasets,Need help with IMDB dataset,ftcqr,DrRansom7469,1290987995.0,https://www.reddit.com/r/datasets/comments/ftcqr/need_help_with_imdb_dataset/,7.0,6.0,"I recently downloaded all the files from IMDB. Once I unzipped the files they are all a .list file extension. My end goal is to put together a large movie dataset in SPSS or Stata, but I don't know how to open these files from IMDB or how to convert them to a useful format.

Any suggestions for what to do? I've tried using Java Movie Database a little bit but haven't figured out how to get that to work.",en
1106865,2011-02-27 03:32:29,MachineLearning,"Sandia National Laboratory's open source machine learning library ""Cognitive Foundry""",ftgkt,b0b0b0b,1144868232.0,https://www.reddit.com/r/MachineLearning/comments/ftgkt/sandia_national_laboratorys_open_source_machine/,27.0,1.0,,en
1106866,2011-02-27 09:27:01,datasets,Publicly available twitter data?,ftlgu,pzauteenk,1279950802.0,https://www.reddit.com/r/datasets/comments/ftlgu/publicly_available_twitter_data/,8.0,14.0,"Anyone knows where I can get about a years worth(more is better) of twitter data. I want to mine the text in twitter at the user level and study it . 
Lotsa folks have told me that the API is the best way to get around this... but I dont know how to write an API call. 
Advice?",en
1106867,2011-02-27 22:03:55,MachineLearning,R 2.12 now has reference classes,ftuab,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/ftuab/r_212_now_has_reference_classes/,10.0,1.0,,en
1106868,2011-02-28 01:37:03,statistics,Statistics: A Remedy for Football Withdrawal,ftxyf,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/ftxyf/statistics_a_remedy_for_football_withdrawal/,0.0,0.0,,en
1106869,2011-02-28 04:38:08,statistics,"Simple experimental design, trouble with the statistical analysis. Can you guys lend me your advice?",fu11x,MacBelieve,1291955283.0,https://www.reddit.com/r/statistics/comments/fu11x/simple_experimental_design_trouble_with_the/,3.0,4.0,"I just finished collecting data and have an idea of what I want to test, but I'm having a hard time figuring out how to test for significance. 

I had each subject run through 4 different conditions in a random order. The response was a integer score in the range 2 to 28. Prior to the condition trials, they gave a guess for what their score would be for each. After the trials, they ranked the difficulty of each the 4 conditions.

I already ran the ANOVA for effect of condition on performance.

I want to next check to see if subjects' predictions were consistent with their performance, and to see if those who ranked specific conditions as easier predicted it beforehand. but I have been drawing a blank on how to test these. Does anyone have an idea?

Also, I'm curious what other tests I'm just outright missing. I feel like I'm very clearly forgetting something.",en
1106870,2011-02-28 06:52:47,statistics,"Hey r/statistics, I thought you might like to see R running on an iPhone (jailbroken, of course)",fu3gy,NinjasInTheWind,1294508333.0,https://www.reddit.com/r/statistics/comments/fu3gy/hey_rstatistics_i_thought_you_might_like_to_see_r/,0.0,3.0,"Here's a screenshot of the results from a simple regression. 

http://imgur.com/1abuK

And here's the load screen

http://imgur.com/FYziD

It's not perfect, and there is no graphical output. It's still cool in that I can perform some basic analyses on the fly.",en
1106871,2011-02-28 19:47:15,statistics,Search Engine Optimization: One Reason On-Line Reviews Stink,fufcj,jeffyablon,1283280097.0,https://www.reddit.com/r/statistics/comments/fufcj/search_engine_optimization_one_reason_online/,2.0,1.0,,en
1106872,2011-02-28 23:27:37,MachineLearning,(x-post from /r/math) Good introductory text on Markov models/chains?,fukk3,[deleted],,https://www.reddit.com/r/MachineLearning/comments/fukk3/xpost_from_rmath_good_introductory_text_on_markov/,4.0,4.0,"I'm an undergrad about to get my B.S. in math, and I don't know much about Markov models. I just started doing image processing research, and they play a large part in that area, so naturally I'd like to know more. Can anyone help me with finding a good text on the subject that approaches from a more applied/algorithmic perspective? Thanks /r/math.",en
1106873,2011-03-01 00:24:50,MachineLearning,New algorithm for computer aided diagnosis of medical images,fuly6,allusionist,1294872860.0,https://www.reddit.com/r/MachineLearning/comments/fuly6/new_algorithm_for_computer_aided_diagnosis_of/,9.0,13.0,"Dr. Balis and his colleagues have produced a new Spatially-Invariant Vector Quantization (SIVQ) algorithm for locating similar areas within a digital image. This has huge implications in medicine and specifically pathology since diagnosing physician pathologists review human biopsy tissue on glass slides using a microscope, and a single tissue slide when scanned at 400x may result in an image up to 30 Gigapixels in size. A single physician pathologist after 3 to 6 years of specialized training usually reviews 50 to 300 glass slides with tissue per day from different patients to diagnose cancer and other diseases. See the images in the paper: [Hipp JD, Cheng JY, Toner M, Tompkins RG, Balis UJ. Spatially Invariant Vector Quantization: A pattern matching algorithm for multiple classes of image subject matter including pathology. J Pathol Inform 2011;2:13](http://www.jpathinformatics.org/article.asp?issn=2153-3539;year=2011;volume=2;issue=1;spage=13;epage=13;aulast=Hipp)",en
1106874,2011-03-01 01:37:51,statistics,Ask r/statistics: I need advice from those who have been drilled on statistics during their interview process,funlu,aguyfromucdavis,1279525151.0,https://www.reddit.com/r/statistics/comments/funlu/ask_rstatistics_i_need_advice_from_those_who_have/,10.0,9.0,"I've been contacted by a college recruiter from Microsoft and they would like to offer me a phone interview. I've been doing some research and most people seem to get barraged by interviewers with questions regarding algorithm design and coding techniques. I don't expect to be treated the same way simply because I'm not shooting for any positions in software engineering. If you've gone through a similar experience, what kind of drilling questions can I expect for someone who's looking to become a statistician? Thanks for any help/advice!",en
1106875,2011-03-01 03:48:18,statistics,"RStudio, new open-source IDE for R",fuqad,[deleted],,https://www.reddit.com/r/statistics/comments/fuqad/rstudio_new_opensource_ide_for_r/,0.0,0.0,,en
1106876,2011-03-01 07:32:03,MachineLearning,Machinelearning masters?,fuuru,stonedfox8,1297009710.0,https://www.reddit.com/r/MachineLearning/comments/fuuru/machinelearning_masters/,14.0,34.0,"Is it worth to spend 2 years in uni to get a masters of computer science where machine learning will just be one of the subjects?
Is it better instead to do some self study?
What is the current job market like for machine learning?",en
1106877,2011-03-01 16:04:33,statistics,R hates me: How to install packages,fv1kd,fatswaller,1297298774.0,https://www.reddit.com/r/statistics/comments/fv1kd/r_hates_me_how_to_install_packages/,5.0,16.0,"Sorry for the elementary question, but I'm about to throw my computer through the window. 

I want to run a mixed-model/random-effects model program, and I downloaded the lmer4 package from CRAN. This is the first package I've downloaded. I tried to run the lmer(....) function but R still can't find it. Am I missing something here? Can someone help me out?",en
1106878,2011-03-01 17:45:12,statistics,Ask r/statistics: Where to start for a beginner.,fv3ho,sdfx,1286514514.0,https://www.reddit.com/r/statistics/comments/fv3ho/ask_rstatistics_where_to_start_for_a_beginner/,10.0,22.0,"I'm a newbie with not a lot of probability and statistics experience, although I am equipped with basic mathematical tools (functions, calculus, algebra).

I was wondering if you guys knew of any excellent resources to speed me through the fundamentals of probability, as I will be taking a regression course soon and I want to have the fundamentals down. I would prefer video resources but books are not out of the question. I've already found khanacademy.org, but do you guys know of any other resources that have a nice flow to the material. 

Thanks. ",en
1106879,2011-03-01 17:53:49,computervision,automatic go board recognizer,fv3p5,ludflu,1182284577.0,https://www.reddit.com/r/computervision/comments/fv3p5/automatic_go_board_recognizer/,3.0,6.0,"For a long time, I've wanted to write a program to generate game records (SGF) for the game of Go.  After reading Bradski &amp; Kaehler's OpenCV book, I feel like I'm closer than ever.

Here's the most recent output of my program:

https://github.com/ludflu/kifu/blob/master/goodsqares.jpg

Source: https://github.com/ludflu/kifu/blob/master/goboard.cpp

I'd love some suggestions on how this could be improved upon, and also input on the next steps to take - detecting stones as they are placed on the board.

To find all the points in a completely automated fashion, here's my recipe:

1. find all the closed polygons with 4 sides. (this step is taken from the OpenCV example code)
2. discard all polygons whose area is more or less than 2 std deviations from the median area
3. Compute the convex hull of the set of points of the remaining quadrilaterals
4. find the four corners by discarding points in the convex hull that form a straight line. (this step needs some cleaning up)
5. compute a homography matrix (H) using the four corner points that correspond to the four corners of an ideal go board.
6. use H to transform all the vertices of the ideal go board into the image coordinates
7. Profit!",en
1106880,2011-03-01 22:15:53,MachineLearning,ICDAR 2011 - Arabic Writer Identification Competition,fv9rf,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/fv9rf/icdar_2011_arabic_writer_identification/,0.0,0.0,,en
1106881,2011-03-01 23:56:01,MachineLearning,First Steps at Building a Classifier with Mahout,fvc36,mynameistaken,1250943921.0,https://www.reddit.com/r/MachineLearning/comments/fvc36/first_steps_at_building_a_classifier_with_mahout/,5.0,2.0,,en
1106882,2011-03-02 02:41:40,artificial,The creator of the Creatures artificial life game series is looking for your help for a new game,fvfl7,nillion42,1221263002.0,https://www.reddit.com/r/artificial/comments/fvfl7/the_creator_of_the_creatures_artificial_life_game/,29.0,2.0,,en
1106883,2011-03-02 16:27:22,datasets,"Complete snapshot of all listings, bids, user profiles, groups and loans ever created on Prosper",fvsiy,reidhoch,1209656850.0,https://www.reddit.com/r/datasets/comments/fvsiy/complete_snapshot_of_all_listings_bids_user/,10.0,0.0,,en
1106884,2011-03-02 22:37:14,MachineLearning,scikit-learn 0.7 machine learning lib for python / numpy is out: what's new?,fw17b,ogrisel,1171218481.0,https://www.reddit.com/r/MachineLearning/comments/fw17b/scikitlearn_07_machine_learning_lib_for_python/,17.0,1.0,,en
1106885,2011-03-03 04:19:10,statistics,Andrew Gelman's thoughts on RStudio,fw8dx,mycatharsis,1294046118.0,https://www.reddit.com/r/statistics/comments/fw8dx/andrew_gelmans_thoughts_on_rstudio/,27.0,8.0,,en
1106886,2011-03-03 15:47:56,statistics,Can someone explain this definition?,fwixu,[deleted],,https://www.reddit.com/r/statistics/comments/fwixu/can_someone_explain_this_definition/,2.0,4.0,,en
1106887,2011-03-03 17:20:12,statistics,filtering time series data in R,fwkw7,ultraspeedz,,https://www.reddit.com/r/statistics/comments/fwkw7/filtering_time_series_data_in_r/,6.0,8.0,"Any ideas on how/packages to filter time series data in R, i.e. i have strings for each observation and I need to filter out some observations that are erroneous. 

Any ideas/input/packages/code?
",en
1106888,2011-03-03 19:58:10,MachineLearning,A news reader that learns what you like,fwonm,rhnet,1299174902.0,https://www.reddit.com/r/MachineLearning/comments/fwonm/a_news_reader_that_learns_what_you_like/,3.0,2.0,,en
1106889,2011-03-03 23:08:02,statistics,LGM vs Random effects regression,fwt6d,Case_Control,1263273609.0,https://www.reddit.com/r/statistics/comments/fwt6d/lgm_vs_random_effects_regression/,6.0,2.0,"I've been trying to find some reading on this subject, and figured this would be a good place to ask around. Is there any difference between binary latent growth model (of the SEM variety) and a random effects logistic regression? I remember seeing something at one point that an LGM was equivalent to a random effects model, but I can't seem to find the source and I'm not sure if it holds when you are talking about non linear models. Any thoughts reddit? ",en
1106890,2011-03-04 02:34:47,statistics,Introductory Video on Creating R Packages - Melbourne R Users,fwxgc,mycatharsis,1294046118.0,https://www.reddit.com/r/statistics/comments/fwxgc/introductory_video_on_creating_r_packages/,10.0,0.0,,en
1106891,2011-03-04 11:46:20,MachineLearning,On the emerging Open-Source Analytics Stack,fx6kr,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/fx6kr/on_the_emerging_opensource_analytics_stack/,11.0,1.0,,en
1106892,2011-03-04 21:39:43,MachineLearning,Ask MachineLearning: Help with recommendation optimization?,fxgst,[deleted],,https://www.reddit.com/r/MachineLearning/comments/fxgst/ask_machinelearning_help_with_recommendation/,2.0,9.0,"I've been working on a web-app (written in PHP) for awhile that basically lets you rate articles you have read from 1-5 stars, and it then gives you recommendations based on those rankings.

So basically, I have a database of articles that look something like this:

          Title     |     Submitter     |     Categories      |
    The Son of Blah |    Mike Elantra   | Science, Technology |
    Getting back t..|   Jacob Delilah   | Science, Programming|
    Stories of Bef..|    Daniel Ellas   | History, English    |
    Friends of Fam..|    Mike Elantra   | Science, English    |

And after you rate a few articles, it looks through all the other articles in the database and looks for how many times you highly rated an article by a certain submitter, or a certain category. So if you 5 starred ""The Son of Blah"" above, it would most likely recommend ""Friends of Fam..."" because the submitter was the same, and it shares 1/2 of the categories of the first article (they both share ""science""). The more you 5 star articles, the more weight it gives to certain categories (so if you five starred the 1st, 2nd, and 4th articles, it would weight ""science"" more highly than other categories, or if you 5 starred the 3rd and 4th articles, it would weight English more highly, etc)

The algorithm seems to work pretty well, but it is ridiculously slow. For 400 articles it took around 20 seconds for the page to load, and for &gt;1000 articles, it took over 40 seconds to load the page, and since I wanted my website to be used by more than one person (and with more than 1000 articles!!), I can just imagine that it will be unusable if 10, 100, or even 1000 people try to use it at once.

The problem, I believe, resides in the fact that it calls the recommendation algorithm every time you visit the recommendation page. I do this because if a user rates a few more articles, I want them to see changes in their recommendations, so  I need to rerun the recommendation algorithm again against all articles to see what the new rankings are. I figure there has to be a better way than to re-rank against every single article, but I can't think of a good way to do so. Is there something I'm not seeing that could help make my page load time go way down?
",en
1106893,2011-03-05 21:21:50,MachineLearning,"doSMP, Parallel processing in R for Windows is finally GPL...",fy0sc,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/fy0sc/dosmp_parallel_processing_in_r_for_windows_is/,6.0,0.0,,en
1106894,2011-03-06 05:35:27,statistics,Ask r/statistics: Robust vs classical statistics,fy87k,[deleted],,https://www.reddit.com/r/statistics/comments/fy87k/ask_rstatistics_robust_vs_classical_statistics/,5.0,5.0,"I have a situation where I'm analyzing some data (results from several labs on the same sample).  I've been told I need to use the robust method for analyzing the data, probably because it's more resistant to outliers.  This is useful for when a particular lab does something wrong and gives a really awful result.

For one of the tests however, the data I received is very homogeneous.  The standard deviation is very small.  When I applied the robust method, the robust standard deviation becomes zero, and the standard uncertainty is also zero.  I therefor cannot perform z-scores etc when the standard deviation is zero (divide by zero errors).

So, my question is, is there a black and white definition of when robust statistics CANNOT be used?  If all my data is so homogeneous, it seems like i should be able to use the classical method of analysis (as my understanding is the sole purpose of using the robust method is that it's resistant to outliers).

**tl;dr**: When is it not okay to use robust statistics?  What if my robust standard deviation turns out to be zero?",en
1106895,2011-03-06 07:42:37,datasets,High resolution pictures of the earth - GeoTIFF/PNG + Torrents (9 and 7GB),fy9v5,voltagex,1230861112.0,https://www.reddit.com/r/datasets/comments/fy9v5/high_resolution_pictures_of_the_earth_geotiffpng/,57.0,2.0,,en
1106896,2011-03-06 13:17:30,datasets,"ScraperWiki, collection of scrapers. Great source of data",fyczy,voltagex,1230861112.0,https://www.reddit.com/r/datasets/comments/fyczy/scraperwiki_collection_of_scrapers_great_source/,15.0,1.0,,en
1106897,2011-03-06 19:49:21,statistics,Ask /r/statistics: What software do you use at work or for your research?,fyh6i,cbrunos,,https://www.reddit.com/r/statistics/comments/fyh6i/ask_rstatistics_what_software_do_you_use_at_work/,6.0,40.0,"Hello,
I am currently a student in statistics and econometrics and I learned to use RATS, Eviews and R. I also learned to use gretl on my own, and will need to know Matlab/Octave for my intership this summer. What software do you use and why? I intend to do a PhD and want to use FLOSS exclusively but only if it is at pairs with proprietary software, and if possible only use one, maximum two different softwares. For now the gretl/R combo gives me all I need, but would I need other software in «real life»?

Precisions: I work on Macroeconomic models.

Thanks!

EDIT: Thanks for all the answers, they are really interesting. I think I'll take a look at python, scipy and numpy and will continue to use R and gretl also.",en
1106898,2011-03-06 22:27:07,datasets,List of all(?) Public Elementary/Secondary Schools in the US,fyk0i,[deleted],,https://www.reddit.com/r/datasets/comments/fyk0i/list_of_all_public_elementarysecondary_schools_in/,1.0,0.0,,en
1106899,2011-03-07 09:18:20,artificial,"Automaton, Know Thyself: [With 2 Brains] Robots Become Self-Aware",fyvfm,kamoylan,1254047922.0,https://www.reddit.com/r/artificial/comments/fyvfm/automaton_know_thyself_with_2_brains_robots/,25.0,3.0,,en
1106900,2011-03-07 19:04:35,statistics,"""Think Stats: Probability and Statistics for Programmers"", free e-book &amp; course materials by Allen B. Downey",fz488,fbahr,1275413362.0,https://www.reddit.com/r/statistics/comments/fz488/think_stats_probability_and_statistics_for/,16.0,0.0,,en
1106901,2011-03-08 03:47:21,statistics,is this autoregressive?,fzfp3,FailedLifeForm,1299165736.0,https://www.reddit.com/r/statistics/comments/fzfp3/is_this_autoregressive/,3.0,12.0,"hi    
i have an experiment where at time t, i feed a machine x, and it generates y. e.g. at time 0, i enter x0, and the machine produces y0; at time 1, i enter x1, and the machine produces y1, ...    
i suspect that y is a linear function of 5-6 previous x's. How do it fit the data so that i can learn what the weights of the previous x's are? Thanks.",en
1106902,2011-03-08 03:54:35,artificial,Software is Replacing Armies of Expensive Lawyers [cue lawyer jokes here...],fzfvb,kamoylan,1254047922.0,https://www.reddit.com/r/artificial/comments/fzfvb/software_is_replacing_armies_of_expensive_lawyers/,14.0,2.0,,en
1106903,2011-03-08 04:27:05,statistics,Multiple Regression resources? ,fzgj7,StarMouse,1280117776.0,https://www.reddit.com/r/statistics/comments/fzgj7/multiple_regression_resources/,2.0,4.0,"Heya /r/statistics, 

I have a final tomorrow in multiple regression. It's entirely open book open notes and mostly conceptual/spss output analysis. I have a good handle on the material, but do you guys have any favorite Internet resources for regression analysis? Extra perspectives/explanations are always helpful to have. I'm familiar with UCLA's SPSS annotations,  but not much else. 

Topics covered include (just a list off the top of my head) : coliniarity/multicoliniarity/tolerance, comparing ANOVA to regression, interactions, data transformations, screening data, dummy coding, interpreting spss regression output (especially in non-stats language),  and general regression equations/formulas/assumptions etc. ",en
1106904,2011-03-08 19:02:08,MachineLearning,"ML-Reddit, how can I determine if using a classifier for a more specific class is beneficial over just using a more generic prediction model?",fzuin,saprian,1175999066.0,https://www.reddit.com/r/MachineLearning/comments/fzuin/mlreddit_how_can_i_determine_if_using_a/,6.0,7.0,"ML-Reddit, I’ve a problem I can’t seem to figure out. Maybe it’s really easy and I’m just missing something. Either way I’d appreciate any pointers anyone can give me. 

Suppose you have a classifier that tells with an Area-under-ROC curve of 0.7 if somebody will develop cancer (any kind of cancer). Now I have a second classifier that tells with an AUC of 0.75 if somebody will develop lung cancer (more specific kind of cancer). The classes are obviously related (one more specific than the other). However, the base-rate of the lung cancer is only a third of what the rate of cancer is in the data set and I'm worried that the little bit of extra might not outweigh the raw chance of somebody just getting cancer. How do I test if my second, more specific classifier picks up on cases that the first one would have missed? To be more specific, how great is my benefit, if any, of using the second classifier in addition to the first one? How can I measure that gain in terms of some AUC-related measure or otherwise?",en
1106905,2011-03-08 19:22:09,artificial,Robots say the damnedest things,fzuzb,Raerth,1211742193.0,https://www.reddit.com/r/artificial/comments/fzuzb/robots_say_the_damnedest_things/,13.0,3.0,,en
1106906,2011-03-09 01:28:36,MachineLearning,March Madness: Structured Output Learning Formulation,g0319,danger_t,1249412021.0,https://www.reddit.com/r/MachineLearning/comments/g0319/march_madness_structured_output_learning/,3.0,0.0,,en
1106907,2011-03-09 03:00:28,statistics,Ask: Nonlinear Confidence Intervals (Re-parametrization),g04yg,sheppa28,1274041127.0,https://www.reddit.com/r/statistics/comments/g04yg/ask_nonlinear_confidence_intervals/,2.0,7.0,"Drawing a blank on how to figure this out. I have data that can modeled with the Beta Distribution with parameters (a,b). I can compute the MLE's for (a,b) along with their confidence intervals for any arbitrary alpha. 


However, the parametrization that is used in the model is (F,p) where a=(1-F)p/F and b=(1-F)(1-p)/F. So, F=1/(a+b+1) and p=a/(a+b).


Knowing the MLE and CI estimates of (a,b) how do I compute the MLE and CI estimates of (F,p)?  The non-linearity had me confused, as it may be correct that the MLE for F is Fhat=1/(ahat+bhat+1) (true?) but the CI's most likely won't follow the same format.

edit: corrected formula for b (F in denominator, not p)",en
1106908,2011-03-09 04:08:13,statistics,Cluster or Stratified? ,g068k,bluengreen7,,https://www.reddit.com/r/statistics/comments/g068k/cluster_or_stratified/,2.0,7.0,"Please bear with me.....I'm a statistics noob.....

Here is the question: A study of the effectiveness of echinacea involved upper respiratory tract infections. One group of infections was treated with echinacea and another group was treated with placebos.  The echinacea and placebo gropus were determined through a process of random assignment. 

Is this cluster because the groups were initially randomly selected? ",en
1106909,2011-03-09 06:28:19,statistics,sigmoidnet,g090s,FailedLifeForm,1299165736.0,https://www.reddit.com/r/statistics/comments/g090s/sigmoidnet/,1.0,0.0,"hi. can someone please explain the meaning of ""sigmoidnet"" as it appear in [MATLAB](http://www.mathworks.com/help/toolbox/ident/ref/nlarx.html) nlARX. What do the number of units mean?",en
1106910,2011-03-09 10:08:23,statistics,Anyone able to give me any suggestions on this stats problem?,g0ay6,[deleted],,https://www.reddit.com/r/statistics/comments/g0ay6/anyone_able_to_give_me_any_suggestions_on_this/,1.0,19.0,"Hey guys!  So basically I have two different groups of patients who both got two different sets of drugs.  I measured both groups at 1 week and at 4 weeks after a procedure to see how they responded.  I compared the same group at one week and four weeks using Ttests, but is there any statistical test to compare the different groups to each other?

Thanks for the suggestions!",en
1106911,2011-03-09 10:15:26,datasets,Time series data on UK household indebtedness?,g0b1u,agwblack,1267537717.0,https://www.reddit.com/r/datasets/comments/g0b1u/time_series_data_on_uk_household_indebtedness/,5.0,1.0,"I was hoping some knowledgable soul might be able to point me in the direction of a dataset containing time series data on household indebtedness, preferably with a breakdown by income groups. Similar data on other countries would also be useful. Thanks!",en
1106912,2011-03-09 16:51:03,MachineLearning,"$5,000 Challenge: Visualizing the US Federal Budget",g0g5t,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/g0g5t/5000_challenge_visualizing_the_us_federal_budget/,11.0,0.0,,en
1106913,2011-03-09 23:13:02,statistics,What to do with an undergraduate degree in math/stat?,g0p99,fuzonc,1296357930.0,https://www.reddit.com/r/statistics/comments/g0p99/what_to_do_with_an_undergraduate_degree_in/,8.0,19.0,"I'm about to graduate with a BS in math with a concentration in statistics. I hear that graduate school is pretty much the next step but I'm not sure I want to go yet. Is there anything I can do other than going to graduate school?

EDIT: Thanks to all the people that responded. I really appreciate the advice and I'm looking into most of your suggestions.",en
1106914,2011-03-10 00:28:46,statistics,Ridiculously specific SPSS problem.,g0qx4,Radjamaki,,https://www.reddit.com/r/statistics/comments/g0qx4/ridiculously_specific_spss_problem/,5.0,10.0,"I have been analysing data from a recent experiment I performed, and can't wrap my head around an odd problem I'm encountered.

Just to simplify things, I am attempting to examine the main effects of A, B and C on X.

If I carry out seperate 'univariate...' tests for each of the independent variables, none of the main effects are significant (p &gt; 0.05). On the other hand, if I carry out a single 'univariate...' test which includes A, B and C all at once, the main effects are shown as significant (p &lt; 0.05).

My understanding of this test, was that sticking in more than one fixed factor would just run the same test on each one individually, with additional interactions noted in the results. This doesn't seem to be the case, and I can't work out what has happened. Can any statistician help me out?

Apologies for the, perhaps, complicated wording.
",en
1106915,2011-03-10 00:55:04,artificial,Free e-book: Artificial Intelligence - Foundations of Computational Agents (r/csbooks cross-post),g0rix,[deleted],,https://www.reddit.com/r/artificial/comments/g0rix/free_ebook_artificial_intelligence_foundations_of/,0.0,0.0,,en
1106916,2011-03-10 02:45:32,statistics,Seeking advice: Running R and JAGS using multiple processors,g0trj,Phaedrus85,1292406495.0,https://www.reddit.com/r/statistics/comments/g0trj/seeking_advice_running_r_and_jags_using_multiple/,3.0,5.0,"I'm looking for advice on doing Bayesian statistics with R and JAGS, using multiple processors to speed up computation. What combination of R packages is needed to do this? Hardware: I'm considering the Amazon EC2 system, so lots of flexibility in this department. Advice on Linux builds and AMI's would also be appreciated.",en
1106917,2011-03-10 07:57:54,statistics,Generating sounds from assorted statistical models using R,g104p,mycatharsis,1294046118.0,https://www.reddit.com/r/statistics/comments/g104p/generating_sounds_from_assorted_statistical/,8.0,0.0,,en
1106918,2011-03-10 08:19:43,MachineLearning,Science Magazine | Special Online Collection: Dealing with Data,g10jd,Troybatroy,1238986431.0,https://www.reddit.com/r/MachineLearning/comments/g10jd/science_magazine_special_online_collection/,4.0,0.0,,en
1106919,2011-03-10 17:12:01,MachineLearning,Help choosing a classifier,g17wi,leondz,1165669130.0,https://www.reddit.com/r/MachineLearning/comments/g17wi/help_choosing_a_classifier/,14.0,33.0,"Hi,

I'm a compsci academic. I have a problem for which I'm attempting to train a binary classifier, with some degree of success. The classifers I've tried are ID3, MaxEnt, and NaiveBayes (the versions built in to NLTK). I have a suspicion that to achieve great performance with my features requires a classifier that doesn't follow the independence assumption - that is, one that can weight feature values dependent on the values assigned to other features.

ID3 intuitively doesn't follow this assumption, and naive Bayes does (that's its naivety). I'mm not sure about MaxEnt. In any case, my error reduction rate is 43% with MaxEnt, 12% with NBayes and 38% with ID3.

Can anyone recommend a classifier that is good with handling noise, doesn't follow the independence assumption, and can accept nominal features without too much coaxing?

Thanks!",en
1106920,2011-03-10 17:31:58,MachineLearning,GraphLab: A Parallel Framework for Machine Learning,g18cl,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/g18cl/graphlab_a_parallel_framework_for_machine_learning/,9.0,2.0,,en
1106921,2011-03-10 18:36:38,statistics,Simple (hopefully) R question: throwing out and replacing a data set in R,g19t9,JohnsOpinion,1286219762.0,https://www.reddit.com/r/statistics/comments/g19t9/simple_hopefully_r_question_throwing_out_and/,3.0,13.0,"I am trying to write a code in R for a class project. My stats background is very heavily skewed to SPSS and I dont have a coding (of any type past basic html) knowledge or experience. Is there a way to have R remove the current data set you are working off and pull in a new data set? Or should i try to think about it as detaching the original data set and then attaching the new data set?

Either way any help would be greatly appreciated.

Thank you!",en
1106922,2011-03-10 22:21:15,statistics,"So, I was reading the comments about this Pew Research Center Poll, and wondered what our learned statistical Redditors thought about the different responses.",g1f2m,vvvvvvvvvvvvv,1299034940.0,https://www.reddit.com/r/statistics/comments/g1f2m/so_i_was_reading_the_comments_about_this_pew/,3.0,5.0,,en
1106923,2011-03-10 23:38:47,MachineLearning,ML Reddit: What's the best method to extract article text from HTML documents?,g1guv,tomazk,1229769587.0,https://www.reddit.com/r/MachineLearning/comments/g1guv/ml_reddit_whats_the_best_method_to_extract/,6.0,8.0,"Given an HTML document, the algorithm must be capable of predicting/detecting article text and extracting it from that document. 

I've covered some [here](http://tomazkovacic.com/blog/14/extracting-article-text-from-html-documents/).

If you're aware of any paper/article/blog describing it or software/library/api capable of doing it, please let me know.

**UPDATE**: I've written a [blog post](http://tomazkovacic.com/blog/56/list-of-resources-article-text-extraction-from-html-documents/) and summarized my quick research.",en
1106924,2011-03-11 00:29:40,statistics,"August 1, 1872. Francis Galton uses statistical methods of inference to demonstrate the inefficacy of prayer.",g1hyl,robotrebellion,,https://www.reddit.com/r/statistics/comments/g1hyl/august_1_1872_francis_galton_uses_statistical/,4.0,2.0,,en
1106925,2011-03-11 06:01:17,statistics,Ask r/statistics: why does logistic regression use natural log?,g1oiq,Jofeshenry,1195748800.0,https://www.reddit.com/r/statistics/comments/g1oiq/ask_rstatistics_why_does_logistic_regression_use/,13.0,13.0,"I don't really understand what the purpose of taking the natural logarithm of odds is, but it seems key to understanding the whole process better.",en
1106926,2011-03-11 09:04:06,statistics,What advice would you give yourself if you could go back in time to your very early undergrad years?,g1ruy,[deleted],,https://www.reddit.com/r/statistics/comments/g1ruy/what_advice_would_you_give_yourself_if_you_could/,8.0,7.0,"I am about to embark on my grand journey into statistics.  My first 400 level statistics course (intro to probability) is as interesting and challenging as I could've hoped.  I can tell that statistics is indeed the major for me.  However, besides two high school level courses, I know little about what awaits me in the future.  I wish someone would have told me last year how important integration would be for my major so I could have taken it more seriously.  

So, now I turn to you, r/statistics.  What areas of math and stat should I begin to take a lot more seriously if I intend to spend the next 2-5 years rigorously studying statistics and hopefully one day becoming an actuary? Your life hacks and pro-tips are greatly appreciated!",en
1106927,2011-03-11 15:32:24,MachineLearning,DATA-MINING-CUP (DMC) 2011 student competition: Registration opened,g1wzz,fbahr,1275413362.0,https://www.reddit.com/r/MachineLearning/comments/g1wzz/dataminingcup_dmc_2011_student_competition/,10.0,2.0,,en
1106928,2011-03-11 17:02:45,MachineLearning,Readability-like API Using Machine Learning ,g1yip,jimktrains,,https://www.reddit.com/r/MachineLearning/comments/g1yip/readabilitylike_api_using_machine_learning/,10.0,0.0,,en
1106929,2011-03-11 17:38:26,datasets,the Enron Corpus,g1z9o,cavedave,1128052800.0,https://www.reddit.com/r/datasets/comments/g1z9o/the_enron_corpus/,11.0,0.0,,en
1106930,2011-03-11 17:59:25,statistics,Multiple system comparison using pair wise comparison method,g1zqy,incredulouspig,1279280990.0,https://www.reddit.com/r/statistics/comments/g1zqy/multiple_system_comparison_using_pair_wise/,3.0,1.0,"Hey /r/stats.  I have a stats problem.

I'm about to start a listening test which compares 3 surround sound systems (A, B and C).  I am using a method of comparison called a ""paired comparison"" test where listeners compare two stimulus (A and B) on a 7 point scale ranging from ""A is much better than B (-3), to A is the same as B (0), to B is much  better than A (3):

-3 -2 -1 0 1 2 3

Once the listener compares A and B, they will be asked to compare A and C, and B and C.

This will be done over a range of 4 stimuli, providing 12 comparisons in total for each subject.

My question to you, Reddit, is this:
Is it possible for me to show the resulting data graphically when comparing all 3 systems?  For example I would like to have the 3 systems on the 'x' axis, and the 7 point scale on the 'y' axis.  If I simply find the mean score for each system, would that be a valid comparison?  

The reason I'm asking is that the paired comparison test is generally done either when comparing just 2 items, or when comparing multiple items to a *single* item (rather than comparing three items to each other with no single 'reference' item, which is what I am trying to do).

If this is not a suitable method of comparison of 3 systems, could you suggest another method better suited to my purpose?

Let me know if I haven't provided enough information.",en
1106931,2011-03-11 20:50:06,datasets,"100,000 web servers located in Japan in XML format - provided by Shodan",g23s2,achillean,1192265468.0,https://www.reddit.com/r/datasets/comments/g23s2/100000_web_servers_located_in_japan_in_xml_format/,5.0,3.0,,en
1106932,2011-03-12 08:48:08,MachineLearning,Ensemble Learning for Variable selection; an easy read too!,g2gts,klavierspieler21,1250104303.0,https://www.reddit.com/r/MachineLearning/comments/g2gts/ensemble_learning_for_variable_selection_an_easy/,7.0,0.0,,en
1106933,2011-03-12 09:16:45,MachineLearning,Rexer Survey: R is used by more data miners than any other tool,g2h77,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/g2h77/rexer_survey_r_is_used_by_more_data_miners_than/,14.0,0.0,,en
1106934,2011-03-12 14:39:02,datasets,"The records of over 20,000 largely Irish soldiers who were at Kilmainham Hospital between 1783 &amp; 1822 are now online",g2ka6,cavedave,1128052800.0,https://www.reddit.com/r/datasets/comments/g2ka6/the_records_of_over_20000_largely_irish_soldiers/,16.0,0.0,,en
1106935,2011-03-12 19:21:57,MachineLearning,"I need to write a paper on ""recommendation 
systems"" (academic name is - collaborative 
filtering?) and looking for good books/articles to 
start with. Can you suggest something?",g2nnh,[deleted],,https://www.reddit.com/r/MachineLearning/comments/g2nnh/i_need_to_write_a_paper_on_recommendation_systems/,12.0,17.0,"Also, keep in mind that I come from the field of computer science, though I took courses on statistics, probability theory, linear algebra. 
Thank you. ",en
1106936,2011-03-12 19:36:47,statistics,Need help using Hotelling's t or Steiger's Z for multiple regression,g2nw9,jeffhughes,1282868135.0,https://www.reddit.com/r/statistics/comments/g2nw9/need_help_using_hotellings_t_or_steigers_z_for/,2.0,4.0,"I was hoping somebody could give me some help with some stats. I'm working on my Honours thesis (in psychology) and I have been running some multiple regression using two predictors. What I would like is to compare the Beta values of these two predictors to determine if they are significantly different from each other (as opposed to just different from zero).

I've been told about Hotelling's t and (alternately) Steiger's Z as a way of comparing correlations composed of overlapping samples. But I'm not sure if I can use Beta values in place of the zero-order correlations. Does the fact that the Beta value controls for the other variables in the regression model make them ""non-overlapping"", or should I be using the zero-order correlations, or what?

Any help would be appreciated. I'm in way over my head here. I've been using [this page](http://psych.unl.edu/psycrs/statpage/biv_corr_comp_eg.pdf) for info, along with the [FZT Computator](http://psych.unl.edu/psycrs/statpage/comp.html) it mentions.",en
1106937,2011-03-13 05:42:25,statistics,"Advanced Data Analysis from an Elementary Point of View (PDFs of lectures, R code, homework, and more)",g2xau,mycatharsis,1294046118.0,https://www.reddit.com/r/statistics/comments/g2xau/advanced_data_analysis_from_an_elementary_point/,11.0,1.0,,en
1106938,2011-03-13 05:46:17,statistics,"Probability, Random Processes,
and Ergodic Properties by Robert M. Gray (EBOOK: PDF)",g2xcz,mycatharsis,1294046118.0,https://www.reddit.com/r/statistics/comments/g2xcz/probability_random_processes_and_ergodic/,7.0,0.0,,en
1106939,2011-03-13 07:28:22,statistics,"Poisson, quasipoisson, gamma, binomial. . . Is there a good resource that will tell me which distribution to use and when to use it?  Bonus for a resource with examples (and R references?!)",g2yqn,daledinkler,1199898909.0,https://www.reddit.com/r/statistics/comments/g2yqn/poisson_quasipoisson_gamma_binomial_is_there_a/,16.0,4.0,,en
1106940,2011-03-13 16:09:50,MachineLearning,An idea for a platform where people can play the Turing game and chat bot developers can gather data and compete.,g33pi,quiteamess,1289768452.0,https://www.reddit.com/r/MachineLearning/comments/g33pi/an_idea_for_a_platform_where_people_can_play_the/,12.0,8.0,"While discussing with nadie854 on his [reserbot](http://www.reddit.com/r/MachineLearning/comments/f3uvz/a_chatterbot_built_using_a_neural_networks/) I had an idea for a platform where people can play the Turing game and people can submit bots that participate in the Turing game. The key Idea is that data is gathered by the conversations. This data can be used to train bots. Another strength is that the chatter bots stand in competition and can be tested.

The data-gathering idea is already there in [cleverbot](http://cleverbot.com/). But there is a key difference in this approach. In the original [Turing game](http://cogprints.org/499/1/turing.html) there is a human interrogator C, a Human A and a bot B. The interrogator tries to find out if A or B is the bot. Therefore, there are conversations between C and B, i.e. humans that can be used as training data. Imagine this on a large scale where many people play this game, as in google image labeler. This would bring a lot of good data with conversations that aim at the identification of chatbots. This data can then be used to further train chatbots. 

Further the idea is not to train a single bot, but to provide a platform for chatbot developers. The bots can stand in competition, as it is already established in the research community (for example the [Loebner prize](http://www.google.com/url?sa=t&amp;source=web&amp;cd=1&amp;sqi=2&amp;ved=0CBcQFjAA&amp;url=http%3A%2F%2Fwww.loebner.net%2FPrizef%2Floebner-prize.html&amp;rct=j&amp;q=turing%20test%20competition&amp;ei=ZMd8TY6uLsbGswaNxsHbBw&amp;usg=AFQjCNGNmtOxBD5jb1UYDbd4Mp53MOKWyA&amp;cad=rja)). The incentive for developers to use this platform would be the possibility to gather data.

I think this idea has quite a potential, if advertised in the AI-community. It is appealing to the general public as a game and appealing for developers as data-source and competition. As I won't have the time to implement this idea I wanted to share it here. ",en
1106941,2011-03-13 21:01:32,statistics,Ordinary least squares,g37xv,[deleted],,https://www.reddit.com/r/statistics/comments/g37xv/ordinary_least_squares/,1.0,0.0,"So I have a design matrix which is 

1, -1, 1    1, -1, -1    1, 1, 1    1, 1, -1 

The original question says the Y-values (transpose) are (2,1,5,3) and to find coefficients (this is simple enough, I know how to do it). Updated question says that each value of Y is an average of 5 values with standard deviations of (2,1,1,2) respectively. How do I go about calculating ordinary least squares coefficients. ",en
1106942,2011-03-13 21:28:03,statistics,Ordinary Least squares question,g38f4,amirightfolks,1280970578.0,https://www.reddit.com/r/statistics/comments/g38f4/ordinary_least_squares_question/,0.0,0.0,"So I have a design matrix which is 

intercept| x1 | x2 |
:-----------|:------------|:------------|
 1  | -1 |   1     
 1  | -1 |  -1    
 1  |  1 |   1     
 1  |  1 |  -1      


The original question says the Y-values (transpose) are (2,1,5,3) and to find coefficients (this is simple enough, I know how to do it). Updated question says that each value of Y is an average of 5 values with standard deviations of (2,1,1,2) respectively. How do I go about calculating ordinary least squares coefficients. ",en
1106943,2011-03-14 00:21:12,statistics,"Using R for Introductory Statistics, The Geometric distribution",g3bee,christopherbare,1291844617.0,https://www.reddit.com/r/statistics/comments/g3bee/using_r_for_introductory_statistics_the_geometric/,4.0,0.0,,en
1106944,2011-03-14 04:18:24,MachineLearning,Selection Sunday: Is your March Madness prediction algorithm ready?,g3ff1,danger_t,1249412021.0,https://www.reddit.com/r/MachineLearning/comments/g3ff1/selection_sunday_is_your_march_madness_prediction/,4.0,1.0,,en
1106945,2011-03-14 06:24:41,datasets,Installing GeoCouch/CouchDB on a Mac and Geo Data (Shape Files) with Shp2Couch,g3ht7,felipera,1286469568.0,https://www.reddit.com/r/datasets/comments/g3ht7/installing_geocouchcouchdb_on_a_mac_and_geo_data/,3.0,0.0,,en
1106946,2011-03-14 12:16:27,MachineLearning,RStudio 0.92.44 Release: another great R IDE release...,g3mfc,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/g3mfc/rstudio_09244_release_another_great_r_ide_release/,21.0,0.0,,en
1106947,2011-03-14 12:16:45,statistics,RStudio 0.92.44 Release: another great R IDE release...,g3mff,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/g3mff/rstudio_09244_release_another_great_r_ide_release/,6.0,7.0,,en
1106948,2011-03-14 16:55:34,statistics,"My girlfriend just posted a question on Stackexchange concerning R. If you have the time and will, please have a look ",g3p3b,jonaseriksson,1280834535.0,https://www.reddit.com/r/statistics/comments/g3p3b/my_girlfriend_just_posted_a_question_on/,2.0,4.0,,en
1106949,2011-03-14 20:57:18,statistics,How The New York Times Graphics Department Uses R ,g3ujk,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/g3ujk/how_the_new_york_times_graphics_department_uses_r/,19.0,3.0,,en
1106950,2011-03-14 21:40:56,statistics,Lebesgue integral,g3vjq,FailedLifeForm,1299165736.0,https://www.reddit.com/r/statistics/comments/g3vjq/lebesgue_integral/,2.0,4.0,"hi. 
Say, f(x) = 2x , for x in [0 1]. 
using Riemann integral, E[X] = integral(x*f(x)*dx) {from 0 to 1} = 2/3.    
How do you set this integral as a Lebesgue integral?",en
1106951,2011-03-15 00:07:15,MachineLearning,2011 March Madness Predictions with Probabilistic Matrix Factorization,g3xz4,danger_t,1249412021.0,https://www.reddit.com/r/MachineLearning/comments/g3xz4/2011_march_madness_predictions_with_probabilistic/,11.0,2.0,,en
1106952,2011-03-15 13:52:21,statistics,"Ask /r/statistics: How to find the best p,d,q of a ARIMA model in R?",g4ca8,cbrunos,,https://www.reddit.com/r/statistics/comments/g4ca8/ask_rstatistics_how_to_find_the_best_pdq_of_a/,6.0,8.0,"Hello,

I have some time series data and I want to find what process this data follows. Is there a way to do this in R? Some kind of test that would return the p,d,q parameters that minimize the AIC for example?

Thanks for your help!",en
1106953,2011-03-15 16:20:26,data,Watch Movies? Like Music? Run a Server? Get Ready to Pay.,g4eu5,jeffyablon,1283280097.0,https://www.reddit.com/r/data/comments/g4eu5/watch_movies_like_music_run_a_server_get_ready_to/,1.0,0.0,,en
1106954,2011-03-15 18:28:39,analytics,I just released a new iPad Google Analytics app! Here's Quicklytics to you all!,g4hrv,[deleted],,https://www.reddit.com/r/analytics/comments/g4hrv/i_just_released_a_new_ipad_google_analytics_app/,1.0,0.0,,en
1106955,2011-03-15 22:14:48,statistics,Getting started with Social Network Analysis,g4mvm,abitofperspective,1272704550.0,https://www.reddit.com/r/statistics/comments/g4mvm/getting_started_with_social_network_analysis/,6.0,10.0,"Hi - I have some data that I'd like to analyse using social network analysis techniques. The data are international migration numbers between countries, they are in the format:

A B X

A C Y

A D Z

etc.

Where A, B, C are country names and X, Y, Z are numbers of people. The network is directed (i.e. AB is not the same as BA) and valued (obviously not binary).

Can anyone suggest a good software package and (more importantly) good tutorials where I can load this data and try running some measures. I'm particularly interested in:

- The centrality of the network - does it become more centralized or spread out over time

- The coreness of certain nodes. Do some countries become more core over time.

- Identifying clusters in the data

Have read about half of the Wasserman/Faust book, but am a little tired of reading and would like to try analysing some data.",en
1106956,2011-03-15 23:42:55,datasets,free HD video sequence to be used as a dataset for machine learning algorithm??,g4ot9,fookmefookyou,1300224457.0,https://www.reddit.com/r/datasets/comments/g4ot9/free_hd_video_sequence_to_be_used_as_a_dataset/,7.0,3.0,"I need a HD video sequence to use as a dataset for my algorithm. It has to be publicly available, as in http://www.cipr.rpi.edu/resource/sequences/ but it has to be HD (&gt;=720p) and simple (fixed background/zoom, few dynamics), e.g. a newscaster talking.
Thnx
",en
1106957,2011-03-16 05:14:52,statistics,Amanda Cox on How The New York Times Graphics Department Uses R,g4vhw,[deleted],,https://www.reddit.com/r/statistics/comments/g4vhw/amanda_cox_on_how_the_new_york_times_graphics/,1.0,0.0,,en
1106958,2011-03-16 13:45:06,statistics,"Hey r/stats, can you shed some light on these schools for graduate study?",g52su,[deleted],,https://www.reddit.com/r/statistics/comments/g52su/hey_rstats_can_you_shed_some_light_on_these/,4.0,14.0,"I'm a senior college student graduating in May and looking to pursue a Master's degree at one of these schools. If you know something about these stats programs at these schools, could you share your knowledge?

* Master's in Statistical Practice at CMU
* Master's in Applied Statistics at Columbia
* Master's of Science in Survey Methodology at University of Maryland
* Master's of Science in Statistics at University of Chicago
* Master's of Science in Statistics at George Washington University

p.s. If any other seniors out there in r/stats are headed to graduate study, feel free to share your future plans!",en
1106959,2011-03-16 14:35:47,artificial,Smarter Than You Think: Poker Bots Invade Online Gambling,g53j0,kamoylan,1254047922.0,https://www.reddit.com/r/artificial/comments/g53j0/smarter_than_you_think_poker_bots_invade_online/,13.0,6.0,,en
1106960,2011-03-16 16:20:09,statistics,Display (n) in excell 2007,g55i9,krakenwastaken,1294310889.0,https://www.reddit.com/r/statistics/comments/g55i9/display_n_in_excell_2007/,2.0,4.0,"Hi, how do I display the number of datapoints which my graph is consisting off, inside the graph itself, in excel 2007? Like, if I have a bar style graph of Y=% and X=a few categories of something. And I want each bar to display how many (n) went into my % calculation. I hope someone can make sense of my badly phrased question. Thanks ",en
1106961,2011-03-16 19:17:14,statistics,Working with a univariate ANOVA: Does normality have to be assumed for the dependent variables?,g59oy,[deleted],,https://www.reddit.com/r/statistics/comments/g59oy/working_with_a_univariate_anova_does_normality/,1.0,0.0," So I'm doing some analysis, and a univariate ANOVA would be perfect for it. I know my dependent variable is normally distributed, but it appears one of my independent variables isn't. Will an ANOVA still work? 
 Many thanks in advance!",en
1106962,2011-03-17 02:10:39,MachineLearning,"In an effort to advance the understanding of market algorithms and Internet economics, Google has launched an academic research initiative focused on the underlying aspects of online auctions, pricing, game-theoretic strategies, and information exchange. (crosspost from /r/science/)",g5ihh,mjanes,1245651052.0,https://www.reddit.com/r/MachineLearning/comments/g5ihh/in_an_effort_to_advance_the_understanding_of/,14.0,1.0,,en
1106963,2011-03-17 04:27:53,MachineLearning,"Have any publications, presentations, or posters demonstrating a contribution to the field of machine learning?  Post it in this thread!",g5l3c,coopster,,https://www.reddit.com/r/MachineLearning/comments/g5l3c/have_any_publications_presentations_or_posters/,2.0,0.0,,en
1106964,2011-03-17 22:35:50,MachineLearning,$3.2M in prizes for predicting hospitalization,g5xmt,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/g5xmt/32m_in_prizes_for_predicting_hospitalization/,9.0,2.0,,en
1106965,2011-03-17 22:36:22,datasets,$3.2M in prizes for predicting hospitalization,g5xnm,talgalili,1271226645.0,https://www.reddit.com/r/datasets/comments/g5xnm/32m_in_prizes_for_predicting_hospitalization/,16.0,0.0,,en
1106966,2011-03-17 22:36:32,statistics,$3.2M in prizes for predicting hospitalization,g5xnu,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/g5xnu/32m_in_prizes_for_predicting_hospitalization/,8.0,3.0,,en
1106967,2011-03-18 03:13:33,MachineLearning,What does ML tell us about teaching kids? ,g646s,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/g646s/what_does_ml_tell_us_about_teaching_kids/,6.0,2.0,,en
1106968,2011-03-18 04:55:16,statistics,Data Set for Fast Food Waiting Time and Queues,g669y,[deleted],,https://www.reddit.com/r/statistics/comments/g669y/data_set_for_fast_food_waiting_time_and_queues/,3.0,4.0,"I've been searching for data sets related to this for a statistics project.  If anyone has time, please help me find some?  I've been looking at archives but I haven't found any.  Please and thank you!",en
1106969,2011-03-18 19:21:34,statistics,Need help with power and sample size calculations.,g6k3g,Griffinheart,1252805594.0,https://www.reddit.com/r/statistics/comments/g6k3g/need_help_with_power_and_sample_size_calculations/,4.0,5.0,"I'm designing an experiment and am unsure what type of power calculation I should use to determine sample size.

My experiment involves subjects lifting boxes in different ways and comparing the levels of spinal loading seen during each type of lift.  Each subject is going to do all of the lifts in a randomized order over a single session.

I am interested in comparing each two-lift combination for these sample size calculations.  I have been looking at tests such as a two-sample t-test but am concerned that my samples are not independent as data for each lift will come from the same subject.

Does anyone have any idea what test I should use?  Your help is greatly appreciated.  If it helps, I'll include some sample data below.

Combined data from two subjects (I'm not worried about detecting differences between subject, just between tasks)

All values are in Newtons.  Raw data are peak forces from multiple trials of each lift.


Lift 1	

* L1 Mean	459.61

* L1 SD	51.51

* L1 Raw	
521.07
506.18
440.00
401.70
429.11
	
Lift 2	

* L2 Mean	815.57

* L2 SD	97.15

* L2 Raw	
873.00
959.83
749.83
741.07
754.14
	
Combined Mean	637.59

Combined SD	201.42
",en
1106970,2011-03-18 20:53:09,datasets,Bimodal benchmark sets,g6m6e,MeowMeowFuckingMeow,1290779070.0,https://www.reddit.com/r/datasets/comments/g6m6e/bimodal_benchmark_sets/,2.0,0.0,"Does anyone know of any benchmark datasets used to evaluate classifier performance which are easily/naturally divided into two streams of data (or come from different modalities?). What I'm using at the moment are data which divide into audio+visual features, but it'd be useful to see if [this method I'm looking at] generalises to other types of data than frontal audiovisual data of humans. 

Thanks!",en
1106971,2011-03-18 21:21:52,MachineLearning,"A couple weeks ago, Huffington Post blogger Dan Mervish noted a funny trend: when Anne Hathaway was in the news, Warren Buffett's Berkshire Hathaway's shares went up. ",g6mu0,self,1135746000.0,https://www.reddit.com/r/MachineLearning/comments/g6mu0/a_couple_weeks_ago_huffington_post_blogger_dan/,26.0,6.0,,en
1106972,2011-03-19 08:02:14,statistics,Pr(4-of-a-kind in Poker),g6xh9,FailedLifeForm,1299165736.0,https://www.reddit.com/r/statistics/comments/g6xh9/pr4ofakind_in_poker/,2.0,6.0,"hi. I am trying to calculate the probability of getting 4-of-a-kind in poker. There are C(52,5) ways of picking 5 cards. this is our denominator. Now there are 52 choices for the first card, 1 choice for the second, third, and forth cards, and 48 choices for the 5th card. So the Pr should be (52x1x1x1x48)/C(52,5). Why is this wrong (according to wikipedia).",en
1106973,2011-03-19 23:01:25,statistics,Pr(no pair in poker),g78ia,FailedLifeForm,1299165736.0,https://www.reddit.com/r/statistics/comments/g78ia/prno_pair_in_poker/,0.0,2.0,"hi. cards must be: all different kinds. no straights. no flushes.    
all different kinds = C(13,5)xC(4,1)^5    
straights =  C(10,1)xC(4,1)^5    
flushes = C(4,1)xC(13,5)   
Pr(no pair) = C(13,5)xC(4,1)^5 - C(10,1)xC(4,1)^5  - C(4,1)xC(13,5)    
this is off by 40. what is missing?",en
1106974,2011-03-20 20:05:50,statistics,"How do you model a non-stationary, discrete valued time series?",g7o6w,roger_,1178076247.0,https://www.reddit.com/r/statistics/comments/g7o6w/how_do_you_model_a_nonstationary_discrete_valued/,6.0,10.0,Is there something equivalent to the ARIMA model when you data can only take on discrete values (say binary)?,en
1106975,2011-03-20 21:00:21,statistics,We throw 3 dice one by one. What is the probability that we obtain 3 points in strictly increasing order?,g7p4r,FailedLifeForm,1299165736.0,https://www.reddit.com/r/statistics/comments/g7p4r/we_throw_3_dice_one_by_one_what_is_the/,0.0,7.0,"Pr(strictly increasing) = Pr(strictly increasing | different numbers) x Pr(different numbers) + Pr(strictly increasing | non-different numbers) x Pr(non-different numbers)    
Pr(strictly increasing | non-different numbers) =0    
Pr(different numbers) = (6/6)x(5/6)x(4/6)    
What is Pr(strictly increasing | different numbers)?",en
1106976,2011-03-20 23:42:01,MachineLearning,Collecting labeled data for Classification Problem: Do we need even amounts for each label?,g7rz6,[deleted],,https://www.reddit.com/r/MachineLearning/comments/g7rz6/collecting_labeled_data_for_classification/,6.0,5.0,"I am working on a classification problem, with 5 different possible labels, let's call them Apple, Banana, Mango, Orange, and Pear.

I have a large set of labeled data(10,000 data points) and the corresponding attributes, but I have an uneven amount of data for each of the fruits.
Apple: 4,000
Banana: 1,000
Mango: 2,000
Orange: 2,000
Pear:1,000

Collecting labeled data is a bit time consuming; I can, but prefer not to collect more Banana and Pear data. Does the amount of labeled data effect the results of the classification?  (I'm guessing SVMs, not so much since we're trying to find only the support vectors, but how about for Tree Based methods like Decision Trees and Random Forest?)",en
1106977,2011-03-21 00:43:47,statistics,"Help me with a HW problem, r/statistics?",g7t11,rustyCrVx,1290128453.0,https://www.reddit.com/r/statistics/comments/g7t11/help_me_with_a_hw_problem_rstatistics/,1.0,4.0,"I have no idea what to do. I've looked through my book and notes for something similar but cannot find anything like this one. Help me out?

A population is normally distributed with a mean of 23.45 and standard deviation of 3.8. What is the probability of taking a sample size of 10 and obtaining a sample mean of 22 or less? What is the probability of taking a sample of size 50 and getting a sample mean of more than 24?",en
1106978,2011-03-21 01:44:34,statistics,It's All Relative,g7u24,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/g7u24/its_all_relative/,5.0,2.0,,en
1106979,2011-03-21 09:33:25,statistics,"Reddit Comments: Estimate facts/opinions of redditor ""population"" using comments ""sample""",g81t6,tempest-fire,1293957671.0,https://www.reddit.com/r/statistics/comments/g81t6/reddit_comments_estimate_factsopinions_of/,3.0,3.0,"I'm taking my first stats course ever (so bear with me, I'm a stats noob), and I was really excited to learn about estimating a population's mean using a sample, because it seems so applicable to real life.  For example: can't we make an approximation of the population of a subreddit by looking at the comments on one of the front-page posts?  

I was thinking of using a Confidence Interval of the Mean, Variance Unknown (T-distribution):  x - ( t(a/2, n-1) * S / (n^.5) ) &lt;= mu &lt;= x - ( t(a/2, n-1) * S / (n^.5) )
Where x is the sample average, S is the sample standard deviation, n is the number of data, mu is the population average, t is the upper 100a/2 percentage point of the t distribution with n-1 degrees of freedom, such that there is a 100(1-a)% confidence interval on mu.

You could then look at the comments on a front-page post, and assign each comment a number based on what the user said, and then use this data to create a confidence interval of that subreddit population's opinions.  One problem I can see is that in my textbook, the examples are all where each datum is a number on a scale (like yield stress or average rainfall), whereas opinions could be yes or no (1 or 0), so what would a 95% Confidence interval of a population average of .55 actually mean?  Maybe you'd have to have each comment based on a 1-10 scale or something-

Any advice or suggestions would be really appreciated!",en
1106980,2011-03-21 19:40:32,datasets,"The housing bubble by city
",g8br0,talgalili,1271226645.0,https://www.reddit.com/r/datasets/comments/g8br0/the_housing_bubble_by_city/,14.0,0.0,,en
1106981,2011-03-21 20:52:15,statistics,R graph label line breaks,g8di2,valen089,1288238268.0,https://www.reddit.com/r/statistics/comments/g8di2/r_graph_label_line_breaks/,4.0,4.0,"I recently that it is possible to force graphic labels to line break using \n and was wondering if there another way of doing this that was more automatic.  Working with a large data set and don't really want to spend time manually inserting \n into each label if there is a more efficient way of doing this.

    label=c(""This is a really really \n really long label"")

",en
1106982,2011-03-22 19:43:51,statistics,why are unity roots so problematic?,g92oq,FailedLifeForm,1299165736.0,https://www.reddit.com/r/statistics/comments/g92oq/why_are_unity_roots_so_problematic/,4.0,4.0,Can someone please explain why unity roots result in divergence in a recurrent series? [eg](http://en.wikipedia.org/wiki/Recurrence_relation#Stability) ,en
1106983,2011-03-22 21:03:26,statistics,Vendors for user-friendly web-based data-collection tools?,g94l0,nipplicious,1268972133.0,https://www.reddit.com/r/statistics/comments/g94l0/vendors_for_userfriendly_webbased_datacollection/,3.0,3.0,,en
1106984,2011-03-22 21:13:55,statistics,Vendors for user-friendly web-based data-collection tools?,g94sb,[deleted],,https://www.reddit.com/r/statistics/comments/g94sb/vendors_for_userfriendly_webbased_datacollection/,1.0,0.0,"Would need to:

* Be able to allow error-checking mechanisms
* Export all data elements into a variety of data formats (SAS, SPSS, Excel)
* Be written using non-proprietary software that can be transferred to another agent

Any recommendations? ",en
1106985,2011-03-23 05:36:14,statistics,Need a little help....,g9fs0,[deleted],,https://www.reddit.com/r/statistics/comments/g9fs0/need_a_little_help/,1.0,1.0,"I'm brand new to stats and I could use a little assistance. Please forgive me ignorance....

For my final project in one of my courses, I have been assigned to conduct a formal evaluation of school program.  I've selected an after-school academic program at a local middle school. To better understand the efficacy of the program, I am using Language Arts and Mathematics classroom grades from 1st and 3rd quarter.  There are two groups of students that I'm looking at: ""regular students"" (who fit the federal definition of regular attendees) and ""non-regular"" (for a lack of a better term) participants. There are 28 regular participants and 59 non-regular participants. 

I've taken the students classroom grades and assigned a numeric value (based on GPA, for example a B is a 3, B- is a 2.66) to find a mean, median and a variance.  However, I'm interested in comparing the mean of the regular participants versus the mean of the non-regular participants.  Would I utilize a t-test?   

Assuming this is the case, wouldn't this be a two-tailed test because I would be interested in either an increase or a decrease? Lastly, because the variances are not identical, I would run a two-sample unequal variance? 

If I set the alpha at .05 and refer to the t distribution chart in the back of the textbook, check the degrees of freedom, I should discover if my values are statistically significant.  

So for example, if I run the t-test in Excel, get the number 0.085, this is actually 8.5%. When I refer to the t-chart it reads 1.990. Am I right that because 8.5% is larger than 1.990, the value is statistically significant? 

If I'm way off based here, I would appreciate any assistance you could provide. 
   ",en
1106986,2011-03-23 06:26:31,statistics,"Statisticians: out of all the things you learned in college, what do you commonly use?",g9gs5,HughManatee,1271624440.0,https://www.reddit.com/r/statistics/comments/g9gs5/statisticians_out_of_all_the_things_you_learned/,17.0,32.0,"I'm currently a graduate student in Statistics and I am just curious what various statisticians use on a daily basis, whether it be a programming language, a type of data analysis, or whatever.  Looking for different fields that may pique my interest as I begin to apply for jobs over the next year or so.",en
1106987,2011-03-23 11:29:48,statistics,Yes Minister -Survey Design,g9l5p,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/g9l5p/yes_minister_survey_design/,5.0,0.0,,en
1106988,2011-03-23 15:48:42,MachineLearning,Screencast: How to successfully compete in data mining competitions (with R and Random forests),g9p55,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/g9p55/screencast_how_to_successfully_compete_in_data/,25.0,2.0,,en
1106989,2011-03-23 15:54:23,statistics,Correlation Nnetwork plot in R,g9p96,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/g9p96/correlation_nnetwork_plot_in_r/,0.0,0.0,,en
1106990,2011-03-23 16:50:23,statistics,Fit statistics for GEE,g9qji,Case_Control,1263273609.0,https://www.reddit.com/r/statistics/comments/g9qji/fit_statistics_for_gee/,3.0,3.0,"Recently started trying to learn some GEE and have a few questions. What I would like to do is be able to answer a question of the form: ""Which variable in the model explains the largest portion of variance in the data?"" If I was using an OLS I'd look at a change in model R squared between the full model and the a model minus one of the explanatory variables; rinse and repeat for every explanatory variable to find the largest change in R squared. I've found a similar approach for mixed models where you can convert the likelihood to an R squared.   The only fit statistic I have been able to find for GEE is the QIC (cant seem to find an R squared equivalent, and I know theres no likelihood). Is there some way to use the QIC to answer this type of question? I had thought about comparing a change in QICs for each variable versus the full model, but I have no idea if this is even interpretable. Anyone know of a way to get at ""explained variance"" in GEE?   ",en
1106991,2011-03-23 17:47:07,statistics,Help finding Standard Deviation on Calculator,g9rut,[deleted],,https://www.reddit.com/r/statistics/comments/g9rut/help_finding_standard_deviation_on_calculator/,1.0,0.0,"Making this quick, don't have too much time. Basically, left my TI-83 at home (I'm a commuter) and have a Statistics exam coming up later. My friend only had his TI-30XS to lend me before he left, so I have to work with it. I've tried to find out how to find the SD using it, but to no avail. I know there's a formula but won't have much time to sit down and do it for a bunch of the problems. I'm also very amateur at math, but I'm hoping there's a way around this. There's a way to insert numbers in the L1, L2 etc columns, but I'm not sure about the Standard Deviation.

",en
1106992,2011-03-23 17:54:15,statistics,"Sociologist going for a ""Statistical Analyst"" position interview. What do you think I should expect/How should I prepare?",g9s0z,[deleted],,https://www.reddit.com/r/statistics/comments/g9s0z/sociologist_going_for_a_statistical_analyst/,3.0,11.0,"Apparently there will be a test in which I must analyse some numbers (data most likely in the form of tables) and then construct a new table or something.

This will all be done by hand.

Dealing most probably with economic or social statistics.

Anyone have experience with this sort of interview test?


**edit:** Regardless of how ""silly"" everyone thinks this is, I'd still appreciate some suggestions to help prepare.",en
1106993,2011-03-23 18:25:35,MachineLearning,"Comparing the Popularity of Data Analysis Software (R vs SAS vs SPSS, etc.)",g9sqy,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/g9sqy/comparing_the_popularity_of_data_analysis/,2.0,0.0,,en
1106994,2011-03-24 00:22:07,statistics,Percentage of Americans who have a passport (by State,ga1jn,bobshoe,1221743590.0,https://www.reddit.com/r/statistics/comments/ga1jn/percentage_of_americans_who_have_a_passport_by/,3.0,3.0,,en
1106995,2011-03-24 01:53:05,datasets,Data Visualization Tools?,ga3kc,Theoretician,1300672455.0,https://www.reddit.com/r/datasets/comments/ga3kc/data_visualization_tools/,1.0,0.0,So what do you guys do for data visualization? I'm relatively new and would like to display some data I recently acquired. What are the best tools in terms of cost as well as functionality?,en
1106996,2011-03-24 07:00:23,MachineLearning,Ireland's blood donations tank as of 2010,ga9t2,[deleted],,https://www.reddit.com/r/MachineLearning/comments/ga9t2/irelands_blood_donations_tank_as_of_2010/,0.0,0.0,,en
1106997,2011-03-24 12:53:09,MachineLearning,So there was a link to JMLR posted yesterday...,gaef5,filox,1196516975.0,https://www.reddit.com/r/MachineLearning/comments/gaef5/so_there_was_a_link_to_jmlr_posted_yesterday/,25.0,31.0,"Although I thought people would dismiss that submission as trivial since everyone already knows about it, I was surprised that some people said they never heard of it.  This is why I'm posting some ML conferences here in case some of you may not know about them.  This list is by no means complete, it's just my personal pick of the ""good"" conferences to follow if you're interested in ML.  So, here goes (I'm too lazy to actually put links):

* ICML
* NIPS
* UAI
* AISTATS
* COLT
* ACL, NAACL, EMNLP (they have pretty strong machine learning tracks)

Other, less ML related, but still possibly interesting conferences (by possibly interesting I mean that they get the odd ML paper that can be very interesting):

* KDD
* ICDM
* SIGIR
* VLDB
* COGSCI",en
1106998,2011-03-24 12:53:40,analytics,Have a look at this brochure for Data Analytics course,gaefb,iqrconsulting,1288244989.0,https://www.reddit.com/r/analytics/comments/gaefb/have_a_look_at_this_brochure_for_data_analytics/,0.0,0.0,,en
1106999,2011-03-24 12:56:23,statistics,I see stats everywhere ,gaegg,cbrunos,,https://www.reddit.com/r/statistics/comments/gaegg/i_see_stats_everywhere/,21.0,3.0,,en
1107000,2011-03-24 17:03:50,MachineLearning,Anybody have a Part of Speech tagged corpus?,gaink,duinn,1267883868.0,https://www.reddit.com/r/MachineLearning/comments/gaink/anybody_have_a_part_of_speech_tagged_corpus/,6.0,5.0,"I'm training my own POS tagger (using Conditional Random Field) and tagging the training corpus is taking a very long tedious time. I need it in a sentence structure (since thats how CRFs work!) e.g.
    My/PRP$ dog/NN also/RB likes/VBZ eating/VBG sausage/NN ./.
Not with these exact tags, but something along those lines!
Ta.

",en
1107001,2011-03-24 21:26:16,artificial,Bayes' Intelligent Abstractions From Meaningful Co-Occurrences,gapfe,Awwware,1289566851.0,https://www.reddit.com/r/artificial/comments/gapfe/bayes_intelligent_abstractions_from_meaningful/,1.0,0.0,,en
1107002,2011-03-24 23:42:03,statistics,"Hey r/Statistics, I have a quick question on Binomial distributions.",gasrz,[deleted],,https://www.reddit.com/r/statistics/comments/gasrz/hey_rstatistics_i_have_a_quick_question_on/,0.0,5.0,"Okay,

I have a candy bar problem: 

If you consume seven 28.35 gram candy bars this week from a brand at the mean contamination level of 0.8371, what is the probability that you consume one or more insect fragments in more than one bar?

I've got X~Bin(7, 0.8371) and P(X&gt;1) = 1 - P(X≤1).  

What is the algebra that gives me the answer of 0.9999? (What comes up after the probability set up?)

Thanks- 
Stats newb",en
1107003,2011-03-25 04:30:55,statistics,Multivariate Experimental Design,gaz1j,MartialLol,1294380290.0,https://www.reddit.com/r/statistics/comments/gaz1j/multivariate_experimental_design/,3.0,13.0,"Excuse me r/stats, but I have a question for you.

I'm a master's student in biology and need to determine how to analyze my thesis data. I haven't started the experiment yet, but my goal is to measure changes in hormone gene expression and steroid secretion in response to acute stress and toxin exposure (fully cross-classified). I'll assay 4 target genes and one steroid, with 4 stress levels, 5 toxin levels, 2 sampling units, and possibly some sub-sampling. 
 
How should this data be analyzed? This is what I've come up with so far:

a) Separate factorial ANOVAs for each dependent variable, with the other DVs as covariables (This seems like a clumsy approximation of multivariate stats, but it's been published in studies similar to mine, so it's my starting point)

b) Factorial MANOVA (I'm least familiar with this method, but it's on the agenda in my stats class later this semester)

c) Separate discriminant analyses for each stress level, using hormone profiles as predictors for toxin treatment

d) I'll definitely be doing some data snooping based on similarity matrices for the hormones. Is there anything in particular to try? 

I realize biologists tend to do things a little differently, so I apologize in advance for my somewhat sketchy understanding of formal stats. I really enjoy the topic though, so anything you have to offer would be greatly appreciated. 

As for software, I have access to SAS, SPSS, SYSTAT, Primer+PERMANOVA, and a rough understanding of R. 

Thanks a lot!
",en
1107004,2011-03-25 16:30:10,MachineLearning,Regression with R,gbac1,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/gbac1/regression_with_r/,16.0,1.0,,en
1107005,2011-03-25 21:48:46,statistics,Do Celebrities Follow the Half Your Age Plus Seven Rule? [R/infographic],gbhz9,bonniemuffin,1176306281.0,https://www.reddit.com/r/statistics/comments/gbhz9/do_celebrities_follow_the_half_your_age_plus/,11.0,1.0,,en
1107006,2011-03-26 04:42:47,statistics,Analysis of The Popularity of Data Analysis Software by Robert A. Muenchen,gbpw4,mycatharsis,1294046118.0,https://www.reddit.com/r/statistics/comments/gbpw4/analysis_of_the_popularity_of_data_analysis/,14.0,2.0,,en
1107007,2011-03-26 10:22:58,MachineLearning,Embedding R inside the Qt framework ,gbukw,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/gbukw/embedding_r_inside_the_qt_framework/,11.0,1.0,,en
1107008,2011-03-26 15:45:48,MachineLearning,Could someone familiar with weka spare a moment to answer a question for me - regarding the representation of missing nominal values in ARFF‏.,gbxl4,[deleted],,https://www.reddit.com/r/MachineLearning/comments/gbxl4/could_someone_familiar_with_weka_spare_a_moment/,7.0,3.0,"Apologies if this is an idiotic question. I've currently got a university project in which I am using weka to apply some classification algorithms to a collection of genotype values. I've had some success using a smaller dataset but now that I am scaling my investigations up to a larger data set with some missing values I am encountering a problem. I've got a list of attributes represented as follows in my arff file:

@attribute blah {AA, CC, GG, TT, AG, AC, AT, CA, CG, CT, GA, GC, GT, TA, TC,TG}

And then my data looks something like: AA,AA,AG,AG,GG,TC,? etc

When I try to open the file in weka it bugs out and tells me that a nominal value has not been declared in the header. I thought that missing values were represented using a '?' character, but do I need to include the '?' as an option in my atrribute list as well?

Thanks in advance for any help.",en
1107009,2011-03-27 02:17:20,statistics,"How do you quantify the ""goodness"" of a fit?",gc7zn,jesusabdullah,1202534664.0,https://www.reddit.com/r/statistics/comments/gc7zn/how_do_you_quantify_the_goodness_of_a_fit/,3.0,29.0,"Hey /r/stats, help a noob with his thesis?

Basically, I have these measurements at various angles:

    x   y
    90	0.87355252855632
    90	0.96510036804794
    90	0.99961155254449
    75	0.83250156448838
    75	0.90994111936252
    75	0.81220807703055
    75	0.88450841597365
    60	0.93325151195878
    60	0.88359117143352
    60	0.85099464726141
    30	0.88641984414472
    30	0.88585837018634
    30	0.86312678397544
    30	0.87395793611538
    90	0.95257033724407
    90	0.96405226327959
    90	1.24511295032759
    75	0.87511994935464
    75	0.909246441656
    75	0.88225329421107
    75	0.92980597874911
    60	0.95485054561817
    60	0.8753713639181
    60	0.8489278433822
    30	0.93234996072948
    30	0.966867344616
    30	0.94658251862214
    30	0.86651964545681

for which I can find a general linear trend, with a slope of 0.00098. The goal here is to show a slight positive slope, so THIS is good.

However, in the grand scheme of things, this isn't very many data points, and the method has a lot of noise to contend with. I can quantify this with averages and standard deviations at each angle:

    angle avg   stdev
    90    0.256 0.032
    75    0.225 0.010
    60    0.228 0.011
    30    0.231 0.010

What I don't know how to do, though, is state, with a number, **how confident I am of the trend I observe.**

So, reddit, I have a stats book and wikipedia at my disposal, but I have no idea where to start. Wat do? D:

*Edit1:* I think what I want is a confidence interval for the slope of my trend.

*Edit2:* I probably did not make it clear: I'm an engineer, not a statistician. I understand univariate distributions, but that's about it. In other words, I don't know what t-tests are.

*Edit3:* You guys are awesome! I feel way out of my element here though. I'll probably give a shout-out to this page in my thesis as a reference. ;)",en
1107010,2011-03-27 09:31:17,MachineLearning,Any MATLAB user interface mods out there?,gcdk3,bciguy,1275791009.0,https://www.reddit.com/r/MachineLearning/comments/gcdk3/any_matlab_user_interface_mods_out_there/,2.0,13.0,"does anyone know of any matlab hacks that let you alter the appearance of the command prompt and text editor beyond what's available in the preferences menu? This isnt super important, but was just wondering if there was any way to spice up my matlab workday.",en
1107011,2011-03-27 12:18:06,statistics,AIC and BIC model fit statistics- how large does a difference need to be to have practical value?,gcf9h,jjrs,1170375384.0,https://www.reddit.com/r/statistics/comments/gcf9h/aic_and_bic_model_fit_statistics_how_large_does_a/,2.0,28.0,"A while back, I found this excellent paper posted on reddit, about assessing model fit with AIC and BIC statistics.

http://www-stat.wharton.upenn.edu/~steele/Courses/434/434Context/ModelSelection/Zucchini.pdf

But I have to wonder...sometimes I see very slight differences in these statistics between two models, say 140,302 and 140,102, and the authors of the paper triumphantly declare model two to have better fit to the data.

I understand that a very good case can be made for the difference *statistically*...but the values seem almost identical. Sometime I worry that people jump on the difference when it becomes publishable, and don't concern themselves with the practical value of the difference as much as they should.

In practical terms, how much difference does there need to be for the second model to differ enough not just to differ significantly, but to likely differ practically? I expect most people will say something along the lines of ""it depends on the situation"", but that case, what are guidelines for assessing particular situations? Do model fit comparisons have a statistic comparable to say an effect size?",en
1107012,2011-03-27 15:07:44,datasets,Is there any legitimate way I can get the WHOIS information for all .com and .org domains?,gcgif,voltagex,1230861112.0,https://www.reddit.com/r/datasets/comments/gcgif/is_there_any_legitimate_way_i_can_get_the_whois/,1.0,1.0,,en
1107013,2011-03-28 02:34:26,statistics,Ten Tactics used in the War on Error,gcqvc,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/gcqvc/ten_tactics_used_in_the_war_on_error/,3.0,3.0,,en
1107014,2011-03-28 03:17:52,MachineLearning,Algorithmic March Madness: Machines Lock in Victory Over Humans,gcro2,danger_t,1249412021.0,https://www.reddit.com/r/MachineLearning/comments/gcro2/algorithmic_march_madness_machines_lock_in/,15.0,1.0,,en
1107015,2011-03-28 13:17:44,statistics,Competitions in Forecasting and Prediction at Kaggle,gd1jh,tudor96,1274895257.0,https://www.reddit.com/r/statistics/comments/gd1jh/competitions_in_forecasting_and_prediction_at/,8.0,0.0,,en
1107016,2011-03-28 15:26:04,statistics,Ten Tactics used in the war on error (corrected link),gd35d,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/gd35d/ten_tactics_used_in_the_war_on_error_corrected/,0.0,0.0,,en
1107017,2011-03-28 22:12:58,rstats,doRedis: redis as dispatcher for parallel R,gdcb7,fxj,1169393085.0,https://www.reddit.com/r/rstats/comments/gdcb7/doredis_redis_as_dispatcher_for_parallel_r/,1.0,0.0,,en
1107018,2011-03-28 23:33:31,statistics,Question about Mean Difference Hypothesis Test?,gdebl,zazagabor,1295488335.0,https://www.reddit.com/r/statistics/comments/gdebl/question_about_mean_difference_hypothesis_test/,2.0,1.0,"Sorry if I shouldn't to ask for help here, but I'm pretty desperate. 

Normally I would just ask my classmates or check the book, but my other classmates are just as lost as I am, and my professor only uses half-hazardly made power-point presentations which aren't any help at all. 

I'm supposed to "" produce a difference of means test of the null hypothesis that average reelection rates were the same before and after the Watergate scandal."" 

Following the prompt, there is a list of incumbent reelection rates in the U.S House and Senate (separately) from 1964- 2006.

So I understand that the null hypothesis would be something like:
Hsuper^script0 = Msuper^script1 = Msuper^script2

and I understand the formula for the z-score:

	z(Xbar1-Xbar2) = (Xbar1 – Xbar2)	
			  √(s12/n1) + (s22/n2)

But I don't know how to lump it all together. If someone could shed some light on how to approach this problem I'd be grateful. Thank you. 


",en
1107019,2011-03-29 01:21:51,MachineLearning,Using R and snow on Ohio Supercomputer Center’s Glenn cluster,gdgvf,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/gdgvf/using_r_and_snow_on_ohio_supercomputer_centers/,9.0,0.0,,en
1107020,2011-03-29 13:03:58,MachineLearning,Critique this visual taxonomy of pattern recognition methods,gdtv3,speciousfool,1151058853.0,https://www.reddit.com/r/MachineLearning/comments/gdtv3/critique_this_visual_taxonomy_of_pattern/,25.0,28.0,,en
1107021,2011-03-29 16:25:20,statistics,How is the famous gender wage gap calculated?,gdwse,casualfactors,1285978895.0,https://www.reddit.com/r/statistics/comments/gdwse/how_is_the_famous_gender_wage_gap_calculated/,6.0,10.0,"Its one of these things I've always figured is at least slightly bogus, but I know Reddit can help me get to the bottom of it once and for all.",en
1107022,2011-03-29 19:59:19,datasets,IMDB Data and Tools,ge1nz,[deleted],,https://www.reddit.com/r/datasets/comments/ge1nz/imdb_data_and_tools/,2.0,2.0,,en
1107023,2011-03-29 22:40:06,MachineLearning,Data Mining versus Data Protection ,ge5k9,locster,1222291024.0,https://www.reddit.com/r/MachineLearning/comments/ge5k9/data_mining_versus_data_protection/,9.0,2.0,,en
1107024,2011-03-30 02:23:52,artificial,THE FUTURE OF IBM’S WATSON,geaka,DivineOmega,1231805729.0,https://www.reddit.com/r/artificial/comments/geaka/the_future_of_ibms_watson/,1.0,1.0,,en
1107025,2011-03-30 03:45:41,statistics,Difference between Durbin-Watson and ACF?,gecbp,applepiety,1295570442.0,https://www.reddit.com/r/statistics/comments/gecbp/difference_between_durbinwatson_and_acf/,3.0,5.0,Is there an important difference between the Durbin-Watson test and the Autocorrelation function (ACF) used to check for time correlations in a data set?,en
1107026,2011-03-30 09:55:20,statistics,I wish I hadn't dropped out of statistics years ago.  Can someone help me understand how these findings are statistically significant as claimed by the author?,geju2,[deleted],,https://www.reddit.com/r/statistics/comments/geju2/i_wish_i_hadnt_dropped_out_of_statistics_years/,3.0,10.0,,en
1107027,2011-03-30 23:17:13,statistics,Log Likelihood of Kernel Density in R,geyn3,Ellzington,,https://www.reddit.com/r/statistics/comments/geyn3/log_likelihood_of_kernel_density_in_r/,2.0,5.0,"For my advanced Econometrics class, I'm trying to program the log likelihood function of a kernel density estimator with a normal kernel for the purpose of estimating slope coefficients of a standard linear model (y=X%*%B + e).  Does anyone have any pointers on how I should be doing this?",en
1107028,2011-03-31 07:01:24,datasets,x/post from r/GIS: GIS project,gf9jx,jvergara799,1289350337.0,https://www.reddit.com/r/datasets/comments/gf9jx/xpost_from_rgis_gis_project/,1.0,0.0,"I wanted to know if motorcycle travelers that commute from south america to north america, if theres data somewhere. travelers that Do not do it for business but those who do it for adventure...am i screwed? I have to do a concept proposal over again, if not.
",en
1107029,2011-03-31 09:00:51,statistics,Wikipedia's list of 15 statistical paradoxes ,gfc10,thegabeman,1239690590.0,https://www.reddit.com/r/statistics/comments/gfc10/wikipedias_list_of_15_statistical_paradoxes/,8.0,4.0,,en
1107030,2011-03-31 18:41:47,MachineLearning,Evaluation Metrics for Text Extraction Algorithms,gfl6b,tomazk,1229769587.0,https://www.reddit.com/r/MachineLearning/comments/gfl6b/evaluation_metrics_for_text_extraction_algorithms/,0.0,2.0,,en
1107031,2011-03-31 18:45:50,datasets,A million syllabi -- what to do with them?,gflat,kev097,1286915567.0,https://www.reddit.com/r/datasets/comments/gflat/a_million_syllabi_what_to_do_with_them/,4.0,0.0,,en
1107032,2011-03-31 18:46:18,MachineLearning,Is there a TSP solution for multiple stops within a location over multiple days?,gflbc,[deleted],,https://www.reddit.com/r/MachineLearning/comments/gflbc/is_there_a_tsp_solution_for_multiple_stops_within/,0.0,3.0,"Here's an image if it helps my explaining: [http://i.imgur.com/e0AAi.png](http://i.imgur.com/e0AAi.png)

You've got a repairman that travels to different cities to resolve issues. You have to *n* sites, with *x* needing to be visited; city boundaries are really irrelevant but I wanted to play with shapes :D. At the end of the day, the repairman needs to return to Home Base (sleep, eat, and browse Reddit).

Is there a functional TSP w/ time constraints solver online that handles time as the *cost* between sites, or should I keep reading journals on the subject?",en
1107033,2011-04-01 06:13:18,statistics,Save the Statistical Abstract of the United States!,gg1ts,[deleted],,https://www.reddit.com/r/statistics/comments/gg1ts/save_the_statistical_abstract_of_the_united_states/,11.0,0.0,,en
1107034,2011-04-01 15:42:06,statistics,Learning R and statistics..,ggc1i,neurobry,1281362067.0,https://www.reddit.com/r/statistics/comments/ggc1i/learning_r_and_statistics/,11.0,11.0,"So I've found myself in a job field (bioinformatics) where knowing R and more than a smattering of my college intro to statistics would be helpful. I'm relatively comfortable with excel for some t-tests and data manipulation, but I also have a background in computer language programming. I've started watching a few videos on writing R scripts and I'm beginning to feel more comfortable, but I wanted to know if anyone here knew of any obviously great resources out there for learning R + Statistics hand in hand.

Any advice would be greatly appreciated!",en
1107035,2011-04-01 20:29:40,statistics,Working with weighted samples in R,ggjht,kiwipete,1171323108.0,https://www.reddit.com/r/statistics/comments/ggjht/working_with_weighted_samples_in_r/,2.0,12.0,"I'm a relative statistical neophyte who has encountered something new: a dataset with a weights column. All of my R smugness is coming back to bite me now, as I'm told by some SPSS users that, in SPSS, you can simply ""turn on"" weights for a dataset and happily go about your analyses as per normal.

By contrast, it appears that, in R, I must find special weighted versions of all the functions I've grown to know and love. ""mean"" becomes ""weighted.mean""; ""length"" becomes ???

Apart from being a royal pain in the ass, this also strikes me as uncharacteristically inelegant. Moreover, what happens when I need a weighted.ggplot2 or a weighted.DoSomethingElse?

I found a [survey package](http://faculty.washington.edu/tlumley/survey/) that I thought looked like it would do the trick. But that one still looks like I have to use its limited set of special functions to do any analysis.

I'm having a small crisis of R identity here. Help? Is there an elegant way of doing this? I'm being told that weighted samples aren't so rare, so it seems likely that R would have some solid provision for working with them.

**EDIT:** Someone asked what the data might look like. Below is a fictional example.

 trait1 | trait2 | trait3 | expweight
:-------|:-------|:-------|:------------|
 red    | 12.7   | True   | 365.2345
 blue   | 13.6   | False  | 342.2356
 green | 12.95 | False   | 365.2345


",en
1107036,2011-04-01 22:23:07,MachineLearning,Is there standard optimization of Non Negative Matrix Factorization for Features?,ggmg4,ml_zealot,1301685204.0,https://www.reddit.com/r/MachineLearning/comments/ggmg4/is_there_standard_optimization_of_non_negative/,5.0,7.0,"Does anyone know of a documented approach to optimizing the number of features to use in applying non negative matrix factorization.  
(don't want features with lowly relevant items contained within, want to optimize the number of features)",en
1107037,2011-04-01 23:56:49,datasets,[request] The Fed emergency discount window data,ggorj,tony_bruguier,,https://www.reddit.com/r/datasets/comments/ggorj/request_the_fed_emergency_discount_window_data/,1.0,0.0,"The United States' Federal Reserved was sued into providing the detail of their emergency loans:

http://marketplace.publicradio.org/display/web/2011/03/31/am-fed-releases-secret-information/

I can't find the information anywhere. Is there data available somewhere? This way we could skip the pointless babble and look at the numbers.

Thanks,
Tony",en
1107038,2011-04-02 00:23:23,MachineLearning,[impressive] Zdenek Kalal's object tracking algorithm learns on the fly,ggpch,tomazk,1229769587.0,https://www.reddit.com/r/MachineLearning/comments/ggpch/impressive_zdenek_kalals_object_tracking/,60.0,12.0,,en
1107039,2011-04-02 09:05:44,MachineLearning,cnc contour cutting machine Offer CHINA Dongguan,ggyf8,techberth,1298470614.0,https://www.reddit.com/r/MachineLearning/comments/ggyf8/cnc_contour_cutting_machine_offer_china_dongguan/,1.0,0.0,,en
1107040,2011-04-02 09:41:03,statistics,Does anyone have institutional access to the IMD World Competitiveness Yearbook?,ggyw7,Brimlomatic,1239639518.0,https://www.reddit.com/r/statistics/comments/ggyw7/does_anyone_have_institutional_access_to_the_imd/,1.0,2.0,"One of my classmates is set on using data from the IMD World Competitiveness Yearbook 2010 for a paper, but by the time the bureaucracy at our library gets around to getting him access, it will be quite close to the deadline. Since I've helped him find stuff online before he asked me about it, but I can't seem to find this data online anywhere, so I turn to Reddit. Can anyone here point me in the right direction? Thanks!",en
1107041,2011-04-02 20:37:52,statistics,Question about selecting cases.,gh6ko,xxgambinoxx,,https://www.reddit.com/r/statistics/comments/gh6ko/question_about_selecting_cases/,2.0,0.0,"Does anyone know how to select a case at the earliest event? 5 dummy variables and for each variable I'm trying to select the time in months that an event the first event took place. Using SPSS.

1 Patients used in the example below but have many more.

    Months P      Y      Q 	G	R
	0	0	1	0	1	0
	3	0	1	0	1	0
	6	1	1	0	1	0
	9	1	1	0	1	0
	12	1	1	0	1	0
	15	1	1	0	1	0
	18	1	1	0	1	0
	21	1	1	0	1	0
	24	1	1	0	1	0
	27	1	1	0	1	0
	30	0	1	0	1	0
",en
1107042,2011-04-03 17:45:31,statistics,Getting the Right Answer,ghngk,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/ghngk/getting_the_right_answer/,0.0,0.0,,en
1107043,2011-04-04 00:21:28,statistics,Relative Risk vs. Lift,ghuid,suckeggmule,1250642053.0,https://www.reddit.com/r/statistics/comments/ghuid/relative_risk_vs_lift/,2.0,2.0,"It has never seemed exactly right to me to use an overall (or marginal) ""rate of X"" for comparison with a sub-population of interest; because the overall probability may be diluted or intensified by the sub-population rate and sort of already includes itself and thus is already stacking the deck.   Why not always split the groups and do the comparison directly?

Case in point:  Relative Risk vs. Lift.

RR compares the two sub-populations directly.  Lift compares a sub-population with the overall rate.

Example:  100 people with low and high blood pressure, and whether they eventually died from cardiovascular disease:

Low BP, No:  36
Low BP, Yes: 4
High BP, No: 40
High BP, Yes: 10

Relative Risk (of dying from High BP) = .2 / .1 = 2

Lift (High BP =&gt; Death) = .2 / .156 = 1.28

Lift apparently yields a diluted effect.  But I doubt it is fair to say this is ""wrong"".

For general understanding, what is the theory/logic difference of these two approaches?
",en
1107044,2011-04-04 03:25:46,statistics,HELP! Time-series analysis in STATA,ghy1l,x540x,1222953769.0,https://www.reddit.com/r/statistics/comments/ghy1l/help_timeseries_analysis_in_stata/,3.0,3.0,"I'm doing statistics homework and I've run into a problem.  We are working with time-series data in STATA.  I have set a up a date variable to indicate that the data is time-series, and the next part of my homework is this:


Using Stata, generate the trend variable and determine whether linear or 
quadratic trend fits the gas sales data better (when looking at the trend models do 
not include the explanatory variables). Explain which model you choose and why. 
Include your Stata outputs. 



My question is: What is the process to create a trend variable? and how do you do this *without* including explanatory variables?  

Thanks for your help.


Edit 1: If you need any additional info please just ask.",en
1107045,2011-04-04 10:37:47,computervision,The project page of GPLed feature tracking algorithm doing rounds in /r/programming,gi64g,irve,1283714060.0,https://www.reddit.com/r/computervision/comments/gi64g/the_project_page_of_gpled_feature_tracking/,7.0,1.0,,en
1107046,2011-04-04 15:24:16,statistics,Pros and Cons of Bayesian/Frequentist aproaches,gi9is,beandipper,1282647848.0,https://www.reddit.com/r/statistics/comments/gi9is/pros_and_cons_of_bayesianfrequentist_aproaches/,12.0,32.0,I would like to get a conversation on the pros and cons of each camp. Hopefully there are enough frequentists on reddit. ,en
1107047,2011-04-04 23:24:34,datasets,"Save the Data: Tell Congress not to pull the plug on Data.gov, USAspending.gov, and more.",gikv5,kev097,1286915567.0,https://www.reddit.com/r/datasets/comments/gikv5/save_the_data_tell_congress_not_to_pull_the_plug/,1.0,0.0,,en
1107048,2011-04-05 04:55:33,datasets,Quora: Where can I get large datasets open to the public,gisk3,ENOTTY,1238700510.0,https://www.reddit.com/r/datasets/comments/gisk3/quora_where_can_i_get_large_datasets_open_to_the/,2.0,0.0,,en
1107049,2011-04-05 06:05:22,datasets,"List of public transit agencies offering Google Transit Data (Bus stop locations, schedules, etc.)",giu0d,jbermudes,1287384893.0,https://www.reddit.com/r/datasets/comments/giu0d/list_of_public_transit_agencies_offering_google/,1.0,0.0,,en
1107050,2011-04-05 09:01:57,MachineLearning,"Amazing object tracking using webcam even if scale changes, unstable camera, rotation etc. Use your webcam as virtual mouse!",gixky,sytelus,1143287988.0,https://www.reddit.com/r/MachineLearning/comments/gixky/amazing_object_tracking_using_webcam_even_if/,7.0,4.0,,en
1107051,2011-04-05 17:11:28,statistics,Help with appropriate regression technique?,gj3qr,abitofperspective,1272704550.0,https://www.reddit.com/r/statistics/comments/gj3qr/help_with_appropriate_regression_technique/,4.0,15.0,"I've reached the outer limits of my statistical knowledge for a research project I'm considering, and was wondering if anyone could help to point me in the direction of what kind of what kind of analysis I should be considering

Say (for example): I want to determine the extent to which trade (T) between two countries (A &amp; B) is explained by governmental links (L) between them (e.g. shared membership in organisations like the UN, European Union, etc). My model would basically be:

T*_AB_* = X*_1_*L*_AB_* - where X*_1_* is a random coefficient

Of course, I would add covariates for gross domestic product of each country (GDP), and population (POP), so it would actually be:

T*_AB_* = X*_1_*L*_AB_* + X*_2_*GDP*_A_* + X*_3_*GDP*_B_* + X*_4_*POP*_A_* + X*_5_*POP*_B_* where X*_1 - 5_* are random coefficients

I actually have a dataset ready that has columns for T, L, Name of A, Name of B, GDP(A), GDP(B), POP(A), POP(B). But my instincts tell me there is a problem with running this as an OLS regression in that each line is not independent, because country A is included multiple times. For example, I have a line for USA-France, another line for USA-Germany, another for USA-Spain - all have the same values for GDP of the USA and POP of the USA.

Any advice on what kind of analysis is appropriate here?",en
1107052,2011-04-05 19:36:40,statistics,How do you check if a die is fair?,gj70x,isarl,1222268050.0,https://www.reddit.com/r/statistics/comments/gj70x/how_do_you_check_if_a_die_is_fair/,9.0,23.0,"There's a very detailed page on Wikipedia about [checking whether a coin is fair](http://en.wikipedia.org/wiki/Checking_whether_a_coin_is_fair), but rolling a die is not a binary outcome.  Let's assume you're using an ordinary six-sided die, but you don't know whether it's fair.  Obviously, you would want to roll it a number of times and count how often each face comes up.  It's also obvious that the expected value for the number of rolls of each face is a sixth of the total number of rolls.

Would the most appropriate test be to treat each the number of rolls possible value (i.e. 1 through 6) as separate binomial distributions and check that they're close enough to the expected value, where ""close enough"" is determined by your desired confidence level?  Or is there a better way to do the test?

*edit:* The test I proposed seems like it generalizes to *n*-sided dice very easily, and would be simple to adapt to dice numbered strangely - for instance, I saw a die the other day numbered 0, 1, 1, 1, 2, 3.  For this die, 1 would have an expected value 3 times higher than the other sides, but if this is taken into account it seems like you could still check it with the test I proposed, if that test is any good.",en
1107053,2011-04-05 20:01:47,statistics,"Studying for the Psychology GRE, need help on a stats question",gj7oz,kramsy,1273239866.0,https://www.reddit.com/r/statistics/comments/gj7oz/studying_for_the_psychology_gre_need_help_on_a/,0.0,10.0,"The question goes:

On a test with a positively skewed distribution, one student recieved the mean score, one student received the mean score and one student received the mode score.  Which of these scores has the greatest percentile ranking?
* The mean
* The Median
* The Mode
* All 3 are equal
* It cannot be determined with the data given

 As such, wouldn't the answer be E-Answer cannot be determined.  The fact that the distribution is positively skewed suggests that the mean is higher than the median, but shows no relation of either tendency compared to the mode.  It is possible that the mode could be higher than the mean, right?  The correct answer on the GRE was A.",en
1107054,2011-04-06 07:08:07,statistics,Having a lil trouble with exponential trend and PHSTAT 2.5,gjnim,chicos240,1222659512.0,https://www.reddit.com/r/statistics/comments/gjnim/having_a_lil_trouble_with_exponential_trend_and/,2.0,0.0,"I need some help, i cant quite get the right results with PHstat.  I doing an anylsys of some data and its suppsed to be exponential trend and the book shows a table that clearly says exponential trend analysis.  I just can seem to find it.  Im sure its not hiding, im just stuck. 
for example, [slides](http://imgur.com/a/dpibj) show four columns.  The last column i just dont get.  which i think is my first problem.  I need to be able to compute the log, in order to create the table.  
 ",en
1107055,2011-04-06 20:04:49,statistics,"Simple Linear Regression, Accuracy of predicted y hat",gk198,dutch245,1209500254.0,https://www.reddit.com/r/statistics/comments/gk198/simple_linear_regression_accuracy_of_predicted_y/,3.0,9.0,"I'm working on a Simple Linear Regression problem on a sample test for my statistics class. I'm able to recreate the ANOVA table, perform hypothesis testing on rho and predict a y hat value given an x. But my professor then asks ""How accurate is your answer?"" How the hell do I figure that out?",en
1107056,2011-04-06 23:04:37,statistics,Randall Munroe understands multiple testing,gk5wu,[deleted],,https://www.reddit.com/r/statistics/comments/gk5wu/randall_munroe_understands_multiple_testing/,49.0,4.0,,en
1107057,2011-04-07 07:22:06,statistics,Probability,gkh7g,tsiegel22,1301436878.0,https://www.reddit.com/r/statistics/comments/gkh7g/probability/,0.0,3.0,"I haven't even taken a stat class since I am only a freshman but I was watching a show and had a question. If you have 1/x chance of picking an item and are giving y chances to pick it, what is you chances of picking that item?",en
1107058,2011-04-07 21:28:06,statistics,What can I do with normal data that was transformed from non-normal data?,gkx9y,halasjackson,1289490537.0,https://www.reddit.com/r/statistics/comments/gkx9y/what_can_i_do_with_normal_data_that_was/,3.0,3.0,"Here's the scenario: I have a large sample (thousands) that is non-normal, but it transforms well (good lambda) into a normal data set.

Now, when I calculate Z-scores to find out that X% of my data is above/below a certain value, how does that translate back to the original data set? Obviously, the range of my tranformed data is now different, so how do the tests (others too, including capability, stability, etc.) I run on the transformed data apply to my original dataset?

Thanks in advance!!!

**Edit 1:** Box-Cox was trasnformation.",en
1107059,2011-04-08 00:51:34,statistics,Regression of count data,gl2bu,Case_Control,1263273609.0,https://www.reddit.com/r/statistics/comments/gl2bu/regression_of_count_data/,5.0,15.0,"So I'm having a disagreement with one of the people I work with when it comes to modeling some count data and I figured I'd get your thoughts reddit.  We are currently running a negative binomial regression on the outcome ""number of sexual partners."" For the sake of argument lets assume that it isnt zero inflated. My co-worker isnt comfortable with this because people who say zero are ""conceptually different"" from those who have had sex; co-worker wants to divide the outcome into 6ish categories and run a multinomial logistic regression.  I have trouble seeing how the fact that the zeros are conceptually different hurts the validity of negative binomial regression for count data, zeros are conceptually different in almost all count data and we run negative binomials and poissons without batting an eye. What say you reddit? ",en
1107060,2011-04-08 02:49:19,statistics,I'm trying to model a discrete valued time series for prediction purposes. Any suggestions? (Sample plot inside),gl4uu,roger_,1178076247.0,https://www.reddit.com/r/statistics/comments/gl4uu/im_trying_to_model_a_discrete_valued_time_series/,1.0,8.0,"[Here's a plot of the data](http://imjoi.com/i/dfgg.png). The y-axis is the count since `t = 0`.

The data is non-stationary (it has daily periodicities which aren't shown) and correlated/""bursty"", so that events tend to happen in sequence (not too obvious in that plot).


What would be a good time series model for this? I've looked at Poisson models, but I'm not sure how to incorporate the correlation.


Any suggestions would be appreciated, thanks.",en
1107061,2011-04-08 07:25:30,statistics,Hadley Wickham on plyr (The Split-Apply-Combine Strategy for Data Analysis in April 2011 issue of Journal of Statistical Software),glapp,mycatharsis,1294046118.0,https://www.reddit.com/r/statistics/comments/glapp/hadley_wickham_on_plyr_the_splitapplycombine/,9.0,2.0,,en
1107062,2011-04-08 11:03:09,statistics,Question: Is there a formula to work out the proof of an event happening via tweets?,gled3,jjmcnab-za,1242291128.0,https://www.reddit.com/r/statistics/comments/gled3/question_is_there_a_formula_to_work_out_the_proof/,0.0,1.0,"Hi there. 

First off a hypothetical.

Lets say there is a shooting, Someone tweets that there has been a shooting (person X). Then from that one tweet, we see people retweeting person X's tweet. From that we also have people reporting on that tweet. But we also see other people tweeting that the there was a shooting ( which they saw, like person X)

So we have

Original first tweeter. (Tweeting first hand information) person x
Other original tweets ( Tweeting first hand Information) People y
People who retweet the tweets of X and Y
Then those that report on the tweets of X and y but do not have first hand information. 


Is there a way to use this information to work out if there hand in fact been a shooting?

Am I missing variables? or am I just talking shit from my very basic understanding of statistics? 

Do you are understand what I am asking?

",en
1107063,2011-04-09 08:16:13,statistics,auto-correlation function for dummy,gm092,Damark81,1274898720.0,https://www.reddit.com/r/statistics/comments/gm092/autocorrelation_function_for_dummy/,7.0,5.0,"Dear reddit stats, 
Could you guys point me to some simple reading materials about analysis using auto correlation function, particularly about deciding whether the level of auto correlation function is significant or not, and also using acf to analyze function that contains multiple hidden signals please?
Thank you very much. ",en
1107064,2011-04-09 11:41:04,MachineLearning,New versions of GGobi and rggobi for Windows users,gm2qf,[deleted],,https://www.reddit.com/r/MachineLearning/comments/gm2qf/new_versions_of_ggobi_and_rggobi_for_windows_users/,2.0,1.0,,en
1107065,2011-04-09 11:41:49,statistics,New versions of GGobi and rggobi for Windows users,gm2qm,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/gm2qm/new_versions_of_ggobi_and_rggobi_for_windows_users/,3.0,0.0,,en
1107066,2011-04-10 00:32:56,statistics,"Ask /r/statistics: If x_t has an order of integration d=1, does dx_t has an order of integration d=0?",gmd5o,cbrunos,,https://www.reddit.com/r/statistics/comments/gmd5o/ask_rstatistics_if_x_t_has_an_order_of/,2.0,2.0,"Hello,

so I am doing some homework and have a question to ask to time series specialists. I have a process, let's call it lm2, which isn't stationary (found out thanks to an augmented Dickey-Fuller test). So now I differentiate it and define a new variable dlm2=lm2_t-lm2_{t-1}. 

dlm2 is stationary (again augmented Dickey-Fuller test). So now I decide to continue my work with dlm2, and fit an ARIMA(p,d,q). Which value does d take? 0? or 1? 

dlm2 is stationary, so it doesn't require any further differentiations, so it should be 0, no? Had I choosen to work with lm2 then it would have been d=1, right?


Thanks to whoever may help me!",en
1107067,2011-04-10 03:27:29,datasets,Data.gov Newest Datasets -- RSS feed,gmg03,zach_will,1283191054.0,https://www.reddit.com/r/datasets/comments/gmg03/datagov_newest_datasets_rss_feed/,1.0,0.0,"I ended up writing an [RSS feed](http://pipes.yahoo.com/pipes/pipe.info?_id=4f0e59d1bb358ad9e8bf9e8b086f5ade) for the new datasets on data.gov — I'm much more likely to check Google Reader every day rather than the actual site.

If you want to know how I wrote it (I just used YQL and Yahoo Pipes), then you can check out my simple [how-to blog post](http://zachwill.posterous.com/datagov-rss-feed).",en
1107068,2011-04-10 07:46:05,statistics,How do I determine if failures are independent or occur in groups?,gmk1q,[deleted],,https://www.reddit.com/r/statistics/comments/gmk1q/how_do_i_determine_if_failures_are_independent_or/,7.0,6.0,"We have a product that consists of 50 components that are produced in one batch. What's the best way about determining whether or not the failure rate is independent? Should I do a GOF test against the binomial distribution?

This is my first real-world stats problem.

Edit: Thanks for the quick responses! I think I will try the bootstrap method, and go from there.",en
1107069,2011-04-10 14:40:22,MachineLearning,Data analysis between laptop and server: Fast Two Way Sync in Ubuntu!,gmo37,[deleted],,https://www.reddit.com/r/MachineLearning/comments/gmo37/data_analysis_between_laptop_and_server_fast_two/,0.0,0.0,,en
1107070,2011-04-10 23:00:29,statistics,"Hey r/stats, question regarding hierarchical(?) data input in SPSS",gmv3p,thishitrighthere,1291357186.0,https://www.reddit.com/r/statistics/comments/gmv3p/hey_rstats_question_regarding_hierarchical_data/,1.0,4.0,"Hey stats, first time posting. I have a question that I haven't found an answer for online...

I have both female and male patients in a study. Some of the females have had 0 children, and some have had 6. For each pregnancy, I am documenting the 1) year of delivery, 2) type of delivery (c-section vs vaginal), and 3) the birth weight of the child. 

How do I include these variables in my database without having to list these variables 6 times? Is there a way to only have to fill out this data if the patient has had at least 1 delivery? Thanks for your time! :)",en
1107071,2011-04-11 01:54:42,MachineLearning,"2011 Predictive Analytics Challenge Winner: Machines Beat Humans predicting the NCAA Tournament
",gmyeo,srt19170,1173365435.0,https://www.reddit.com/r/MachineLearning/comments/gmyeo/2011_predictive_analytics_challenge_winner/,8.0,1.0,,en
1107072,2011-04-11 05:59:33,statistics,Question RE: Elimination of Factors in Factorial Design,gn3dr,willowthewizard,1302193398.0,https://www.reddit.com/r/statistics/comments/gn3dr/question_re_elimination_of_factors_in_factorial/,6.0,2.0,"Greetings /r/statistics,

First time venturing into a subreddit and would like to know if someone can provide a bit of guidance on a small (i hope) problem I'm grappling with.  

I've collected some raw data on various sports teams and pared some of it down to what I feel could be potential relevant factors on the end-response (wins).  However, when I run this through Minitab for analysis (Stat|ANOVA|GLM) I am never able to actually get any relevant data.  I get a bunch of sequential SS's but nothing more (ie, no useful p-values).  

I am collecting this data to make a binary logistic regression.  Essentially what it boils down to is my result show that NONE of my factors/interactions are significant UNTIL I remove a factor.

TLDR: Are there any other ways to eliminate factors from a design without using an ANOVA?  I had initially thought that fitted line plots could be a solution; however, rethinking this, this solution doesn't really take into effect potential interactions between factors.  Ideas?

*EDIT: Added further details of regression model.*",en
1107073,2011-04-11 17:54:51,datasets,Where can i download netflix database for my experiments?,gnfca,Exibus,1296002350.0,https://www.reddit.com/r/datasets/comments/gnfca/where_can_i_download_netflix_database_for_my/,1.0,0.0,.,en
1107074,2011-04-11 18:43:42,statistics,A new version of the plyr package (1.5),gngho,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/gngho/a_new_version_of_the_plyr_package_15/,9.0,0.0,,en
1107075,2011-04-11 21:44:36,data,Trademarcx - An online database of over 8 million trademarks,gnl1d,housepage,1214782425.0,https://www.reddit.com/r/data/comments/gnl1d/trademarcx_an_online_database_of_over_8_million/,2.0,0.0,,en
1107076,2011-04-12 04:51:32,statistics,"Want to earn an average of $8,455 more per year - move to a slow growth metro area.",gnv46,msum,1296090716.0,https://www.reddit.com/r/statistics/comments/gnv46/want_to_earn_an_average_of_8455_more_per_year/,6.0,3.0,,en
1107077,2011-04-12 05:56:36,statistics,Help with Lognormal Distributions,gnwnh,thunderhorse222,1302576598.0,https://www.reddit.com/r/statistics/comments/gnwnh/help_with_lognormal_distributions/,7.0,9.0,"I'm having some difficulty in thinking through this.  I have a list of values that represent a sample for a population: 1.16, 1.24, 1.36, 1.46, 1.85, 2.33, 2.53.  The mean of the ln(x) is 0.49, with a std. dev of 0.31.  However, it is known that the sample represents the tail of the distribution and that by definition, the mean of the sample is cumulative probability of 10% of the population.  How do I calculate the true population mean, and population standard deviation?",en
1107078,2011-04-12 13:47:10,MachineLearning,"A huge, up to date tutorial about using graphical models for web-scale applications, by Alex Smola. Starts out fairly basic, but gets pretty deep. [Warning: 300 page PDF]",go4g6,urish,1221689900.0,https://www.reddit.com/r/MachineLearning/comments/go4g6/a_huge_up_to_date_tutorial_about_using_graphical/,33.0,3.0,,en
1107079,2011-04-12 15:52:56,statistics,School Project: Looking for Ideas,go6cf,roger_pct,1232823670.0,https://www.reddit.com/r/statistics/comments/go6cf/school_project_looking_for_ideas/,1.0,12.0,"I am taking a statistics class in which I have a term project to do which is fairly open ended. I am looking for ideas, and thought the reddit community might be interested in helping...as I may be using Reddit as a tool for gathering data.

Do any ideas pop out to you as an interesting question to answer?

Examples

Does gender affect political choice?

Does education level affect time spent reading leisurely?

Does anonymity make it easier to lie? easier to tell the truth?

(How) Does annual income affect debt levels?


I am just beginning to explore this project, so I haven't given it much thought. I just thought it might be fun if there were some ideas out there that are not springing to mind that others would like to see a statistical analysis of.",en
1107080,2011-04-12 16:13:35,statistics,EC2 AMI for scientific computing in Python and R,go6p8,agconway,1228006618.0,https://www.reddit.com/r/statistics/comments/go6p8/ec2_ami_for_scientific_computing_in_python_and_r/,12.0,6.0,,en
1107081,2011-04-13 01:28:24,statistics,I'm teaching myself statistics. Does anyone have textbook or open courseware suggestions? [x-post from askreddit],goks7,avianattackarmada,1274660347.0,https://www.reddit.com/r/statistics/comments/goks7/im_teaching_myself_statistics_does_anyone_have/,12.0,25.0,"I'm a biologist and somehow didn't take statistics in college. I'm at a pretty basic level with stats, but I have a good grasp on mathematics and calculus. I would like a text and course that is thorough and challenging. I'm interested in understanding what I am doing and why.

Does anyone have any suggestions? I'm also looking for suggestions for more advanced courses/texts after I get through the basics.

Thanks (just found this subreddit)

*Edit:* Thanks for everyone's suggestions.  I ordered a few books.",en
1107082,2011-04-13 02:54:08,statistics,I'm teaching myself statistics. Does anyone have textbook or open courseware suggestions? [x-post from askreddit],gompd,[deleted],,https://www.reddit.com/r/statistics/comments/gompd/im_teaching_myself_statistics_does_anyone_have/,1.0,0.0,"I'm a biologist and somehow didn't take statistics in college. I'm at a pretty basic level with stats, but I have a good grasp on mathematics and calculus. I would like a text and course that is thorough and challenging. I'm interested in understanding what I am doing and why.

Does anyone have any suggestions? I'm also looking for suggestions for more advanced courses/texts after I get through the basics.

Thanks (just found this subreddit)",en
1107083,2011-04-13 12:40:08,computervision,Help needed: image registration of slightly rotated apples.,goy15,Feesje,1278158276.0,https://www.reddit.com/r/computervision/comments/goy15/help_needed_image_registration_of_slightly/,3.0,10.0,"Hey guys. I'm having a lot of problem with registering two apples who are imaged from close by. 

Just to give you a sense of the problem I'm facing I've uploaded some data examples : http://imgur.com/a/uq0TS#ICkue
First two pictures showing a successful match and then a sequence where the same apple is slightly rotated three times. 

My current procedure exists of Sift feature detection, matching and using ransac to apply a transformation model to it. 

This procedure only works for apples with lots of features. And I'm a bit stuck for ideas. 
So I was hoping if any of you guys have any good ideas. The literature wasn't of much help for me.

The data is currently being used for a bio project together with it's chlorophyll images. 
If it doesn't work out then we'll probably add markers to the apples next time and ignore the current data.

Note: It doesn't matter that the pictures perfectly align(impossible anyway), but that the general structure is about the same spot.
",en
1107084,2011-04-13 17:34:48,statistics,"Need help on a statistics problem.  don't want answer, just want to know what it's called so I can figure it out myself.",gp2uy,scr1be,1230923108.0,https://www.reddit.com/r/statistics/comments/gp2uy/need_help_on_a_statistics_problem_dont_want/,1.0,5.0,"xx% of ABC are owned by men.

sample of 300 ABCs is taken.

what is the probability that the sample proportion will be (+)(-) .04 of the population proportion?

-------------


Can somebody tell me what area of statistics this is or direct me to an online tutorial in this area?  

Been a long time since I've taken stats.",en
1107085,2011-04-13 17:41:01,computervision,Which fast face detection algorithms are used on consumer cameras?,gp2zy,0ld,1261047157.0,https://www.reddit.com/r/computervision/comments/gp2zy/which_fast_face_detection_algorithms_are_used_on/,5.0,5.0,"Face detection is a very old problem and even though it is considered ""solved"" by many, getting face detection to run in what I would consider ""real-time"" (30 fps at 640x480) on a regular desktop computer seems almost impossible to me.

Which clever algorithms run on consumer cameras? They seem to be capable of detecting faces in real-time with very limited computing power. 

The fastest algorithms I know of are built around boosted cascades as used by Viola&amp;Jones. But these still need to scan all possible face positions on different scales, which takes too long for my definition of real-time performance unless you parallelize this heavily. So what did I miss?",en
1107086,2011-04-13 18:08:36,statistics,"Publication bias attenuates effect estimates away from the null, invalidating most meta-analyses [pdf, x-post from /r/science]",gp3pg,[deleted],,https://www.reddit.com/r/statistics/comments/gp3pg/publication_bias_attenuates_effect_estimates_away/,20.0,4.0,,en
1107087,2011-04-13 21:41:38,MachineLearning,A stand alone simulated annealing package in python,gp9gg,pwoolf,1168034507.0,https://www.reddit.com/r/MachineLearning/comments/gp9gg/a_stand_alone_simulated_annealing_package_in/,14.0,2.0,,en
1107088,2011-04-13 21:53:53,statistics,Card shuffling analysis with real human data and python scripts!,gp9s6,pwoolf,1168034507.0,https://www.reddit.com/r/statistics/comments/gp9s6/card_shuffling_analysis_with_real_human_data_and/,1.0,0.0,,en
1107089,2011-04-13 22:23:24,statistics,An introduction to survival analysis inspired by xkcd.,gpakv,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/gpakv/an_introduction_to_survival_analysis_inspired_by/,18.0,2.0,,en
1107090,2011-04-14 00:37:20,statistics,Software to discover trends in data and output an equation of them?,gpe34,realigion,1289539552.0,https://www.reddit.com/r/statistics/comments/gpe34/software_to_discover_trends_in_data_and_output_an/,2.0,2.0,"Hey everyone. I have a job with 2 sets of data (inventory and price) and the task is to come up with an optimum ""predicted"" price. I have access to all previous data (inventory amounts and the price at which that inventory sold). 

I think this is kind of an odd task, but the only way I imagine doing it is by finding the trends between x (inventory) and y (price of sale). I remember stumbling upon software that does this sort of thing a long while ago on Reddit but apparently didn't favorite it. 

I know I could just use best-fit-line but I'm not sure if the data is (or always will be) linear and I presume that it wouldn't. 

**TL;DR: 2 sets of data, need to find what sort of trend they make and the equation that defines that trend.**

EDIT: Also, I do not have any sort of programming experience related to this area :/",en
1107091,2011-04-14 02:51:52,statistics,Slutsky and Dickey-Fuller are the least sexual people I can imagine,gph90,javes1,1288135284.0,https://www.reddit.com/r/statistics/comments/gph90/slutsky_and_dickeyfuller_are_the_least_sexual/,0.0,1.0,,en
1107092,2011-04-14 04:11:18,statistics,Can anyone help me understand d prime statistics and Signal Detection theory?,gpj0n,ckvp,1271808954.0,https://www.reddit.com/r/statistics/comments/gpj0n/can_anyone_help_me_understand_d_prime_statistics/,3.0,2.0,"I understand why they are used, but how do they work, how do I do them, etc. I would love for a crash course in them.",en
1107093,2011-04-14 11:39:03,MachineLearning,R 2.13 released - with impressive gains in performance!,gprs8,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/gprs8/r_213_released_with_impressive_gains_in/,5.0,0.0,,en
1107094,2011-04-14 13:26:33,MachineLearning,Rstat x-post: new R release compiles code.,gpt3i,[deleted],,https://www.reddit.com/r/MachineLearning/comments/gpt3i/rstat_xpost_new_r_release_compiles_code/,15.0,2.0,,en
1107095,2011-04-14 14:24:38,statistics,Modeling marketing research problem - dependent variables?,gptrz,[deleted],,https://www.reddit.com/r/statistics/comments/gptrz/modeling_marketing_research_problem_dependent/,3.0,3.0,"Hi,
My goal is to estimate the sales of 3 different companies in a city market, please assume that there is no preliminary data for this. The city is divided into 4 zones, and I wish to get estimates for all the zones.

For this, I plan to sample certain stores and extract information of the 3 companies' sales. The stores are classified based on size and location.

Hence,
Dependent variables = 3 ? (because of each of 3 companies's sales)

Independent variables = 2 ? (location and size)

Please tell me if I have the right idea about dependent variables! Thanks",en
1107096,2011-04-14 16:03:50,MachineLearning,"If You Can Tell Boys From Girls, the Air Force May Give You 20 Grand",gpvec,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/gpvec/if_you_can_tell_boys_from_girls_the_air_force_may/,9.0,10.0,,en
1107097,2011-04-14 17:04:29,statistics,Recommended books on marketing research,gpwo0,[deleted],,https://www.reddit.com/r/statistics/comments/gpwo0/recommended_books_on_marketing_research/,3.0,2.0,Could you all suggest books on this? Muchos thanks!,en
1107098,2011-04-14 18:55:56,statistics,Has anyone run an ARIMA model using R? I could use some assistance with interpretation.,gpzkx,Recamen,1291391511.0,https://www.reddit.com/r/statistics/comments/gpzkx/has_anyone_run_an_arima_model_using_r_i_could_use/,6.0,19.0,"So long story short, I'm running some ARIMA models for a class project, but I'm uncertain as to how to interpret the models in order to properly convey what's going on. What I have right now is an ARIMA model with orders (14,1,5), where 14 is the AR order, 1 is the degree of differencing, and 5 is the MA order. I tried putting in mod.arima$model (I named my model ""mod.arima""), but the resulting output is overwhelming. I get what phi and theta are for, but nothing beyond that. Is there anyone/where I could get an explanation from?

(In addition, the ARIMA models are run on time series data involving closing daily stock prices. The reasoning behind the ARIMA models is that in doing so, I get forecasting data that I can then use in a distortion risk measure statistical estimator developed in another paper. Why ARIMA models? Well, it's because that's what I said I would use in my project proposal, so that's what I have to stick with. In addition, I am using three months' worth of data--I've tried two years' worth and one year's worth, and in both cases the resulting models would be far too large to really say much about. I'm therefore going small in order to keep it simple.)",en
1107099,2011-04-15 00:24:21,MachineLearning,"Prospects for a Phd with a bad GPA, without paper and slightly overaged(26)? ",gq8dy,agz,1297369294.0,https://www.reddit.com/r/MachineLearning/comments/gq8dy/prospects_for_a_phd_with_a_bad_gpa_without_paper/,12.0,39.0,"Hi guys,

I will graduate in June and several years back then, I thought that I would try to apply for Phd programme in Europe. Now the time has come so I'm looking at my options and I would ask for your thoughts on it. About my chances the bad thing is that I have a horrible gpa(3.7 out of 5, in a not top500 university), not really good at math (I can understand proofs like Hopfield Net convergence, although never was top in class), and I'm quite old (turned into 26 yesterday). The good news are that I had several side projects, maybe I'm not braindead(iq ~131, standard mensa, although I assume it's normal within phd), and I have a quite good thesis from which I could extract a paper if would want to(at least I think). So my questions:

1. What do you think, what are my odds getting into a funded phd programme in Europe(ger, gb, ireland, nl, ..)? I've searched for phd-sites but I haven't managed to find, so is there a unified ""phd-search-in-europe"" site? :O

2. A friend interested in doing it in the USA and have done GRE and all the stuff. What are the chances having a phd there(not into ivy league, ""just"" top50 us university - or it's not worth it?) Also odds in other parts of the world?

3. Am I too old, so I should get a life instead? :) I don't have problems with the industry, I just try to avoid ending up with a boring ""business logic"" job. Also I'm not in love with science or anything, I just like to think long on problems(walking in circles at midnights) and I would be dissatisfied if I should do this with problems I don't think worth it. Yeah it sounds lame and mediocre, and I'm aware that I'm not a top shot, but anyway I do this stuff. 

I've made my cv for an internship position, so I would humbly ask you guys, what do you think about my prospects/what should I do? Thank you for any answer or sharing an experience related!



Projects (reverse time order):
- Thesis:
  :: Audio signal classification by computer vision techniques:	
  - Energy based learning: Restricted Boltzmann Machines, KMeans, CMeans 
  - Support Vector Machines  

- Erasmus Summer Scholarship at a top100 university:
  :: Evolving Neural Nets as Controllers done in clojure 
  - Boid/flocking environment simulation ""library""
  - Functional neural net ""library""
  - Topological evolution (NEAT)
  - Framework and representation for evolving neural nets

- Erasmus Scholarship at a top100 university:
  - Data Mining, Bioinformatics, Cognitive Categorization, ML, SVM courses

- Laboratory Work:
  :: Algorithms for a GSM based positioning system
  - Bayesian filtering and inference

- Other side projects I had:
  - Evolutionary Algorithms: Differential Evolution and it's derivatives
  - Compiler for a functional language idea I have (not succeed yet)
  - Several Sport predictor systems (not succeed)


Technical Expertise:
- Programming Languages:
  - Good(1+ year continuous use): Common Lisp, Clojure, Matlab, C++

- Interests:
  - Machine Learning, Ai, Data Mining 
  - Neurobiology, Evolutionary Psychology, ""Mind"" stuff
  - Evolutionary Algorithms
  - Functional Programming
  

EDIT: Several grammar errors here and there(Although I'm sure there are still more - anyway this is time for justice :)

EDIT2: Thank you guys for the comments, they are really valuable for me! 
I'm not that harsh on myself, I simply doesn't have a clear picture and wanted feedback(and yes maybe presented myself lower to prevent real dissapointment). Thanks again and please keep commenting, I think It can be valuable for prospective students.     
  ",en
1107100,2011-04-15 08:23:16,statistics,Zero-inflated poisson and negative binomial models for count data. Anyone want to discuss GLMM's?,gqj33,[deleted],,https://www.reddit.com/r/statistics/comments/gqj33/zeroinflated_poisson_and_negative_binomial_models/,7.0,8.0,,en
1107101,2011-04-15 14:56:25,statistics,"BASIC Simple Linear Regression, 4 hours to submit paper, real screwed.  Help, anyone, please?",gqogj,[deleted],,https://www.reddit.com/r/statistics/comments/gqogj/basic_simple_linear_regression_4_hours_to_submit/,4.0,7.0,"I am so lost.  I'm basically doing a study on consumer habits regarding Starbucks, beceause in some cultures (like mine) people stay longer in the store to study/hang out.

I'm basically seeing if people spend more if they stay longer, based on surveys.  I REALLY need your help as I can't understand anything anymore. :( 

[More information on this document I've started to write, but I can't really tell if what I'm doing is right. :|  You have to download as Google viewer doesn't display the mathematical signs.](http://ge.tt/7sdlGmx)

Please and thank you!!!",en
1107102,2011-04-15 20:46:49,computervision,"Hipster detection: given an image of a person, how could you identify if they were wearing wayfarer frames?",gqwfh,projector,1269210852.0,https://www.reddit.com/r/computervision/comments/gqwfh/hipster_detection_given_an_image_of_a_person_how/,6.0,2.0,"Serious question, lighthearted topic. How would you go about doing this?",en
1107103,2011-04-15 20:59:41,MachineLearning,Machine Learning for Complex Language Entry,gqws7,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/gqws7/machine_learning_for_complex_language_entry/,11.0,1.0,,en
1107104,2011-04-16 02:34:48,statistics,Trying to compare statistics from two years and one treatment.  NOT good with stats.  Can I get some guidance?   ,gr4ha,hoyfkd,1248928403.0,https://www.reddit.com/r/statistics/comments/gr4ha/trying_to_compare_statistics_from_two_years_and/,2.0,3.0,"Hi /r/stats,

So I am doing a research paper, but it's been a while (returning student) since I did stats.  I stumbled upon some numbers that are incredibly useful, but I am not sure how to go about measuring them.

Basically, I have the number of states that have submitted a certain type of record to the federal gov't in both 2006 and 2010, as well as the number per 100,000 residents.  Each state is coded with whether or not they use a specific protocol for information exchange.  

In other words, I have two sets of numbers (the number of records) and within each, some will have used this protocol and some will have not used the protocol.

I then took the difference for each state between 2006 and 2010 for the number of records per 100k residents.  Finally, I separated the difference column into a group that used the protocol and a group that didn't and took the mean for each. 

Now I don't know what to do.  I have the mean for each group relating to the change in submissions / residents.  How do test for statistical significance?  Is there a test I should be running at this point?  Is there something else entirely that I should be looking at doing?  Thanks!!",en
1107105,2011-04-16 06:36:54,MachineLearning,Introductory tutorial on data munging with basic Unix tools,gr8o7,mycatharsis,1294046118.0,https://www.reddit.com/r/MachineLearning/comments/gr8o7/introductory_tutorial_on_data_munging_with_basic/,10.0,2.0,,en
1107106,2011-04-16 08:23:12,statistics,Streaks in Binomial Sequences: Odds of getting x heads in a row given n tosses of a coin,grahj,[deleted],,https://www.reddit.com/r/statistics/comments/grahj/streaks_in_binomial_sequences_odds_of_getting_x/,7.0,9.0,"This is a seemingly simple problem which a friend and I were discussing the other day. Specifically, we wanted to test a claim that a streak in game wins was a sign of skill and not due to random variance. Unfortunately, the only way we could answer the question was with mass simulations via a Java Program I wrote one night - hardly an accurate result. Does anyone here know how to compute the theoretical probability of getting a streak of a given length in a binomial sequence? I tried talking to the math teachers - High school - and they knew nothing. Google was little help either.

The exact example was:
What is the probability that Bob wins 6 games in a row at least once in a set of 150 games, given that Bob has a 50% chance of winning each game.",en
1107107,2011-04-16 21:01:52,MachineLearning,Rank is an open source project consisting of various Machine Learning algorithms which uses Regression Trees,grkfk,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/grkfk/rank_is_an_open_source_project_consisting_of/,19.0,0.0,,en
1107108,2011-04-17 00:11:35,MachineLearning,Is social network analysis useless?,gro5b,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/gro5b/is_social_network_analysis_useless/,26.0,9.0,,en
1107109,2011-04-17 01:50:39,statistics,Biostats PhD vs Epidemiology PhD?,grpxt,raptorgirl,1267041047.0,https://www.reddit.com/r/statistics/comments/grpxt/biostats_phd_vs_epidemiology_phd/,2.0,4.0,"Hey Reddit Stats! I could do with some advice.

I'm a 23 year old female enrolled in a M.S Applied Statistics program in Mexico, my undergrad degree was in Biotechnology Engineering. I'm a decent enough student although my GRE quantitative score of 720 is nothing to write home about, should I retake it?

I want to have a shot at better job offerings after completing a PhD. I'm interested in disease control and spread but I'm still exploring the broad scope of epidemiology research. I'm planning on applying for next year, 2012. 

What do you think? Can a PhD in Biostats do everything an Epidemiologist can? I've been told that with a Biostats PhD you end up working in epidemiology mostly, anyway. Can you recommend programs to me? Do you know how good the programs at Mcgill are or how hard is it to get into John Hopkins or Washington University? 

Thanks for the help!",en
1107110,2011-04-17 03:26:14,statistics,p-value from bootstrap,grrmv,[deleted],,https://www.reddit.com/r/statistics/comments/grrmv/pvalue_from_bootstrap/,1.0,5.0,"say i have a set of data {x} and set {y}. I want to see if the mean of x is different from the mean of y. I can bootstrap the difference of means and calculate the confidence interval on the difference of means, but how do i get a p-value for the difference?",en
1107111,2011-04-17 13:20:52,statistics,Exporting R graphics as LaTeX code (the tikzDevice package),grzuc,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/grzuc/exporting_r_graphics_as_latex_code_the_tikzdevice/,4.0,2.0,,en
1107112,2011-04-17 15:17:01,MachineLearning,What we really need is a showmedo type video tutorial for retards(like me) who want to get into machine learning.,gs0t9,stonedfox8,1297009710.0,https://www.reddit.com/r/MachineLearning/comments/gs0t9/what_we_really_need_is_a_showmedo_type_video/,1.0,1.0,"The thing is that I am having a lot of trouble getting a simple machine learning program up and running. Once I get this done I know that I can run. 
Is there any such resource out there which explains things in a really simple maner and where I don't have to scratch my head. ",en
1107113,2011-04-17 21:19:19,MachineLearning,"rt-rank - An open source project consisting of various Machine Learning algorithms which uses Regression Trees (implemented to compete in Yahoo's ""Learning to Rank"" Challenge)",gs674,[deleted],,https://www.reddit.com/r/MachineLearning/comments/gs674/rtrank_an_open_source_project_consisting_of/,3.0,0.0,,en
1107114,2011-04-17 23:16:03,artificial,podcasts and lectures,gs8jb,XenonDown,1287651652.0,https://www.reddit.com/r/artificial/comments/gs8jb/podcasts_and_lectures/,7.0,8.0,Do you have any podcast and/or lectures which you found especially good regarding AI?,en
1107115,2011-04-18 02:43:48,MachineLearning,"After leaving my laptop on overnight yet again to train several models, I got to thinking: is it possible to rent a powerful remote desktop computer, hosted on a server elsewhere?",gsclp,chillage,1251759220.0,https://www.reddit.com/r/MachineLearning/comments/gsclp/after_leaving_my_laptop_on_overnight_yet_again_to/,26.0,14.0,"This would be similar to the Amazon Cloud, but for personal use.  I tried renting server time at Amazon Web Services, but they seem to be focused entirely on web hosting, etc. I'm just looking to have a fully functional version of Windows/Linux hosted on a powerful server. I log into it and it behaves like a normal interactive desktop interface (with a bit of lag of course, since it's being processed elsewhere). But the upside is that it's so much more powerful since it's hosted on a very fast server elsewhere.

This would make my work so much easier... Does this exist anywhere? And my school does give me access to some very mediocre Linux servers, but I can only ssh in through putty and then I have to deal with a command line interface. I would really like to find a fully functional version of Windows/Linux that I can log into remotely.



EDIT:
thanks DimeShake! Looks like this is exactly what I was looking for:

http://blog.restbackup.com/how-to-use-amazon-ec2-as-your-desktop

I'll try it out when I have some time soon",en
1107116,2011-04-18 06:51:25,MachineLearning,Solving Pursuit-Evasion Games,gshlc,[deleted],,https://www.reddit.com/r/MachineLearning/comments/gshlc/solving_pursuitevasion_games/,3.0,5.0,"Hey Reddit,

I've got a pursuit evasion game that I'm trying to solve for fun and I'm having trouble figuring out how to go about solving it.

The particular games goes like this. We have a fixed undirected graph with edges pretty much randomly distributed, with the exception that each node has at least two edges.

Then a monster, a princess, and an exit are each assigned a random location on the graph. The princess and monster are both informed of the starting location of each other. The princess wins if she can reach the exit. The monster wins if he reaches the princess. Each turn, both characters may move to an adjacent node, with the sole exception that the monster may not move onto the exit. Lastly, if a path consisting of at most 3 edges can be made from the princess to the monster, the two are said to hear each other, and both are informed of this event.

My intuition says both should race to the exit. If the princess starts in a good location she can simply beat the monster to it. However, if the monster gets there first, then he should simply patrol around and try to listen for the princess. If he can hear her, he should be able to do a bit of triangulation by realizing that the first time he hears her she is exactly within three nodes of him. (Given this, if the princess cannot beat the monster, she should wait a while before trying to reach the exit.) If he hears her again, he can simply try to move inbetween the exit and the princess as best he can while simultaneously trying to capture the princess.

So basically, my goal is to write the optimal bots for both the princess and monster. Links and insight are very much appreciated.",en
1107117,2011-04-18 11:59:22,MachineLearning,Data Mining Drug Interactions,gsmpp,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/gsmpp/data_mining_drug_interactions/,0.0,0.0,,en
1107118,2011-04-18 14:26:37,MachineLearning,"HDPE Manufactures Mumbai, HDPE Manufactures India, Plastic Raw Materials Mumbai
",gsoet,timmckay,1285823561.0,https://www.reddit.com/r/MachineLearning/comments/gsoet/hdpe_manufactures_mumbai_hdpe_manufactures_india/,1.0,0.0,,en
1107119,2011-04-18 14:59:29,statistics,Welcome to a Little Book of R for Biomedical Statistics! — Biomedical Statistics v0.1 documentation,gsovh,mycatharsis,1294046118.0,https://www.reddit.com/r/statistics/comments/gsovh/welcome_to_a_little_book_of_r_for_biomedical/,10.0,0.0,,en
1107120,2011-04-19 00:51:37,MachineLearning,A short introduction to Sparse Coding and Dictionary Learning,gt2v6,ogrisel,1171218481.0,https://www.reddit.com/r/MachineLearning/comments/gt2v6/a_short_introduction_to_sparse_coding_and/,5.0,2.0,,en
1107121,2011-04-19 01:40:58,statistics,What could I do with school cafeteria revenue data that would be valuable?,gt41m,msmiitz,1274669312.0,https://www.reddit.com/r/statistics/comments/gt41m/what_could_i_do_with_school_cafeteria_revenue/,7.0,8.0,"I have two sets of data: One from my school that contains each day's revenue from the school cafeteria for the past school year, and the other from the other high school in the district.

My project is design a statistical investigation, which means making up my own question to investigate.  

Any ideas on what I could do with this data that would have any sort of merit? Testing whether one of the schools earns significantly more per day has already been rejected by my teacher.
",en
1107122,2011-04-19 01:58:18,MachineLearning,"Using R, Sweave and Latex to integrate animations into PDFs",gt4gy,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/gt4gy/using_r_sweave_and_latex_to_integrate_animations/,3.0,0.0,,en
1107123,2011-04-19 04:59:51,statistics,"I don't have a lot of knowledge of statistics, but I want to get a basic idea of what on earth is going on with hierarchical Bayesian space-time modeling. Does anyone have any good resources?",gt8nr,[deleted],,https://www.reddit.com/r/statistics/comments/gt8nr/i_dont_have_a_lot_of_knowledge_of_statistics_but/,3.0,6.0,"Though I'm an undergrad with not much statistics knowledge, I've taken a graduate time series course, regression, and probability theory. ",en
1107124,2011-04-19 12:50:36,statistics,How to upgrade R on windows 7 (common problems and their solutions),gthpn,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/gthpn/how_to_upgrade_r_on_windows_7_common_problems_and/,3.0,0.0,,en
1107125,2011-04-19 14:51:16,artificial,Artificial Plants Collection,gtj8p,a2boffice,1290240496.0,https://www.reddit.com/r/artificial/comments/gtj8p/artificial_plants_collection/,1.0,0.0,,en
1107126,2011-04-19 21:51:38,statistics,"I want to a cumulative distribution curve in R, but want to show the y axis in percent and/or count. Is it possible? plot.ecdf does not have any options for that.",gtt48,oblivious_human,1209884190.0,https://www.reddit.com/r/statistics/comments/gtt48/i_want_to_a_cumulative_distribution_curve_in_r/,5.0,8.0,Thanks for any help.,en
1107127,2011-04-19 22:34:17,MachineLearning,"How Kaggle competitors use R
",gtu75,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/gtu75/how_kaggle_competitors_use_r/,21.0,2.0,,en
1107128,2011-04-20 01:28:07,statistics,How to predict marathon times (and then fail to achieve them),gtyb4,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/gtyb4/how_to_predict_marathon_times_and_then_fail_to/,11.0,2.0,,en
1107129,2011-04-20 08:51:45,statistics,A neat little world population meter set in real time (I'm sure its an estimation),gu8jm,brockwallace,1285170907.0,https://www.reddit.com/r/statistics/comments/gu8jm/a_neat_little_world_population_meter_set_in_real/,0.0,0.0,,en
1107130,2011-04-20 09:08:00,datasets,"hat are some example of public datasets that have randomized instruments? Sometimes they ask questions in different orders, or use different prompts",gu8w9,inquilinekea,1274654696.0,https://www.reddit.com/r/datasets/comments/gu8w9/hat_are_some_example_of_public_datasets_that_have/,3.0,0.0,"Or datasets with instruments (with at least one variable randomized)?

I would like to use at least one of them for my causal modelling course (Stat 566), whose syllabus is at https://www.stat.washington.edu/tsr/s566/syllabus566.pdf",en
1107131,2011-04-20 15:40:30,MachineLearning,Penguin tickling,gues4,[deleted],,https://www.reddit.com/r/MachineLearning/comments/gues4/penguin_tickling/,1.0,0.0,,en
1107132,2011-04-20 23:56:38,statistics,"A wiki article I wrote, any comment will be welcomed:   Cramér's V - a measure of association between two nominal variables",gusa6,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/gusa6/a_wiki_article_i_wrote_any_comment_will_be/,10.0,2.0,,en
1107133,2011-04-21 03:04:27,statistics,Question about finding a P-Value using a t statistic,guwnh,[deleted],,https://www.reddit.com/r/statistics/comments/guwnh/question_about_finding_a_pvalue_using_a_t/,1.0,0.0,"I am working through a problem and I have came across the t statistic .58 and I want to find the P-Value.
The degrees of freedom are 25 but I can't seem to get the answer correct (.277)  It seems I am reading the t table incorrectly, could someone explain?",en
1107134,2011-04-21 05:53:06,statistics,Examples of articles that correctly interpret the p-value,gv09o,jp134711,1300151480.0,https://www.reddit.com/r/statistics/comments/gv09o/examples_of_articles_that_correctly_interpret_the/,3.0,1.0,"Does anyone have examples of such articles? An alternative is one that incorrectly interprets it. Thanks.

Edit: articles which are published scientific studies, not commentaries on interpretation of p-value",en
1107135,2011-04-23 06:14:19,MachineLearning,Using zlib as a similarity metric,gvhyw,lrwiman,1241371191.0,https://www.reddit.com/r/MachineLearning/comments/gvhyw/using_zlib_as_a_similarity_metric/,11.0,6.0,,en
1107136,2011-04-23 08:13:54,statistics,how do I get the standard deviation from a mean and n,gvk9q,[deleted],,https://www.reddit.com/r/statistics/comments/gvk9q/how_do_i_get_the_standard_deviation_from_a_mean/,0.0,4.0,"I'm studying for my stats final, here's one of the questions

""Opinion polls find that 16% of American adults claim that they ""never have time to relax"" Suppose you take a random sample of 400 American adults and count the number X in your sample that claims they never have time to relax""

Question before is ""The mean of X is...""
A) 16
B) 54
C) 64
D) 72

it's C


The next question is 
""The standard deviation of X is 
A) 3.46
B) 7.33
C) 12
D) 54

One more question.

The probability that X is at least 80 is 
A) less than 0.0001
B) about .0078
C) about 0.0116
D) about 0.0146

thanks!",en
1107137,2011-04-23 10:53:55,MachineLearning,Visualizing iPhone location tracking with Google Maps and R,gvmii,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/gvmii/visualizing_iphone_location_tracking_with_google/,14.0,2.0,,en
1107138,2011-04-23 18:54:12,statistics,"Revolution R, your experience?",gvrsk,tmalsburg2,1226925597.0,https://www.reddit.com/r/statistics/comments/gvrsk/revolution_r_your_experience/,12.0,7.0,"In my daily work I spend a lot of time waiting for R to finish complex calculations.  I'm already using a lot of optimization tricks and write critical code in C instead of in R.  Still, R is much too slow for many thing I'm doing.  Recently I became aware of Revolution R which supports multicore and cluster computing.  I might have access to a cluster soon, but I'm mainly interested in multicore support using the foreach package.  My questions:

1. Can I use Revolution R as a drop in replacement for standard R? 

2. Are there any drawbacks or problems?

3. If not, why isn't everybody using it?

4. And in general: what's your experience with Revolution R?

Many thanks!

**Edit:**

I just tried the community version of Revolution R (from official Ubuntu repositories).
Parallelizing my code was trivial using the foreach package.  Now my
script uses both cores of my machine!  As far as I understand, I can run this
code also on a cluster if Revolution R Enterprise is installed there.

Converting the code comes down to replacing for-loops and lapplys by
the foreach construct:

    foreach (i=1:100) %dopar% {
      heavy.lifting(i)
    }

In order to make this parallel you have to register the parallel
computing backend for foreach:

    library(multicore)
    library(doMC)
    library(foreach)

    registerDoMC(2)

The last line initializes to worker processes.  The iterations of a
foreach call are distributed to these workers.  The nice
thing: if you want to run your code in a standard R, you just load the
foreach package without registering a backend.  The iterations of the
loop are then executed sequentially.

I like it!",en
1107139,2011-04-24 04:16:42,statistics,Running into a weird problem with multiple linear regression. Help?,gw1o2,DifferanceQ,1213301630.0,https://www.reddit.com/r/statistics/comments/gw1o2/running_into_a_weird_problem_with_multiple_linear/,4.0,23.0,"I'm an English major who sort of accidentally ended up in a statistics course, and I'm having a hard time making sense of an issue I'm having with regression. 

I'm doing a research paper testing three theoretical models that could explain variation in sexual dimorphism cross-culturally. So basically, I have three predictor variables (two continuous, one nominal), and my dependent variable (continuous). First, I did simple linear regression with each dependent variable. Then, as per my prof's recommendation, I ran multiple regression using all three. When I did simple regression, none of the predictor variables came out as significant. But, when I ran multiple regression, the two continuous variables came out as really significant. I also found that the two continuous variables were significantly correlated (although the collinearity table didn't show any problems). I couldn't find anything in my textbooks about it, but I did some googling, and I think one of my continuous variables must be acting as a ""suppressor"" variable. 

My question is: is it normal and ""okay"" to get such different results from simple and multiple regression? Or am I right in thinking there's a suppressor variable or some other problem that's messing up multiple regression? If so, what can I do about it? Do I just scrap the idea of multiple regression totally, and assess each predictor variable on its own, or can I somehow ""fix"" things? 

**EDIT: My results look like this:**
Simple linear regression

Female height= beta value: 0.167, significance: 0.224

Subsistence method= beta value: -0.033, significance: 0.811

Polygyny rate= beta value: -0.245, significance: 0.089

Pearson’s correlation between female height and polygyny rate: 0.339, significance: 0.017

Multiple regression:

Female height=  beta value: 0.297, significance: 0.053

Subsistence method= beta value: -0.078, significance: 0.580

Polygyny rate= beta value: -0.345, significance: 0.024


Thanks in advance for any guidance you can offer!",en
1107140,2011-04-24 09:18:39,MachineLearning,Visualizing Android location tracking with Google Maps and R,gw6cp,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/gw6cp/visualizing_android_location_tracking_with_google/,0.0,0.0,,en
1107141,2011-04-24 20:23:48,computervision,A question for those with a good understanding of Multi-view geometry,gwe4h,gatman02,1282134360.0,https://www.reddit.com/r/computervision/comments/gwe4h/a_question_for_those_with_a_good_understanding_of/,1.0,0.0,"I'm trying to figure out how [this paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.144.8869&amp;rep=rep1&amp;type=pdf) creates the ground truth correspondes on a 3D scene.

From the paper: 
""The transfer of an interest point is done by point transfer using the trifocal tensor. For an image sequence I1 , ..., In we need a geometric correct mapping for every detected location in I1 to the other images of the sequence. Having a set of three images I1, I2, I3 it is possible to calculate the point coordinates x′′ in I3 of a corresponding point pair x ↔ x′ from I1, I2. The method is explained in detail in [5]. However, this requires that for every detected location in I1 we want to evaluate we need the corresponding position in an additional image. This is achieved by using a dense stereo matching between the first two images I1, I2 (see Fig. 3). To transfer the point	x	from	I1	to	images	I3 , ..., In	**we	first	have	to	find the closest entry from the set of the matched points. This is done by nearest neighbor search** . Now the trifocal tensor and the point pair can be used to compute a homography which projects the point from image I1 to the target image.""

I understand, at least conceptually, the construction of the trifocal tensor from the dense matching between two images. However, the next step seems a bit odd. They find the matching point (using SIFT) of x in I3, then use this point pair (x, xMatch) along with the trifocal tensor to compute x'', but doesn't x'' = xMatch anyways. From the best of my knowledge, the exact position of the 3rd camera/view is unknown (if it was known, then we could find the position in the 3rd image without matching, right?).

Thanks for the help

reference for the quoted paragraph above:
[1]	F. Fraundorfer and H. Bischof, “A novel performance evaluation method of local detectors on non-planar scenes,” p. 1-10, May. 2005.",en
1107142,2011-04-24 23:44:35,statistics,Logistic regression in R,gwi0z,amirightfolks,1280970578.0,https://www.reddit.com/r/statistics/comments/gwi0z/logistic_regression_in_r/,7.0,13.0,"I have a couple questions

* When you do a logistic regression in R, what kind of residuals do they give you. Are they deviance residuals or Pearson residuals?
* Let's say I call my fitted percentages ""fits"". Is it correct to look at 2*asin(sqrt(fits)) vs. the residuals. 

Also, I'm trying to use the ""nlme"" package. Isn't the only thing I have to do is type ""library(nlme)"". For some reason, I'm unable to use the functions from the package after doing this.",en
1107143,2011-04-25 02:03:26,statistics,Quick conceptual question.,gwkr2,healthfood,1274747510.0,https://www.reddit.com/r/statistics/comments/gwkr2/quick_conceptual_question/,2.0,5.0,"I don't want to get into the context for this question unless it's necessary.     
If we have some data that's normally distributed, and we look at the highest value in a certain size sample, we would expect the highest value to rise at a decreasing rate as the sample size increases, right?     
Essentially, I'm curious if we'd expect to get [something like this.](http://i.imgur.com/p0U9z.jpg)",en
1107144,2011-04-25 07:48:47,statistics,Need some stats HW help! (ANOVA),gwrw2,statshalp,1303702199.0,https://www.reddit.com/r/statistics/comments/gwrw2/need_some_stats_hw_help_anova/,0.0,0.0,"N = 24 K (conditions) = 3 (control, 1-day condition, 2-day condition) alpha = .05 Fobtained = 3.5 MSE = 3.00

Researcher used ANOVA.

Was there significant differences between conditions?

Which condition did the best? Why?

How can i create with an ANOVA summary table with what was given to me?
",en
1107145,2011-04-25 10:52:26,MachineLearning,Machine Learning PYthon (mlpy) - A high-performance Python library for predictive modeling,gwuuw,fbahr,1275413362.0,https://www.reddit.com/r/MachineLearning/comments/gwuuw/machine_learning_python_mlpy_a_highperformance/,25.0,9.0,,en
1107146,2011-04-25 17:36:13,MachineLearning,The Deloitte/FIDE Chess Competition: Play by Play,gx0ae,llimllib,1142360191.0,https://www.reddit.com/r/MachineLearning/comments/gx0ae/the_deloittefide_chess_competition_play_by_play/,5.0,1.0,,en
1107147,2011-04-25 18:36:59,statistics,Does anyone have extensive experience calling compiled languages from R?,gx1mq,BirthDeath,1295140816.0,https://www.reddit.com/r/statistics/comments/gx1mq/does_anyone_have_extensive_experience_calling/,1.0,16.0,"I have started to try to avoid loops by familiarizing myself with the vectorization commands in R such as apply() and outer() to speed up my R code, but I have found that they can be a bit slow when dealing with very large data sets (requiring a very large number of operations).  

It seems like the idea of exporting some of these computationally intensive tasks to a complied language is starting to gain traction, especially with the development and publicity of the [RCPP](http://dirk.eddelbuettel.com/code/rcpp.html) package, which allows for calling C++ code from R.

I am also aware of functions to call C and FORTRAN code from R.  

I haven't worked with a compiled language since my intro to CS class several years ago, when I used Java, so I have no idea where to start.  

I have heard that most C++ calls are generally canned routines that are incredibly easy to learn once you pick up the syntax (the seminars for rcpp do not even require familiarity with C++).

Should I try to learn a low-level language like C or FORTRAN, learn C++, or just concentrate on getting better with R until the need arises to learn a compiled language?   ",en
1107148,2011-04-25 18:54:36,statistics,Quartile Analysis (w/ ref group) Methodology,gx20l,[deleted],,https://www.reddit.com/r/statistics/comments/gx20l/quartile_analysis_w_ref_group_methodology/,1.0,0.0,"I'm attempting to perform a quartile analysis (first time doing such) and would like to know how to set up a reference group.  I have been using Hosmer and Lemeshows ""Applied Logistic Regression"" for guidance but I wasn't totally sure on how to apply a reference group to the rest of my data.  Right now, all I have is raw ""quartalized"" data (yes it is a continuous variable) and am not really sure if it would be appropriate to perform any sort of analysis without first setting up a reference group.

It *appears* that I should just divide Odds-Ratios of all other groups by ref-group Odds-Ratio and subtract ref-group estimated coefficients from all the others.  However, I do not know if this is correct.",en
1107149,2011-04-25 19:18:06,artificial,So this may be a stupid thought...,gx2iw,radagasthebrown,1284856474.0,https://www.reddit.com/r/artificial/comments/gx2iw/so_this_may_be_a_stupid_thought/,1.0,0.0,"So I was pondering this thought the other day, while in a somewhat altered state admittedly. This thought being; is there any software currently available that would allow me to verbally interact with my computer? 

The closest I've been able to find of this kind of technology is through a company called [Quantum Flux](www.qflux.net) who are currently in beta for an application called the VIWonder, which is essentially a limited artificial intelligence entity who can understand and respond to basic commands from an external user. The other example close to what I'm looking for is from the video game Mass Effect. In the game the player encounters a computer network/system loaded with a Virtual Intelligent system designed to assist users and make retrieving Information easier.

Essentially what I'm looking for is a program that takes the voice command capabilities used in the Mac standard voice command program, and then adds in a personality that can respond with relevant information to commands. 

I realize that the Mac voice command can already do things like this (""what time is it?"" &amp; ""tell me a joke."") but what I'm wondering is if there was a way to link this with other applications. Something that could read article titles in an RSS feed. Or giving weather information based on the computers current location. Spoken notifications ""So and so has posted to your Facebook wall."" 

Anyway, just curious if anyone has any thoughts or info.

Tl;dr I want to turn my computer into GLaDOS.

",en
1107150,2011-04-25 21:35:53,statistics,help:Logistic Regression End-Model Selection,gx5h5,willowthewizard,1302193398.0,https://www.reddit.com/r/statistics/comments/gx5h5/helplogistic_regression_endmodel_selection/,2.0,3.0,"Okay, /r/statistics, I need help with making a judgement call on selecting one model vs. the other.  

Model A: Log-Likelihood:-27.750; DF=4; 75% factors have a significant p-value
vs.
Model B (inclusive of 1 2-way interaction): Log-Likelihood:-26.732; DF=5; 40% of factors have a significant p-value

Is there any quick and easy way to determine which one I should choose over the other?  Should I look strictly at Goodness-of-Fit Tests which is better for B or should I factor in DF vs Log-Likelihood which seems more tilted towards A.",en
1107151,2011-04-25 22:42:14,statistics,dmFit and Excel,gx75w,AUBeastmaster,1301078207.0,https://www.reddit.com/r/statistics/comments/gx75w/dmfit_and_excel/,2.0,4.0,"Hey, first time poster (and flustered grad student).  I'm trying to use dmFit to fit curves to some data I have in excel to compare nonlinear death curves for pathogen populations.

Questions (for those who have experience with it):
1. In what format should I have my data?  I have 2 replicates that could be combined, 13 sampling points within each replicate, 3 samples per sampling point, and 8 different treatments (big experiment).  I'd like to be able to construct graphs that have a general line with standard deviation.
2. Does Excel have the capacity to compare data between curves (i.e., between treatments)?  Once I obtain the value that defines my curves, will Excel be able to show p values or anything between them, or should I analyze those in something like SAS?

Thanks so much for your input!",en
1107152,2011-04-26 03:05:37,statistics,Developing techniques to simulate random variables.,gxcyr,thecircleofreddit,1297270218.0,https://www.reddit.com/r/statistics/comments/gxcyr/developing_techniques_to_simulate_random_variables/,1.0,0.0,"Hey guys, I was wondering if anyone could give me some assistance, or some hints on this problem. It is a homework problem, so I am not asking for anyone to do it. I would love to be able to understand the problem and solution. 

Develop a technique for simulating a random variable having density function:

f(x) = e^(2x) if -infinity &lt; x &lt; 0

and 

f(x) = e^(-2x) if 0 &lt; x &lt; infinity

I figure I am supposed to use the inverse transformation method, but I am having trouble with the details. Thank you.",en
1107153,2011-04-26 08:02:32,statistics,Psych Experiment- Statistics Help ,gxjpw,[deleted],,https://www.reddit.com/r/statistics/comments/gxjpw/psych_experiment_statistics_help/,0.0,2.0,"This is pretty basic I'm sure, but I have no idea what I'm doing. 

I have four different environmental settings and I have to test whether the settings have affect on human memory. I’m doing this by testing how many words from lists are recalled in each setting, and also writing down the probability of recalling words in a certain position in the list (like, what is the probability of recalling the 7th term in a list of 15). 

1. What test do I use to compare the averages of the number of terms recalled in each setting if I want to show that one or two of them are significantly lower than the others? Do I have to deem one of them the control and compare the other three to that setting? 

2. If there were a lower/greater recall of the beginning terms but not in the end terms in one of the settings, which test would I use to show that? Or is it sufficient to just compare the graphs at a qualitative level for a research paper? 

I don’t even know if I’m being clear about this, but, help? ",en
1107154,2011-04-26 12:57:45,MachineLearning,Cheese Winding Machine India,gxnz1,vijaygroups,1303811264.0,https://www.reddit.com/r/MachineLearning/comments/gxnz1/cheese_winding_machine_india/,0.0,1.0,,en
1107155,2011-04-26 15:17:33,statistics,Walmart Shooping Online | WebandRank.com Blog,gxpt3,webandrank,1210916379.0,https://www.reddit.com/r/statistics/comments/gxpt3/walmart_shooping_online_webandrankcom_blog/,0.0,0.0,,en
1107156,2011-04-26 17:32:28,statistics,A book on data visualization,gxsgo,Rym_,1273654478.0,https://www.reddit.com/r/statistics/comments/gxsgo/a_book_on_data_visualization/,16.0,12.0,"Heya /statistics. I'm getting more and more interested in statistics, data, and everything involved. I was wondering if you know any good books on proper data visualization (maybe there is a definitive guide?) For example what kind of charts to use with what kind of data. Also, to accompany it, possibly a book on data visualization in R?",en
1107157,2011-04-26 17:42:20,statistics,[question] gene-gene interaction: is logistic regression the way?,gxspi,[deleted],,https://www.reddit.com/r/statistics/comments/gxspi/question_genegene_interaction_is_logistic/,3.0,5.0,"hi. i have two biallelic genes (A, and B) and a binary behavioral trait (Y). Gene A is encoded as 0, 1, or 2 (depending on genotype), and similarly, Gene B is encoded as 0, 1, or 2. The behavioral trait is encoded as 0 or 1. I want to see if a genotype of A, B, or a combined genotype can predict Y. I can run a logistic regression with A and B, but how do I capture the interaction? ",en
1107158,2011-04-26 17:47:29,statistics,Discussion of StatWeave and other technologies for weaving document text and analyses,gxsuc,mycatharsis,1294046118.0,https://www.reddit.com/r/statistics/comments/gxsuc/discussion_of_statweave_and_other_technologies/,6.0,0.0,,en
1107159,2011-04-26 18:44:56,statistics,Heteroscedasticity,gxu9x,jack47,1214711994.0,https://www.reddit.com/r/statistics/comments/gxu9x/heteroscedasticity/,7.0,34.0,"I am a physicist, so my data is almost always heteroscedastic. I generally have an estimate of the variances which can vary wildly from data point to data point. This is very common in the sciences, but when I open most statistics book they almost always make an assumption of homoscedasticity. 

Does /r/statistics/ have any thoughts on this? Do you know of any good introductions to regression in the presence of heteroscedasticity?

EDIT: I am currently using weighted least squares for regression and [chi squared reduced](http://en.wikipedia.org/wiki/Goodness_of_fit#Example) to define the goodness of fit, but I'm not sure whether this is correct and would like to know how statisticians deal with such situations. ",en
1107160,2011-04-26 21:01:41,statistics,"Total n00b at R, can someone help show me how I'm supposed to get started simulating a sample with different random variables?",gxxro,[deleted],,https://www.reddit.com/r/statistics/comments/gxxro/total_n00b_at_r_can_someone_help_show_me_how_im/,3.0,10.0,"So these are my instructions. I'm not asking anyone to do my homework for me, just help me get started because I have no idea wtf to even do as I've never used this software before and my prof. wants us to use R. How do I get up and running?

&gt; Use R for the following illustration of Omitted Variable Bias. Construct a sample of 500 simulated individuals with the following random variables:

&gt; i. Prior to running any other commands, use the command set.seed(20900)

&gt; ii. X_1 = t(5), X_3 ~ 6 + N(0,1), and X_4 ~ 4 + 4 x t(15)

&gt; iii. X_2 = 3 + 5X_1 + U_1 where U_1 ~ N(0, sigma^2 = 9)

&gt; iv. **Population regression relationship.** The response variable equals:

&gt; Y = 10 + 2X_1 + 7X_2 + 17X_3 + W, where W ~ N(0, sigma_w^2 = 100)

&gt; v. Given these r.v.'s, creata a **data.frame** object by binding (Y, X_1, X_2, X_3, X_4) together into a single data fram. Use **cbind()** and **as.data.frame()**

&gt; vi. Run the **summary()** command on the data frame you constructed and comment briefly on how the summary statistics from this simulated population match with the parameter values you used.

Thanks so much guys. Any help or links to resources are appreciated.  ",en
1107161,2011-04-27 02:17:27,MachineLearning,best  object recognition algorithm?,gy5x9,qwsazxerfdcv,1287517243.0,https://www.reddit.com/r/MachineLearning/comments/gy5x9/best_object_recognition_algorithm/,0.0,4.0,"hey, 
I have heard of Sift and Surf , but i was wondering what is the best in object/pattern recognition technique out there ?",en
1107162,2011-04-27 04:15:47,statistics,Can someone please explain the little bar that appears in R's periodogram?,gy8j9,brews,1263715301.0,https://www.reddit.com/r/statistics/comments/gy8j9/can_someone_please_explain_the_little_bar_that/,0.0,7.0,,en
1107163,2011-04-27 04:17:03,MachineLearning,Best Java/Ruby/JRuby Machine Learning Libraries?,gy8ke,lrwiman,1241371191.0,https://www.reddit.com/r/MachineLearning/comments/gy8ke/best_javarubyjruby_machine_learning_libraries/,1.0,10.0,"I'm pretty familiar with the available Python machine learning libraries, but for various reasons, I'm thinking of moving to Ruby.  Unfortunately, Ruby doesn't seem to have great library support, but JRuby is pretty straightforward to set up.  I'm working on document clustering and classification, and looking for libraries that support standard algorithms (linear SVC, naive Bayes, perceptron, logistic regression, k-means, etc).  Something with the level of functionality of scikits.learn (or better), with simple, consistent interfaces that's still being actively developed would be ideal.  Any advice on libraries people have used would be greatly appreciated.",en
1107164,2011-04-27 09:52:50,statistics,SAS certification- is it worth getting or just master R?,gyftj,[deleted],,https://www.reddit.com/r/statistics/comments/gyftj/sas_certification_is_it_worth_getting_or_just/,11.0,20.0,I've done a fair bit of data analysis using SPSS before of biological data. I am looking to pursue either R or SAS to help me land an industry job position. Any recommendations on which path I should pursue?,en
1107165,2011-04-27 22:30:06,statistics,Data manipulation in R,gyn0r,[deleted],,https://www.reddit.com/r/statistics/comments/gyn0r/data_manipulation_in_r/,0.0,3.0,"I have an lm model fitting 3 predictor variables. I used the influence.measures function to check which points might be problematic. The asterisk tells me which points are problematic. Let's say I wrote:

'inf&lt;-influence.measures(reg)', then I wrote

'ind&lt;-which(inf$is.inf,arr.ind=T)' to get the indices where the influential points are. 

So what I want to do is set up a smaller table that shows the observation numbers on the rows, the header(dfbetas, covr, dffits, etc.), and the values that are influential with maybe 0's in the other spots on the table. 

Any ideas?



Edit:I actually wrote a function that works fairly well

&gt; inftable&lt;-function(reg){

inf&lt;-influence.measures(reg) #influence measures

indi&lt;-which(inf$is.inf,arr.ind=T) #array matrix of influential points

inf1&lt;-inf$infmat #get influence matrix by itself

vals&lt;-inf1[indi] #actual values for various influence measures

num&lt;-indi[,2] #column numbers of the array matrix

colna&lt;-colnames(inf1)[num] #get column names (dfbetas, cov.r, etc)

matty&lt;-cbind(indi,vals,colna) #combine array, actual values, and stat name

matty

}
",en
1107166,2011-04-27 23:28:36,statistics,need help tuning an algorithm,gyotw,shaggorama,1233555004.0,https://www.reddit.com/r/statistics/comments/gyotw/need_help_tuning_an_algorithm/,0.0,3.0,"I'm working on a procedure that matches fields in a table to input values using some fancy techniques. Each field gets a match score, but certain fields are of more importance than others, so I'm using a weighted average of the fields to score each record. 

My problem is that this method favors records with empty, unimportant fields. To counteract this, I'm docking points for null fields (subtracting different amounts depending on which field is null).

Is there a better way to do this? If not, how do I balance the amount I'm subtracting against the weighted average to achieve the best results?

thanks y'all",en
1107167,2011-04-28 00:07:06,data,Belgian trademarks online now!,gyq2o,housepage,1214782425.0,https://www.reddit.com/r/data/comments/gyq2o/belgian_trademarks_online_now/,2.0,0.0,,en
1107168,2011-04-28 17:53:45,MachineLearning,Please help me understand CRF's better :),gzc9t,[deleted],,https://www.reddit.com/r/MachineLearning/comments/gzc9t/please_help_me_understand_crfs_better/,10.0,8.0,"I don't know what to do about formatting, so please take lower-case as subscript, bold as vectors, and capital E as big epsilon.

Here is where I am at (chain CRF's):

We have P(**X**|**Y**) = 1/Z exp( En Ei Li Fi() ) for features F(Yn,Yn-1,N,**X**)

Z = Ey exp( En Ei Li Fi() )

where Ey is the sum over all possible outputs **Y**. Training is done by maximizing the log likelihood over i/o pairs (**X**m,**Y**m), or

T = Em log( P(**Y**|**X**) )

using partial derivatives of Lk's

dT/dLk = Em (

En Fk() -

Ey( Fk() exp( En Ei Li Fi() ) )

/

Ey( exp( En Ei Li Fi() ) )

)

( I hope that is readable :( )

Right now I am trying to find the maximum by gradient ascent. I am finding results for arbitrary input **X** by finding argmax(y) P(**Y**|**X**).

Is this right? I'm having some success with it on algorithmically generated datasets, but it seems like the results are giving me too many mistakes even when the i/o pairs are Yn = Xn+1 and the features are only 1.0 for those equivalences.

Can someone help me understand how many i/o training pairs I should have for satisfactory answers, how exactly to use Viterbi/Forward-Backward to reduce training time, and/or how to generalize CRFs to non-linear graphs (like it is used in images somehow)? I have read a number of papers and each uses different symbols and includes variables that they assume the reader would know (but I do not).",en
1107169,2011-04-28 20:19:30,MachineLearning,"Pigs, Bees, and Elephants: A Comparison of Eight MapReduce Languages (cross-post from /r/bigdata)",gzg5j,ohsnaaap,1270647929.0,https://www.reddit.com/r/MachineLearning/comments/gzg5j/pigs_bees_and_elephants_a_comparison_of_eight/,2.0,0.0,,en
1107170,2011-04-29 01:38:08,statistics,Google Summer of Code Student Project Wins Statistical Software Award,gzomy,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/gzomy/google_summer_of_code_student_project_wins/,26.0,5.0,,en
1107171,2011-04-29 06:04:45,artificial,Artifical intelligence a long way off from Skynet: University of Toronto professor Geoffrey Hinton,gzump,kamoylan,1254047922.0,https://www.reddit.com/r/artificial/comments/gzump/artifical_intelligence_a_long_way_off_from_skynet/,2.0,3.0,,en
1107172,2011-04-29 19:17:48,MachineLearning,"my stupid nlp comic, maybe people here will enjoy it",h087t,ohashi,1220481495.0,https://www.reddit.com/r/MachineLearning/comments/h087t/my_stupid_nlp_comic_maybe_people_here_will_enjoy/,0.0,0.0,,en
1107173,2011-04-29 23:55:03,statistics,CVlm function in R,h0f06,amirightfolks,1280970578.0,https://www.reddit.com/r/statistics/comments/h0f06/cvlm_function_in_r/,2.0,5.0,"I'm using the CVlm function to do cross-validation on my model. I was wondering how it splits the data sets into the test group because I tried to use the function multiple times and it split it up the same exact way each time. Is there a different function that uses a more random algorithm. Thanks. 

Edit: OK, I can change the seed number to get different groupings. 

How do I interpret the graph that the function produces. If the lines are very close together does that mean the model has pretty good predictive power. 
",en
1107174,2011-04-30 08:05:43,statistics,homework help me please,h0o1s,furrytyrant,1304139553.0,https://www.reddit.com/r/statistics/comments/h0o1s/homework_help_me_please/,0.0,1.0,"Please help me with my homework! My stats partners are MIA, and I can't do it alone.

-Two out of five adult smokers acquired the habit by age 14. If 400 smokers are randomly selected, find the probability that 170 or more acquired the habit by age 14.

-If a sample of 100 people is chosen, and the researcher desires to have a maximum error of estimate of 4% on the specific proportion who favor gun control, find the degree of confidence. A recent study showed that 50% were in favor of some form of gun control.

-Find the 95% confidence interval of the variance and standard deviation of the nicotine content of cigarettes manufactured if a sample of 20 cigarettes has a standard deviation of 1.6 milligrams. 

Your help is much appreciated! (Will gladly return favors with MTG, ragnorak and/or LOL discussions...)",en
1107175,2011-04-30 20:54:25,statistics,Statistical significance in linear vs logistic regression,h0x8z,2203,1302803806.0,https://www.reddit.com/r/statistics/comments/h0x8z/statistical_significance_in_linear_vs_logistic/,5.0,9.0,"Hey all,

I'm running a multiple linear regression with a continuous Y variable and five continuous X variables (including a continuous-continuous interaction variable). In the linear model, all X variables are highly statistically significant. 

However, I tried dichotomising my Y variable and running a logistic regression (just for shits and giggles, mostly). In the logistic model, only one of my X variables is statistically significant. I'm trying to figure out why.

Can someone help explain what causes this? Any assistance is much appreciated. Thanks!",en
1107176,2011-04-30 21:29:12,statistics,"BA in psych, maybe want to get a MA in Biostatistics? 
- x-posted from askreddit",h0xuu,displacingtime,1299284762.0,https://www.reddit.com/r/statistics/comments/h0xuu/ba_in_psych_maybe_want_to_get_a_ma_in/,2.0,3.0,"I have a BA in psychology. I did really well in college, so I have a strong GPA.

I enjoyed my exposure to statistics much more than I would have expected, because high school math was very discouraging for me. I wasn't a very good student in high school in general though.

I love research and had thought I wanted to get into it through the psych phd angle, but I'm wondering if a statistics degree would be a more practical way to get into that same area of research.

I'm wondering if maybe getting a MA in biostatistics is an option for me. The problem is that looking at schools offering it I see that I don't have close even to the needed prerequisites. I've taken 1 college level statistics course and two research methodology courses that included stats content (avovas, multiple regression, some factor analysis) but I have never taken Calculus (or pre-calculus) and Algebra 2 Freshman year of high school was the farthest I got in Algebra. Sadly, the school where I got my BA did not have any more advanced Statistics classes beyond what I took so I wasn't able to pursue it further.

Is this just way to drastic of a switch? Would a statistics degree be so diferent from what I've been exposed to from the psychology angle that I can't properly judge if I would really like this?

Is it possible for me to take some classes and then apply to a program? Does anyone have suggestions of what classes might be the best ones for me to focus on?",en
1107177,2011-05-01 07:11:07,statistics,"Multiple response variables, Multiple explanatory variables: Multivariate Regression?",h17wp,[deleted],,https://www.reddit.com/r/statistics/comments/h17wp/multiple_response_variables_multiple_explanatory/,1.0,8.0,"Ok, I'm a stats noob (also working in R, just fyi)
I have measured 12 ""response variables"" (species abundances) and 25 ""explanatory variables"" (various food abundances), and I'd like to look at how they are related, seeing if any response variables are related particularly strongly to any explanatory variables, and well, I'd like to learn more about the data in general, to be honest. I'm sure that there is some covariance going on in the explanatory  and response variables (not sure how to handle that) , and I'll probably end up throwing out a few of the ""explanatory variables"" because they have too many 0 entries.  I've only got a sample size of 15 measurements (15 years) for each variable, and I cannot get anymore data.  I'm trying multivariate regression using the mvr() function from the pls package.  Have any suggestions or better ideas?",en
1107178,2011-05-01 14:59:34,statistics,Academic Publishing- a subreddit for young academics trying to publish in peer-reviewed journals ,h1cs6,jjrs,1170375384.0,https://www.reddit.com/r/statistics/comments/h1cs6/academic_publishing_a_subreddit_for_young/,12.0,0.0,,en
1107179,2011-05-01 15:48:46,analytics,Analytics mystery... where could a massive spike of direct traffic be coming from?,h1d76,Paranoidmarvin,1284649744.0,https://www.reddit.com/r/analytics/comments/h1d76/analytics_mystery_where_could_a_massive_spike_of/,3.0,1.0,"http://i.imgur.com/maV1M.png
For no apparent reason I've had a (relatively) huge spike in traffic that has all come directly to my site. Most of my traffic before tended to be from stumbleupon and other referring sites, but I've had about 7k of direct hits on top of the roughly steady 1k a day I was getting.
Any ideas?",en
1107180,2011-05-01 21:44:13,statistics,Where can I find a list of time series processes?,h1inl,[deleted],,https://www.reddit.com/r/statistics/comments/h1inl/where_can_i_find_a_list_of_time_series_processes/,0.0,0.0,"I'm working on a project at the moment and we need a list of all published time series processes.


Thanks
",en
1107181,2011-05-01 22:18:46,statistics,STATA: How do I define my own regression function.,h1jci,[deleted],,https://www.reddit.com/r/statistics/comments/h1jci/stata_how_do_i_define_my_own_regression_function/,3.0,6.0,"Basically I need to regress on this function 
z=(((Yi - L)^2 ) ) - Yi)/(L*sqrt(2))

I know there is a way to do this in stata I just cannot figure out how. 

Edit: I did figure out how to do what I needed to do. Unfortunately I could not clear up my syntax error.
",en
1107182,2011-05-02 05:31:53,statistics,Making a probability dendrogram in R.,h1ruq,Trevallion,1304301731.0,https://www.reddit.com/r/statistics/comments/h1ruq/making_a_probability_dendrogram_in_r/,5.0,3.0,"Hello reddit! I'm trying to make a probability dendrogram in R to illustrate my answer for a homework problem. It's not required for the work, but I thought it would be a fun way to illustrate my answer and a good way to learn something new in R. However, I'm having the worst time getting it to do what I want. The question is this: Steve is involved in an accident and there were three things he could have done to avoid the accident: drive slower, pay less attention to his phone, or pay more attention to the other cars on the road. The probability that he is negligent in any one of those things is 0.3. 

I want to create a dendrogram that shows each branch of each level of Steve's potential negligence, so the highest level shows his negligence for driving slower with a 0.3 and a 0.7 branch, and each of those has a 0.3 and 0.7 branch for paying less attention to his phone, and then each of those has a 0.3 and 0.7 branch for paying more attention to the other cars. The final level has a branch for every combination of every level. My most functional idea was to create a data frame with 3 sample() objects and then use makeDendrite() and plot.dendrite() from the Plotrix package like so:

    drive &lt;- sample(c(rep(""at fault"",30),rep(""not at fault"",70)))
    cell &lt;- sample(c(rep(""at fault"",30),rep(""not at fault"",70)))
    attention &lt;- sample(c(rep(""at fault"",30),rep(""not at fault"",70)))
    steve &lt;- data.frame(drive=drive,cell=cell,attention=attention)
    stevedendrite &lt;- makeDendrite(steve)
    plot.dendrite(stevedendrite,xlabels=c(""Drove Too Fast"",""Watched Cell Phone"",""Didn't Pay Attention to Other Cars""),main=""Probability of Steve's Negligence"")

This is really close to what I want, but because I used whole numbers for the sample() objects, the numbers on the last level are slightly off. For example, the 0.7 * 0.7 * 0.7 entry should be 0.343 or 34.3, but instead it is 32. Similarly, the 0.7 * 0.3 * 0.7 entry is 16, while the 0.3 * 0.7 * 0.7 entry is 15. They should both be 14.7 (or 15, I guess). I tried using sample(c(""at fault"", ""not at fault""),100,prob=(0.3,0.7)) for the sample objects but the numbers were still off by a bit. Is there a better way to do this? It seems like there should be a better way to construct the sample() objects but I couldn't figure it out. Can I use another function for the objects? I also looked at other dendrogram and tree functions but they all looked REALLY complicated for what I'm trying to do. I think most of them are used for visualizing regression results. I'm just trying to make a pretty picture for a homework problem ;)",en
1107183,2011-05-02 10:28:18,statistics,"REGRESSION FANTASIES: PART III
Is Your Regression Model Telling the Truth?
",h22td,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/h22td/regression_fantasies_part_iii_is_your_regression/,11.0,2.0,,en
1107184,2011-05-02 10:48:35,analytics,Every second website is using Google Analytics now,h237p,JackS08,1250705804.0,https://www.reddit.com/r/analytics/comments/h237p/every_second_website_is_using_google_analytics_now/,4.0,0.0,,en
1107185,2011-05-02 13:49:20,statistics,Postdoc on adaptive MCMC in Paris,h25tt,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/h25tt/postdoc_on_adaptive_mcmc_in_paris/,2.0,0.0,,en
1107186,2011-05-02 23:05:21,analytics,Small Business Solutions: Marchex,h2iwa,BasilPuglisi,1296671987.0,https://www.reddit.com/r/analytics/comments/h2iwa/small_business_solutions_marchex/,1.0,1.0,,en
1107187,2011-05-02 23:17:21,MachineLearning,Unicode symbols in R,h2j94,[deleted],,https://www.reddit.com/r/MachineLearning/comments/h2j94/unicode_symbols_in_r/,0.0,2.0,,en
1107188,2011-05-02 23:33:31,statistics,Do significantly different means automatically imply significantly different medians?,h2jnn,tree_or_up,1263601594.0,https://www.reddit.com/r/statistics/comments/h2jnn/do_significantly_different_means_automatically/,7.0,16.0,"I'm still getting my feet wet, so please accept my apologies if this is a silly question or I'm not stating things clearly enough.

Using a t-test, I found that the difference in means between groups was statistically significant (p-value of around 0.01). Let's say the mean of group A was 25 and the mean of group B was 21. I then notice that the median of group A is 8 and the median of group B is 6.

Is it safe to say that the this difference in medians is also statistically significant?",en
1107189,2011-05-03 09:15:26,statistics,Next year I graduate with a Master's in Stats - No idea what I want to do,h2y7w,MrDannyOcean,1303187964.0,https://www.reddit.com/r/statistics/comments/h2y7w/next_year_i_graduate_with_a_masters_in_stats_no/,12.0,24.0,"I was just wondering what kind of different jobs statisticians actually do in the business world.  I've thought about becoming an actuary, but would like to explore options other than just working in the insurance industry.  So what other kinds of work or positions are available for somebody with a Master's in Statistics?",en
1107190,2011-05-03 10:42:43,statistics,Case Study of Introducing R into the Enterprise,h2zm5,mycatharsis,1294046118.0,https://www.reddit.com/r/statistics/comments/h2zm5/case_study_of_introducing_r_into_the_enterprise/,7.0,2.0,,en
1107191,2011-05-03 13:53:25,MachineLearning,Help for naming a Machine Learning problem,h321s,pecatti,1289158391.0,https://www.reddit.com/r/MachineLearning/comments/h321s/help_for_naming_a_machine_learning_problem/,6.0,17.0,"Hi, i am a MS student in CS and need your help for identifying a topic.What is the name of the concept to teach operators to computers?
Like to teach binary (+) operator to computer, it does not need to be operators any kind of functions not data based classifiers?

Any keywords for searching is appreaciated. thanx.

Edit:
I mean for example when we want to build a car classifier. We provide machine features from car like length,
height, shape etc and label these features.And we train our classifier like SVM, neural network.
But if we need to do teach an operator or a function so there will be no data matrix. In this case what we should do?

Second Edit:
Sorry for ambiguity but simply what i want is a program which can learn mathematical operators like addition, multiplication by given input.

Example input can be several addition operations and results.

it can be supervised or unsupervised, does not matter. 
",en
1107192,2011-05-03 17:50:42,statistics,Do Percentages Mean What We Think They Mean?,h3691,Harpsichord,1274891179.0,https://www.reddit.com/r/statistics/comments/h3691/do_percentages_mean_what_we_think_they_mean/,6.0,3.0,,en
1107193,2011-05-03 18:39:06,MachineLearning,“The Data Science Toolkit (DSTK)” R package,h37k0,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/h37k0/the_data_science_toolkit_dstk_r_package/,8.0,1.0,,en
1107194,2011-05-03 18:39:38,analytics,TIL Why Gmail referrals don't show up in Google Analytics,h37kq,searchengineoptimist,1282518032.0,https://www.reddit.com/r/analytics/comments/h37kq/til_why_gmail_referrals_dont_show_up_in_google/,3.0,1.0,,en
1107195,2011-05-03 23:04:32,statistics,Running R on an iPad/iPhone (with RStudio),h3epb,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/h3epb/running_r_on_an_ipadiphone_with_rstudio/,8.0,8.0,,en
1107196,2011-05-04 02:29:52,MachineLearning,SVMs and a little prior info on naughty words improve ML attempts to classify jokes,h3juy,[deleted],,https://www.reddit.com/r/MachineLearning/comments/h3juy/svms_and_a_little_prior_info_on_naughty_words/,12.0,1.0,,en
1107197,2011-05-04 08:04:20,rstats,"Introduction to using R with org-babel, Part 1",h3rc2,[deleted],,https://www.reddit.com/r/rstats/comments/h3rc2/introduction_to_using_r_with_orgbabel_part_1/,1.0,0.0,,en
1107198,2011-05-04 21:39:50,MachineLearning,bigkmeans also works well for ordinary matrix objects (via biganalytics package),h46xu,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/h46xu/bigkmeans_also_works_well_for_ordinary_matrix/,7.0,2.0,,en
1107199,2011-05-04 21:45:06,datasets,Central clearinghouse for datasets published by the City &amp; County of San Francisco,h4735,TheMunch8,1277148344.0,https://www.reddit.com/r/datasets/comments/h4735/central_clearinghouse_for_datasets_published_by/,1.0,0.0,,en
1107200,2011-05-05 05:15:13,statistics,Data analysis problem. Help please :),h4hmx,[deleted],,https://www.reddit.com/r/statistics/comments/h4hmx/data_analysis_problem_help_please/,3.0,6.0,"Hey r/statistics! 

What I would like to do is take some relative power scores I have for each state (directed-dyad dataset) and fit each power score for 1816-2000 with somewhere between a 3rd and 9th degree polynomial, depending on how good the fit ends up being. Then I need to calculate upper and lower asymptotes and inflection points where the second derivative is = to 0. I have R and STATA available for this. I am aware of the lnlm and the 'by' package in R which seem to kind of what I am looking for, but I am not really sure how to go about writing a function to do this, if that is in fact what I need to do. Any help would be appreciated.",en
1107201,2011-05-05 10:57:54,MachineLearning,Machine Learning cheat sheet ,h4oc5,mycatharsis,1294046118.0,https://www.reddit.com/r/MachineLearning/comments/h4oc5/machine_learning_cheat_sheet/,54.0,11.0,,en
1107202,2011-05-05 11:04:04,analytics,Google Analytics: measure page load time,h4ofr,luigiagosti,1292702077.0,https://www.reddit.com/r/analytics/comments/h4ofr/google_analytics_measure_page_load_time/,7.0,0.0,,en
1107203,2011-05-05 16:45:09,datasets,Compilation of a variety of data processing APIs as a downloadable VM or AMI,h4szd,[deleted],,https://www.reddit.com/r/datasets/comments/h4szd/compilation_of_a_variety_of_data_processing_apis/,1.0,0.0,,en
1107204,2011-05-05 18:11:53,statistics,"Newbie looking for overview of statistical concepts, when and why you would use them",h4v0a,[deleted],,https://www.reddit.com/r/statistics/comments/h4v0a/newbie_looking_for_overview_of_statistical/,3.0,18.0,"I don't have a tremendous need for statistics, but I find it interesting and have some opportunities to apply it within my job.  Can anyone recommend a well put together (i.e. Khan Academy style -- I didn't see anything there) overview of various concepts/methods and when you might want to use them?  I'm looking for pretty high level overviews.

Edit: data mining techniques would be most useful",en
1107205,2011-05-05 18:47:01,statistics,"Reddit, can you please clarify interpreting coefficients for a regression test for me?",h4vuv,jaboi,1270668381.0,https://www.reddit.com/r/statistics/comments/h4vuv/reddit_can_you_please_clarify_interpreting/,7.0,7.0,"http://i.imgur.com/Y26cc.jpg

I'm running a regression test comparing % of a population in poverty with % of a population that has associate's degrees. Using poverty as Y and associates as X, the coefficients (shown in the picture above) were calculated. I am not entirely sure how to interpret this. 

A 1% increase in %associate's degrees leads to a 12% decrease in %poverty? ",en
1107206,2011-05-05 19:39:14,statistics,"Reddit, may I have help interpreting the steps of a do-file in STATA?",h4x7s,Parmeniscus,1294346329.0,https://www.reddit.com/r/statistics/comments/h4x7s/reddit_may_i_have_help_interpreting_the_steps_of/,1.0,3.0,"The data set is the world values survey. The input is below: 

. gen first=v231
. recode first 2=-1 3 4=0
. gen second=v232
. recode second 2=-1 3 4=0
. gen pref=v233a
. recode pref 2=-1 3/5=0
. egen partyn=rsum(first second pref)


My main issue is with the ""egen"" command. What exactly is that doing?
",en
1107207,2011-05-05 21:32:36,MachineLearning,"Building a TWSS Classifier, Applying it to Fairy Tales",h502x,jasonrosen,1304619652.0,https://www.reddit.com/r/MachineLearning/comments/h502x/building_a_twss_classifier_applying_it_to_fairy/,19.0,0.0,,en
1107208,2011-05-05 23:02:23,statistics,Math illiterate here: I need some help with a graph.,h52cn,xenmate2,1291074295.0,https://www.reddit.com/r/statistics/comments/h52cn/math_illiterate_here_i_need_some_help_with_a_graph/,1.0,17.0,"I work organising concerts in London. We regularly have 15-20 concerts we are working on at any one time, and to be able to see easily which ones need more work I'd like to start mapping them on charts so I can see the ticket sales progression as a value and as the percentage of the venue capacity over time. If anyone would be so kind as to help this ignoramus I would be forever endebted.",en
1107209,2011-05-05 23:47:16,statistics,Probability Help,h53hv,TruckStopGloryHole,1294932064.0,https://www.reddit.com/r/statistics/comments/h53hv/probability_help/,1.0,2.0,"given that a student after completing a class has (completely made up)

28% - get an A
25% - B
12% - C
35% - D

If there are 42 students, what is the probability that (9 - A, 13 - B, 5 C, 15 D)

Should I do binomial(n,p) where for A: n=42, p = .28 and x = 9
then B: n = 42-9, p = .25/(1-.28) and x = 13
then C: n = 42 -9 - 13, p = .12/(1-.28-.25) and x = 5
then D: don't need

Then multiply P(A)P(B)P(C)?
That way I get: .092 * .1206 * .2020 = 0.00224

Any advice, I feel like I am doing this completely wrong?
",en
1107210,2011-05-06 03:52:57,statistics,xpost from AskReddit; I need help doing multivariable statistical analysis,h5986,joepls,1274171208.0,https://www.reddit.com/r/statistics/comments/h5986/xpost_from_askreddit_i_need_help_doing/,0.0,5.0,,en
1107211,2011-05-06 07:42:00,statistics,False applications of normalcy in modern parametric statistics.,h5duk,[deleted],,https://www.reddit.com/r/statistics/comments/h5duk/false_applications_of_normalcy_in_modern/,0.0,0.0,"Do any of you pure statistics guys and gals get the feeling that, especially in social sciences, false assumptions of normalcy are quite rampant? Of four undergrad stats courses I've taken, focused on a social science major, all but one of them have been strictly parametric statistics and Gaussian distributions. This is all well and good when addressing the simplest of questions, but even in my brief experience it kills exploration into the real truths of social sciences. Very few questions have a single central value to cluster around, and often there are discoveries to be found in the clusters and outliers. I know these kids will be taught non-parametric methods and Bayesian probability etc. in grad school, but this not only leaves a portion that will stop after undergrad with a horrid misunderstanding of stats, but also primes everyone to search for the Gaussian distribution regardless. 

Do you guys commonly see Gaussian methods applied to data inappropriately? I know I've personally read a few posts on here concerning people trying to ""normalize"" their data (owww that makes my head hurt), which is all well and good if there is an actual set of parameters under which you can isolate a single central mean, but that doesn't seem to be the case in these examples or examples I've seen IRL. 

What basic stat methods do you think could be better incorporated into an intro class or set of intro classes to properly educate neophytes on the full spectrum of statistical analysis?",en
1107212,2011-05-06 16:47:37,MachineLearning,Analyse Marvels social graph with With Pig and Wukong ,h5lue,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/h5lue/analyse_marvels_social_graph_with_with_pig_and/,8.0,1.0,,en
1107213,2011-05-07 16:10:57,statistics,95% CI of MLE of ARGUS Distribution,h624v,sheppa28,1274041127.0,https://www.reddit.com/r/statistics/comments/h624v/95_ci_of_mle_of_argus_distribution/,0.0,1.0,"I am trying to solve for the MLE of chi of the ARGUS Distribution using the formula [here](http://en.wikipedia.org/wiki/Argus_distribution#Parameter_estimation). If you let z_i=(x_i/c)^2 then the R.H.S. is just normalized and [L.H.S.]=AVG[Z]

The MLE equation part I figured out and can solve, now trying to solve for the 95% CI of the MLE. Is it correct to just solve for chi of [L.H.S.] = AVG[Z] +/- 1.96*STD(Z)/sqrt(n) ? Just want to make sure I'm not missing something. Thanks!
",en
1107214,2011-05-08 00:38:49,MachineLearning,Heritage Health Prize Demands Exclusivity; Others Call for a Boycott,h6b5c,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/h6b5c/heritage_health_prize_demands_exclusivity_others/,17.0,7.0,,en
1107215,2011-05-08 01:30:31,statistics,Care to help with analyzing data on refugee claim grant rates in Canada?,h6c2v,[deleted],,https://www.reddit.com/r/statistics/comments/h6c2v/care_to_help_with_analyzing_data_on_refugee_claim/,3.0,2.0,"Dear Reddit,

I am in need of help to analyze some data from the Immigration and Refugee Board of Canada. I've got some data that gives me information on what the decision of an asylum application was, the type of claim that was made (i.e. political persecution, gender persecution), the applicants country of origin, the gender of the applicant, the office that reviewed the claim, and the name of the member of the IRB reviewing the claim. I'm trying to figure out whether the name of the member reviewing the claim, is statistically significant in predicting the outcome of the hearing, basically whether some members have ridiculously high or low grant rates in comparison to other members reviewing similar claims. So I want to look at the variation between the grant rate of any one member, in comparison to the mean grant rate holding all else constant (i.e. similar claim type, gender and country of origin).  However I'm a little confused how to model this using STATA and haven't taken statistics in a while. I think it has something to do with the XI command, but I could be wrong. Here's a link to the data (i'm using the 2006 data at the moment) http://ccrweb.ca/documents/rehaagdatamarch10.htm and here is a paper that was written based on the 2006 data http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1468717. Does anyone know what STATA command might help me figure this out? Or does anyone have some general statistical advice on what I should do?

Thanks.",en
1107216,2011-05-08 03:39:27,statistics,Connect 4 in pure R (follow up from my R-tac-toe game).,h6e8g,randomsample,,https://www.reddit.com/r/statistics/comments/h6e8g/connect_4_in_pure_r_follow_up_from_my_rtactoe_game/,13.0,9.0,,en
1107217,2011-05-09 00:49:06,statistics,Quick Question about multiway ANOVA,h6v4d,[deleted],,https://www.reddit.com/r/statistics/comments/h6v4d/quick_question_about_multiway_anova/,7.0,10.0,"I have a quick question about Multiway ANOVA and/or Logistic regression with multiple predictors.  Let's say I have a 4Way ANOVA model with predictors A, B, C, and D and all of their interactions and my stats program chunks out that A x C is a p value of .803.  However when I do a 2 Way ANOVA the p value for A x C is .014.  Why/How does this happen?  Is one of the p values more reliable?  When I graph the data it looks like it should fit the results of the 2-way but I don't want to just go with the wrong data.  Any help would be most appreciated.  Thanks!",en
1107218,2011-05-09 01:12:42,statistics,Help with a problem,h6vm1,gatman02,1282134360.0,https://www.reddit.com/r/statistics/comments/h6vm1/help_with_a_problem/,5.0,4.0,"*if this is not the right place to post something like this please let me know*

I am working on a project evaluating image feature detection methods (methods to find similar points in image pairs) on a certain type of image pair. The performance of detectors in these circumstance is not very good and I would like to figure out if it does better than the case of randomly detecting points in each image. At first I was thinking that I could do this by actually generating random points, then assessing their point location correspondence. Then I realized that I this could be done using probability/statistics. 

I was wondering if someone might be able to construct the problem and some guidance in its solution. This is what I have so far:
Consider that we have 2 images, A and B. Each image has a resolution of 800*600. If I were to randomly select 1000 points from each image, what percentage of points from A would correspond to points from B? Furthermore, how would this change if I allow some error between how I defined corresponding points? For example, if one of the 1000 points in A was (2,2), I could say that it corresponds to point (3,2) in B because the euclidean distance is below some tolerance value.

I guess from a probability standpoint this doesn't sync very well. Maybe I could redefine it as ""what is the probability that 10% of the 1000 points will correspond"". The number 1000 is arbitrary, just so you know.

I'm hoping that someone might know an example that is analogous to this problem that I could look at. Also, any keywords that might point me in the right direction in determining a solution (perhaps I should be looking at a specific method).

Thanks in advance for your help.
",en
1107219,2011-05-09 03:15:14,MachineLearning,One pitched advantage of probabilistic problem framing is that there is a natural way of dealing with sensor fusion. This presentation implies that there are other straight-forward/naive methods of sensor fusion. What are they? ,h6xyz,senseorconfusion,1304899565.0,https://www.reddit.com/r/MachineLearning/comments/h6xyz/one_pitched_advantage_of_probabilistic_problem/,0.0,2.0,,en
1107220,2011-05-09 04:15:03,statistics,Path Analysis Help,h6z6p,Parmeniscus,1294346329.0,https://www.reddit.com/r/statistics/comments/h6z6p/path_analysis_help/,1.0,1.0,I am creating a path analysis with three exogenous and three endogenous variables. Some of the relationships are not significant. Do I still calculate the P's for insignificant relationships?,en
1107221,2011-05-09 04:23:50,statistics,Regression Equation HW help,h6zdb,brs_bts_bsgallactica,1304903735.0,https://www.reddit.com/r/statistics/comments/h6zdb/regression_equation_hw_help/,0.0,3.0,,en
1107222,2011-05-09 08:19:59,artificial,Abstract symbol manipulation (and random musings),h74c8,ArmchairArmada,1277576047.0,https://www.reddit.com/r/artificial/comments/h74c8/abstract_symbol_manipulation_and_random_musings/,1.0,0.0,"Forgive me I'm not an expert, so bear with me for a bit.  If self-teaching core AI, such as with a neural network or by some other means, is designed to have an interface to some form of abstract symbol manipulation (even if it is only something as simple as access to an array of floating point numbers) I would assume the AI could subsequently form a more robust generalized artificial intelligence.  What research is currently being done in this area?

What I mean is, I've seen artificial intelligence that has been trained to directly do specific tasks, and I've seen them do so though evolutionary means, but what if a less direct method is taken?  What if the task the AI is evolved for is reading, writing, and interpreting symbols that it itself has full control over.  Then it could, in turn, store precomputed data, memories, algorithms, and other useful information if it's choosing.

As an off topic side note, I am also curious about AI that is trained for predicting future input patterns.  I would think that AI that is capable of determining with some accuracy what inputs it will receive in the future would have a better understanding of the nature of those inputs (such as a better understanding of the world it interacts with).  Also, I would assume, such AI would be potentially better at making plans as it could try to predict the impact certain decision might make.

I know this may all seem like I'm rambling a bit, but it's late at night and I'm in a speculative mood.",en
1107223,2011-05-09 11:39:41,analytics,Google Analytics Advanced Segments are badly broken and are reporting incorrect data,h7799,jamiembrown,,https://www.reddit.com/r/analytics/comments/h7799/google_analytics_advanced_segments_are_badly/,4.0,0.0,,en
1107224,2011-05-09 14:05:20,statistics,Can someone explain the use of Expected Values taken to higher powers?,h792j,lDeadeyersl,1303039548.0,https://www.reddit.com/r/statistics/comments/h792j/can_someone_explain_the_use_of_expected_values/,0.0,6.0,"I Get E(X), and I even get using E(X^2) for its value in finding variance, however I overlooked an old practice exam for my final and I see it asking about E(X^3) and E(-e^X), what do these functions find? In addition would you solve them the same way you solve E(X^2)?
",en
1107225,2011-05-09 17:42:58,statistics,How to determine sample size (overall and group) for a one way anova?,h7cwh,Evolutionarybiologer,1281584636.0,https://www.reddit.com/r/statistics/comments/h7cwh/how_to_determine_sample_size_overall_and_group/,1.0,10.0,"Is there a set formula? I have three groups, so I know my degrees of freedom for group are going to be 2. I can see from the F distribution that for a fixed Numerator df of 2 the critical values of decrease by 0.01 units after 27 Error/Denominator df. So I am guessing I would need about 27 or more individuals per group? Is this the right way to go about this?",en
1107226,2011-05-09 17:43:46,statistics,Expected Value Question Confirmation Needed,h7cx5,rwestergren05,1301665654.0,https://www.reddit.com/r/statistics/comments/h7cx5/expected_value_question_confirmation_needed/,0.0,5.0,"Hello fellow Redditors! I have a question I got wrong on a quiz that I'd like a 2nd opinion on (in addition to the professor's.)


In a TV game show, 4 prizes are hidden on a game board that contains 20 spaces.  One prize is worth $10,000, two prizes are worth $5,000 (each), and one prize is worth $1,000.  The remaining spaces contains no prizes.  The show's host offers a contestant their choice of $1,000 (for not playing) or the opportunity to pick one space on the game board.  How much greater is the expected value of picking a space than not playing the game?


	
**My Incorrect Answer**:  The expected value is greater for not playing the game than it is for playing it.

**Correct Answer**: $50

&gt; The expectced value if you play the game is:

&gt; (10,000)(1/20) + (5,000)(2/20) + (1,000)(1/20) = 500 + 500 + 50 = $1,050

&gt; Since you always get $1,000 if you don't play, the expected value of not playing is $1,000. 

&gt; Comparing these, you will have an expected value that is $1,050 - $1,000 = $50 greater if you play the game &gt; &gt; than if you don't.""


I believe the solution omits that there are (-1000)(16/20) values on the game board, given that you lose the $1000 if you do play the game and do not pick of the of the four spaces with a prize. This would make the expected value of choosing a space 250.

Could anyone confirm?

Thanks!",en
1107227,2011-05-09 21:45:31,statistics,Logistic regression diagnostics,h7igs,[deleted],,https://www.reddit.com/r/statistics/comments/h7igs/logistic_regression_diagnostics/,1.0,1.0,"I fit a model that tries to predict whether someone is likely to go into academia based on whether they were born in the U.S. or not. What kind of diagnostics can I use for this though? My predictor variable, whether they are born in the U.S. is an indactor variable, so diagnositcs seem a little tricky. My results say it is a significant factor but I don't know how to check it. ",en
1107228,2011-05-10 00:30:52,MachineLearning,Looking for overviews of ways of measuring similarity,h7mjn,projector,1269210852.0,https://www.reddit.com/r/MachineLearning/comments/h7mjn/looking_for_overviews_of_ways_of_measuring/,17.0,28.0,"If I have two sets of things - let's take fruit as an example - how might I go about measuring how similar those sets are? I'm curious about approaches taken to this problem and the technical, statistical and philosophical underpinnings of it.

For example, if my sets are identical and both contain one banana, I could say they are 100% alike. But if one has a banana and apple, the other a banana and pear, how can I measure this?

As a starting point, I could give 1 point for a match and divide by the average length of the sets. In this case, my sets have 0.5 similarity, while the two bananas score 1. But I can see there are problematic implications here - sets with four matching items and length nine have a lower score but more matches. I'm not sure if this is what I want or not. Similarly, I can see there are consequences to deducting points when items don't match.

I'm asking such a broad question seeking general overviews of approaches and options. Are there any good articles about similarity which you could recommend? I'd also be interested to see some examples of different similarity measures used in different applications and discussion of why they were chosen.

tl;dr there's a lot to the concept of similarity - can you suggest some overviews?

",en
1107229,2011-05-10 00:37:33,MachineLearning,Their model said that there was a 80.9 percent chance that bin Laden was hiding in the town of Abbottabad,h7mq3,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/h7mq3/their_model_said_that_there_was_a_809_percent/,0.0,5.0,,en
1107230,2011-05-10 02:56:35,statistics,AP Statistics exam is on Wednesday.  Any advice for us overachieving high school Statistics scholars?,h7pwd,beaverteeth92,1292728016.0,https://www.reddit.com/r/statistics/comments/h7pwd/ap_statistics_exam_is_on_wednesday_any_advice_for/,0.0,3.0,,en
1107231,2011-05-10 05:35:17,statistics,Help with unusual density function,h7tj0,MrDannyOcean,1303187964.0,https://www.reddit.com/r/statistics/comments/h7tj0/help_with_unusual_density_function/,0.0,2.0,"A random variable X has density A*sin(x) + B*cos(x)

if 0 &lt; x (pi/2)

find a and b.  Where to go from here?",en
1107232,2011-05-10 12:12:17,analytics,Customized your Yahoo! Store with Yahoo! Web Analytics,h80su,percept09,1239109799.0,https://www.reddit.com/r/analytics/comments/h80su/customized_your_yahoo_store_with_yahoo_web/,3.0,0.0,,en
1107233,2011-05-10 19:40:56,artificial,"What advanced artifical intelligence books should I 
read",h885i,mozzyb,1228491801.0,https://www.reddit.com/r/artificial/comments/h885i/what_advanced_artifical_intelligence_books_should/,1.0,0.0,"I am taking a master in Information Science and Artifical Intelligence. Assuming a moderate to advanced knowledge of artifical intelligence and machine learning in general; what books should I read?

I am interested in Neural Nets, Evolutionary Algorithms (GA, GP etc.), Reinforcement Learning, Bayesian Networks ...

I am thinking about implementing AI in a one player game and see if I can beat a human players high score.",en
1107234,2011-05-11 04:15:16,statistics,"There's probably an embarrassingly simple solution to my question, but I can't phrase my question correctly to Google it - pertaining to regressions with similar variables but under different circumstances. ",h8ksm,tatowtot,1291014599.0,https://www.reddit.com/r/statistics/comments/h8ksm/theres_probably_an_embarrassingly_simple_solution/,1.0,5.0,"I am trying to run a regression to test my hypothesis, but am not quite sure how to go about it when using the same independent and dependent variables, but under different circumstances.

My hypothesis essentially predicts a positive relationship between the perception of social consensus of an event (IV) and ethicality of people (DV), but predicts different relationship strength between the two variables between people with high levels of religiousness and people with low levels of religiousness. 

Or another example that I just came up with is...I predict that the hungrier you are, the more likely you are to go out and buy food (I know a multiple regression is used for this), but this relationship may change if the weather is (a) sunny or (b) rainy. (This is where I get lost). 

Any tips will be greatly appreciated and thanks in advance!",en
1107235,2011-05-11 05:17:15,datasets,Library Hack - Records and images from Australian and NZ libraries,h8m72,HeapsGood,1267759042.0,https://www.reddit.com/r/datasets/comments/h8m72/library_hack_records_and_images_from_australian/,1.0,1.0,,en
1107236,2011-05-11 06:19:55,statistics,gains chart question,h8nga,lemongrenade,1300836292.0,https://www.reddit.com/r/statistics/comments/h8nga/gains_chart_question/,3.0,1.0,"so i have this table

 actual:   T     T     T     F    F
predict:   T     F     T     F     F


how do i go about drawing the gains chart both with and without the confidences
",en
1107237,2011-05-11 16:30:57,statistics,Interview Question,h8wyg,katiekara,1301538801.0,https://www.reddit.com/r/statistics/comments/h8wyg/interview_question/,22.0,12.0,"Hey Reddit - A friend of mine recently received an interview question that none of us can be 100% certain of the answer (and the interviewer wouldn't tell  him!). What do y'all think?

""The median is two standard deviations below the mean. What percent of the distribution is above the mean?""",en
1107238,2011-05-11 17:51:57,statistics,Probability of someone picking themselves in a Secret Santa,h8yqr,Captchca_ca_KA,1291820902.0,https://www.reddit.com/r/statistics/comments/h8yqr/probability_of_someone_picking_themselves_in_a/,1.0,3.0,"Hello, here is an off-season question that has been annoying me since December 23rd 2010 when myself and my 3 room-mates were picking names from a hat to see who got whom in the Secret Santa. 

If all four of us picked a name at the same time, what was the probability that at least one of us would get our own name?

Is is as simple as 1 in 4?

Thanks!",en
1107239,2011-05-11 18:21:16,statistics,"Hi r/statistics, I'm looking for a (free?) online 
Statistics book. Any recommendations?",h8zge,philogb,1170270404.0,https://www.reddit.com/r/statistics/comments/h8zge/hi_rstatistics_im_looking_for_a_free_online/,6.0,13.0,,en
1107240,2011-05-12 04:04:57,computerscience,Computer scientists are the history teachers of the year 2500. [7},h9e8r,[deleted],,https://www.reddit.com/r/computerscience/comments/h9e8r/computer_scientists_are_the_history_teachers_of/,1.0,0.0,I'm a computer science student. Disprove me. Or any who come to my rescue. ,en
1107241,2011-05-12 08:38:48,statistics,Cannabis connecting to higher education survey (Details inside),h9jx2,[deleted],,https://www.reddit.com/r/statistics/comments/h9jx2/cannabis_connecting_to_higher_education_survey/,2.0,8.0,"I'm posting this survey in a few places over the internet and would really appreciate the input. It's for school and I'm trying to get a variety of places involved so as not to be biased.

Here's the survey:https://spreadsheets.google.com/viewform?hl=en&amp;formkey=dF9JdzlEZ3MyNmQxR29hYlJfTFFzVEE6MQ
#gid=0",en
1107242,2011-05-12 09:51:24,statistics,Statistics in the environment ,h9l72,thecircleofreddit,1297270218.0,https://www.reddit.com/r/statistics/comments/h9l72/statistics_in_the_environment/,1.0,0.0,"Hi guys/girls, I am thinking about applying for a scholarship directed towards statistics majors/minors. Part of the application requires me to write a one-two page essay on how the study of the environment may be impacted by statistics. The only problem is, I have no experience with the combination! I love the environment, and statistics, but up until now they have been relatively unrelated to me. I understand right off the bat how important statistics would be, and am planning on doing my own research. I was just wondering if anyone with some experience would be interested in discussing or sharing anything with me. I would really appreciate it. Thank you",en
1107243,2011-05-12 11:22:07,analytics,"Google Analytics data error is getting worse, not better - serious problems with Google's Advanced Segments",h9mj1,jamiembrown,,https://www.reddit.com/r/analytics/comments/h9mj1/google_analytics_data_error_is_getting_worse_not/,5.0,2.0,,en
1107244,2011-05-12 18:51:43,statistics,Statistically speaking is there a 'favourite' answer for a Multiple Choice A B C D question.,h9u8y,Boblert,1179864888.0,https://www.reddit.com/r/statistics/comments/h9u8y/statistically_speaking_is_there_a_favourite/,6.0,7.0,"Has there ever been a large scale statistical test to see if A,B,C or D comes up at at a higher rate then the others in multiple choice papers (a subconscious favourite.) Much the same as 17 comes up at a high rate when asking somebody to pick a number between 1 and 20, http://scienceblogs.com/cognitivedaily/2007/02/is_17_the_most_random_number.php . ",en
1107245,2011-05-12 19:37:41,MachineLearning,"Product recommendations competition. Maximum Prize: $1,000,000",h9vfg,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/h9vfg/product_recommendations_competition_maximum_prize/,14.0,6.0,,en
1107246,2011-05-13 00:50:08,MachineLearning,Predicting web spam with HTTP session information,ha3m8,lrwiman,1241371191.0,https://www.reddit.com/r/MachineLearning/comments/ha3m8/predicting_web_spam_with_http_session_information/,0.0,0.0,,en
1107247,2011-05-13 02:18:29,statistics,10 plushies of your favorite statistical distributions (very cute) x/post from r/shutupandtakemymoney,ha5cs,dmanww,1260935032.0,https://www.reddit.com/r/statistics/comments/ha5cs/10_plushies_of_your_favorite_statistical/,1.0,0.0,,en
1107248,2011-05-13 11:15:20,statistics,"r/statistics, i do believe this is the ",hafbi,[deleted],,https://www.reddit.com/r/statistics/comments/hafbi/rstatistics_i_do_believe_this_is_the/,1.0,0.0,,en
1107249,2011-05-13 15:41:47,MachineLearning,"$45,000 algorithmic contest!",hai95,m3g4n3,1277388269.0,https://www.reddit.com/r/MachineLearning/comments/hai95/45000_algorithmic_contest/,23.0,6.0,,en
1107250,2011-05-13 17:33:25,MachineLearning,"$45,000 Competition for identification of substances from electromagnetic signatures",hake4,[deleted],,https://www.reddit.com/r/MachineLearning/comments/hake4/45000_competition_for_identification_of/,0.0,0.0,,en
1107251,2011-05-13 17:33:48,datasets,"$45,000 Competition for identification of substances from electromagnetic signatures",hakeg,[deleted],,https://www.reddit.com/r/datasets/comments/hakeg/45000_competition_for_identification_of/,0.0,0.0,,en
1107252,2011-05-13 18:40:51,MachineLearning,"What is the relationship between Neural Networks and Turing Machines (or equivalently lambda calculus), if any?",halz7,[deleted],,https://www.reddit.com/r/MachineLearning/comments/halz7/what_is_the_relationship_between_neural_networks/,0.0,1.0,,en
1107253,2011-05-13 19:41:28,computervision,Digital Images of Yale’s Vast Cultural Collections Now Available for Free,hang1,cavedave,1128052800.0,https://www.reddit.com/r/computervision/comments/hang1/digital_images_of_yales_vast_cultural_collections/,5.0,0.0,,en
1107254,2011-05-13 23:53:25,MachineLearning,New Ways to Exploit Raw Data May Bring Surge of Innovation,hattj,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/hattj/new_ways_to_exploit_raw_data_may_bring_surge_of/,8.0,6.0,,en
1107255,2011-05-14 17:23:46,statistics,20-sided dice and probability,hb8w2,jonaseriksson,1280834535.0,https://www.reddit.com/r/statistics/comments/hb8w2/20sided_dice_and_probability/,8.0,10.0,"Hi. I posted this in r/math but then I figured this subreddit might be slightly more relevant..

If someone has the answer, this will help settle a discussion.

If I have a dice with 20 sides, the chance of not rolling a number 2 in 55 rolls is 0,95^55 = 6 percent, right?

But what is the probability of this happening to any of the 20 numbers with 55 dice rolls? I.e. any of them not showing up. I'd first say it's still 6%, but a friend did not agree (""there are 20 numbers with a 6% chance so it must be more!"") and another said it was more like 74% based on simulations he did.

Help pls :)

edit: to clarify. the probability that, if I before i start rolling the dice ask myself ""what is the probability that after 55 rolls there will be any number of the 20 that has not been rolled"". i.e. not a fixed number just any of them.",en
1107256,2011-05-14 19:01:21,MachineLearning,machine learning for beginners ,hbaf9,qwsazxerfdcv,1287517243.0,https://www.reddit.com/r/MachineLearning/comments/hbaf9/machine_learning_for_beginners/,27.0,21.0,"I am more of a graphics guy , but recently I am picking up machine learning and I want to do it right.I will be dealing with tons of medical image data.
my  questions are as follows 
if i am doing machine learning in large scale what options for programming do i have?
i talked to a few people and got the following options 
1&gt;R 2&gt;matlab 3&gt;python
O kings of ML
what do you think is the way forward?",en
1107257,2011-05-14 20:19:50,MachineLearning,Ask ML: Face Detection question,hbbtr,g23f,1293501540.0,https://www.reddit.com/r/MachineLearning/comments/hbbtr/ask_ml_face_detection_question/,3.0,2.0,"I've recently been reading about Viola and Jones face detection algorithm and I was wondering a few things. I've read that it splits the image into 24x24 sub windows and I understand that. I also understand how the integral image thing makes it efficient to calculate things on the image.

The algo uses 2,3, or 4 rectangles to calculate a feature score. I'm wondering over what parts of the image it calculates this.

Does it calculate this feature score for each sub 24x24 image and then classify each of those sub windows as having a face and not? I read something about zoom levels and it seemed kind of confusing.

If it takes say a 640x480 image does it then split that into 27x20 sub boxes, apply the 2,3, and 4 feature things using the integral image for efficiency, and then using Adaboost does it say ""boxes 15 and 16 may contain a face"" is that basically how this works?

What if the face is split by two sub boxes, is that why Adaboost is used to ""guess"" if there is a face?",en
1107258,2011-05-15 16:39:47,statistics,"I have a large number of samples randomly obtained, close to 2000. Can the variables be not normally distributed?",hbtvq,mutabilis,1284179253.0,https://www.reddit.com/r/statistics/comments/hbtvq/i_have_a_large_number_of_samples_randomly/,0.0,8.0,"I read that due to the law of large numbers, my data should be normally distributed. However, when I ran the sktest in stata it rejects that the data is normally distributed.",en
1107259,2011-05-15 20:51:02,MachineLearning,"Patrick Winston Lectures on ""Neural Nets, Back Propagation, Support Vector Machines""",hbxvk,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/hbxvk/patrick_winston_lectures_on_neural_nets_back/,44.0,4.0,,en
1107260,2011-05-16 02:09:03,statistics,"Dear r/Statistics, do you know of a residual variance calculator?",hc4ac,Joshpho,1300996450.0,https://www.reddit.com/r/statistics/comments/hc4ac/dear_rstatistics_do_you_know_of_a_residual/,4.0,3.0,"I'm writing a scientific paper where I'm measuring growth rates of different chicken breeds, so I'm using simple linear regression to find b (growth rate/slope). I'm also trying to find if the difference between each of these growth rates are significant, which entails finding the residual variance. Does anyone know an online calculator (or software) I could use to calculate the value?

For reference, here is the formula:

s^2 r= 1/n-2 * ( sum of squares of y - (sum of products)^2 /sum of squares of x )

Where n is the number of observations.

",en
1107261,2011-05-16 08:59:21,MachineLearning,"What accuracy can I reasonably expect for document classification, and what can I do to improve it?",hccgp,tymekpavel,1190535699.0,https://www.reddit.com/r/MachineLearning/comments/hccgp/what_accuracy_can_i_reasonably_expect_for/,9.0,33.0,"I am currently trying to classify 30,000 full-text articles from the NY Times using 30 discrete categories that represent war, healthcare, elections, etc. I use the SVM implementation in e1071 available via CRAN to train on a Term-Document Matrix generated using the tm package, and then classify on a similar matrix.

Currently, accuracy hovers around 40-50% through random sampling on a 3000 article training sample, 2000 article classification sample. I have tried stemming words, removing stopwords, and removing sparse terms, but nothing seems to affect accuracy substantially. Are there any suggestions for how I can improve accuracy, or is this about as good as it well get for such an application?",en
1107262,2011-05-16 14:11:27,datasets,Price of weed,hcgkb,cavedave,1128052800.0,https://www.reddit.com/r/datasets/comments/hcgkb/price_of_weed/,4.0,1.0,,en
1107263,2011-05-16 20:21:07,statistics,"Hi r/statistics, need your help with a statistics/math question regarding raffles ",hcnxg,dumbhaas7,1305562683.0,https://www.reddit.com/r/statistics/comments/hcnxg/hi_rstatistics_need_your_help_with_a/,4.0,8.0,"hi, my first ever post on reddit so please excuse any mistakes. just wanted to get everyones thoughts on the mechanics of raffle draw that we are working on. basically, we have to issue 1 raffle ticket for every 1$ purchased. now we have a ton people who buy in $1- 9$ basket sizes and at the same time we have a ton of people who buy in 80$ - 90$ basket sizes of the same item. the problem is, its easy for the those who get 1- 9 raffle tickets to write their name and phone number on every single ticket but for those who get 80 - 90 or even more it becomes a real pain. my question is if we devise a way for the 80-90 to enter the same raffle number 80 times (so they only have to write it once, we just tell the system to enter that one code 80 times) will this still be fair? or am i missing something? 

im not sure if im being clear about this so il try another way too. 
if i had 10 raffle tickets from 101, 102, 103...110 or 10 raffle tickets with the same number, say 101.. the chances of being selected are the same right? and this will be fair for all other participants right? 

the promo allows for the same person to win multiple prizes and raffle tickets will be picked at the end of the promo. ",en
1107264,2011-05-17 06:39:58,statistics,"Dear r/Statistics, looking for a good resource for cluster analysis.",hd3dv,tonecapone3001,1298277107.0,https://www.reddit.com/r/statistics/comments/hd3dv/dear_rstatistics_looking_for_a_good_resource_for/,3.0,9.0,I am trying to self teach myself cluster analysis in SPSS. Does anyone have any good resources?,en
1107265,2011-05-17 08:00:37,statistics,Using if: command in SPSS to select only a few specific respondents to run a mean test? To take only respondent 4 and respondent 5 to run a mean for their income. How does it work?,hd538,atomofconsumption,1205649727.0,https://www.reddit.com/r/statistics/comments/hd538/using_if_command_in_spss_to_select_only_a_few/,3.0,3.0,"The variable I want to use is called ""geocode"" and it has a numerical value associated with a certain geographical area (for example, 247606531). I want to be able to run a mean command to compare the means of ""average income"" by only ""geocode value =  247606531"".

Seriously can't figure it out.

Is there a good SPSS guide for this that you know about? I'm working pretty blindly over here.




Not sure if this makes sense...


**Edit** tried this and it worked, not sure if it's correct:


select if (gcode2006 GT 2476054 and gcode2006 LT 2476066).
means h31v015 by gcode2006.
",en
1107266,2011-05-17 16:15:07,artificial,"Bacterial Wisdom As Template for Artificial Free Will
",hdc8a,Awwware,1289566851.0,https://www.reddit.com/r/artificial/comments/hdc8a/bacterial_wisdom_as_template_for_artificial_free/,1.0,0.0,,en
1107267,2011-05-17 19:05:53,MachineLearning,Research Directions for Machine Learning and Algorithms,hdg52,[deleted],,https://www.reddit.com/r/MachineLearning/comments/hdg52/research_directions_for_machine_learning_and/,29.0,0.0,,en
1107268,2011-05-17 22:23:44,statistics,Cities Rank-Size Distribution,hdlf5,dudarev,1267204011.0,https://www.reddit.com/r/statistics/comments/hdlf5/cities_ranksize_distribution/,4.0,0.0,,en
1107269,2011-05-17 22:49:33,statistics,"Comparing ""means"" for categorical variables with &gt;2 values; one-way ANOVA? Chi-Square test?",hdm48,saturnight,1276976518.0,https://www.reddit.com/r/statistics/comments/hdm48/comparing_means_for_categorical_variables_with_2/,6.0,17.0,"My data can be divided into 4 samples. I wanted to test whether the distribution of a binary variable differs significantly between the samples, so I did 6 t-tests - one for each pair of samples.

However, I also need to analyze how another categorical variable differs between the samples, this time with 3 possible values. 

Is it okay to compute new t-tests, each time for a pair of values and a pair of samples, for a total of 6*3 tests? The problem is that this is too detailed: I don't want to test each pair of values, only each pair of samples.

SAS also gives me the option (in *proc freq*) of calculating a chi-square test for all 4 samples, but this is not enough information: it only tells me that not all of the means are equal.

And then there's a one-way ANOVA test, which can be used to compare the means of &gt;2 samples, but apparently can't be used for categorical variables with &gt;2 values. Or can it? 

How would you analyze this data?",en
1107270,2011-05-18 08:33:16,datasets,Where can I find a large dataset of n-grams?,hdzox,jasonrosen,1304619652.0,https://www.reddit.com/r/datasets/comments/hdzox/where_can_i_find_a_large_dataset_of_ngrams/,1.0,0.0,"I'm looking for a large dataset of n-grams (preferably 4-grams or 5-grams, though bigrams or trigrams might also work), in order to get some cooccurrence data. Are there any such datasets (freely) available?

Sadly, it looks the Google n-grams datasets (http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html -- not the more recently released book n-grams) now cost $150 + shipping.

I thought I'd heard that Bing also provided n-grams datasets (http://research.microsoft.com/en-us/collaboration/focus/cs/web-ngram.aspx), but it looks like all the access is through some limited API, so (unless I'm missing something) I can't find all the cooccurrences for a given word X.

Are there any other datasets available / am I misunderstanding what Bing provides? Or is my only option to just pony up and pay for the Google data?",en
1107271,2011-05-18 14:27:10,statistics,Misuse of Statistics,he4n5,roger_pct,1232823670.0,https://www.reddit.com/r/statistics/comments/he4n5/misuse_of_statistics/,7.0,18.0,"I have heard it said often that with statistics, you can prove anything...and while I mostly disagree with this statement, there is something behind it.

For example, say ""Disease A"" afflicts 10 people in 100,000 in the year 2010, and then in the year 2011 ""Disease A"" afflicts 15 people in 100,000. Statistically, you could say there has been a 50% increase in cases of this particular disease. Mathematically this is true, but it is a very misleading use of statistics.... the sort that would be used for sensationalized news headlines.

I am interested in seeing if any redditors have links to any great articles about ""abusing the power of statistics"" in a similar vain. It does not have to be a case identical to this. I, however, would like to see what you might come up with when asked for an article about statistics being used to misinform the public.

Thanks in advance.",en
1107272,2011-05-18 20:32:26,computervision,metric rectification from circles,heck7,jeanlucpikachu,1159634949.0,https://www.reddit.com/r/computervision/comments/heck7/metric_rectification_from_circles/,2.0,0.0,,en
1107273,2011-05-18 23:42:27,statistics,Determining the Enrichment of Samples in a Complex Pool,hehpz,chicken_fried_steak,1274130970.0,https://www.reddit.com/r/statistics/comments/hehpz/determining_the_enrichment_of_samples_in_a/,6.0,3.0,"A coworker of mine posed this problem to me a few weeks ago regarding some of her research, and I was wondering what reddit thought of it.  I posted this in r/math a day or so ago but decided that it may make a bit more sense here:

You begin with a rather large pool of samples - let's say there are n types of samples in a pool, each making up a proportion p_i of the pool. You then perform some experiment that enriches some of these samples to the exclusion of others, so the proportion is now q_i where it used to be p_i. We can assume that both the pre- and post-enrichment pools are of extremely large (relative to the sampling we do) size.

You then take the pre-enrichment pool and draw M samples from it, categorizing each into one of the n categories you're looking at for a set {pre_1,pre_2,...,pre_n}, and similarly you draw N samples from the post-enrichment pool and categorize them to get counts {post_1,post_2,...,post_n}.

The question is then how to get information on the enrichment process, more specifically how to determine which samples are significantly enriched relative to the rest of the pool - that is, which are abundant at levels significantly higher than you would expect by chance alone.  My original thought on this was that you could take each sample count to be binomially variant, so that the variance on, say, pre_10 can be treated as pre_10*(1-pre_10/M) and then the enrichment computed as (post_10 + d) / (pre_10 + d) * M / N, where d is some pseudocount to account for undersampling (in practice, many of the pre and post values are simply 0 due to insufficient sampling of the distribution - in this case there are about 13,000 classes of sample and M and N are in the hundreds of thousands to millions, so undersampling for rare events is a risk in many cases).  Doing things this way, though, doesn't seem to discriminate very well in picking out what's significantly enriched versus what isn't.  I was curious if there was a more refined analysis that could be done that might be better at picking out what's really significant within the data set.

Thanks!",en
1107274,2011-05-19 03:11:58,data,“Data Mercenary” Factual officially releases U.S. POI &amp; business listings,hemt0,[deleted],,https://www.reddit.com/r/data/comments/hemt0/data_mercenary_factual_officially_releases_us_poi/,1.0,0.0,,en
1107275,2011-05-19 05:18:33,statistics,"r/statistics, what are your thoughts on our final for the year-long statistics course in our psychology dept?",heplm,nucleusaccumbens,1304493444.0,https://www.reddit.com/r/statistics/comments/heplm/rstatistics_what_are_your_thoughts_on_our_final/,7.0,12.0,,en
1107276,2011-05-20 00:34:19,MachineLearning,Learning to Compress Images and Video,hfdih,wally_fish,1272829059.0,https://www.reddit.com/r/MachineLearning/comments/hfdih/learning_to_compress_images_and_video/,5.0,0.0,,en
1107277,2011-05-20 12:40:08,statistics,"Gathering data for a simple statistics project, please take this simple survey (9 quick questions). Thank you",hfrrv,roger_pct,1232823670.0,https://www.reddit.com/r/statistics/comments/hfrrv/gathering_data_for_a_simple_statistics_project/,4.0,3.0,,en
1107278,2011-05-20 12:52:12,statistics,Notable statistics departments in the area of statistical machine learning?,hfrx4,shazbotter,1181932997.0,https://www.reddit.com/r/statistics/comments/hfrx4/notable_statistics_departments_in_the_area_of/,10.0,16.0,"Hi everyone,

I thought about posting this in /r/machinelearning as well but thought I would post here first.  I'm interested in the statistical side of machine learning and I am contemplating applying for graduate schools next fall.  Most of the lists of notable schools for machine learning tend to be lists of notable CS departments.  What are some schools I should be looking at if I'm interested in the statistical side of machine learning?

Besides the obvious departments of statistics at stanford, berkeley, UCLA and CMU, which schools should I be looking for?  Those are certainly fine schools but very competitive; what are some mid-ranked/lower ranked programs that have a decent research record in the area of statistical machine learning? ",en
1107279,2011-05-20 18:29:26,statistics,Statistical probability of the rapture on any given day,hfxx0,gimme_dat_bbq,1288620760.0,https://www.reddit.com/r/statistics/comments/hfxx0/statistical_probability_of_the_rapture_on_any/,0.0,2.0,In honor of tomorrow's rapture (heh) here is a thought experiment for you stats geniuses.  Please assume that a rapture will happen sometime in the span of human existence.,en
1107280,2011-05-20 21:10:00,analytics,How to identify which Google Analytics account is tracking my site?,hg2g9,GoogleAnalyticsQ,1305914727.0,https://www.reddit.com/r/analytics/comments/hg2g9/how_to_identify_which_google_analytics_account_is/,3.0,3.0,"Hey all, my gf is having trouble with GA and hasn't gotten any response in 3 days from posting in the Google help forums, so I figure I'd try here. Question as follows:

""I have a client that we coded a website for and Google analytics was plugged into it.  We would like to look at the statistics for the site, and no one can identify what account is associated with the tracking code that is embedded.

I have pulled the user account number from the source code, I'm just not sure how to identify what the login associated with it is.

Can anyone help?   This is a fairly urgent request.""

Thanks in advance for any help!",en
1107281,2011-05-20 22:52:51,statistics,Make 10k with you Stats Awesomeness,hg5bc,thentherewerelimes,1277293201.0,https://www.reddit.com/r/statistics/comments/hg5bc/make_10k_with_you_stats_awesomeness/,1.0,0.0,"From this week's innocentive.

Detection of Differences between Non-Standard Distributions 
(#9932646)
The Seeker is looking for statistical approaches to describe changes (or shifts) in the distributions of data on experimental subjects between two different treatments. There are numerous possible choices for statistical approaches, but ultimately the Seeker needs an approach, which can be easily communicated to non-technical consumers of the research. Many more details are available within the Challenge details section of this Challenge.
This Challenge requires only a written proposal.
Deadline: July 18, 2011
Reward: $10,000",en
1107282,2011-05-21 05:14:08,datasets,the R datasets package,hgdwa,Lors_Soren,1282243358.0,https://www.reddit.com/r/datasets/comments/hgdwa/the_r_datasets_package/,1.0,0.0,,en
1107283,2011-05-21 07:48:13,datasets,Sensory data from a french fries experiment,hggr3,Lors_Soren,1282243358.0,https://www.reddit.com/r/datasets/comments/hggr3/sensory_data_from_a_french_fries_experiment/,1.0,0.0,,en
1107284,2011-05-21 12:57:46,MachineLearning,Rock Paper Scissors Programming Competition,hgl72,byronknoll,1287719594.0,https://www.reddit.com/r/MachineLearning/comments/hgl72/rock_paper_scissors_programming_competition/,7.0,6.0,,en
1107285,2011-05-21 19:18:54,MachineLearning,I have a short internship doing data mining...and am getting overwhelmed,hgqdv,Tiomaidh,1256863695.0,https://www.reddit.com/r/MachineLearning/comments/hgqdv/i_have_a_short_internship_doing_data_miningand_am/,26.0,10.0,"I'm a high school senior, and for my senior project I'm spending 3-4 weeks working at a company that does data mining. I've just finished my first week, and have:

* Read a bunch of online material to get an overview of data mining.

* Read several chapters from [*Data Mining* by Witten and Frank](http://www.amazon.com/Data-Mining-Practical-Techniques-Management/dp/0120884070/ref=sr_1_2?s=books&amp;ie=UTF8&amp;qid=1305991819&amp;sr=1-2).

* Played with WEKA, mostly to learn how to use it. (And sworn at the ARFF file format).

* Worked a bit with MS SQL Server and Excel PowerPivot.

* Installed Excel's Data Mining plugin.

There's a *lot* of data for the project, but I myself have downloaded and am working with six databases, each of which has approximately six million rows. It's about some pretty complex engines, and has several years' worth of readings from several dozen sensors. I think it's supposed to be tied to events (engine turns on, turns off, overheats, fails, etc.) but that column is blank.

Recently, I had the following conversation:

&gt;Me: I have all the software installed and more-or-less know how to use it. So...what am I supposed to be *doing*?

&gt;Boss: Oh, just play around with the databases and see what you learn.

&gt;Me: Well, I started doing that, but I feel like I can't really discover anything useful without having the readings tied to events...

&gt;Boss: No, there's a lot you can learn without the events.

&gt;Me: I mean, while learning WEKA I made really trivial observations, like, ""The temperature and pressure have a strong positive correlation"", but--

&gt;Boss: No, that's a good thing to know!

&gt;Me: Oh, well, um, okay.

For reference, here's a simplified version of the data I have:


Timestamp| SensorID  | Value | Unit
----------|----------|---     |
1/1/01 0:01| 1          | 478    | PSI
1/1/01 0:01| 2          | 50 | Ft/sec
1/1/01 0:01| 3          | 0 | 0=off; 1=on
1/1/01 3:23| 1          | 485 | PSI
1/1/01 3:23| 2          | 55 | Ft/sec
1/1/01 3:23| 3          | 1 | 0=off; 1=on


And here are my problems:

* I have a number of issues with actually manipulating the data into a useful form. I can figure it out eventually, it's just not streamlined at all. If you're interested, [here](http://pastebin.com/mj4epk8N)'s a description.

* I feel like I'm obsessing over stuff like my ARFF files just because I'm not actually sure what I'm going to do with them once they've actually been loaded into WEKA. It's also a much easier problem to solve, and tricks me into feeling productive when I do solve it.

* My boss told me to read up on Naive Bayesian classifiers, which I did, so I'm probably the most knowledgeable about how to classify stuff. However, as far as I know, I have no classes in which to put stuff (if I had the EventIDs, I'd imagine it'd be massively useful to be able to classify things into events...) I'm actually a bit unsure about what I *could* do...I guess I could make a model to predict the reading of one sensor based on the readings of the other--but when would that be useful? If you can get all but one reading, odds are you can get the last one too. I guess I could also try to predict stuff based on the previous readings (which would probably be useful, actually)...but without knowing if the engine is on or off or whatever, I'm guessing it wouldn't be as accurate as one would hope.

* Ooh, I guess I could discretize everything, and *then* I could classify stuff, right? Is this a good idea?

So I'm unclear about what my objective is, and I have only a shallow knowledge about how to achieve any objective I might have.

In case it's useful--I *do* know how to program (in Python, Java (I even have a Real Job doing Java development), and Lisp (I won prizes for a pretty cool genetic algorithm that generates AIs for games)), I took AP Statistics (which so far has not been remotely useful, unfortunately). I'm just quite clearly in very new territory (and don't get me wrong--I'm really happy to be learning new things. It's just pretty uncomfortable, too.)

So I'm sorry if this has been kinda long and unfocused. I'm just overwhelmed and am hoping that you guys will have some words of wisdom.

Thanks in advance.


---------------

TL; DR - I have a stack of data that I'm supposed to get information from. The data is readings from sensors on a large engine (and thus has the pressure, temperature, and other parameters from several different locations) taken periodically over the course of several years. There is no *specific* thing I'm trying to find out. I think it'd be useful to be able to take a set of readings and predict the next one, but that's just me--I could be wrong. Rather than blindly running promising-looking algorithms and seeing what happens, I'd like suggestions on what I should be looking for. I'd rather have an objective and then select a method based on it than do it the other way around.",en
1107286,2011-05-21 20:59:21,statistics,Questions Regarding Modelling [x-post from /r/math],hgsci,someone13,1281015435.0,https://www.reddit.com/r/statistics/comments/hgsci/questions_regarding_modelling_xpost_from_rmath/,1.0,1.0,,en
1107287,2011-05-22 02:05:51,statistics,Whats a pirate's favorite command based statistics program...?,hgyf4,latortilla,1299276404.0,https://www.reddit.com/r/statistics/comments/hgyf4/whats_a_pirates_favorite_command_based_statistics/,41.0,6.0,"... STATA. 

(because you can't pirate R)",en
1107288,2011-05-22 14:08:57,statistics,"Someone must have experienced this problem, but I'm stumped",hh9g2,sanity,1149365629.0,https://www.reddit.com/r/statistics/comments/hh9g2/someone_must_have_experienced_this_problem_but_im/,11.0,24.0,"Sorry about the non-descriptive title, but I can't think of a concise way to summarize this problem.

I'm trying to find as many golden tickets as I can, so that I can sell them to rich, spoilt kids that want to go on a tour of Wonka's chocolate factory.

Fortunately, I have a machine that can tell me the probability that a ticket is present within a chocolate bar without having to open it.  I can use this machine on chocolate bars before I must buy them, so I can use it to select the chocolate bars most likely to have tickets.

Of course, the machine isn't accurate, so the probability it gives me will have some error, but this error will be distributed evenly around the real probabilities, so the positive error will cancel out the negative error across multiple chocolate bars.  This means that if I tested every chocolate bar, I could determine the overall probability of finding a winning ticket by averaging the predictions produced by the machine.

So I test 10,000 chocolate bars (its Costco), and pick the 100 with the highest probabilities as predicted by the machine.

Now, to test the machine's accuracy, I average its predictions for these 100 bars, and I get 0.2 - so there is a 1 in 5 chance that any given chocolate bar will have a ticket - so I expect to find 20 in this group of 100.

The problem?  The average of the predictions is *above* the actual rate at which I discover tickets in this group of 100.

In fact, if I repeat this experiment many times, the average of the predictions is **always** higher than reality.

I understand why this is, because when I select those chocolate bars with the highest predicted probabilities, I'm more likely to select those with positive error, and less likely to select those with negative error.

My question is: is there any way I can correct for this bias, so that I can accurately estimate the number of chocolate bars in the group of 100, based only on the predicted probabilities?

*Edit:* Here is the [Java code](https://gist.github.com/985440) I wrote to test my hypothesis, and [here](https://gist.github.com/985441) is an example of its output.  As you can see, the average of the predictions is always higher than the average of the actual probabilities.

Oh, and [here](https://gist.github.com/985442) is what happens if, rather than selecting the highest predictions, we select the lowest.  As we would expect, the average of the predictions are now consistently lower.

Lastly, [here](https://gist.github.com/985443) is what happens if we average across *all* predictions - as you can see they are very close.",en
1107289,2011-05-22 17:22:20,statistics,Opinionated guide to R?,hhbfn,kiwipete,1171323108.0,https://www.reddit.com/r/statistics/comments/hhbfn/opinionated_guide_to_r/,8.0,7.0,"I just read [this thread](http://www.reddit.com/r/statistics/comments/hgzxb/anybody_speak_r_213/) in which someone asks for help with a plot with facets. I know that the solution could have been accomplished with the Lattice package, or possibly even with the core R graphics, but instead the solution [gravitated toward the excellent ggplot2](http://www.reddit.com/r/statistics/comments/hgzxb/anybody_speak_r_213/c1vbpju).

That would have been my response too, as I feel that ggplot2 is the Right Way to do most graphics in R (barring 3d?). Do other people have similar areas where they feel that there are:

* multiple ways of accomplishing something in R, but
* one of those ways is the Right Way

Just curious to hear folks' opinions. To start things off I'll go with another Hadley package. I think that most people are better off using Plyr instead of the built-in apply functions.",en
1107290,2011-05-22 17:47:34,MachineLearning,A Simple Approach to Ranking Differentially Expressed Gene Expression Time Courses through Gaussian Process Regression (Crosspost from /r/bioinformatics) ,hhbrw,alkalait,1272855639.0,https://www.reddit.com/r/MachineLearning/comments/hhbrw/a_simple_approach_to_ranking_differentially/,5.0,0.0,,en
1107291,2011-05-22 18:27:14,statistics,Question with maps in r and combining with heatmaps.,hhcdy,metamorphaze,1245678351.0,https://www.reddit.com/r/statistics/comments/hhcdy/question_with_maps_in_r_and_combining_with/,2.0,6.0,"I've got some data in R about the number of farms in Maine.  I'm trying to make it so that as the number of farms decrease, the colors on a bunch of different maps will change.  I can make the map, and I've been messing around with the colors, BUT I cannot get it to combine the maps functions and the heatmaps functions in any useful way.

Ideally, I could imagine a matrix of colors formed by the farm numbers in different counties over time, and then pulling the columns for the colors, but I can't seem to make the color matrix or table or what I would need to be able to pull.  Once that's made, the map part should be pretty easy (I think).

Any ideas?",en
1107292,2011-05-22 18:32:38,MachineLearning,An IInteresting Visual tree Map for introducing Data Mining.,hhchh,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/hhchh/an_iinteresting_visual_tree_map_for_introducing/,28.0,3.0,,en
1107293,2011-05-22 20:51:05,MachineLearning,Collection of 10 Distribution Plushies,hhf13,[deleted],,https://www.reddit.com/r/MachineLearning/comments/hhf13/collection_of_10_distribution_plushies/,1.0,0.0,,en
1107294,2011-05-22 22:41:23,statistics,Contrast in SAS MANOVA is not estimable,hhhai,f4m1n3,1290015129.0,https://www.reddit.com/r/statistics/comments/hhhai/contrast_in_sas_manova_is_not_estimable/,3.0,3.0,"I've searched around and from what I have found there just is nothing I could do to fix the ""Contrast '...' is not estimable"" error.

My IV are X1 and X5.  X1 has three factors ( &lt;1, 1-5, 5+).  X5 has two factors (direct and indirect).  

The DV are X19, X20, X21.  They are continuous variables that range from 0-10.  They are normal distributed with no outliers.  

The first two contrasts work perfectly fine but the third is where I'm getting the error.  If someone could just look at the code and see if there is some error I've made that is contributing to the ""not estimable"" error I would appreciate it.


    proc glm data=stuff ;
    class x5 x1;
    model x19 x22 = x5|x1 / ss3 ;
    contrast 'direct vs indirect' x5 1  -1 /e;
    contrast '&lt;1 vs 1-5 vs 5+' x1 2 -1 -1 /e;
    contrast 'Direct &lt;1  VS  Indrirect &lt;1' x1*x5 1 0 0 -1 0 0 /e ;
    manova h = _all_ ;
    run;

The coefficients for contrast that I get for the third contrast are as follows:

                Intercept                                                        0

                x5        Direct to customer                                     0
                x5        Indirect through broker                                0

                x1        1 to 5 years                                           0
                x1        Less than 1 year                                       0
                x1        Over 5 years                                           0

                x5*x1     Direct to customer 1 to 5 years                        1
                x5*x1     Direct to customer Less than 1 year                    0
                x5*x1     Direct to customer Over 5 years                        0
                x5*x1     Indirect through broker 1 to 5 years                  -1
                x5*x1     Indirect through broker Less than 1 year               0
                x5*x1     Indirect through broker Over 5 years                   0


",en
1107295,2011-05-23 04:24:07,statistics,Which statistical concepts should I be proficient at to solve this paper?,hhocj,dassouki,1215439790.0,https://www.reddit.com/r/statistics/comments/hhocj/which_statistical_concepts_should_i_be_proficient/,3.0,5.0,,en
1107296,2011-05-23 05:28:44,statistics,Request for papers: How does your field apply statistics to its data,hhpqx,[deleted],,https://www.reddit.com/r/statistics/comments/hhpqx/request_for_papers_how_does_your_field_apply/,1.0,0.0,"Whether it's gaussian mixture models to classify true and false positives in biological data, or a dickey-fuller test to identify AR(1) processes in econometrics, statistical methods are applied to nearly every field of science.

So what do you do? Are there papers (famous or not, important or not, reviews or not) that demonstrate canonical methods that dominate (or that you think will come to dominate) your field?",en
1107297,2011-05-23 19:30:31,MachineLearning,AskML: Where do you start when you've got a big pile of data?,hi55g,[deleted],,https://www.reddit.com/r/MachineLearning/comments/hi55g/askml_where_do_you_start_when_youve_got_a_big/,1.0,0.0,"Just trying to get my head around what the experts do when you're first sat down with a dataset and told 'see what you can find'?

The situation I'm in, via my job, is that I've got a chunk of data about horse racing, which I've always wanted to analyse for trends, and now I've been given the opportunity to spend some time on it but I'm a bit stuck on where to start.

There are just so many approaches; I could try to build a classifier to separate winners from losers, or a decision tree to determine whether to place a bet, or a genetic algorithm where the fitness function is profit.... The actual brief is very open-ended - we just want to extract some interesting information about how the different factors typically influence the outcome of a race, we're not looking to create some amazing prediction algorithm or anything. At the moment I'm thinking that EM is a good place to start.

My background: the machine learning course I did at uni (MSc) was geared more towards understanding the underlying mathematics than towards eg learning how to use weka. So I do understand the core concepts, but currently I'd be more likely to write my own implementation than use something off-the-shelf. Which I know is a bad idea!

I'd really appreciate your help MLdit!",en
1107298,2011-05-23 20:02:17,statistics,"Principles of Uncertainty - A free ebook by (former, now retired) CMU stats professor Joseph B. Kadane, covering the Bayesian approach to statistics &amp; stuff",hi5w4,fbahr,1275413362.0,https://www.reddit.com/r/statistics/comments/hi5w4/principles_of_uncertainty_a_free_ebook_by_former/,28.0,8.0,,en
1107299,2011-05-23 20:10:33,MachineLearning,Advice on industry jobs for PHD's in Machine Learning..,hi63q,stopthemachines,1306119205.0,https://www.reddit.com/r/MachineLearning/comments/hi63q/advice_on_industry_jobs_for_phds_in_machine/,37.0,51.0,"Hi. I'm about to start a PHD in the fall in computer science focusing on Machine Learning and Information Extraction. I am aware that faculty positions are hard to come by and may not be the best fit for me in the future, and I was wondering what the industry jobs prospects are there for people trained in Machine Learning at the advanced graduate level. 

The program I am accepted into is a well ranked one and has some famous professors in the field I can work with. 

If you have one of these sorts of jobs I would like to know the following:

* Do you regret the time taken to get your PHD?
* Are you happy with your salary?
* Do you find the work stimulating?
* Do you find this sort of work isolating (Could someone who defines themselves as an extrovert work in your job)?
* Is there an ability to move up and progress at your job or is it dead end?

Thanks in advance.",en
1107300,2011-05-24 00:25:54,statistics,Distribution incorporating skewness and kurtosis,hicvd,[deleted],,https://www.reddit.com/r/statistics/comments/hicvd/distribution_incorporating_skewness_and_kurtosis/,1.0,2.0,"I have the return of an asset that has skewness and kurtosis. I would like to be able to run similar statistical analysis (cdf, etc) on it as I can with something that is normally distributed. The return is close to normal save for the skewness and kurtosis (not lognormal or anything weird). I have the mean, std dev, kurtosis and skewness, is there a way to do this? I have exel and R at my disposal. Thanks.",en
1107301,2011-05-24 01:56:20,MachineLearning,Overfitting Entire Companies with Statistical Modeling,hif2e,pgroves,1306191110.0,https://www.reddit.com/r/MachineLearning/comments/hif2e/overfitting_entire_companies_with_statistical/,8.0,1.0,,en
1107302,2011-05-24 05:33:24,statistics,"Chi-square analyses - if the cell count is less than 5, is the result still valid?",hik10,turtfan,1287365218.0,https://www.reddit.com/r/statistics/comments/hik10/chisquare_analyses_if_the_cell_count_is_less_than/,8.0,14.0,"I'm running a series of chi-square analyses and for several of them, I'm getting a message which states that ""# cells have an expected count less than 5"".  If the results are significant, are they still worth reporting in spite of this message?  I'm comparing participant selections of one of three alternatives when assigned to one of four conditions, examining pairs of comparisons.",en
1107303,2011-05-24 15:19:06,MachineLearning,How positive or negative are the ultimate consequences of the creation of a  human‐level (and beyond human-level) machine intelligence likely be?,hiua0,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/hiua0/how_positive_or_negative_are_the_ultimate/,11.0,16.0,,en
1107304,2011-05-24 17:37:21,MachineLearning,Finding trending topics with Hadoop Pig,hix2q,henri_11,1306247472.0,https://www.reddit.com/r/MachineLearning/comments/hix2q/finding_trending_topics_with_hadoop_pig/,0.0,0.0,,en
1107305,2011-05-24 22:04:08,MachineLearning,Classic papers in Machine Learning?,hj4cx,CuriouslyStrongTeeth,1262029271.0,https://www.reddit.com/r/MachineLearning/comments/hj4cx/classic_papers_in_machine_learning/,34.0,22.0,"I am trying to get into machine learning, and one thing that has been helpful to me in other fields has been reading some of the ""classic"" papers in the field. I don't have a mentor for machine learning to ask, so I turn to you guys. What are some of the ""classic"" machine learning papers that helped establish the field?",en
1107306,2011-05-25 06:03:58,computervision,Tell me how to do my job,hjggy,IAmTimHearMeRoar,1306292496.0,https://www.reddit.com/r/computervision/comments/hjggy/tell_me_how_to_do_my_job/,2.0,8.0,"I'm a semester from graduating college and got a summer job. As part of this job, I wrote my first computer vision application from scratch, having never done any research in the field previously. After two weeks of stumbling around wikipedia, I produced a working application but I feel that my implementation must be so naive that it's laughable. I would like criticism of my method.

My task:  
Provided an image of a 4 7-segment displays, figure out which ones are on and which ones are off.

My method:  
1. Run across the gray-scale image in the x direction, looking for steep differences in pixel values. If this difference crosses a certain threshold, currentlyInDarkArea = !currentlyInDarkArea. Mark the ""dark areas"".  
2. Do the same thing in the y direction.  
3. AND the results from steps 1 and 2 together. This results in something like [this](http://i.min.us/icJym6.png).  
4. Put [this thing](http://i.min.us/icOxno.png) through various perspective transformations until it optimally fits the image from step 3. The result is something like [this](http://i.min.us/icKjfw.png).  
5. Find the percentage of each segment that is obstructed. If this rises above a certain threshold, count it as on.

Comments?

p.s. This is an account I made recently as a ""I don't care if people know who I am"" account. I've been a redditor for years.",en
1107307,2011-05-25 08:11:09,statistics,Question on poker probabilities,hjjfq,sirsosay,1254514768.0,https://www.reddit.com/r/statistics/comments/hjjfq/question_on_poker_probabilities/,2.0,4.0,"This isn't a homework problem, trust me.  My feeble mind is attempting to pass the first actuarial exam.

My question is pretty simple, yet I can't wrap my head around it.  Instead of writing them out, I'll refer you to the math at the poker probability wiki: [here](http://en.wikipedia.org/wiki/Poker_probability)

Why is it that a full house has (13 choose 1)*(12 choose 1) and the two pair has (13 choose 2).  The rest of the math I understand, but I'm having a hard time recognizing the difference between the two even though they have drastically different numbers.

For the full house, out of 13 you're choosing one rank for the three-of-a-kind, and then 1 rank for the two-of-a-kind.  But when calculating the two pair, you're choosing the two ranks simultaneously.  My question is, what is the difference?

Thanks for any help.",en
1107308,2011-05-25 08:19:37,statistics,"Statistics gear for you, your prof, a friend.  Proceeds help me pay my tuition =) ",hjjm3,TraptInaCommentFctry,1288736832.0,https://www.reddit.com/r/statistics/comments/hjjm3/statistics_gear_for_you_your_prof_a_friend/,0.0,0.0,,en
1107309,2011-05-25 15:20:30,statistics,Dealing with a LARGE data set (30+GB) in R or what other software?,hjq5d,ultraspeedz,,https://www.reddit.com/r/statistics/comments/hjq5d/dealing_with_a_large_data_set_30gb_in_r_or_what/,22.0,32.0,"I have time series data it has the time column and the price column, I also have this for quotes, Bid-offer and time.

R keeps on telling me that my ram is used up (I have 8gb), what are some ideas on how to get it to go through the entire set..

",en
1107310,2011-05-25 16:11:07,rstats,R4ALL - a great course and book for the life sciences r novice.,hjr34,FactoidHunter,1284634166.0,https://www.reddit.com/r/rstats/comments/hjr34/r4all_a_great_course_and_book_for_the_life/,8.0,3.0,"Took this course and went from frightened to able.

Can't recommend it enough, if you are lucky enough for them to come to your institution.  If not they have a book coming out soon.

EDIT:  here's the [link](http://www.r4all.group.shef.ac.uk/) ",en
1107311,2011-05-25 19:36:43,statistics,Mining patterns in search data with Google Correlate,hjw5r,mjanes,1245651052.0,https://www.reddit.com/r/statistics/comments/hjw5r/mining_patterns_in_search_data_with_google/,1.0,0.0,,en
1107312,2011-05-25 21:14:18,statistics,Google Correlate finds search patterns which correspond with real-world trends,hjyup,urish,1221689900.0,https://www.reddit.com/r/statistics/comments/hjyup/google_correlate_finds_search_patterns_which/,2.0,0.0,,en
1107313,2011-05-26 14:46:02,analytics,VideoLectures Recommender System Competition is half way through,hkkvv,m3g4n3,1277388269.0,https://www.reddit.com/r/analytics/comments/hkkvv/videolectures_recommender_system_competition_is/,2.0,0.0,,en
1107314,2011-05-26 17:35:07,MachineLearning,Approaching ML from a statistics background.,hko9j,RA_Fisher,1299707119.0,https://www.reddit.com/r/MachineLearning/comments/hko9j/approaching_ml_from_a_statistics_background/,12.0,21.0,"I've been doing a lot of lurking around this subreddit, Kaggle, and r-bloggers and I'm very interested in machine learning.

My background is in econometrics, and statistics.  I'm proficient in SAS, and R.  I'm currently in university for a MS in stats.  As part of our program we have to choose a minor, I wanted to take an ML course, but there were at least four prereqs. including learning C++, advanced data structures, mastering recursive algorithms, etc..

My question is, how much do I need to know to start doing reasonable stuff with ML in R using existing packages?  Are there any good books for people specifically lacking in a CS background?  While I believe that I can accomplish anything I set my mind to, how far behind am I?  Is it reasonable to think I could be competing (at least middle of the pack) in Kaggle competitions using ML in two years?

Any and all advice appreciated!  Please be frank!",en
1107315,2011-05-26 19:04:47,MachineLearning,Stock prices in csv form,hkqly,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/hkqly/stock_prices_in_csv_form/,0.0,0.0,,en
1107316,2011-05-26 19:38:07,analytics,Hack for Google Analytics Beta: How to export all keywords to CSV (workaround for the 500 keyword export limit),hkri7,eponymouse,1303762090.0,https://www.reddit.com/r/analytics/comments/hkri7/hack_for_google_analytics_beta_how_to_export_all/,5.0,1.0,,en
1107317,2011-05-26 20:20:04,statistics,Zscore in R? Any package?,hksmb,[deleted],,https://www.reddit.com/r/statistics/comments/hksmb/zscore_in_r_any_package/,1.0,0.0,"I can't seem to find a package with a zscore function, i'd like to calculate the z-score of matrix or data.frames, but since i'm a newbie in r i have difficulties. If anyone can help!",en
1107318,2011-05-26 22:28:22,MachineLearning,Python and Machine Learning,hkw4l,joejoe500,1291995007.0,https://www.reddit.com/r/MachineLearning/comments/hkw4l/python_and_machine_learning/,0.0,0.0,,en
1107319,2011-05-26 22:33:25,MachineLearning,Mining patterns in search data using Google Correlate,hkw9n,dmdude,1250681632.0,https://www.reddit.com/r/MachineLearning/comments/hkw9n/mining_patterns_in_search_data_using_google/,10.0,0.0,,en
1107320,2011-05-27 01:11:33,MachineLearning,IBM’s Watson Now A Second-Year Med Student,hl0k8,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/hl0k8/ibms_watson_now_a_secondyear_med_student/,44.0,8.0,,en
1107321,2011-05-27 16:11:40,statistics,On Chomsky and the Two Cultures of Statistical Learning,hlhcf,vorushin,1251454537.0,https://www.reddit.com/r/statistics/comments/hlhcf/on_chomsky_and_the_two_cultures_of_statistical/,36.0,4.0,,en
1107322,2011-05-27 17:14:20,MachineLearning,VIMs in Random Forests,hliob,teaspoon2008,1306433523.0,https://www.reddit.com/r/MachineLearning/comments/hliob/vims_in_random_forests/,0.0,0.0,"Hi,

I'm confused about why correlated predictors don't mask each other when the Mean Decreased Accuracy variable importance method (where the values of the predictor are permuted over the out of bag set) is used. Under my current understanding of this method, correlated predictors, even if strongly correlated with the target variable, would show a low importance because the other would act as a surrogate when the one is permuted. Maybe someone could point me to a link that explains this. Thanks!

Breiman says the following, but I don't understand the last sentence:
""My definition of variable importance is based on prediction. A
variable might be considered important if deleting it seriously
affects prediction accuracy. This brings up the problem that if two
variables are highly correlated, deleting one or the other of them
will not affect prediction accuracy. Deleting both of them may degrade
accuracy considerably. The definition used in random forests spots
both variables.""

Never posted here before, is there a better place to ask questions like this?",en
1107323,2011-05-27 17:46:28,statistics,Setting Rapache in Natty Narwhal (Ubuntu/Linux),hljfz,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/hljfz/setting_rapache_in_natty_narwhal_ubuntulinux/,1.0,0.0,,en
1107324,2011-05-27 22:46:27,statistics,Anybody else going to JSM this summer?,hlrdl,CommentSense,1292583891.0,https://www.reddit.com/r/statistics/comments/hlrdl/anybody_else_going_to_jsm_this_summer/,2.0,4.0,"Maybe a /r/statistics meet up?

And did I mention it's gonna be in Miami? Don't forget your sunscreen.",en
1107325,2011-05-28 01:05:07,MachineLearning,Why is machine learning not more widely used for medical diagnosis?,hlukz,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/hlukz/why_is_machine_learning_not_more_widely_used_for/,5.0,9.0,,en
1107326,2011-05-28 03:32:30,statistics,Advice on stat Ph.D. ,hlxa7,[deleted],,https://www.reddit.com/r/statistics/comments/hlxa7/advice_on_stat_phd/,11.0,7.0,"I am graduating from a top 10 school with a BS/MS in statistics, honors BS in math and a quite good GPA (3.8 is pretty good around here). What sort of statistics schools should I be looking at? I want to stuff on the more mathematical side of stat, but I've also been doing genetics and econ and like both those applications. Also, should I go straight into grad school? I graduate in a year.",en
1107327,2011-05-28 19:26:10,statistics,Help With New Probability Command In Latex,hmass,[deleted],,https://www.reddit.com/r/statistics/comments/hmass/help_with_new_probability_command_in_latex/,1.0,0.0,"I'm not satisfied with the new command I've made. My code:

    \newcommand{\ps}[2]{\mbox{P}_{#1}\left(#2\right)}

It displays like this

[; \mbox{P}_{Y}\left(y\right)} ;]

There's too much space between the subscripted [;Y;] and the left parenthese. Does anyone know how to get these two closer together?",en
1107328,2011-05-29 06:17:09,MachineLearning,SVM's codify the differences between the sexes.,hmm29,[deleted],,https://www.reddit.com/r/MachineLearning/comments/hmm29/svms_codify_the_differences_between_the_sexes/,0.0,0.0,,en
1107329,2011-05-29 11:11:27,MachineLearning,"Infer.NET, a framework for running Bayesian inference in graphical models « MSR Cambridge",hmq9c,fbahr,1275413362.0,https://www.reddit.com/r/MachineLearning/comments/hmq9c/infernet_a_framework_for_running_bayesian/,20.0,2.0,,en
1107330,2011-05-29 19:55:15,statistics,What is it called?,hmw7c,svdasein,1201388077.0,https://www.reddit.com/r/statistics/comments/hmw7c/what_is_it_called/,1.0,0.0,"A few years ago I came across this theory / postulate /theorem (I don't even know precisely what it was) that stated essentially this:

If population of the human species follows a bell curve from its beginning to its end, it is most likely that your lifespan will take place during the period of greatest population.

The idea has a proper name - I can't for the life of me remember it and I've been unable to come up with the google foo that locates it.  Anyone in r/statistics familiar with this?  (Honestly I'm not sure this question isn't better posed in /r/philosophy or something but it really is a notion born of statistics).",en
1107331,2011-05-29 20:49:27,statistics,Test for an association between time of day and a response variable?,hmx8g,poochwheels,1295037110.0,https://www.reddit.com/r/statistics/comments/hmx8g/test_for_an_association_between_time_of_day_and_a/,6.0,16.0,"A reviewer wants me to test if various behavioural assays I performed were influenced by the time of day I performed them (i.e. biased). 

The assays have either binary or continuous responses. What is the best approach to this? It is as simple as fitting a GLM, response~time, and testing for a non-zero slope? That seems overly simplistic to me. I've also tried GAMs with gaussian or binomial error functions, but it doesn't seem to be doing what I expect it to do... 

Any thoughts would be greatly appreciated. I'm sure its stupidly simple, but I'm having trouble wrapping my head around the problem!

Edit: more details:

We assayed the behaviour of six species of spiders. The purpose of the study was to compare the behaviour of some species to others. For various reasons, we could not balance the time of day at which we assayed the behaviours for each species (the wonders of field biology...). It may be the case that these spiders have some sort of daily behavioural cycle, e.g. more active at night (however it could be the opposite, or really any function of time; we have no idea). I think the reviewer's concern is that this could influence the comparisons, e.g. if we assayed species 1 more often at night than species 2?

I suppose the explicit hypothesis I am interested in testing is that time of day has a significant effect on behavioural response, but I am unsure how to construct a model to test this, given that I have no idea if the relationship will be linear or some complex function.",en
1107332,2011-05-30 13:06:59,MachineLearning,(XPost) Recommender systems and movie hack day in Berlin next weekend. MARK IT ZERO!!!!,hne04,happyteapot,1250881365.0,https://www.reddit.com/r/MachineLearning/comments/hne04/xpost_recommender_systems_and_movie_hack_day_in/,1.0,0.0,,en
1107333,2011-05-30 16:28:45,MachineLearning,Visualization of Machine Learning algorithms,hngqr,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/hngqr/visualization_of_machine_learning_algorithms/,74.0,23.0,,en
1107334,2011-05-30 21:17:09,MachineLearning,Induction Magnetometer prediction of earthquakes ,hnmnk,IP0,1266550960.0,https://www.reddit.com/r/MachineLearning/comments/hnmnk/induction_magnetometer_prediction_of_earthquakes/,3.0,0.0,"While researching HAARP to try to debunk some weather control conspiracies, (I always forget the 'never argue with an idiot' mantra) I came across some people talking about high fluctuations in the HAARP Induction Magnetometer readings and a correlation with seismic activity. Further research found one study showed a statistically valid results in predicting earthquakes from EM activity. cite http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=1549778

Now given that not all EM activity is from earthquakes, something more is needed to give accurate detection/prediction.
Now for my questions:Is there publicly minable data of EM occurrences, and seismic activity? Has much research been done using Support Vector Machines/Genetic Algorithms/Neural Nets as a training model for earthquake prediction, and if so, what problems have arose from these techniques?

disclaimer - Google queries yield nothing but non-science results.",en
1107335,2011-05-31 07:42:13,statistics,Diversity Indices,hnzk6,patrickj86,1284705179.0,https://www.reddit.com/r/statistics/comments/hnzk6/diversity_indices/,1.0,0.0,"I'm using tests of diversity for my Master's thesis in Historical Archaeology to essentially show how the extent to which different communities made different ceramics. At this point, I've used the [Paleontological Statistics Package](http://folk.uio.no/ohammer/past/) to show dominance and evenness values, as well as Shannon, Menhinick, Fisher alpha, and Margalef indices. I'll probably end up using [Biodiversity Pro](http://gcmd.nasa.gov/records/NHML_Biopro.html) to do rarefaction. 

Any advice or resources would be great! In particular, I've only found limited discussion of the limitations of each index, so more details would be very helpful. Thank you very much. ",en
1107336,2011-05-31 17:12:45,artificial,Why Are Spy Researchers Building a 'Metaphor Program'?,ho8ej,[deleted],,https://www.reddit.com/r/artificial/comments/ho8ej/why_are_spy_researchers_building_a_metaphor/,1.0,0.0,,en
1107337,2011-05-31 20:50:34,statistics,"""Data Analysis for Politics and Policy"" by Edward Tufte (Prentice-Hall, 1974) in PDF format",hodpe,fbahr,1275413362.0,https://www.reddit.com/r/statistics/comments/hodpe/data_analysis_for_politics_and_policy_by_edward/,25.0,5.0,,en
1107338,2011-05-31 22:56:12,statistics,[Game Theory?] Dice question.,hoh5b,Mobius_One,1263232881.0,https://www.reddit.com/r/statistics/comments/hoh5b/game_theory_dice_question/,1.0,0.0,"**Background:** The game system in general is DnD 4e, if anyone is familiar with it, you can go ahead and skip the background.  Anyways, in this game, there are certain attacks that a player can make which include multiple enemies.  In order for the attack to succeed on any enemy, the roll must exceed a constant defense value for each enemy.  

Going by the rules, a player should make separate attack-rolls against each enemy to determine if it gets hit.  With my rules, I would rather they just roll once and then check this value against all of the enemies defenses.  

**Question:**  Is there any difference, probabilistically speaking, in rolling for each enemy separately or figuring out whether any of them get hit given a single dice roll?

**Ex1:**  A player uses Fireball against 3 enemies.  They are all the same type of enemy, so whatever the player has to roll (say a 10 or higher[out of 20]) to hit them is the same across the board (multiple independent events).  Is there an advantage to rolling 3 separate times to see if any/some/all of the attacks hit?  Or does it not matter due to the events being identical independent repetitions?

**Ex2:**  A player uses Fireball against 3 enemies.  They are all the different types of enemy, so whatever the player has to roll (say a 10 or higher[out of 20]) to hit one of them is different than what he must roll for the other two (say an 11 and 12 respectively).  Is there an advantage to rolling 3 separate times to see if any/some/all of the attacks hit?  Or does it not matter due to the events being independent?

**Hypothesis:  I think that there isn't a difference, after doing some binomial tests, but I may be interpreting them wrong.  Also, it seems kind of silly for the game's creator (Wizards of the Coast) to have this ""roll separately"" rule if all it does is give the player the illusion of an advantage.**

**TL;DR** Can the rules in 4th Edition Dungeons and Dragons be changed so that any AoE attack is rolled only once?  (In an effort to speed up combat without having an effect on the balance of the game)

**Edit1:** It seems at this point that the hypothesis is correct in that there is 0 difference in the long run, unless I made an error.  If there is such an error that you see, please let me know.",en
1107339,2011-06-01 02:09:55,statistics,"Reddit, I need SPSS recoding help using syntax?",holuf,tonecapone3001,1298277107.0,https://www.reddit.com/r/statistics/comments/holuf/reddit_i_need_spss_recoding_help_using_syntax/,7.0,2.0,"Hi Reddit, I am trying to turn a string variable (with many categories) into a numeric variable (with only few categories). 

i will use the following scenario to explain what I am trying to accomplish. Say I have 20 stores (the string variable) across and what to categorize them into 5 numerical categories. Using spss syntax, how would I categorize 5 stores (the string variable) to = 1 in a numerical variable, categorize 5 stores to =2, etc...

Thank you in advance for any of your help",en
1107340,2011-06-01 04:35:11,statistics,"Hypothesis testing made easy: only one test, simulate and count.",hop4y,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/hop4y/hypothesis_testing_made_easy_only_one_test/,25.0,9.0,,en
1107341,2011-06-01 05:32:16,statistics,Protege looking for a stats Mentor,hoqib,thats_a_negative,1306459310.0,https://www.reddit.com/r/statistics/comments/hoqib/protege_looking_for_a_stats_mentor/,1.0,0.0,"Hi all, long time lurker here who has finally come around to creating an account. This is probably a long shot but wanted to see if it’s possible to find a mentor here on r/statistics. I have developed a passion for statistics during the past 12 months and really would like to get some real hands on experience with all the coding languages and analytic software. My goals are to learn R, Python, and SQL (and SAS if there’s a way to gain access to it).

I have seen many posts that recommend fabulous materials to help those who are self-learning. While those are great, I get a sense that I have no idea what I'm doing nor do I know where to start. I feel like I need someone who can teach me the important basics and practical usage scenarios. It’s probably just my personal learning style, to have someone walk me through initially, and then I can run with it on my own.

A bit on myself I have a background in web analytics and have about 3 month experience playing around in SAS. I am planning to enroll for a Masters in Statistics beginning 2012.

**TL;DR: Looking for a mentor to teach and help me understand the correct usage of R, SQL, Python, (and SAS if possible) with hands-on work. Need someone to hold my hand and walk me through first, so I can run with it and learn more on my own later.**

PS: Should I post this in r/MachineLearning as well?

Thanks!",en
1107342,2011-06-01 13:54:21,MachineLearning,The pinnacle of recommender technology,hozcd,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/hozcd/the_pinnacle_of_recommender_technology/,9.0,4.0,,en
1107343,2011-06-01 14:09:45,computervision,Using photos to match dogs to owners,hozjo,cavedave,1128052800.0,https://www.reddit.com/r/computervision/comments/hozjo/using_photos_to_match_dogs_to_owners/,3.0,0.0,,en
1107344,2011-06-01 15:44:21,artificial,Why Are Spy Researchers Building a 'Metaphor Program'?,hp0xx,r721,1258928336.0,https://www.reddit.com/r/artificial/comments/hp0xx/why_are_spy_researchers_building_a_metaphor/,15.0,5.0,,en
1107345,2011-06-01 21:32:59,MachineLearning,"Whilst IBM's Watson got all the hype, the @cyc_ai actually ""does"" Twitter",hp9hy,[deleted],,https://www.reddit.com/r/MachineLearning/comments/hp9hy/whilst_ibms_watson_got_all_the_hype_the_cyc_ai/,0.0,0.0,,en
1107346,2011-06-02 00:19:07,statistics,restrictions/assumptions of the binomial test,hpdxk,punchygorilla,1306881197.0,https://www.reddit.com/r/statistics/comments/hpdxk/restrictionsassumptions_of_the_binomial_test/,6.0,7.0,"question regarding the appropriateness of using a binomial test on some data im trying to analyze. 

ive collected 6 data points from each of 22 individuals. each observation is binary: either yes or no. each observation is an independent event. is it appropriate to collapse all the observations together and then run a binomial test to determine if overall the observed proportion of yes responses is greater than that expected by chance (here 25%)?

not sure if the binomial test requires that all data come from either a single individual or each individual contributes just a single data point, or if, given independence, it doesn't matter.

uhhh hopefully this is semi clear.

thanks in advance",en
1107347,2011-06-02 07:02:02,MachineLearning,Any RapidMiner experts out there want to help a n00b?,hpnid,boost2525,1285250186.0,https://www.reddit.com/r/MachineLearning/comments/hpnid/any_rapidminer_experts_out_there_want_to_help_a/,0.0,7.0,"I've been struggling with this for a few days and could use some gentle prodding in the right direction. 

I have approx. 24k example rows, with 25 real (double's) attributes, and a nominal label. Each example represents a snapshot of scientific measurements at a moment in time (g-forces, magnometer, etc.) and the nominal label is essentially a boolean (""was the event happening?""). I'm trying to build a model (preferably a formula) that can predict the boolean output, or provide some sort of numerical ""confidence"". 

Here's the issues I'm having:

* Almost everything I do is running out of memory, I have 3GB of RAM devoted to the RapidMiner JVM.
* In the event that I *do* get a model to self learn, I end up with something that has ""97% accuracy"", but always predicts one of the boolean values (e.g. it's 97% accurate and not 100% accurate, because it always predicts ""false"" and never predicts a ""true"")

I'm thinking some of my attributes are insignificant to the boolean result, but I don't understand how to identify which ones and eliminate them. I also think I'm wasting a lot of time trying each model type out (LibSVM, Neural Net, etc.) when the guru's would probably know which model applies to this type of data/problem. 

Thanks for any help.",en
1107348,2011-06-03 01:21:13,statistics,"Hypothesis testing made easy: part 2, case studies",hqa2a,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/hqa2a/hypothesis_testing_made_easy_part_2_case_studies/,15.0,4.0,,en
1107349,2011-06-03 18:33:08,statistics,Binomial Data: How to set sample size?,hquf4,[deleted],,https://www.reddit.com/r/statistics/comments/hquf4/binomial_data_how_to_set_sample_size/,0.0,1.0,"I could use some help here - (this is a side project at work, not homework).

We have a process that produces a product. The product consists of n identical components, and each component has an independent failure rate p. Only m components need to be functional for the product to work (built in redundancy). 

How do I go about setting a sample size k such that if we test k components we are 99% confident that at least m components are functional?

normal approximation won't work since np &lt;1.",en
1107350,2011-06-03 18:52:03,statistics,"Statistics Lie. Are You Listening, Hubspot and Shareaholic?",hquwx,jeffyablon,1283280097.0,https://www.reddit.com/r/statistics/comments/hquwx/statistics_lie_are_you_listening_hubspot_and/,1.0,0.0,,en
1107351,2011-06-04 08:40:00,statistics,Need help with R homework assignment,hrc90,[deleted],,https://www.reddit.com/r/statistics/comments/hrc90/need_help_with_r_homework_assignment/,0.0,0.0,"I have a homework assignment in R, and I know nothing about the software. The assignment involves estimating a finite mixture poisson regression model.

The DV is the number of packages bought on each purchase occasion.
The IV is the price per package.

The assignment involves 
1. Formulate the log-likelihood for the a single segment poisson regression
2. programing it in R
3. Calculate each parameter estimates and their standard errors 
4. Then do the same only with a two-segment finite mixture.

------

The second part of the assignment is modify Beta Geometric NBD Model to allow for death before the first purchase and report the 
conditional and unconditional likelihoods results.",en
1107352,2011-06-04 09:35:34,statistics,Conditional vs. Unconditional Likelihoods (BG-NBD Model),hrd3o,ninchnate,1274674761.0,https://www.reddit.com/r/statistics/comments/hrd3o/conditional_vs_unconditional_likelihoods_bgnbd/,2.0,0.0,Anyone know the difference? I need to figure out a how to write the conditional and unconditional likelihoods the Beta Geometric NBD Model where death has occured before the first purchase.  ,en
1107353,2011-06-04 09:46:04,statistics,Need help with R Homework - estimate a finite mixture poisson regression model,hrd9k,[deleted],,https://www.reddit.com/r/statistics/comments/hrd9k/need_help_with_r_homework_estimate_a_finite/,1.0,0.0,"I know absolutely nothing about R and am supposed to estimate a finite mixture poisson regression model. The assignment is as follows:
1. Formulate the log-likelihood for the a single segment poissonregression
2. program it using R
3. Give the parameter estimates and their standard errors 
Then repeat for a two-segment finite mixture. 

I have two files:
The first is the data file that has 2777 observations from 200 individuals. There are three columns: (1) id; (2) DV; (3) IV.

The second file contains 200 rows. Row 1 is for individual 1 and so on. The first column in row gives the row # in the data file indicating for the first observation of person i. The second column gives the row for the last observation for person i.

I can provide more details as needed, including the .dat files. I would really appreciate any help.",en
1107354,2011-06-04 19:23:59,statistics,Clustering for customer segmentation: is there a good statistical reason to wish to avoid including too many inputs?,hrk9j,statsthrowaway,1307204157.0,https://www.reddit.com/r/statistics/comments/hrk9j/clustering_for_customer_segmentation_is_there_a/,1.0,0.0,"A colleague convinced himself that as the number of inputs goes up, more noise creeps into the model and the clusters become less meaningful (this is more an intuition than an empirical observation -- he didn't actually test it on the data). Neither of us are very well versed in clustering or machine learning (I am trained as an econometrician, and he as a mathematician), so I thought it would be nice to have some external thoughts on this.

I believe that his intuition may be right in some situations. For example, if I want to identify different species of flowers based on a number of predictors, some of which are more noisy than the others: obviously I do not want to give too much weight to the noisy predictors, or the accuracy of the algorithm would suffer.

However, the situation I just described is quite different from the one in which we find ourselves. Indeed, we wish to segment the customers to a large corporation in a small number of homogeneous clusters; we have a considerable amounts of information about each customer (about 30 variables in total). The information is collected with great accuracy; there is no measurement error to speak of, and there are good ways of making sure outliers are not an issue.
In my opinion, the choice of which inputs to include, and what weight to give them, depends only on what we think is meaningful for the analysis, or easy to interpret. I see no statistical reason to wish to reduce the dimensionality to, say, 5 inputs or so. In fact, isn't the point of clustering, in this case, precisely to reduce the dimensionality of our data?
(I understand the computational burden, but that is a different question. Let us assume, for the sake of this question, that we have access to infinite ram and cpu speed.)

Reddit, what do you think? Am I missing something here?",en
1107355,2011-06-04 22:32:08,computerscience,"My genius son explains the cloud to his father. 
",hrny6,[deleted],,https://www.reddit.com/r/computerscience/comments/hrny6/my_genius_son_explains_the_cloud_to_his_father/,1.0,0.0,,en
1107356,2011-06-04 23:50:01,statistics,Need help with R Homework - estimate a finite mixture poisson regression model,hrpe3,ninchnate,1274674761.0,https://www.reddit.com/r/statistics/comments/hrpe3/need_help_with_r_homework_estimate_a_finite/,1.0,0.0,"Okay have another question:
I know absolutely nothing about R and am supposed to estimate a finite mixture poisson regression model. The assignment is as follows: 1. Formulate the log-likelihood for a single segment poisson regression 2. program it using R 3. Give the parameter estimates and their standard errors Then, repeat for a two-segment finite mixture.
I have two files: The first is the data file that has 2777 observations from 200 individuals. There are three columns: (1) id; (2) DV; (3) IV.
The second file contains 200 rows. Row 1 is for individual 1 and so on. The first column in row gives the row # in the data file indicating for the first observation of person i. The second column gives the row for the last observation for person i.
I can provide more details as needed, including the .dat files. I would really appreciate any help.",en
1107357,2011-06-05 20:56:50,statistics,Testing Different Methods for Merging a set of Files into a Dataframe,hs83o,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/hs83o/testing_different_methods_for_merging_a_set_of/,1.0,0.0,,en
1107358,2011-06-05 20:59:18,MachineLearning,Using R for performing SVD on the the Netflix prize dataset (a short video and links),hs85w,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/hs85w/using_r_for_performing_svd_on_the_the_netflix/,12.0,3.0,,en
1107359,2011-06-05 23:28:40,MachineLearning,"Quora :""What are some introductory resources for learning about Large Scale machine learning?""",hsbfj,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/hsbfj/quora_what_are_some_introductory_resources_for/,19.0,0.0,,en
1107360,2011-06-06 20:39:47,statistics,Get me on the right track!,hszzd,peetar,1287015536.0,https://www.reddit.com/r/statistics/comments/hszzd/get_me_on_the_right_track/,5.0,9.0,"Here's a simple version of the problem I'm trying to solve:

There are 12 socks in a drawer, 10 white, 2 red.  I draw 5 socks, what is the probability that I get 2 reds?

My assumption was that i could get the probability of all white, which is pretty easy (10/12)(9/11)(8/10)(7/9)(6/8) = 7/22
and add the probability of 1 red. But I couldn't even figure out how to get that probability.",en
1107361,2011-06-06 21:34:38,statistics,Fittesmodel.com: A user-friendly way to conduct empirical research together,ht1lu,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/ht1lu/fittesmodelcom_a_userfriendly_way_to_conduct/,1.0,0.0,,en
1107362,2011-06-06 21:41:43,statistics,I would like to learn more about statistics can anyone recommend a good book to learn and ease me into this world,ht1t3,um_nevermind,1290021094.0,https://www.reddit.com/r/statistics/comments/ht1t3/i_would_like_to_learn_more_about_statistics_can/,5.0,9.0,,en
1107363,2011-06-07 03:23:56,datasets,Anybody know where I can find stats on cable subscribers by age group?,htart,jesusabdullah,1202534664.0,https://www.reddit.com/r/datasets/comments/htart/anybody_know_where_i_can_find_stats_on_cable/,1.0,0.0,"I've been spending time with my older relatives, and they watch TV *all the time*. Meanwhile, a lot of my friends (myself included) don't even *own* a TV.

I'm wondering if I'm the only one. :)",en
1107364,2011-06-07 04:36:07,statistics,How are the electoral projections made?,htcik,pdhborges,1301817620.0,https://www.reddit.com/r/statistics/comments/htcik/how_are_the_electoral_projections_made/,1.0,0.0,What kind of statistics are used?,en
1107365,2011-06-07 08:00:37,analytics,Better Campaign Tracking in Google Analytics,hthi8,tomdiggity,1306637897.0,https://www.reddit.com/r/analytics/comments/hthi8/better_campaign_tracking_in_google_analytics/,0.0,0.0,,en
1107366,2011-06-07 08:24:26,MachineLearning,The pros and cons of robust data characterizations,hti03,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/hti03/the_pros_and_cons_of_robust_data_characterizations/,11.0,0.0,,en
1107367,2011-06-07 15:04:20,statistics,A/'The' Book on Data mining,hto7q,Rym_,1273654478.0,https://www.reddit.com/r/statistics/comments/hto7q/athe_book_on_data_mining/,0.0,0.0,Continuing my reading spree I was wondering what are good/great books on data mining? Possibly the topic itself and on data mining with R (I'm going to assume there's a book called just that :P),en
1107368,2011-06-07 19:21:20,AskStatistics,Error estimates on least squares fits,htu1y,iorgfeflkd,1255212906.0,https://www.reddit.com/r/AskStatistics/comments/htu1y/error_estimates_on_least_squares_fits/,5.0,3.0,"I have some data, and an equation with some free parameters, and I use a least squares fitting algorithm to find the parameters that make the equation best match the data. Is there a way to get error bars on the values that the algorithm spits out?

I'm using MATLAB's lsqcurvefit algorithm.",en
1107369,2011-06-07 20:52:05,statistics,ttest for qualitative biological data,htwk0,BigOx,1296023654.0,https://www.reddit.com/r/statistics/comments/htwk0/ttest_for_qualitative_biological_data/,8.0,12.0,"I am revising a paper submitted to a peer reviewed biological journal and, as always, the reviewers want to know if the observed phenomena are ""statistically significant"". I am generally not a fan of relying on statistically significance for experimental biology, as I think it is generally used incorrectly. For instance, people will do experiments until they get p&lt;.05, then stop; people will change the parameters of the test from unpaired to paired or incorrectly say that the have equal variance; people will do many, many assays but only report on the ""significant"" results. In my opinion, typically with experimental biology the differences should be obvious without a statistical test for them to be meaningful (with obvious exceptions).

So here is my experiment: expose nematodes to a condition and see whether they live or die, with no intermediate phenotypes. I will assay different genetic mutants and find that some genetic mutants are unable to survive when wildtype animals can. In a previous paper I had a condition where every wildtype animal survived and every mutant animal died. The reviewer suggested I change to a noisier assay because there was no variance in the data and thus I could not do ttest (luckily the editor stepped in).

**When I am required to perform a ttest, I typically repeat the experiment 3 times and compare the average survival for each replicate. So my data might be wildtype survival (100%, 100%, 100%) and mutant (80%, 90%, 70%). If I perform a ttest, as the reviewer has asked, do I say these data sets have equal or unequal variance?**

Thanks!",en
1107370,2011-06-08 03:54:13,MachineLearning,Explaining a Neural Net by fitting a binary tree to inputs and outputs,hu7r4,TraptInaCommentFctry,1288736832.0,https://www.reddit.com/r/MachineLearning/comments/hu7r4/explaining_a_neural_net_by_fitting_a_binary_tree/,16.0,11.0,"In Leo Breiman's paper ""Statistical Modeling: The Two Cultures"" (PDF [here](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.4933&amp;rep=rep1&amp;type=pdf)), Breiman relates this anecdote, as part of his response to Bruce Hoadley's response:  

&gt; A computer scientist working in the machine learning area joined a large money management
company some years ago and set up a group to do portfolio management using stock predictions given by large neural nets. When we visited, I asked how he explained the neural nets to clients. “Simple,” he said; “We fit binary trees to the inputs and outputs of the neural nets and show the trees to the clients. Keeps them happy!” In both stock prediction and credit rating, the priority is accuracy. Interpretability is a secondary goal that can be finessed.

I want to make sure I understand what this computer scientist at the large money mgmt company is doing.  He's taking the predicted outputs of the neural net - the yhats - and using them as the dependent variable for a single binary tree model.  Is that right?  I understand that this is solely for client interpretability, and the tree wouldn't be used for prediction, but it still feels a little funny.  The tree wouldn't fit the neural net's predictions perfectly, so you'd have situations like this:

you: ""well the tree predicts this, but that's actually not our prediction, because our real model is a neural net.""  

client: ""why does the net make a different prediction?""

you: ""it's a black box, I can't say""  

and you're back to square one.  
Thoughts?  ",en
1107371,2011-06-08 05:04:08,MachineLearning,"I know R and Matlab.  If I had to choose one language to add to my toolkit, what should it be?  ",hu9ft,TraptInaCommentFctry,1288736832.0,https://www.reddit.com/r/MachineLearning/comments/hu9ft/i_know_r_and_matlab_if_i_had_to_choose_one/,9.0,40.0,"While I can do Machine Learning in R and Matlab, I feel like I should add a ""real"" programming language to my toolkit to increase the range and attractiveness of my employment opportunities.  
Is this feeling wrong?
If not, what would you suggest?  Python?  

A bit more context, if it helps:  I did Econ as an undergrad, took a lot of Statistics classes.  I'm starting a MA in Statistics this fall, and I'm using it to take as many Machine Learning classes as I can.  
I'm interested in Machine Learning generally; at the moment, there isn't a specific application I have in mind.  

Thanks! ",en
1107372,2011-06-08 17:30:59,statistics,Hierarchical Bayesian models made easy: how to model pond scum.,hun04,[deleted],,https://www.reddit.com/r/statistics/comments/hun04/hierarchical_bayesian_models_made_easy_how_to/,1.0,0.0,"Ok, maybe not easy, but this was my first attempt at a HBM in Python and everything went better than expected.",en
1107373,2011-06-08 17:32:20,statistics,Hierarchical Bayesian models made easy: how to model pond scum.,hun1l,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/hun1l/hierarchical_bayesian_models_made_easy_how_to/,25.0,4.0,,en
1107374,2011-06-08 19:30:01,statistics,How do I calculate the proportion of the variability in correlations?,huq62,[deleted],,https://www.reddit.com/r/statistics/comments/huq62/how_do_i_calculate_the_proportion_of_the/,1.0,0.0,"Ex. Suppose the correlation between hot chocolate sales and weather temperature is -0.80. What proportion of the variability is predicted by the relationship with weather?

The back of the book says the answer is 64%, but I'm not sure how to calculate that.

I tried reading some things online, but it was kind of overwhelming to look at.



If anyone could help me out, that'd be very much appreciated. Thanks!",en
1107375,2011-06-08 20:02:15,datasets,SQL dump with 200+MM tweets from 13+MM users,hur2f,dwdwdw2,1253592990.0,https://www.reddit.com/r/datasets/comments/hur2f/sql_dump_with_200mm_tweets_from_13mm_users/,1.0,0.0,,en
1107376,2011-06-08 22:27:45,datasets,"200 million tweets from 13 million users, 543 million rows, 2gb compressed",huvc9,user24,,https://www.reddit.com/r/datasets/comments/huvc9/200_million_tweets_from_13_million_users_543/,30.0,8.0,,en
1107377,2011-06-08 22:30:15,MachineLearning,"Twitter dataset - 200 million rows, 13 million users, 2gb compressed, get it while it's hot. (/r/datasets repost)",huves,user24,,https://www.reddit.com/r/MachineLearning/comments/huves/twitter_dataset_200_million_rows_13_million_users/,42.0,20.0,,en
1107378,2011-06-09 02:31:23,statistics,"Help needed, unsure of appropriate test",hv1zq,DastardlyNYC,1276544955.0,https://www.reddit.com/r/statistics/comments/hv1zq/help_needed_unsure_of_appropriate_test/,2.0,6.0,"Hey Reddit, I'm finishing up a final paper for psych and I am super stumped as to which test would be most appropriate. 

The study is about the decision to run for office, and the variables I tested were presence of a recruiter, mentorship help, nature of mentorship (formal informal) and gender. The study is made, up I can crate data, I just need to choose the best method of analysis to show the effects (that mentorship and recruitment, for example, increase the probability of running for office and more so for women than for men).

I usually would just say an ANOVA or a multiple regression, but given that the DV is categorical, I'm a little confused. Would simple correlational data cut it? Please help out if you can, thank you!",en
1107379,2011-06-09 08:42:30,statistics,Making Simple R Packages on Windows,hvacv,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/hvacv/making_simple_r_packages_on_windows/,1.0,0.0,,en
1107380,2011-06-09 15:43:40,statistics,Help.I don't remember enough college statistics to solve this question.,hvgn0,[deleted],,https://www.reddit.com/r/statistics/comments/hvgn0/helpi_dont_remember_enough_college_statistics_to/,0.0,1.0,"What are the odds of having 3 children born in sequential minutes?  First child born at 6:27 am.  Years later, twins born at 6:25 am and 6:26 am. So I'm thinking the variable for 6:25 am would be x, then x+1, and x+2.  There are 2,400 minutes per day.  How would you set up this equation?
What are the odds?  Thanks.",en
1107381,2011-06-09 15:48:21,MachineLearning,Leafsnap cost about US$2.5 million to develop,hvgq9,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/hvgq9/leafsnap_cost_about_us25_million_to_develop/,9.0,4.0,,en
1107382,2011-06-09 17:35:39,MachineLearning,What is this method called?,hvj4k,the_cat_kittles,1287171936.0,https://www.reddit.com/r/MachineLearning/comments/hvj4k/what_is_this_method_called/,10.0,8.0,"use some of your features to cluster your data, regress on each cluster. Kind of like a M5P, but flat and unsmoothed. Alternatively, is there a way of doing locally weighted linear regression with discrete features that have more than 2 possible values?",en
1107383,2011-06-09 22:41:18,statistics,Data misclassification challenge.,hvrvx,RA_Fisher,1299707119.0,https://www.reddit.com/r/statistics/comments/hvrvx/data_misclassification_challenge/,7.0,11.0,"All, I'm currently working with a database that has misclassification problems.

As part of data cleaning it is possible to visually ID some of these misclassifications, and also develop a limited set of procedures to clean the data (based on intuition about how how the data generating process should function IRL).

My question is, where do I start, what field of statistics deals with data misclassification? Data mining? Decision tree analysis? Cluster analysis?  My ultimate hope is to create a set of procedures/model that will allow me to pop in 'dirty' data and get out 'clean' data.  I plan to test this by inserting misclassifications in data, run it through the process, and make sure that such misclassifications are taken care of.

I'm not sure if this is to abstract, please help me out with some feedback.

",en
1107384,2011-06-10 00:57:19,MachineLearning,Hit me with some pointers on news aggregator algorithms,hvvk5,aire111,1307656325.0,https://www.reddit.com/r/MachineLearning/comments/hvvk5/hit_me_with_some_pointers_on_news_aggregator/,12.0,7.0,"
Apart from simple text clustering and classification, what other algorithms are there for news aggregation? I want to, for example, take a (metadata-free) news feed and figure out what the events of the day are.

",en
1107385,2011-06-10 01:26:30,MachineLearning,Automating R Scripts - on Amazon EC2,hvw9e,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/hvw9e/automating_r_scripts_on_amazon_ec2/,7.0,3.0,,en
1107386,2011-06-10 01:52:18,MachineLearning,Qustion about Split in Random Forest Algorithm,hvwu5,by_321,1304395033.0,https://www.reddit.com/r/MachineLearning/comments/hvwu5/qustion_about_split_in_random_forest_algorithm/,2.0,6.0,"Hi,

I've been reading about random forest, here's the wikipedia entry:

  http://en.wikipedia.org/wiki/Random_forest

Step 4 is: For each node of the tree, randomly choose m variables on which to base the decision at that node. Calculate the best split based on these m variables in the training set.

What I don't understand is:

1. What's the ""best split"" here for classification and regression problems ? And how do you calculate it ?

2. Is it saying the best split using all m variables, or just the one among m that gives you the best split ?

Thanks.",en
1107387,2011-06-10 16:43:56,datasets,I'm looking for a GPS-annotated roadmap/graph of any major city. ,hwd72,jfasi,1215745765.0,https://www.reddit.com/r/datasets/comments/hwd72/im_looking_for_a_gpsannotated_roadmapgraph_of_any/,2.0,4.0,"I'd like to perform a large state space search to find optimal traffic light patterns. Basically, I'd like to try to compute a scheduling for traffic lights that would optimize traffic flow, and I'd like to have a dataset to train on. ",en
1107388,2011-06-10 21:17:54,statistics,"Having trouble understanding why ""N"" affects these calculations differently.",hwker,[deleted],,https://www.reddit.com/r/statistics/comments/hwker/having_trouble_understanding_why_n_affects_these/,0.0,1.0,,en
1107389,2011-06-11 00:37:04,statistics,Need advice on finding jobs in analytics with MA degree and no prior work experience,hwpl6,entryjob,1307741533.0,https://www.reddit.com/r/statistics/comments/hwpl6/need_advice_on_finding_jobs_in_analytics_with_ma/,1.0,0.0,"Hi people of reddit/r/statistics!

Can you give me some advise on where to begin looking for jobs with MA in Math (emphasis in statistics) without much of the ""real"" world experience? 

Everyone is asking for years and years of working experience, where I had about 1.5 years of research experience and publication in works plus some conference talk invites, etc. 

May be I am doing something wrong, but I am not getting even phone interview as of now...

Any advise on the course of actions is very much appreciated! 

:)",en
1107390,2011-06-11 04:47:29,statistics,"Does anyone have guidance on multi-level logistic 
regression?",hwuqd,TheShadowBanned,1299528014.0,https://www.reddit.com/r/statistics/comments/hwuqd/does_anyone_have_guidance_on_multilevel_logistic/,5.0,10.0,"Hey everyone,

So it looks like I'm going to have run a multilevel logistic regression, but I'll be damned if I know how, and the documentation for it is neither easy to come by nor clearly explained. A description of the data follows:  
  
&gt; The dataset consists of two-levels. The first level consists of webpage viewers, and thus the rows of the dataset represent these viewes. The second level are the webpages that are being viewed. Each webpage has between 20 and 500 viewers nested within it. All of the variables I'm analyzing are at the second level (i.e., no viewer-level effects). The DV is a binary variable representing whether the viewer clicked a link within webpage. No viewer is represented more than once in the entire dataset, and there are approximately 80,000 viewers.  
  
I know MPlus, R and Stata are capable of multi-level logistic regression. I also know SPSS 19 now has that capability as well. Unfortunately I didn't learn mixed models in my grad work though I'm planning on taking a class next January, and thus there appear to be a lot of decision points and terminology I'm not comfortable with deciphering. And, as I said before, existing documentation is not amazing.   
 
  Does anyone have experience with these kinds of analyses? If so, would you be willing to point me in the right direction? Thanks!  
  
  
-------------------  

**Edit:** For the sake of context I should maybe note that while I'm not an idiot in statistics, I'm probably not as smart as many here. My graduate degree was in a statistics-heavy field as far as social sciences go, but it certainly wasn't primarily stats-focused. I took Advanced Stats I and II (ANOVAs, regressions, etc.), Multivariate Stats (discriminant analysis, profile analysis, MANOVAs, etc.), Factor Analysis and SEM, and Analysis of Financial Time Series. I also had some training in a technique called utility analysis, but it not really helpful. Most of my training was in SPSS, but I did use SAS for the time series class and MPlus for SEM.   
  
HLM was sadly not a course option while I was taking classes, so now I'm paying for my own class next year. It's quite a shame.  
  
--------------
  
**EDIT 2** - I made an error. I do have a couple level 1 variables acting as moderators. ",en
1107391,2011-06-11 08:30:34,MachineLearning,Learning ML after graduating,hwyus,visarga,1166994643.0,https://www.reddit.com/r/MachineLearning/comments/hwyus/learning_ml_after_graduating/,27.0,12.0,"There might be many like me - I want to pick up machine learning but I finished university 10 years ago. Of course there are great books and video courses online (I watch Stanford Machine Learning CS 229 with Andrew Ng) but being alone in front of this monumental field is difficult.

So I was wondering if there is a way to get personal tutoring. A student could probably do 99% of the work on his own with the videos and books, and ask for external help only for that last 1%. What is needed here is the ""coaching"" / ""question answering"" part more than the actual instruction part.

Do you think this could work? Any idea where to look for help?


Edit: Thank you for the links. Anyone willing to offer me (paid) tutoring over email and skype?",en
1107392,2011-06-12 00:42:23,statistics,GRE score for a Master's degree in Stats?,hxdui,valen089,1288238268.0,https://www.reddit.com/r/statistics/comments/hxdui/gre_score_for_a_masters_degree_in_stats/,6.0,9.0,"What kind of GRE score would be considered competitive for a Master's program in Stats? Higher is better and I assume more weight is given to the Quant score, but what is the cut point?",en
1107393,2011-06-12 13:52:36,datasets,Looking for: Cars Dataset,hxpml,cdemi,1277845034.0,https://www.reddit.com/r/datasets/comments/hxpml/looking_for_cars_dataset/,1.0,0.0,"Hi guys, I'm looking for a car dataset ie. Manufacturer, Model Type, Year, Fuel Type and Transmission etc...

Can anyone tell me where to find such dataset or what I have to do to obtain/buy this type of data?",en
1107394,2011-06-12 16:26:42,MachineLearning,I asked HN on how to effectively start on Machine Learning (I have web dev background),hxr8g,phektus,1266479689.0,https://www.reddit.com/r/MachineLearning/comments/hxr8g/i_asked_hn_on_how_to_effectively_start_on_machine/,16.0,5.0,,en
1107395,2011-06-12 17:19:47,MachineLearning,Apps every Mac user should check out #1,hxrv9,destruktive,1301855308.0,https://www.reddit.com/r/MachineLearning/comments/hxrv9/apps_every_mac_user_should_check_out_1/,0.0,0.0,,en
1107396,2011-06-12 20:50:20,statistics,An argument for why your Twitter Klout score is meaningless ,hxvi4,Wonnk13,1287022683.0,https://www.reddit.com/r/statistics/comments/hxvi4/an_argument_for_why_your_twitter_klout_score_is/,6.0,1.0,,en
1107397,2011-06-12 23:50:51,MachineLearning,Improving Malaria Treatment using Clustering,hxzf2,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/hxzf2/improving_malaria_treatment_using_clustering/,9.0,1.0,,en
1107398,2011-06-13 00:33:01,datasets,"Twitter data containing questions, 436k tweets",hy0b6,snippetin,1307914152.0,https://www.reddit.com/r/datasets/comments/hy0b6/twitter_data_containing_questions_436k_tweets/,0.0,1.0,,en
1107399,2011-06-13 04:14:43,MachineLearning,How machine learning techniques are helping to solve important problems?,hy4q4,[deleted],,https://www.reddit.com/r/MachineLearning/comments/hy4q4/how_machine_learning_techniques_are_helping_to/,2.0,0.0,"I'm reading the book ""Programming Collective Intelligence"" in order to have a basic idea about ML algorithms, but the algorithms are used to solve problems like: price models, better recommendations, trip planning... 

I'd like to know about interesting projects(open source would be great) that apply ML to help solving problems in fields like: health, education, nutrition, politics, harvesting...

Thank you in advance for your replies.",en
1107400,2011-06-13 23:21:15,statistics,"""Some Mathematical Notes On Three-Way Factor Analysis"", aka, ""Tucker Invented Everything. We Can All Go Home, Now.""",hysrh,[deleted],,https://www.reddit.com/r/statistics/comments/hysrh/some_mathematical_notes_on_threeway_factor/,1.0,0.0,,en
1107401,2011-06-14 00:02:11,datasets,Vaccine funds - get the data,hytvn,cavedave,1128052800.0,https://www.reddit.com/r/datasets/comments/hytvn/vaccine_funds_get_the_data/,1.0,0.0,,en
1107402,2011-06-14 06:22:21,artificial,"Interview: Dr. Ben Goertzel on Artificial General Intelligence, Transhumanism and Open Source (Part 2/2)",hz34v,[deleted],,https://www.reddit.com/r/artificial/comments/hz34v/interview_dr_ben_goertzel_on_artificial_general/,9.0,2.0,,en
1107403,2011-06-14 13:57:09,artificial,I'm an EE undergrad seeking tips from someone working on the AI field,hzb7n,humectant,1308047815.0,https://www.reddit.com/r/artificial/comments/hzb7n/im_an_ee_undergrad_seeking_tips_from_someone/,22.0,12.0,"Hello. In 2 years, I'll be choosing Control Systems as my major, which has mostly AI/Robotics courses. I figured it's time to start educating myself on the subject and maybe come up with a project or two. My questions are the following:

* What books do you recommend?

* I'm already proficient in C/C++ and Matlab. Should I learn Python or skip to a logic programming language?

* What are the major branches of AI?

* Is it difficult to find a job outside of academic research?

Thanks.",en
1107404,2011-06-14 16:06:49,MachineLearning,Machine Learning with Apache Mahout,hzd9p,joejoe500,1291995007.0,https://www.reddit.com/r/MachineLearning/comments/hzd9p/machine_learning_with_apache_mahout/,13.0,1.0,,en
1107405,2011-06-14 20:35:38,statistics,Should I take the math GRE if I'm going to stat grad school?,hzkef,[deleted],,https://www.reddit.com/r/statistics/comments/hzkef/should_i_take_the_math_gre_if_im_going_to_stat/,10.0,18.0,,en
1107406,2011-06-15 00:37:35,statistics,Help with determining what test to use ,hzrbf,[deleted],,https://www.reddit.com/r/statistics/comments/hzrbf/help_with_determining_what_test_to_use/,1.0,0.0,I am comparing two different ways to calculate vulnerability (Variable A and B) and would like to be able to tell how similar the variables are to each other.  Both variables are independent of each other so regression is out.  Thank you for your time.,en
1107407,2011-06-15 02:58:25,MachineLearning,Is anyone using Revolution Analytics?   ,hzuto,TraptInaCommentFctry,1288736832.0,https://www.reddit.com/r/MachineLearning/comments/hzuto/is_anyone_using_revolution_analytics/,7.0,4.0,"[Revolution Analytics](http://www.revolutionanalytics.com/) claims that ""R is Ready for Business.""  

From what I can gather, they offer a new file format that lets R stream through data files too big to fit in memory, and accompanying algorithms designed to optimize (a small set of) operations on these streaming files.  

They also say:

""Take Hadoop and Other Big Data Sources to the Next Level

Revolution R Enterprise fits well within the modern ‘Big Data’ architecture by leveraging popular sources such as Hadoop, NoSQL or key value databases, relational databases and data warehouses. These products can be used to store, regularize and do basic manipulation on very large datasets—while Revolution R Enterprise provides advanced analytics at unparalleled speed and scale"" 

I can't find any more detail on how they ""leverage"" Hadoop.  Is it possible to use their product to distribute over a cluster?  In their examples, they use multiple cores on a single server/pc. 

Also, they claim ""Extend Existing Analyses by writing user- defined R functions to “chunk” through huge data sets."" Anyone have experience with this? 

It's currently only available for PC so I can't try it.  

Edit: Clarity",en
1107408,2011-06-15 09:57:23,computervision,Open-source FPGA Stereo Vision Core released,i047f,snapperplug,1290082618.0,https://www.reddit.com/r/computervision/comments/i047f/opensource_fpga_stereo_vision_core_released/,16.0,1.0,,en
1107409,2011-06-15 12:22:03,MachineLearning,Book draft: Introduction To Machine Learning (by Smola &amp; Vishwanathan) [PDF],i06a0,fbahr,1275413362.0,https://www.reddit.com/r/MachineLearning/comments/i06a0/book_draft_introduction_to_machine_learning_by/,47.0,9.0,,en
1107410,2011-06-15 16:11:31,MachineLearning,Machine Learning and Social Media,i09ss,joejoe500,1291995007.0,https://www.reddit.com/r/MachineLearning/comments/i09ss/machine_learning_and_social_media/,0.0,0.0,,en
1107411,2011-06-16 00:10:52,statistics,Statistical Distribution of Credit Scores,i0n6x,Saterus,1289896299.0,https://www.reddit.com/r/statistics/comments/i0n6x/statistical_distribution_of_credit_scores/,1.0,0.0,"Does anyone have any data on the statistical distribution of credit scores for any of the major credit bureaus? Seeing basic figures like mean, standard deviation, etc., would be really interesting.

Purely curiosity, I'm wondering how rare are high credit scores.",en
1107412,2011-06-16 01:19:12,statistics,Is there a subreddit for R users?,i0p47,RA_Fisher,1299707119.0,https://www.reddit.com/r/statistics/comments/i0p47/is_there_a_subreddit_for_r_users/,25.0,9.0,Enlighten me.,en
1107413,2011-06-16 01:26:32,computervision,shape from shading,i0pb3,mish4,1286697468.0,https://www.reddit.com/r/computervision/comments/i0pb3/shape_from_shading/,0.0,1.0,I'm studying shape from shading and wrote a [blog post](http://valserb.blogspot.com/2011/06/understanding-shape-from-shading-part-1.html) on the subject. I am learning this material on my own and don't have much feedback on whether I understand things correctly. If anyone wants to discuss or improve my understanding feel free to comment. Thanks!,en
1107414,2011-06-16 04:23:14,statistics,Determining inter-rater reliability,i0tmb,valen089,1288238268.0,https://www.reddit.com/r/statistics/comments/i0tmb/determining_interrater_reliability/,2.0,3.0,"I'm trying to determine the inter-rater reliability on some categorical data.  Currently the data set is arranged like so:

    case#    rater    item1    item2 …
    1         A        Y         N   …
    1         B        Y         Y   …
    2         A        Y         N   …
    2         B        N         N   …
    ⋮         ⋮        ⋮          ⋮

I want to analyze the inter-rater reliability in a way such that it accounts for agreements/disagreements on individual items, not the total scores for each case (as one would do using the irr package).

Does anyone have any suggestions as how to approach the data?",en
1107415,2011-06-16 05:06:56,datasets,Anyone have a dataset with national median income for countries?,i0unh,neolduser,,https://www.reddit.com/r/datasets/comments/i0unh/anyone_have_a_dataset_with_national_median_income/,1.0,0.0,I'm having a terrible time finding it. You'd think it'd be easy.,en
1107416,2011-06-16 05:48:16,MachineLearning,Is there a top machine learning hangout on the web?,i0vol,the_cat_kittles,1287171936.0,https://www.reddit.com/r/MachineLearning/comments/i0vol/is_there_a_top_machine_learning_hangout_on_the_web/,41.0,12.0,"like some place where most serious lads/lasses hang out? #machinelearning on irc isnt really happening, #R is ok too, reddit is good, but i wonder, am i missing a party somewhere?

Edit: metaoptimize is fantastic, thank you!",en
1107417,2011-06-16 15:05:05,statistics,Does anyone know how to get the gamma value in STATA after a stochastic frontier analysis?,i15lw,mutabilis,1284179253.0,https://www.reddit.com/r/statistics/comments/i15lw/does_anyone_know_how_to_get_the_gamma_value_in/,1.0,0.0,"So, I just ran my model and haven't the faintest idea on how to estimate gamma nor test its significance. Please help.

After I ran the model omitting the variables which pertain to inefficiencies, I get the following:

Likelihood-ratio test of sigma_u=0: chibar2(01) = 72.94  Prob&gt;=chibar2 = 0.000

So it means that U is significantly different from 0, now I wish to test if gamma (percentage contribution of u) is significantly different from 0.

I know that I can manually calculate gamma, sigma_u^2/sigma^2, but I wish to conduct the significance test.
",en
1107418,2011-06-16 20:32:40,statistics,[[URGENT]] Question about CI/Best-Fit!!!!,i1dtx,radiodank,1233858817.0,https://www.reddit.com/r/statistics/comments/i1dtx/urgent_question_about_cibestfit/,0.0,5.0,"Hi reddit!

I have a statistics question that I hope some of you can help me with.

I can't make a forum-like post with pictures ect. so I made the post here-
http://www.physicsforums.com/showthread.php?t=507430

If you could just read it and put your response here on reddit, or physicsforum, I don't mind.

Thank you VERY much!",en
1107419,2011-06-17 17:53:47,statistics,Logistic Regression Help,i25y8,celeritatis,1295307865.0,https://www.reddit.com/r/statistics/comments/i25y8/logistic_regression_help/,1.0,0.0,"This is not a request for homework help. I wish this one was as easy to deal with as my homework.

I am a HS ex stats student(AP), and I am poking around prediction of voting. Specifically, from true or false variables on past elections(whether they voted or not) I want to return the probability of someone voting. What would be the appropriate statistical method here? 

If it is logistic regression, could someone explain how that is practically implemented? I get some of the theory, but not how you would make it work.

Thank you all in advance.",en
1107420,2011-06-17 19:12:30,MachineLearning,Review: Machine Learning: An Algorithmic Perspective by Stephen Marsland - A *great* introductory ML book,i286p,robintw147,1276775262.0,https://www.reddit.com/r/MachineLearning/comments/i286p/review_machine_learning_an_algorithmic/,3.0,0.0,,en
1107421,2011-06-18 03:25:49,MachineLearning,Machine Learning Summer School Slides,i2kxe,leonoel,1284436447.0,https://www.reddit.com/r/MachineLearning/comments/i2kxe/machine_learning_summer_school_slides/,28.0,4.0,,en
1107422,2011-06-18 08:06:36,statistics,[X-Post] [Excel] Constructing a cell reference with the cell row number and cell column letter using a function.,i2qif,Mottebayo,1283551388.0,https://www.reddit.com/r/statistics/comments/i2qif/xpost_excel_constructing_a_cell_reference_with/,0.0,0.0,,en
1107423,2011-06-18 21:21:13,statistics,Multiple imputation for multilevel models (heteroskedastic data),i326e,mountaindrew_,1295105069.0,https://www.reddit.com/r/statistics/comments/i326e/multiple_imputation_for_multilevel_models/,5.0,5.0,Does anyone know a software which imputes multilevel data while taking into account the heteroskedastic nature of the data? AMELIA II would do the job really well but it's only a single-level software.,en
1107424,2011-06-20 03:03:59,statistics,Multiple Imputation in the presence of Gaussian errors with non-constant variance,i3wd2,[deleted],,https://www.reddit.com/r/statistics/comments/i3wd2/multiple_imputation_in_the_presence_of_gaussian/,1.0,1.0,,en
1107425,2011-06-20 17:11:57,statistics,Math refresher topics for mathematical statistics?,i4bta,djobouti_phat,1231775524.0,https://www.reddit.com/r/statistics/comments/i4bta/math_refresher_topics_for_mathematical_statistics/,0.0,0.0,"Hi statit,

I'm going to be starting a self-study program in mathematical statistics (i.e., buying a good textbook and working through it). My only problem is that while I took a lot of math for phy sci in college (diff eq., linear algebra, analysis, etc.), it's been a long time since I used it, so I've forgotten nearly all of it. 

Fortunately, I still have all my old books, but I'd prefer not to work all the way through them to pick out what I need. Can someone help me out a bit and tell me which topics I would find most helpful to brush up on? Besides just plain old multivariable calculus, I know that at least part of the linear algebra will be helpful. What else? Taylor series? Fourier?

Thanks!",en
1107426,2011-06-20 17:23:59,MachineLearning,Calling R lovers to work together on “The R Programming wikibook”,i4c34,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/i4c34/calling_r_lovers_to_work_together_on_the_r/,26.0,9.0,,en
1107427,2011-06-20 18:24:53,statistics,Any hockey fans in here...? Looking for ideas to add to a spreadsheet of hockey stats ,i4dpc,randude,1286197010.0,https://www.reddit.com/r/statistics/comments/i4dpc/any_hockey_fans_in_here_looking_for_ideas_to_add/,4.0,2.0,"My buddy and i play PS3 hockey and have compiled a wide array of stats for our games - I ask if there is anyone who has any ideas of what stats we can add in.

There's multiple tabs on the sheet, the ""NHL10 Games"" tab is where i enter in our game by game info and then it gets calculated into the ""NHL10 Stats' tab where we have Totals, minimums, maximums &amp; averages for every statistical category &amp; trending analysis and then also the ""NHL10 Summary"" tab where we have an assortment of stats, mostly pertaining to after 2 periods analysis games won/lost.

There's also tabs that show stats from previous versions that all calculate out to the ""Overall"" tab where we have stats that encompass all the versions combined.

Thank you for your help - the file is in 2007 Excel format (.xlsx)

I have it linked here:

http://randude.com/ps3/2_buds_hockey_stats.xlsx",en
1107428,2011-06-20 18:43:24,statistics,Help? Variance estimation of gaussian with known mean and noisy observations.,i4e8a,motional,1278430649.0,https://www.reddit.com/r/statistics/comments/i4e8a/help_variance_estimation_of_gaussian_with_known/,3.0,10.0,"Bayesian experts? I'm stuck with my pet project, which is a bayesian framework for signal analysis.

The problem that i've stumbled upon is estimating the variance of a gaussian distribution. What makes it hard is that the observations themselves contain gaussian noise. I have not been able to find any solutions from the internet, my brains or my books. Can anyone help?

The generative distribution is defined by mean m and variance s^2: 

P(x|m, s^2 ) ~ N(m, s^2 )

Now, the observation can contain additional noise, so that:

P(x|obs) ~ N(m_x, s_x^2 )

In order to estimate the variance (MAP), we make use of the bayesian rules:

P(s^2 |obs)  ~  int[ P(s^2 |x)P(x|obs) ]dx
  = P(s^2 ) int[ P(x|s^2 )P(x|obs) ]dx

I'm no bothering you with further details, just trying to describe the problem. The real problem lies further ahead, where we encounter an equation which can not be solved in closed form. Has anyone solved/encountered something like this, or possibly could guide me to some relevant source?


--------------
EDIT: It was suggested that I add more details. So here we go!

If we ignore the observation noise for a while. The typical way, I guess, to estimate variance of normal distribution with known mean is to set inverse gamma as the prior. It would look something like this:

      P(s^2 ) ~ (s^2 )^-v/2 exp(-v*s_0^2 /2s^2 )

The generative distribution looks quite similar:

      P(x|s^2 ) ~ (s^2 )^-1/2 exp(-(x-m)^2 /2s^2 )

Now, the posterior density (after observing one sample) for s^2 looks like,

      P(s^2 |x) ~ P(s^2 )P(x|s^2 )
                    = (s^2 )^-v/2 exp(-v*s_0^2 /2s^2 ) * (s^2 )^-1/2 exp(-(x-m)^2 /2s^2
                    = (s^2 )^-(v+1)/2 exp( (-v*s_0^2 - (x-m)^2 ) /2s^2 )

This again looks like inverse gamma. We wish to use maximum a posteriori point (MAP) as the estimate for s^2. We could look from the tables or find it our selves. First, we take the logarithm

      log P(s^2 |x) = -(v+1)/2 log (s^2 ) - (v*s_0^2 + (x-m)^2 ) /2s^2

Now, to find the maximum we derivate wrt. s^2 and set to zero

      d/ds^2 log P(s^2 |x) = -(v+1)/2s^2 + (v*s_0^2 + (x-m)^2 ) /2s^4 = 0

We note that s^2 != 0, and multiply both sides by 2s^4:

      -(v+1)s^2 + (v*s_0^2 + (x-m)^2 ) = 0

This results to

      s^2  = (v*s_0^2 + (x-m)^2 ) / (v+1)

Now, back to our problem. We have noisy observations:

      P(s^2 |obs)  ~ P(s^2 ) int[ P(x|s^2 )P(x|obs) ]dx

The integration part of the above equation solves to:

      int[ P(x|s^2 )P(x|obs) ]dx ~ N(m_x, s_ i^2 + s^2 )

If we try a similar approach, as the typical approach above without noise, the posterior distribution becomes

      P(s^2 |obs) ~ (s^2 )^-v/2 exp(-v*s_0^2 /2s^2) * (s^2 )^-1/2 exp(-(m_x-m)^2 /2(s^2 + s_x^2 )
                       = (s^2 )^-(v+1)/2 exp(-v*s_0^2 /2s^2 - (x-m)^2/2(s^2 + s_x^2 ) )

As we can see, this does not solve to inverse gamma form (problem?). Again, we take the logarithm:

      log P(s^2 |obs) ~ -(v+1)/2 log(s^2 ) - v*s_0^2 /2s^2 - (x-m)^2/2(s^2 + s_x^2 )

Then derivate wrt. s^2 and set to zero:

      -(v+1)/2s^2 + v*s_0^2 /2s^4 + (x-m)^2/2(s^2 + s_x^2 )^2 = 0

      (s^2 + s_x^2 )^2/s^2  = -(x-m)^2/(v*s_0^2 -v-1)

      s^2 + 2 s_x^2 + s_x^4 /s^2  = -(x-m)^2/(v*s_0^2 -v-1)

Multiply by s^2,

      s^4 + (2 s_x^2 -(x-m)^2/(v*s_0^2 -v-1) )s_^2 + s_x^4  = 0

A second order equation, when we ignore the negative root, it solves to

      s^2 =  (-(2 s_x^2 -(x-m)^2/(v*s_0^2 -v-1) ) + sqrt((2 s_x^2 -(x-m)^2/(v*s_0^2 -v-1) )^2 - 4s_x^4)) / 2

Okay, we reached a closed form solution, but it does not look nice..









",en
1107429,2011-06-21 00:26:38,MachineLearning,AGI and the Emerging Peer-to-Peer Economy: Ben Goertzel Interviews AI Researcher Mohamad Tarifi ,i4o82,[deleted],,https://www.reddit.com/r/MachineLearning/comments/i4o82/agi_and_the_emerging_peertopeer_economy_ben/,0.0,0.0,,en
1107430,2011-06-21 02:39:19,statistics,So I suck at probability...,i4rmu,TheDyingDandy,1293034864.0,https://www.reddit.com/r/statistics/comments/i4rmu/so_i_suck_at_probability/,2.0,3.0,"Bucket has 100 marbles, 70 white and 30 black. If I pick one I have a 70% chance to pick a white one. Question: If I pick two marbles without looking, what's the chance that at least one is white?",en
1107431,2011-06-21 02:53:22,computervision,How do I speed up my ImageSegmentation code? ,i4rz7,[deleted],,https://www.reddit.com/r/computervision/comments/i4rz7/how_do_i_speed_up_my_imagesegmentation_code/,3.0,10.0,"I have a code on image segmentation based on kmeans clustering. It works for low density images, but for slightly high density images, BOOM! It gets atrociously slow- more than half an hour for one image, &lt;1 second for low density images.

I tried pre-allocation of memory for arrays and cell structures, vectorization of the for loops, replacing the inbuilt kmeans function with one I found on Matlab central which the author claimed to be ~100 times faster than the original code, EVERY SINGLE DAMN THING.

I love MATLAB and feel frustrated that even after ~3 years experience with it cannot make a simple code run fast. WTF. Any suggestions would be welcome, guys!

Thanks for your time. This means a lot to me.
",en
1107432,2011-06-21 19:20:02,MachineLearning,Algorithmic probability and Solomonoff Induction,i5cus,ogrisel,1171218481.0,https://www.reddit.com/r/MachineLearning/comments/i5cus/algorithmic_probability_and_solomonoff_induction/,21.0,6.0,,en
1107433,2011-06-21 21:37:13,MachineLearning,Non- or Lightly Technical Introduction to Data Mining/Machine Learning,i5guy,akg_67,1306991466.0,https://www.reddit.com/r/MachineLearning/comments/i5guy/non_or_lightly_technical_introduction_to_data/,0.0,5.0,"Hello,

I would like to understand data mining/machine learning domain. My primary interest is to gain enough knowledge on applied aspects so that I can figure out technique/problem fits and for a problem to be able to determine potential solution and applicability of a specific technique before handing off the problem to an expert in that domain.

I reviewed first three lectures of Andrew Ng available online and found them to be more technical/implementation level than the applied level that I need.

Can you please suggest any specific introductory and applied level material that I should should read as a start?

*Something at the level of NUMB3RS TV series*

Thanks.",en
1107434,2011-06-22 00:42:07,rstats,"Code problem.  Help, much appreciated.",i5me3,RA_Fisher,1299707119.0,https://www.reddit.com/r/rstats/comments/i5me3/code_problem_help_much_appreciated/,3.0,3.0,"My problem is that I'm trying to take the difference between any increasing value in the series and the previous value and add that difference back into each previous value.

For example:

Dataframe A:

| C | B | Diff (0 where decreasing) | B2 (w/ calculations) |
| ------------- | ------------- | ------------- | ------------- |
| Factor A  | 2   | 1  |  5 &lt;- (2 + (1+2+1))    
| Factor A  | 3 | 2  | 5 &lt;- (3 + (2+1))    
| Factor A  | 5  | 0  |  5 &lt;- (5 + (1))    
| Factor A  | 2  | 1  | 2 &lt;- (2 + (1))    
| Factor A  | 1   | 0  | 1 &lt;- (1 + (0))    

Code:

## A is a data.frame
## B is a numeric column of A
## C is a factor column of A

cumSumBack &lt;- function(x)
    x + Reduce(sum, c(pmax(diff(x), 0), 0), right=TRUE, accumulate=TRUE)

A &lt;- within(A, A$B &lt;- ave(A$B, A$C, FUN=cumSumBack))


Gives me:

Error in pmax(diff(x), 0) : cannot mix 0-length vectors with others

I can't figure out how to solve this one, help would be much appreciated!!",en
1107435,2011-06-22 04:37:24,MachineLearning,ML for loan granting decisions,i5skq,mfalcon,1264563760.0,https://www.reddit.com/r/MachineLearning/comments/i5skq/ml_for_loan_granting_decisions/,12.0,14.0,"I've a friend who runs a finance institution. The business is growing and I was thinking of using ML to help the institution make about loan granting decisions. I'm searching for papers, code or other materials to help as a base line but I can't find something really useful. 

It'd be great to have some similar case to learn from, how they implemented it, which technique/algorithm do they use and why, which attributes of the loan data are particularely useful to raise conclusions, how much information is needed in order to make useful decisions... ",en
1107436,2011-06-22 12:26:27,statistics,Statistics software without installation,i62al,njuv,1308734546.0,https://www.reddit.com/r/statistics/comments/i62al/statistics_software_without_installation/,7.0,9.0,"I can't install software on my work computer. Is there any webbrowser statistics package, or possibility to download a live CD for statistics? I used to have a live CD for Eviews, but I lost it... Thks",en
1107437,2011-06-22 21:12:03,MachineLearning,Machine Learning opportunities at Google ,i6e7i,[deleted],,https://www.reddit.com/r/MachineLearning/comments/i6e7i/machine_learning_opportunities_at_google/,1.0,0.0,,en
1107438,2011-06-22 22:02:00,statistics,What type of statistical analysis could I use for this survey data?,i6fss,mikechaney1,1301172358.0,https://www.reddit.com/r/statistics/comments/i6fss/what_type_of_statistical_analysis_could_i_use_for/,0.0,7.0,"I am writing an article on the FDA's ban on caffeinated alcohol (four loko). What I'm really doing is going over all of the studies that the FDA used to support their claim.

After reading all of them, I was shocked as to how bad a lot of these studies/surveys were. 

The FDA included in their letter that people who drank ""caffeine and alcohol were more likely to drink and drive."" They got this from a survey set up outside of a bar area where they asked the participant what they drank and if they intend to drive a private vehicle. Here is what I want to analyze:

38 people drank caffeine and alcohol. 18 of them intended to drive.
450 people only drank alcohol. 135 of them intended to drive.

18/38=47% and 135/450=30%

Is there a way to tell how statistically significant this data is? I can solve it, I just dont know what test I could use...if there is one at all.

Thanks!
",en
1107439,2011-06-23 00:27:39,statistics,Doing Good With Data – Data Without Borders,i6kd2,[deleted],,https://www.reddit.com/r/statistics/comments/i6kd2/doing_good_with_data_data_without_borders/,1.0,0.0,via: http://flowingdata.com/2011/06/21/data-without-borders/,en
1107440,2011-06-23 00:37:43,statistics,Doing Good With Data – Data Without Borders ,i6knm,jdw25,1220557205.0,https://www.reddit.com/r/statistics/comments/i6knm/doing_good_with_data_data_without_borders/,1.0,0.0,,en
1107441,2011-06-23 16:46:36,statistics,The R Journal Vol. 3/1 just went online,i759l,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/i759l/the_r_journal_vol_31_just_went_online/,18.0,2.0,,en
1107442,2011-06-23 18:25:19,datasets,Anyone have datasets from social networks which include spam?,i77w6,[deleted],,https://www.reddit.com/r/datasets/comments/i77w6/anyone_have_datasets_from_social_networks_which/,1.0,0.0,"I would like to experiment with spam filters. Social networks could include Reddit, Digg, Facebook etc.

Thanks for your help!",en
1107443,2011-06-23 22:15:01,MachineLearning,Data without borders: why I want to change the world,i7exy,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/i7exy/data_without_borders_why_i_want_to_change_the/,26.0,3.0,,en
1107444,2011-06-23 23:25:55,computervision,The website for a working draft of an upcoming book on computer vision. Looks excellent.,i7h8o,urish,1221689900.0,https://www.reddit.com/r/computervision/comments/i7h8o/the_website_for_a_working_draft_of_an_upcoming/,2.0,0.0,,en
1107445,2011-06-24 05:35:35,statistics,MANCOVA v ANCOVA v ANOVA help!,i7qxy,ThePaleBlueDot,1298352423.0,https://www.reddit.com/r/statistics/comments/i7qxy/mancova_v_ancova_v_anova_help/,1.0,1.0,"Hey Reddit, need some quick advice as I'm need to submit some very basic statistics for a grant proposal.

The problem: I have 3 known predictors of the outcome - say (a, b, c) all of these are continuous variables, but in theory could be easily converted to categorical.

The purpose if to determine if 4 (say W, X, Y, Z) different methods/interventions alter this outcome. It is possible people used more than 1, and different degrees of each. 2 of these are continuous, 2 are categorical.

It is likely that those who used W would have preformed better even if they didn't use this method.

Thus I want to control for this confounding factor by adjusting for the predictors a,b,c.

I feel like ANCOVA is the correct methods since I have 1 dependent variable (outcome) with multiple covariants (a,b,c) and independent variables (W, X, Y, Z). Is that correct?

Thanks!
",en
1107446,2011-06-24 14:23:18,statistics,How to compare proportions within a Chi-Squared test?? Any help soonish much appreciated!,i7zia,Cognitive86,1308913962.0,https://www.reddit.com/r/statistics/comments/i7zia/how_to_compare_proportions_within_a_chisquared/,9.0,12.0,"Hello, 

I have a categorical dependent variable and a categorical independent variable. I have experience with conducting a Chi-Squared test and checking to see if there is a significant non-independence result. I also have experience with checking if the absolute value of the standardised residual is greater than 2 for any individual value (which would show that it is driving the non-independence result). However, I have found something that I would like to do, and that is NOT one of the above. 

I would like to compare two different proportions within the Chi Squared analysis (or if anyone can think of another test, that is fine!). For example, in my basic analysis I have a table with 'region' (Scotland, England etc) along the side and 'stream type' (either 1,2 or 3) along the top. In this table I have the proportions calculated along the rows (so 1,2 and 3 add up to 100 percent). I would like to compare (Scotland,2) to (England,2). These proportions differ, 38% to 29% respectively and I would like to know if the difference is significant. 

Any help MUCH appreciated, 

Best wishes, 
C86",en
1107447,2011-06-24 22:41:30,MachineLearning,Cluster analysis for concise results in eye tracking studies,i8cft,iwas,,https://www.reddit.com/r/MachineLearning/comments/i8cft/cluster_analysis_for_concise_results_in_eye/,3.0,0.0,,en
1107448,2011-06-25 03:14:29,datasets,Does anyone have access to a dataset of clinical trials (RCTs) and their outcomes?,i8jcb,[deleted],,https://www.reddit.com/r/datasets/comments/i8jcb/does_anyone_have_access_to_a_dataset_of_clinical/,1.0,0.0,,en
1107449,2011-06-25 08:26:22,MachineLearning,Why Robbie Can't Learn: The Difficulty of Learning in Autonomous Agents [2001 Lecture],i8pzl,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/i8pzl/why_robbie_cant_learn_the_difficulty_of_learning/,5.0,2.0,,en
1107450,2011-06-25 21:32:46,artificial,Japan's newest popstar outed as CGI creation. No one could tell by the pixels.,i91ko,systmshk,1282827984.0,https://www.reddit.com/r/artificial/comments/i91ko/japans_newest_popstar_outed_as_cgi_creation_no/,55.0,11.0,,en
1107451,2011-06-26 05:51:56,MachineLearning,Anyone going to ICML in Bellevue next week?,i9aua,GetsEclectic,1201624029.0,https://www.reddit.com/r/MachineLearning/comments/i9aua/anyone_going_to_icml_in_bellevue_next_week/,12.0,10.0,"I'll be there all week, I'm sure there will be other redditors there as well, so we should hang out.",en
1107452,2011-06-27 13:08:33,statistics,Stop the spread of the ecological fallacy ,ia7yc,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/ia7yc/stop_the_spread_of_the_ecological_fallacy/,5.0,2.0,,en
1107453,2011-06-27 18:19:01,statistics,"How many times R was cited in the past 12 years (short answer: 18,605 times)",iadx4,[deleted],,https://www.reddit.com/r/statistics/comments/iadx4/how_many_times_r_was_cited_in_the_past_12_years/,0.0,0.0,,en
1107454,2011-06-27 20:55:07,statistics,Estimating number of events in multinomial distribution?,iailp,[deleted],,https://www.reddit.com/r/statistics/comments/iailp/estimating_number_of_events_in_multinomial/,5.0,8.0,"This has no particular application for me, but it was an interesting question that I don't know how to solve. 
Say that there are a finite but unknown number k outcomes of a particular trial. I do not have any prior information about a maximum for k nor the individual probability for any outcome.
After observing some number of events, is there a way that I could come up with an estimate for the number of possible outcomes? My guess is no, but if there's anything I've learned throughout my math education, it's that math will always be surprising me with what it can do.

Thanks for your help in advance!",en
1107455,2011-06-27 21:01:05,statistics,"What is a large enough random sample? - problems, rule of thumb, and a technical paper",iaith,jambarama,1130472000.0,https://www.reddit.com/r/statistics/comments/iaith/what_is_a_large_enough_random_sample_problems/,2.0,0.0,,en
1107456,2011-06-28 01:24:26,statistics,"I know so little, I am incapable of even asking this question I have. But I'll try in the text. If you can help, I'd be grateful.",iaqmi,maybe-tomorrow,1308562245.0,https://www.reddit.com/r/statistics/comments/iaqmi/i_know_so_little_i_am_incapable_of_even_asking/,0.0,1.0,"So I have done some internal audit and found that our employees are doing ""x"" 21% of the time. I KNOW however that they should have only done this 4% of the time. 

Since I have not looked at every order, there is a chance that I just got unlucky with my sample. (For what it is worth, I looked at every 9th order for a total of 500 orders reviewed out of a population of 4500).

What I am hoping to be able to state is something like, ""The chances of the result being 21% in my sample due to chance is 1 in xxx."" I would be content with a confidence error of up to 10%. 

I hope this makes sense. If you could help, I would appreciate it. If you could point me in the right direction so that I could understand your answer, I'd appreciate that more...because I am going to have to be able to explain my statement. 

Thanks!",en
1107457,2011-06-28 07:02:45,computervision,published computer vision papers,iazdw,mish4,1286697468.0,https://www.reddit.com/r/computervision/comments/iazdw/published_computer_vision_papers/,1.0,0.0,,en
1107458,2011-06-28 07:07:20,MachineLearning,Sarah Palin e-mail browser (organized based on Topic Modeling),iazhz,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/iazhz/sarah_palin_email_browser_organized_based_on/,0.0,1.0,,en
1107459,2011-06-28 17:32:54,statistics,"I want to do real world statistical work, what do I have to include in my resume?",ibbk1,[deleted],,https://www.reddit.com/r/statistics/comments/ibbk1/i_want_to_do_real_world_statistical_work_what_do/,9.0,17.0,"Quick question for you professional statisticians out there. I guess my question is what do I need to know to do statistics in the real world? I have been in graduate school for the last year and will be finishing in December, but often I'm told that no one can tell if I am qualified for a position or not because my descriptions of my work is so vague. Can anyone orient me towards a proper description?",en
1107460,2011-06-28 18:56:27,MachineLearning,Graph structure learning,ibdyl,[deleted],,https://www.reddit.com/r/MachineLearning/comments/ibdyl/graph_structure_learning/,11.0,14.0,"I have a question related to graph structure learning.

Basically I have several graphs with undirected edges and each node can have an attribute from a set of possible attributes. Typically the graphs consists of 15-30 nodes and are moderately connected.

What I want to do is given a partial graph, I want to predict the most likely ""completion"" of missing parts. 
I've looked into things such as vectorizing graphs to project them to a N dimensional space and do clustering there and so on. But this gives the nearest neighbor and I want to estimate the missing connections and nodes given a partial graph with a probability attached to it.

It sounds like it should be a quite well researched problem. I am also interested in software packages so that I can quickly try out something...

Thanks a lot!


",en
1107461,2011-06-28 19:53:03,rstats,Loading RJSONIO Help? (cross post on stackflow),ibfr6,ummreally,1270958671.0,https://www.reddit.com/r/rstats/comments/ibfr6/loading_rjsonio_help_cross_post_on_stackflow/,4.0,1.0,"I'm trying to load the RJSONIO library into R so I can use googleVis, but I get an error:
     Error: package 'RJSONIO' is not installed for 'arch=x64'

Is there a way to get around this? I'm using R  2.13.0 on Win7 64-bit. Also I'm a total n00b.  Thanks everyone!

Here's the link to the stackoverflow page: http://stackoverflow.com/questions/6509827/using-rjsonio-with-r-2-13-win7-64-bit

UPDATE: Someone on stackoverflow suggested using the 32-bit version of R. I tried that and it worked perfectly. ",en
1107462,2011-06-29 11:29:47,statistics,Why do cars cluster?,ic4z4,njuv,1308734546.0,https://www.reddit.com/r/statistics/comments/ic4z4/why_do_cars_cluster/,5.0,6.0,Have you ever noticed that cars on a road are usually not equally distibuted? Instead they come in groups of several vehicles. Is there a statistical reason for that? Does que theory have anything to say on the matter?,en
1107463,2011-06-29 13:51:46,datasets,List of words with affective labels,ic71r,fuckitfuckitfuckitfu,1309344507.0,https://www.reddit.com/r/datasets/comments/ic71r/list_of_words_with_affective_labels/,1.0,0.0,"(anger, confusion....)
Better if freely available.
I m also interested if someone has an idea how to create one without manual operations.
Thanks",en
1107464,2011-06-29 19:40:08,statistics,Build a predictive model that predicts future editing activity on Wikipedia. Prize pool: USD 10000,icfgj,utcursch,1175517803.0,https://www.reddit.com/r/statistics/comments/icfgj/build_a_predictive_model_that_predicts_future/,19.0,9.0,,en
1107465,2011-06-30 00:28:53,statistics,SPSS Help for stressed out student,icnsf,20andcounting,1266541476.0,https://www.reddit.com/r/statistics/comments/icnsf/spss_help_for_stressed_out_student/,0.0,0.0,"I have been analysing data from a recent experiment I performed, and can't wrap my head around a problem I'm encountered that is more than likely very simple to figure out.
I have 4 groups in my experiment. I have 3 time bins. My data is not normal so i transformed it. How do I check that my transformed data is normal? Do I just do descriptive statistics or can I get away with it if I use Mauchys test of sphericity in repeated measures and p is above .05  Can any statistician help me out?
Apologies for the, perhaps, complicated wording.",en
1107466,2011-06-30 03:22:19,artificial,Last day for early bird tickets for Singularity Summit Melbourne Australia 20-21 August 2011,ics4v,wavegeekman,1252921750.0,https://www.reddit.com/r/artificial/comments/ics4v/last_day_for_early_bird_tickets_for_singularity/,1.0,0.0,,en
1107467,2011-06-30 03:46:22,artificial,Last day for early bird tickets for Singularity Summit Melbourne Australia 20-21 August 2011,icsqn,wavegeekman,1252921750.0,https://www.reddit.com/r/artificial/comments/icsqn/last_day_for_early_bird_tickets_for_singularity/,1.0,0.0,,en
1107468,2011-06-30 18:58:46,rstats,How to return number of entries for given factor?,idcft,RA_Fisher,1299707119.0,https://www.reddit.com/r/rstats/comments/idcft/how_to_return_number_of_entries_for_given_factor/,2.0,3.0,"I'm working with data that looks like:

| Factor Variable | Numeric Variable |
| ------------- |
| Factor A  |  1 
| Factor A  |  2 
| Factor B  |  1 
| Factor C  |  1 
| Factor C  |  2 

I have a function that operates on the 'numeric variable' given a 'factor' in the 'factor variable', but the function is not necessary for factor variables single entry factors.  So, for example I would like my function to look at Factor B and just skip it --- I've been working on this, but I can't figure it out without breaking my function!

This is my function:

cumSumBack &lt;- function(x)
    x + Reduce(sum, c(pmax(diff(x), numeric(1)), 0), right=TRUE, accumulate=TRUE)

Any help would be appreciated!

",en
1107469,2011-06-30 21:36:33,statistics,"What data will help me figure out how much an hour of work is worth in the U.S., in converting dollars to consumer goods, over the past few decades?",idhen,casualfactors,1285978895.0,https://www.reddit.com/r/statistics/comments/idhen/what_data_will_help_me_figure_out_how_much_an/,6.0,6.0,"I'm having trouble with the horrific data sets BLS, BEA, and BER make available - my horse for a fucking ""control for inflation from index year t"" button - so can anyone help me figure out how I'd make this assessment honestly?",en
1107470,2011-06-30 22:49:06,datasets,Marvel Universe Social Graph data set | Infochimps,idjp8,winniechimp,1309463088.0,https://www.reddit.com/r/datasets/comments/idjp8/marvel_universe_social_graph_data_set_infochimps/,2.0,0.0,,en
1107471,2011-07-02 00:46:54,MachineLearning,"How to add ""R"" as one of your skills on linkedin",iejzw,[deleted],,https://www.reddit.com/r/MachineLearning/comments/iejzw/how_to_add_r_as_one_of_your_skills_on_linkedin/,0.0,0.0,,en
1107472,2011-07-02 01:38:40,statistics,"Hi Reddit, need your help in comparing web versus paper responses?",iel4q,tonecapone3001,1298277107.0,https://www.reddit.com/r/statistics/comments/iel4q/hi_reddit_need_your_help_in_comparing_web_versus/,5.0,1.0,"I am looking at customer satisfaction surveys that are collected in both paper and web based (Both have the same exact questions). I am using SPSS to analyze the data. I am wondering if there is any special analysis that need to be conducted or is it a simple independent t-test with my IV being the survey source (paper vs. web) and my DV is the score.

Thanks in advance!",en
1107473,2011-07-02 09:19:23,MachineLearning,Predict future edit rates in Wikipedia - a new Kaggle competition for $5K,ietyz,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/ietyz/predict_future_edit_rates_in_wikipedia_a_new/,27.0,0.0,,en
1107474,2011-07-02 14:43:54,statistics,AIC or BIC for model selection?,iexvm,tmalsburg2,1226925597.0,https://www.reddit.com/r/statistics/comments/iexvm/aic_or_bic_for_model_selection/,0.0,0.0,,en
1107475,2011-07-02 18:58:59,statistics,I want to be a master statistician...,if1gi,maverick566,1295662983.0,https://www.reddit.com/r/statistics/comments/if1gi/i_want_to_be_a_master_statistician/,10.0,28.0,Where do I begin?,en
1107476,2011-07-03 06:00:19,statistics,Looking for a probability book for novice! Thx,ifedb,matgre,1292963580.0,https://www.reddit.com/r/statistics/comments/ifedb/looking_for_a_probability_book_for_novice_thx/,0.0,0.0,Hi!,en
1107477,2011-07-03 21:42:20,MachineLearning,Learning an absolute order from pairwise comparisons?,ifsgj,sbirch,1268510446.0,https://www.reddit.com/r/MachineLearning/comments/ifsgj/learning_an_absolute_order_from_pairwise/,11.0,16.0,"Say I have N training examples each with a bunch of features. For training I have access to a list of pair-wise comparisons: a is less than b, etc. We will (naively and inaccurately) assume that these decisions are a deterministic function of the features. How can I learn a function which performs the same thing? This seems very similar to regression analysis, so the natural extension is to learn the same in the face of inconsistent pair-wise decisions. Any thoughts/references/RTFMs appreciated!",en
1107478,2011-07-04 04:48:52,MachineLearning,Gradient Boosted Decision Trees in Python?,ig1cw,TraptInaCommentFctry,1288736832.0,https://www.reddit.com/r/MachineLearning/comments/ig1cw/gradient_boosted_decision_trees_in_python/,4.0,8.0,"Does anyone know of a machine learning package in Python that will do Gradient Boosted Decision Trees?  Something analogous to gbm package in R?

I've looked at a bunch of machine learning modules in Python and so far I haven't found one.  

I'm thinking of making a parallelized version a la [Jerry Ye, et al. (2009). Stochastic gradient boosted distributed decision trees](http://delivery.acm.org/10.1145/1650000/1646301/p2061-ye.pdf?key1=1646301&amp;key2=5312144621&amp;coll=GUIDE&amp;dl=ACM&amp;CFID=15151515&amp;CFTOKEN=6184618) using [ElasticWulf](http://code.google.com/p/elasticwulf/).",en
1107479,2011-07-04 05:58:19,statistics,Pretty much the best graph ever,ig2q2,therealprotonk,,https://www.reddit.com/r/statistics/comments/ig2q2/pretty_much_the_best_graph_ever/,0.0,0.0,,en
1107480,2011-07-05 03:40:40,MachineLearning,What are good ways of dealing with/organizing data when building a model?,igs3k,rm999,1175510936.0,https://www.reddit.com/r/MachineLearning/comments/igs3k/what_are_good_ways_of_dealing_withorganizing_data/,16.0,14.0,"Often data sets come in many parts that are all relevant to a modeling task. Usually I preprocess the files into a flat file database and then deal with the data line-by-line. This often works, but is fairly time-consuming and inflexible. Other times I will just load up all the separate files into a hash table, but this is inflexible in its own way, especially on big datasets. 

I've read a little on SQL and noSQL, but have no clue how to start using them in a python/C++ model. Nor do I really understand how they conceptually fit into a model or what their benefits are. Can anyone explain all this to me?",en
1107481,2011-07-05 04:10:29,statistics,"""Do men have a wider variance of intelligence than women? - Quora"" - could effects at the upper extreme be the result of a sampling error? (please see comments for discussion)",igsqc,inquilinekea,1274654696.0,https://www.reddit.com/r/statistics/comments/igsqc/do_men_have_a_wider_variance_of_intelligence_than/,0.0,1.0,,en
1107482,2011-07-05 16:32:36,MachineLearning,cloudnumbers - 4 steps (in 5 minutes) for using R in the cloud (freemium),ih5hc,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/ih5hc/cloudnumbers_4_steps_in_5_minutes_for_using_r_in/,0.0,0.0,,en
1107483,2011-07-05 23:18:04,statistics,Am I missing the obvious or do really 68.2% of South Africans don't have any income?,ihgx7,phond,1295191796.0,https://www.reddit.com/r/statistics/comments/ihgx7/am_i_missing_the_obvious_or_do_really_682_of/,4.0,5.0,,en
1107484,2011-07-06 00:18:57,statistics,Google Correlate Certainly Does Not Imply Causation,ihise,[deleted],,https://www.reddit.com/r/statistics/comments/ihise/google_correlate_certainly_does_not_imply/,1.0,0.0,,en
1107485,2011-07-06 00:30:38,statistics,Google Correlate Certainly Does Not Imply Causation,ihj3y,jambarama,1130472000.0,https://www.reddit.com/r/statistics/comments/ihj3y/google_correlate_certainly_does_not_imply/,23.0,8.0,,en
1107486,2011-07-06 06:41:13,MachineLearning,"EM-PPCA, anyone have a working version?",ihsj3,kraemahz,1207724861.0,https://www.reddit.com/r/MachineLearning/comments/ihsj3/emppca_anyone_have_a_working_version/,8.0,5.0,"I'm trying to implement the EM probabilistic PCA algorithm concurrently in MATLAB and numpy and I've run into a problem with the weight vectors homogenizing. Since I'm basically just going straight by Bishop I'm afraid I'm a bit stuck on where the problem is. The LL increases and sigma approaches a reasonable value but the result is clearly wrong, I'm basically just getting back something close to the first eigenvector in every column.

I was wondering if anyone had an implementation of this lying around I could compare against to work out where my bug is. I'm not going to bore anyone asking them to find it for me.

Edit: Fixed it! Since the time of posting I found the [Dimensionality Reduction Toolbox](http://homepage.tudelft.nl/19j49/Matlab_Toolbox_for_Dimensionality_Reduction.html) which contained an implementation of EM-PPCA for me to compare against. It was a pretty minor bug after all. Thanks everyone for your help!",en
1107487,2011-07-06 09:18:49,MachineLearning,Activity classification using accelerometers,ihw7p,Dummas,1298123807.0,https://www.reddit.com/r/MachineLearning/comments/ihw7p/activity_classification_using_accelerometers/,6.0,12.0,"Dear Reddits,

I'm currently on Internship and I was given a task to get in touch with the latest jobs in activity classification.

Anyone knows a good papers about this topic?

Interesting papers:
- http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5346042&amp;tag=1

Thank You",en
1107488,2011-07-06 14:08:22,statistics,All-Island Regional Research Observatory (AIRO),ii0ol,scareymonster,1231233908.0,https://www.reddit.com/r/statistics/comments/ii0ol/allisland_regional_research_observatory_airo/,1.0,1.0,,en
1107489,2011-07-06 20:57:13,statistics,Some simple SAS questions ,iiaet,javes1,1288135284.0,https://www.reddit.com/r/statistics/comments/iiaet/some_simple_sas_questions/,5.0,9.0,"I am working with some survey data and have split the survey in several data sets. I would like to compare the mean of a smaller subset of the data against the total mean. 

How would I go about calling the mean and standard deviation from proc means into a new data set? Basically using a t test. Sample is large, 6000 total, and subset is about 250. 

",en
1107490,2011-07-07 13:33:14,statistics,Comparing adjusted relative odds in a multinomial logistic regression,iix9y,Cognitive86,1308913962.0,https://www.reddit.com/r/statistics/comments/iix9y/comparing_adjusted_relative_odds_in_a_multinomial/,8.0,1.0,"Hi, 

Hopefully someone can help. I'm looking at the results of a published study today and it is based upon a multinomial logistic regression model. 

There are 3 levels to the independent variable of interest (a number of other controls) and there are multiple levels to the dependent variable.  Level 1 of our IV is the reference. The adjusted relative odds of level 2 vs 1 is 1.67 for one of the outcomes (CI: 1.28 to 2.17). Equally the adjusted relative odds of level 3 vs level 1 is 2.14 (CI:1.46 to 3.16). 

I'm trying to infer as much as I can about the difference between level 2 and level 3 from the study. The aspect I am really interested in is whether there is a significant difference between both of their relative odds for the exact same model (i.e. same IVs and same DV). Can I assume that, as in both cases the values are included in the others' confidence interval, there will likely not be a sig difference in relative odds between these levels?

Many thanks in advance guys,

C86",en
1107491,2011-07-07 15:54:45,statistics,Applets for demonstrating signal-noise?,iizix,konold,1261530500.0,https://www.reddit.com/r/statistics/comments/iizix/applets_for_demonstrating_signalnoise/,5.0,2.0,"I'm looking for applets for demonstrating the idea of signal/noise to use in a course I'm teaching.  What I have in mind is an image or sound hooked to a noise slider.  So we could begin with a very noisy image, slowly decrease the noise until some recognizable signal was detectable in the image.  Ideally, the applet would have several images/sounds so that we could do this a few times with different signals -- make a bit of a game of it.  I've been looking a while now and not found quite what I'm looking for.",en
1107492,2011-07-07 17:11:07,statistics,"Write a R-bloggers guest post on the statistics, rstats, and machinelearning subreddits!",ij1dl,RA_Fisher,1299707119.0,https://www.reddit.com/r/statistics/comments/ij1dl/write_a_rbloggers_guest_post_on_the_statistics/,4.0,0.0,"I recently e-mailed with Tal, the host/manager of R-bloggers.com.  If you're not familiar with R-bloggers.com, check it out, it's a great site that aggregates blog post on R, statistics, and machine learning.

The purpose of my e-mail was to see if we could promote the statistics, rstats, and machinelearning subreddits on his site.  He agreed and asked that I/we write a guest blog post that talks about what we're doing here and why R-bloggers readers should come participate.

So, I'm asking you to help me draft a good, succinct, and persuasive guest post describing and selling these sub-reddits.  I'm only one single user of these sub-reddits and I think collaboration is the way to go.

I've setup a Google docs file here that anyone can access without logging in:

https://docs.google.com/document/d/1UCSsyjpy4_ju6C8uIexudQwmOK_4ZLSEC1zGK5IYBMM/edit?hl=en_US

Thanks for your help, input, and participation!",en
1107493,2011-07-07 17:11:54,MachineLearning,"Write a R-bloggers guest post on the statistics, rstats, and machinelearning subreddits! (x-cross from r/statistics)",ij1ec,RA_Fisher,1299707119.0,https://www.reddit.com/r/MachineLearning/comments/ij1ec/write_a_rbloggers_guest_post_on_the_statistics/,8.0,3.0,"I recently e-mailed with Tal, the host/manager of R-bloggers.com.  If you're not familiar with R-bloggers.com, check it out, it's a great site that aggregates blog post on R, statistics, and machine learning.


The purpose of my e-mail was to see if we could promote the statistics, rstats, and machinelearning subreddits on his site.  He agreed and asked that I/we write a guest blog post that talks about what we're doing here and why R-bloggers readers should come participate.


So, I'm asking you to help me draft a good, succinct, and persuasive guest post describing and selling these sub-reddits.  I'm only one single user of these sub-reddits and I think collaboration is the way to go.


I've setup a Google docs file here that anyone can access without logging in:

https://docs.google.com/document/d/1UCSsyjpy4_ju6C8uIexudQwmOK_4ZLSEC1zGK5IYBMM/edit?hl=en_US

Thanks for your help, input, and participation!",en
1107494,2011-07-07 18:42:28,rstats,"Write a R-bloggers guest post on the statistics, rstats, and machinelearning subreddits! (x-cross from r/statistics)",ij42w,RA_Fisher,1299707119.0,https://www.reddit.com/r/rstats/comments/ij42w/write_a_rbloggers_guest_post_on_the_statistics/,11.0,0.0,"I recently e-mailed with Tal, the host/manager of R-bloggers.com. If you're not familiar with R-bloggers.com, check it out, it's a great site that aggregates blog post on R, statistics, and machine learning.

The purpose of my e-mail was to see if we could promote the statistics, rstats, and machinelearning subreddits on his site. He agreed and asked that I/we write a guest blog post that talks about what we're doing here and why R-bloggers readers should come participate.

So, I'm asking you to help me draft a good, succinct, and persuasive guest post describing and selling these sub-reddits. I'm only one single user of these sub-reddits and I think collaboration is the way to go.

I've setup a Google docs file here that anyone can access without logging in:
https://docs.google.com/document/d/1UCSsyjpy4_ju6C8uIexudQwmOK_4ZLSEC1zGK5IYBMM/edit?hl=en_US

Thanks for your help, input, and participation!",en
1107495,2011-07-08 09:35:44,MachineLearning,What kind of hardware enhances machine learning tasks,ijt76,the_cat_kittles,1287171936.0,https://www.reddit.com/r/MachineLearning/comments/ijt76/what_kind_of_hardware_enhances_machine_learning/,4.0,3.0,"If you could design a computer for machine learning, what kind of hardware would you get? I would think a solid state hard drive would help speed up database queries, and having a ton a ram would be a boost. Are there other things? I don't know much about hardware.",en
1107496,2011-07-08 20:35:39,statistics,Can a ratio have a standard deviation?,ik6q1,johnnythebiochemist,1259382408.0,https://www.reddit.com/r/statistics/comments/ik6q1/can_a_ratio_have_a_standard_deviation/,6.0,16.0,"I have some positive signal data with replicates, and some baseline signal data with replicates. I can calculate the standard deviation for both, but what happens to that population statistic when I try to find the signal to noise ratio?

Perhaps more generally, where do you learn to do ""population parameter"" operations?",en
1107497,2011-07-08 21:13:54,MachineLearning,"Hiring multiple full-time Data Scientists for a flourishing Big Data startup in the Bay Area, CA and Cambridge, MA. One Junior position and one more Senior Technical Lead position.",ik7v0,ohsnaaap,1270647929.0,https://www.reddit.com/r/MachineLearning/comments/ik7v0/hiring_multiple_fulltime_data_scientists_for_a/,0.0,4.0,"please email me your resume if you are interested. tanyacash@gmail.com.  Will kick off initial phone interviews and explain more about the positions then. 

Looking forward to finding some great data junkies via reddit!",en
1107498,2011-07-09 00:55:32,statistics,"How do you go about finding ""something interesting"" in the data?",ikeqc,statguy,1271026910.0,https://www.reddit.com/r/statistics/comments/ikeqc/how_do_you_go_about_finding_something_interesting/,8.0,21.0,"Not sure if there is a better subreddit for this, but as an analyst I come across this request a lot, where the client would be interested in seeing something interesting. If I know what I am looking for the problem becomes trivial (still requires a lot of work), but how do I go about exploring the data to find something unexpected or interesting?

Is there a philosophy around that? How do you go about it?",en
1107499,2011-07-09 05:52:17,MachineLearning,"ML Reddit, what do you make of this algorithm by Google? PHIL - Google's Second Most Important Algorithm",iklvc,HalNavel,1211842976.0,https://www.reddit.com/r/MachineLearning/comments/iklvc/ml_reddit_what_do_you_make_of_this_algorithm_by/,26.0,1.0,,en
1107500,2011-07-09 08:14:31,statistics,Standard Deviation in non-Euclidean Vector Spaces,ikp25,GilbertSullivan,1286372915.0,https://www.reddit.com/r/statistics/comments/ikp25/standard_deviation_in_noneuclidean_vector_spaces/,1.0,0.0,"Is there some concept of a standard deviation in non-Euclidean vector spaces?  


Looking at the formula for standard deviation, the sqrt( (x1 - m)^2 + (x2 - m)^2 ) part looks to me like, essentially, a Euclidean distance.  My understanding of the formula is that the standard deviation is essentially the average distance between each observation and the mean.

Suppose my observations come from a non-Euclidean vector space (for instance, suppose we use L1 norm instead).  Would we then calculate the standard deviation differently?  (i.e. would it be sum(|xi - m|)?)",en
1107501,2011-07-09 23:04:07,statistics,Chi square question/biostatistics?,il3hu,[deleted],,https://www.reddit.com/r/statistics/comments/il3hu/chi_square_questionbiostatistics/,1.0,0.0,"Hello all, I would really appreciate it if someone could look over this for me and let me know if I am on the right track.

As some background, I am working a summer internship at a hospital and somehow found myself doing statistical analysis despite initially thinking I'd be doing something else. I've taken a couple of stats courses, but I am not really confident yet in what I am doing as I am an anthropology major.

Alright, onto the problem at hand. I work in a pathology lab, and I am trying to determine whether results for a cancer diagnostic test vary with respect to the amount of time it takes for the tested specimen to be put in formaldehyde after being excised from a patient. The current recommended standard is that this should occur within one hour.. 

So here's what I did..
I looked at pathology reports since 2008, recorded collection times and times into formaldehyde for all breast excisions then subtracted formaldehyde time from collection time to get the overall times I was looking for. I also recorded whether a particular test result was positive or negative for each case. 

I then split this data into two categories-results for under/equal to one hour and results for greater than one hour, and found the number of negative results and positive results for each category. Then I ran a Chi-square test for homogeneity of proportions between the two time categories and got a significant result (using Yates correction since it was a 2x2 table that was set up as shown below)). 
                       ≤60 minutes         &gt;60 Minutes
Test Negative        26                        49

Test Positive         130                      129

Essentially, is this an OK way to see if the two categories are different? I am not sure how else to go about it. If I am completely off base here please feel free to correct me, it would be very very much appreciated.",en
1107502,2011-07-10 00:55:51,statistics,What is your position on pie charts?,il60o,TedFromTheFuture,,https://www.reddit.com/r/statistics/comments/il60o/what_is_your_position_on_pie_charts/,3.0,4.0,"In college, I was told NEVER to use pie charts. My 2nd year statistics professor said that if any student used a pie chart, they would fail the class instantly. This has been ingrained into my brain. Now that I am out in the ""business"" world, I am demanded constantly to use pie charts. This has turned into a serious internal dilemma. Should I stand up and fight against the tyrannical pie chart, or were my professors just nuts? 

Edit: Thanks everybody for your feedback. I will hold back the urge to complain about pie charts. ",en
1107503,2011-07-10 06:55:54,statistics,path analysis help,ildbp,forever_erratic,1256947469.0,https://www.reddit.com/r/statistics/comments/ildbp/path_analysis_help/,6.0,0.0,"Hello stats people! I have a few questions, but I'll start with a basic one. Why might path analysis be better than multiple regression that includes interactions?",en
1107504,2011-07-10 21:29:49,statistics,Benefits of MLE?,ilqkm,[deleted],,https://www.reddit.com/r/statistics/comments/ilqkm/benefits_of_mle/,7.0,19.0,"I can't find a convincing argument for using the MLE. I realise it's a mode of the likelihood function, but even ignoring the possibility of multimodal likelihoods, why the mode? (other than ease) The median of the posterior minimises absolute deviation and the mean minimizes squared deviation, of course. Those seem grounded in good reasoning. 

Can anyone explain why maximising likelihood is a good method for parameter estimation? 

EDIT: Sorry, I should have mentioned- I'm open to working in either a Bayesian or frequentist framework. ",en
1107505,2011-07-10 21:58:07,MachineLearning,Bayesian Reasoning and Machine Learning free ebook,ilr8w,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/ilr8w/bayesian_reasoning_and_machine_learning_free_ebook/,42.0,6.0,,en
1107506,2011-07-11 05:15:49,computervision,what kind of camera should I use for CV projects?,im1lw,ludflu,1182284577.0,https://www.reddit.com/r/computervision/comments/im1lw/what_kind_of_camera_should_i_use_for_cv_projects/,1.0,1.0,"I have a [CV project](http://www.reddit.com/r/computervision/comments/fv3p5/automatic_go_board_recognizer/) I attempted to execute using a consumer grade digital video camera - but the results were not good. Everything works great with still images taken with an SLR, but when I grab frames from a digital video camera, the quality is so poor that I can't really work with it. Are there digital cameras made for Computer Vision that can be had for less than say, $500 ? I'm not sure where to look. Thanks!",en
1107507,2011-07-11 19:36:22,statistics,Correlated.org and sample correlations vs. population correlations,imiyn,casualfactors,1285978895.0,https://www.reddit.com/r/statistics/comments/imiyn/correlatedorg_and_sample_correlations_vs/,7.0,3.0,,en
1107508,2011-07-11 19:48:59,statistics,"I need to pull down stock data for the last 3 years, for a variety of companies. Any suggestions as to how to go about it?",imjbt,prionattack,1257312771.0,https://www.reddit.com/r/statistics/comments/imjbt/i_need_to_pull_down_stock_data_for_the_last_3/,18.0,21.0,"I've tried Wolfram (Mathematica &amp; Alpha) as well as Yahoo Finance, etc. I need the opening(or closing) price, debts, and asset data. I don't know any languages except for R (statistical software).",en
1107509,2011-07-11 21:42:13,statistics,[URGENT REQUEST]Hypothesis Testing,immq8,Zippo16,1306205124.0,https://www.reddit.com/r/statistics/comments/immq8/urgent_requesthypothesis_testing/,0.0,4.0,,en
1107510,2011-07-12 00:16:04,MachineLearning,"Are there any non-math heavy classifier books out 
there?",imrll,alfiejohn,1235330361.0,https://www.reddit.com/r/MachineLearning/comments/imrll/are_there_any_nonmath_heavy_classifier_books_out/,0.0,6.0,"It's been years since I've done any math but I am wanting to implement my own classifiers (in particular non-linear SVMs). However, most books seem too math-heavy for me and Bishop seems impenetrable.

Are there any non-math heavy classifier books (especially focusing on non-linear classifiers) or am I going to have to bite the bullet and scrub up on linear algebra and calculus first?

*edit: Thanks guys... I was hoping on not to have to read math books again, but I think necroforest put it succinctly",en
1107511,2011-07-12 03:40:39,datasets,"Looking for a free, recent zip code database",imxfw,tallbaker,1302037850.0,https://www.reddit.com/r/datasets/comments/imxfw/looking_for_a_free_recent_zip_code_database/,1.0,0.0,"I am looking for a zip code database for a project I am working on - but I can't seem to find one that is free. It seems absurd that the USPS charges for this information.  

All the sites I find charge or are scammy - or have data that is seven or eight years old.  Does anyone know of a place I can find the zip, city, state, lat and long for free? ",en
1107512,2011-07-12 04:49:54,datasets,~25k reddit stories with vote data; ones about meat marked,imza0,Xodarap,1218923849.0,https://www.reddit.com/r/datasets/comments/imza0/25k_reddit_stories_with_vote_data_ones_about_meat/,1.0,0.0,,en
1107513,2011-07-12 11:45:34,statistics,Andrew Gelman on evaluating a new and wacky claim?,in92g,mycatharsis,1294046118.0,https://www.reddit.com/r/statistics/comments/in92g/andrew_gelman_on_evaluating_a_new_and_wacky_claim/,13.0,7.0,,en
1107514,2011-07-12 12:04:38,statistics,Data sources for global (!) industrial/manufacturing production and energy use?,in9dh,latortilla,1299276404.0,https://www.reddit.com/r/statistics/comments/in9dh/data_sources_for_global_industrialmanufacturing/,7.0,2.0,"Hi, I am looking for data on industrial/manufacturing production and energy use (e.g. % of energy coming from coal, nuclear etc), my problem is that it needs to be on a global scale, i.e. world production, world energy use and ideally monthly since the 1960s. As it turns out, these things aren't that easy to find, the World Bank and International Energy Agency has some, but not enough. Does anyone have any ideas where to find that kind of data? Cheers!",en
1107515,2011-07-12 20:41:16,statistics,What are the 3-dimensional versions of quadratic and cubic functions?,inliy,robotrebellion,,https://www.reddit.com/r/statistics/comments/inliy/what_are_the_3dimensional_versions_of_quadratic/,6.0,7.0,"For my research I am testing the fit of linear, quadratic, and cubic functions to my data. I then need to add a third dimension and generate similar tests. What are these 3D (XYZ) functions called, or what form would they take?

Edit: It looks like this may be the 3D quadratic equivalent:

http://en.wikipedia.org/wiki/Quadratic_polynomial#Two_variables_case

However I am still clueless about a 3D cubic function.",en
1107516,2011-07-12 21:51:35,statistics,What exactly is the name of the type of regression analysis where you try to see if the model is significant over *multiple* endpoint values?,innun,inquilinekea,1274654696.0,https://www.reddit.com/r/statistics/comments/innun/what_exactly_is_the_name_of_the_type_of/,1.0,2.0,"An example is here: http://www.reddit.com/r/askscience/comments/ine4x/regarding_the_recent_lapse_of_global_warming_in/c2554al

I'm sure it's related to robust statistics. But I'm sure that there's a label more specific than that. ",en
1107517,2011-07-13 02:49:45,statistics,I need to whine,inx35,new_england,1202694183.0,https://www.reddit.com/r/statistics/comments/inx35/i_need_to_whine/,1.0,0.0,,en
1107518,2011-07-13 06:35:53,statistics,Basic Hypothesis Testing Question,io383,mkh01,1305265426.0,https://www.reddit.com/r/statistics/comments/io383/basic_hypothesis_testing_question/,3.0,2.0,"So I'm enjoying my introductory statistics course so far, but I'm having a little trouble conceptualizing this problem. 

""After completing sales training for a large company, it is expected 
that the salesperson will generate a sale on at least 15 percent of the 
calls he or she makes. To make sure that the sales training process 
is working, a random sample of n = 400 sales calls made by sales 
representatives who have completed the training have been selected and 
the null hypothesis is to be tested at 0.05 alpha level with a p-value. 
Suppose that a sale is made on 36 of the calls. Based on this 
information, can the company determine whether they are generating 
sales on at least 15% of the calls?""

The way the test was set up in class: 
null hypothesis: Pi &lt;=.15
alt. hypothesis: Pi &gt;.15
(Lower Tail Test)
Ztest = -3.36
P-Value = 1, accept null

The way I intuitively set it up in practice:
null hypothesis: Pi&gt;=.15
alt. hypothesis: Pi&lt;.15
(Upper Tail Test)
Ztest = -3.36
P-Value = 0, reject null

I understand that these are two different ways of saying the same thing. But I've got two questions that I'm trying to wrap my head around. 
-Which of these tests was set up more accurately? 
-I understand that p-value is the probability of another sample being more extreme (farther from the mean) from the current sample mean...but am having trouble applying the concept in the context of this problem. Is it the claimed mean or the sample mean they're referring to?",en
1107519,2011-07-13 21:42:36,MachineLearning,"Is there a free ""website text extraction"" software out there?",ioo9g,eltranslator,1234930111.0,https://www.reddit.com/r/MachineLearning/comments/ioo9g/is_there_a_free_website_text_extraction_software/,3.0,5.0,"Don't know if this is the right subreddit, but you're more tech savvy than I'm for sure. I'm a translator of webpages, and sometimes it is difficult to extract all the text of a certain website. Is there a software out there that does this task? Like, input url -&gt; get .txt file. Possible or naive?",en
1107520,2011-07-14 10:44:37,MachineLearning,Advice: Software to model a tumor from a sequential group of 2d slides into 3d,ip955,pervball,1305606290.0,https://www.reddit.com/r/MachineLearning/comments/ip955/advice_software_to_model_a_tumor_from_a/,9.0,13.0,"Hi, I'd ideally like to take a group of thin slice pathology slides and then  model them into a 3d view of the tumor and its surrounding blood and lymphatic supply. I'm looking for recommendations of software that could accomplish this. I've been playing around with ImageJ but no luck. The tumor type in question is still debated on whether lymphatic invasion or blood invasion is first. ",en
1107521,2011-07-14 20:19:12,statistics,I need some stats help for my dissertation involving questionnaire data,ipl9q,[deleted],,https://www.reddit.com/r/statistics/comments/ipl9q/i_need_some_stats_help_for_my_dissertation/,0.0,7.0,"Hi Redditors, 

I am doing research involving asking villagers their opinions on conservation and consuming sea turtle eggs. I have produced 100 questionnaires, each with 9 questions and all answers are tick responses based on a similar style to a Lickert scale. 

I have half of them, sitting next to me, but I am a biologist and I really struggle with stats even though they are the backbone of my science. I wish to learn and I am all the time. 

I really need help to know what I should do with the data, how I should transfer it onto the computer and what is the most suitable stats. I think ANOVA and maybe Chi squared is best? 

I'm using excel as the data is simple. 

I know I should ask my dissertation supervisor, but I am in  Malaysia and she is in England. Also, she is very busy and a large douche...

Thanks",en
1107522,2011-07-15 07:46:39,statistics,Help with basic hypothesis/significance problem,iq509,SnorlaxAttacks,1310704453.0,https://www.reddit.com/r/statistics/comments/iq509/help_with_basic_hypothesissignificance_problem/,1.0,8.0,"The average GPA for students last year was 2.4 
The standard deviation is 0.3 
This year a simple random sample of 10 students had a mean GPA of 2.6
Is this evidence the GPA has improved over the first year?

A) State hypotheses using mathematical notation
   
For this part so far I have Ho μ=2.4
                                   Ha μ&gt;2.4  

B) Calculate the value of the test statistic

I have Z= (2.4-2.6)/(0.3/10) Z=6.66666667

C) Determine the P-value

Really confused at what to do now..

D) State your conclusion in terms of the problem

Part C has snowballed into D

E) Explain if the result is or is not significant at the 1% level of significance



Am I doing the first couple parts right? Can someone help me out further with solving it? been using Khan academy but am very confused


",en
1107523,2011-07-15 08:33:01,statistics,Taking introductory statistics. Can't unsee.,iq60n,Boobzilla,1284686076.0,https://www.reddit.com/r/statistics/comments/iq60n/taking_introductory_statistics_cant_unsee/,0.0,0.0,Is this considered [normal](http://imgur.com/MTvLC)?,en
1107524,2011-07-15 10:03:37,artificial,Objective measure of brain's response to uncanny valley,iq7sx,kali3000,1179676976.0,https://www.reddit.com/r/artificial/comments/iq7sx/objective_measure_of_brains_response_to_uncanny/,10.0,0.0,,en
1107525,2011-07-15 11:44:44,MachineLearning,Python for brain mining: (neuro)science with state of the art machine learning and data visualization,iq9if,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/iq9if/python_for_brain_mining_neuroscience_with_state/,24.0,5.0,,en
1107526,2011-07-15 13:48:54,MachineLearning,"Graph and Network Analysis. Tutorial, datasets and code",iqbcc,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/iqbcc/graph_and_network_analysis_tutorial_datasets_and/,37.0,1.0,,en
1107527,2011-07-15 17:52:00,statistics,"This should be an easy problem ... a system of simultaneous linear equations, but with uncertainties",iqg6y,[deleted],,https://www.reddit.com/r/statistics/comments/iqg6y/this_should_be_an_easy_problem_a_system_of/,3.0,7.0,"I have a set of experimental measurements on a system with up to 3 variables: a, b, and c.  I have measurements of the system when a+b+c are present, a few different systems when a+b are present, and a few when just a or just b are present.  

The measurements on each system are normally distributed about a mean, and I know the standard deviation.  Clearly, the system is ""overdetermined"", but is there a proper way to determine the best values of a, b, and c, as well as the uncertainties in their values?

... cross-posted to math and stats.  Thanks!",en
1107528,2011-07-15 22:06:42,statistics,Trouble with hiring statistics,iqnyz,distinctsummer,1302816063.0,https://www.reddit.com/r/statistics/comments/iqnyz/trouble_with_hiring_statistics/,0.0,1.0,"I'm given some stats: 
In 2009, there were 128 terminations within the first 6 weeks of the year.  There were 383 new hires.  In 2010 there were 126 terminations within the first 6 weeks.  There were 370 new hires.  I have the breakdown of how many employees were hired and how many were terminated on a week by week basis.  I was asked to run some statistical tests and write a report ""detailing the variances among those weeks, some percentages (i.e. % of new hires terminated within various periods, and overall), and maybe look for any statistically significant variations between the 09 and 10 data for some of those week periods.""  What kinds of tests do I do to go about this?  Thanks. ",en
1107529,2011-07-15 22:42:18,statistics,Recommendations for advanced statistics book?,iqp25,xel02,1304403087.0,https://www.reddit.com/r/statistics/comments/iqp25/recommendations_for_advanced_statistics_book/,6.0,7.0,"I've just finished an undergraduate degree in statistics, and computer science. I feel comfortable with the concepts covered in undergraduate statistics degree as well as the machine learning concepts.

I'm trying to find a good general book on advanced topics in statistics such as design of experiments, multilevel regression, Poisson regression, that might be useful to a masters student who wants to focus on the applied side of statistics as opposed to mathematical statistics.

What would be some good books you would recommend to a masters student in statistics that plans to work in industry?",en
1107530,2011-07-16 00:13:19,statistics,Zero-Truncated Negative Binomial modeling,iqrtf,[deleted],,https://www.reddit.com/r/statistics/comments/iqrtf/zerotruncated_negative_binomial_modeling/,2.0,4.0,I have been trying to run a ZTNB model in stata and the lousy thing will not output results. I am running out of ideas and I am wondering if there is anyone out there who deals with count models and survey data that might have an idea that I have not had. ,en
1107531,2011-07-16 02:58:02,MachineLearning,"This is probably the wrong place to post, but IMO this subreddit has some of the best minds I can easily reach on the net.  [Hiring] tech/finance, NYC.",iqvvp,[deleted],,https://www.reddit.com/r/MachineLearning/comments/iqvvp/this_is_probably_the_wrong_place_to_post_but_imo/,0.0,4.0,Hiring posted [here](http://www.reddit.com/r/forhire/comments/iqvho/hiring_nyc_400day_for_data_warehouse_mis_sql/).,en
1107532,2011-07-16 06:07:33,statistics,Help with Plotting Empirical Survival Function in R,ir00n,[deleted],,https://www.reddit.com/r/statistics/comments/ir00n/help_with_plotting_empirical_survival_function_in/,10.0,5.0,"I'm trying to plot the empirical log survival function for my set of data in R. I've been using ecdf(data) to get the empirical cdf but I can't manipulate the data from ecdf(data) to fit the survival function: S_n(t) = 1-F_n(t), where F_n(t) is the ecdf. I don't seem to be able to find any package/function that calculates the empirical survival function like the function ecdf() does. Can anyone give me some suggestions here? Thanks for looking!

The data I've been using is just a n=100 sample from an exponential dist with lambda=1
    data = rexp(100,1)

EDIT: 
I've done 
    x = rexp(100,1)
    f = ecdf(x)
    plot(1-f(x),x,log=""y"")
This seems to give me what I want.

 ",en
1107533,2011-07-17 10:50:07,artificial,Peter Norvig vs Noam Chomsky on the future of AI,irvnn,secret_town,1279292087.0,https://www.reddit.com/r/artificial/comments/irvnn/peter_norvig_vs_noam_chomsky_on_the_future_of_ai/,28.0,6.0,,en
1107534,2011-07-17 18:43:52,statistics,Difference between distance-based linear modeling and distance-based redundancy analysis?????,is1ji,mycelialgroove,1306514225.0,https://www.reddit.com/r/statistics/comments/is1ji/difference_between_distancebased_linear_modeling/,3.0,2.0,"Hey there statistical wizards,

I have been conducting some statistics using Primer on some ecological data incorporating environmental characteristics as predictor variables and species abundance as response variables.  It's obvious to me that non-parametric statistics are the way to go on this...avoiding assumptions of normality, capability of using distance measures besides Euclidean etc.   therefore, I have been using the DISTLM function (distance-based linear modeling) in Primer to determine the relative importance of my predictor environmental variables.  I've subsequently used a multi-model inference method called AICc weighting that basically gives higher credence (relative variable importance (RVI) to variables that appear in better and more models.  I subsequently used distance-based redundancy analysis (in primer again) including only those variables that received high RVI scores.  

From what I can tell from the literature, there is very little difference between distance-based linear modeling and distance-based redundancy analysis, however I am having a difficult time finding ANY differences between the two.  This is because there are no journal or book references to DISTLM.  The only source on the internet that speaks of DISTLM refers me to a 2001 paper by McArdle and Anderson that speaks only of db-RDA. I'm beginning to think that DISTLM is just another name for db-RDA that PRIMER made up, but I don't understand why they created two different analyses in the program if they are the same.  

Does anyone have any insight into this matter?  If they are different, why and if they are not different, why the two separate analyses in PRIMER?  

Thanks so much for your help.",en
1107535,2011-07-18 02:11:33,statistics,Can someone please explain to this dummy (i.e. me) a basic coin toss probability question?,isbr5,tReP2pHu,1280851017.0,https://www.reddit.com/r/statistics/comments/isbr5/can_someone_please_explain_to_this_dummy_ie_me_a/,10.0,30.0,"Okay, this is clearly a stupid question but I don't any longer want to think about this each time I'm high.

Say I have a fair coin i.e. heads and tails have equal (0.5) probability as the result of a coin toss. I take my coin and start taking a sample. And I see H, H, H, H, H, H etc. Now, given enough coin tosses, I should get closer to the true probability of H and T, right? So, if I see H, H, H, H (let's say 100 times in a row), doesn't each subsequent H increase the probability that the next coin toss will result in a T? Because we're getting further and further away from the true probability, but we know it's a fair coin, so I should start getting a number of Ts to reflect the true probability?
 
EDIT: Wikipedia to the rescue. [Gambler's fallacy](http://en.wikipedia.org/wiki/Gambler%27s_fallacy). Thanks all.",en
1107536,2011-07-18 09:44:51,statistics,Interpretation of Skewness and Kurtosis,ism37,plast1c,1310969776.0,https://www.reddit.com/r/statistics/comments/ism37/interpretation_of_skewness_and_kurtosis/,2.0,2.0,"hello,
i got a  ich habe einen homogeneous power test without time limits and with dichotomous items Items (right/wrong), which are attributed to three different scales. i want to have a look at the distribution of the scores now. the kolmogorov smirnov-test for the 3 scales says that none of them is a normal distribution. the skewness is -1.517, -.577 and -1.125. this means, that they are left-skewed. Now my Questions: What does this mean exactly? That there are just a few persons with low scores and even less with really high scores? is the test too easy? does a power test has to be normal distributed? what can i do to get a normal distribution?

Thank you for your answers in advance",en
1107537,2011-07-18 19:01:17,analytics,"I noticed a Google Analytics -UA-XXXX-Y javascript code on most websites, does this necessarily mean that tracking is conducted?",iswed,SeeDerekRun,1293615839.0,https://www.reddit.com/r/analytics/comments/iswed/i_noticed_a_google_analytics_uaxxxxy_javascript/,1.0,0.0,"I was just wondering if I saw this code on a website, if it meant anything. Do all adword accounts use the same thing?",en
1107538,2011-07-18 23:46:25,statistics,"Real analysis, measure and probability theory introductory books for non-mathematicians?",it5hx,raptorgirl,1267041047.0,https://www.reddit.com/r/statistics/comments/it5hx/real_analysis_measure_and_probability_theory/,9.0,7.0," My undergrad was on engineering and although I've already worked through Statistical Inference by Casella, I'd like to understand better a number of topics. Needless to say I'm pants at pure mathematics so I don't wanna go over my head. Any recommendations?",en
1107539,2011-07-19 04:02:42,statistics,People should learn statistics. ,itcw9,[deleted],,https://www.reddit.com/r/statistics/comments/itcw9/people_should_learn_statistics/,1.0,0.0,,en
1107540,2011-07-19 13:59:50,artificial,"Is anyone working on ""true"" AI",itpql,redditnoveltyaccoun2,1305836392.0,https://www.reddit.com/r/artificial/comments/itpql/is_anyone_working_on_true_ai/,34.0,32.0,"Is anyone having any success on machines that actually think? normally AI seems to be about searching problems spaces with heuristics or algorithms that take lots of input-output data into account. What about developing something that does actual reasoning from scratch?
",en
1107541,2011-07-19 18:19:42,statistics,How does PPS sampling in a survey study affect data analysis?,itvj7,taciturnbob,1304227801.0,https://www.reddit.com/r/statistics/comments/itvj7/how_does_pps_sampling_in_a_survey_study_affect/,1.0,0.0,"I am using SAS's survey toolbox to take into account the Probability proportional to size sampling method, but the documentation is short of the technical details on what assumptions are being used and how the data analysis differs. Can anyone with experience with PPS comment?",en
1107542,2011-07-19 20:46:03,statistics,How do I explain statistical significance to a lay person?,iu0b9,tonecapone3001,1298277107.0,https://www.reddit.com/r/statistics/comments/iu0b9/how_do_i_explain_statistical_significance_to_a/,18.0,23.0,So last week I was presenting results for a major client. I presented information that showed before and after snapshot of the effects of training. For one training program the raw data showed a decline in performance by (one point from a 100 point scale) but the drop was not significant. When I tried explaining that the results were significant the client was arguing with me that it is significant. I tried explaining significance level to him but it was going over his head. Does anyone have a good lay person explanation for significance that I can use?,en
1107543,2011-07-20 02:19:08,statistics,Need help finding the probability.,iuad3,set123,1282059646.0,https://www.reddit.com/r/statistics/comments/iuad3/need_help_finding_the_probability/,1.0,3.0,"Let's say I have a bag of 6700 marbles. All but five of the marbles (6695) are white; five are black. If I let you pick, at random, 134 marbles out of the bag (EDIT: and without putting them back after you pick them), what is the probability that you will have four out of the five black marbles when you're done?

I don't know what the etiquette is here; this is a real-world problem I'm trying to figure out (not math homework or anything). I actually used to be a math major, and would even be fine with someone pointing me in the direction of the right formula.

Thanks in advance.",en
1107544,2011-07-20 04:25:54,statistics,"/r/stat, how many of you gamble?",iudsz,blue_horse_shoe,1259214000.0,https://www.reddit.com/r/statistics/comments/iudsz/rstat_how_many_of_you_gamble/,21.0,31.0,,en
1107545,2011-07-20 04:55:17,computervision,Reinventing the Wheel: Face Detection in Python - Detecting a Face (crosspost from /r/Python),iuel9,rCX12,1273937703.0,https://www.reddit.com/r/computervision/comments/iuel9/reinventing_the_wheel_face_detection_in_python/,4.0,0.0,,en
1107546,2011-07-20 10:34:38,statistics,"Three guides on ""Making Data Meaningful"" from  United Nations Economic Commission for Europe",iumua,mycatharsis,1294046118.0,https://www.reddit.com/r/statistics/comments/iumua/three_guides_on_making_data_meaningful_from/,2.0,0.0,,en
1107547,2011-07-20 16:53:15,statistics,Computing confidence in a set of ratings,iut8d,jon_smark,1253799834.0,https://www.reddit.com/r/statistics/comments/iut8d/computing_confidence_in_a_set_of_ratings/,3.0,12.0,"I'm a programmer, not a statistician by training.  Nevertheless, I'm trying to solve a problem in a way that's not statistics-illiterate, which is why I'm hoping this subreddit can lend me a hand...

I have a set of independent ratings for a given item (a book, for example). Each rating is an integer in the range 1..7.  Moreover, even before the first rating is cast, I have some tentative prior expectation of the item's worth, which is essentially a float in the range [1.0; 7.0].

Below are the two key problems I want to solve.  Note that they are closely related.

* Given the current set of ratings, I want a single value which summarises how confident I can be that the item's ""true"" worth will be within a short interval (say [-0.5; 0.5]) of the current estimate of its worth.
* Also given the current set of ratings, I want a single value which summarises how confident I can be that the item's ""true"" worth will be higher than a given value.

To illustrate, suppose I had a book whose worth I suspected was 4.0. I then had a fairly large set of ratings whose mean was close to 4.0, and had furthermore a very narrow deviation from that mean.  In this scenario, I expect the confidence metric to be pretty close to 100%.

On the other hand, suppose I had a book whose worth I initially suspected was also 4.0.  However, in this case my set of ratings is not as large, and they are all over the place.  Therefore, I expect the confidence metric for this book to be quite low.

Now, my problem is how do I go about actually computing these confidence metrics? The fact that I have some prior expectations and that I wish to revise them as new ratings are cast leads me to suspect some sort of Bayesian analysis will be in order.  Moreover, I'm pretty sure this sort of problem is not new, and that somewhere out there is a good explanation of how these calculations can be made. Your thoughts?  (Many, many thanks in advance!)
",en
1107548,2011-07-20 17:09:32,statistics,"Figuring out which variable(s) best describe a dataset when, for some data points, data is missing?",iuto2,Bitruder,1145203871.0,https://www.reddit.com/r/statistics/comments/iuto2/figuring_out_which_variables_best_describe_a/,4.0,9.0,"Say I had a dataset of high-dimensional data that mapped onto a scalar value.  So, for example f(a,b,c,d) = y and I have this for a ton of a,b,c,d values. I want to figure out which of those variables (there are way more than 4) or which combination of variables, best predict the value for y.  Here's the catch though, sometimes I don't have a value for one of the variables so I only have f(a,b,__,d) = y.  What methods are available to me here that can take this into account?

What I think I'd end up with is a weighting matrix where I can ask first ""what is the best variable"" and then I could ask ""what is the best pairs of variables"", etc.",en
1107549,2011-07-20 23:05:06,computervision,"i have been studying optical flow for a few days and decided to write about it. feel free to criticize, or add to my understanding. ",iv4ua,mish4,1286697468.0,https://www.reddit.com/r/computervision/comments/iv4ua/i_have_been_studying_optical_flow_for_a_few_days/,2.0,0.0,,en
1107550,2011-07-21 03:15:00,statistics,So how is statistics as a career?,ivc33,[deleted],,https://www.reddit.com/r/statistics/comments/ivc33/so_how_is_statistics_as_a_career/,16.0,16.0,"So years ago I got a BS in mathematics but couldn't find a job so I went into the military.  I'm out now and the last year I've been going back to school preparing for pursuing a master's in computer science.  So far I've been taking undergraduate courses in CS and I've done quite well.

The problem is that I'm not a computer science/programmer guy.  I can do it but I don't enjoy it.  I've always been more into math and lately I've been thinking of applying to a graduate statistics program.

So how is statistics these days as a career field?  Is it difficult to find work?

(I spoke with  the director of the math dept at my school and he said that his stats graduates are in demand but the dept doesn't collect any data about its graduates)",en
1107551,2011-07-21 09:34:20,statistics,"O'Reilly Book ""Probability and Statistics for Programmers"": Creative Common with full PDF available; lots of python code examples",ivlov,mycatharsis,1294046118.0,https://www.reddit.com/r/statistics/comments/ivlov/oreilly_book_probability_and_statistics_for/,50.0,3.0,,en
1107552,2011-07-21 19:47:50,statistics,How should I analyze this data?,ivymc,[deleted],,https://www.reddit.com/r/statistics/comments/ivymc/how_should_i_analyze_this_data/,0.0,3.0,"So I've never taken a statistics course and my knowledge is very rudimentary. I'm analyzing some data I took over the summer in Dominica on snails. For this particular experiment, the data was qualitative: it was either (+) phototaxis, (-) phototaxis, or inconclusive. How do I go about analyzing this data for significance?


",en
1107553,2011-07-21 21:50:01,statistics,Probability of Passing a Test (Bayesian?),iw2kr,evilmaus,1278545501.0,https://www.reddit.com/r/statistics/comments/iw2kr/probability_of_passing_a_test_bayesian/,1.0,6.0,"I have been studying for a certification exam, which starts out naturally with doing some pretest work to assess areas of weakness.  The exam is composed entirely of multiple choice questions with four possible answers.  I was able to get 64% of the questions right in the pretest.  As it turns out, I won't be able to take it for a few more years, but I got to thinking.  I can model my odds of passing the test with the binomial distribution.  There are 175 questions that matter.  A passing grade is getting 106 of them correct (roughly 61%, ""D"" for ""diploma"" it seems)  Pulling out the binomial distribution, I plugged in n=175, k=105, and p=.64  and got 15% as my chance of failure.  Then I started thinking that, whereas n and k are givens and perfectly certain, the p parameter is really based off of an estimate from taking a few practice questions.  Do I just take the 64% probability as a given, or to be more accurate do I need to also construct some distribution representing what my actual chance of being correct on a given problem to properly solve this?  

I know how Bayesian reasoning can apply to a situation with given probabilities, but know that instead of specific values, one can use bayesian reasoning with whole probability distributions.  This seems to be a case where it applies, but I don't know beans about combining the two.",en
1107554,2011-07-21 22:11:07,statistics,A Roadmap for Rich Scientific Data Structures in Python,iw3af,agconway,1228006618.0,https://www.reddit.com/r/statistics/comments/iw3af/a_roadmap_for_rich_scientific_data_structures_in/,20.0,1.0,,en
1107555,2011-07-21 23:27:19,statistics,What is it called when you delete rows of observations with missing values?,iw5sj,scopegoa,1245090274.0,https://www.reddit.com/r/statistics/comments/iw5sj/what_is_it_called_when_you_delete_rows_of/,3.0,8.0,"Hello!

I have searched all over the Internet and I always thought this technique was called a crosswise deletion or something like that. It's starting to drive me crazy!


Example:

ColumnNames= Name, Age, Height, Occular Acuity

                   Dude,   5,   40,      Awesome
                   Teen,  15,   69,      Excellent
                   Adult, 40,   69,      N/A
                   Older, 75,   65,      NotSoExcellent


In this example ""Adult"" would be deleted by this process because of its missing value.
                     ",en
1107556,2011-07-22 00:45:48,statistics,"Chance favors the concentration of wealth, U of M study shows",iw88n,acusticthoughts,1157109878.0,https://www.reddit.com/r/statistics/comments/iw88n/chance_favors_the_concentration_of_wealth_u_of_m/,4.0,0.0,,en
1107557,2011-07-22 01:08:26,statistics,Simpson’s Paradox in regression,iw8ww,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/iw8ww/simpsons_paradox_in_regression/,13.0,0.0,,en
1107558,2011-07-22 16:37:47,statistics,How do you measure the accuracy of a probability forecast?,iwsip,John_P_Hackworth,1306990634.0,https://www.reddit.com/r/statistics/comments/iwsip/how_do_you_measure_the_accuracy_of_a_probability/,9.0,13.0,"For example, a series of rain forecasts that are 50% chance of rain, 75% chance of rain, etc. How do you measure the accuracy of the forecaster? ",en
1107559,2011-07-22 18:20:17,data,The Howler Project: Get Loud!,iwv91,winniechimp,1309463088.0,https://www.reddit.com/r/data/comments/iwv91/the_howler_project_get_loud/,2.0,0.0,,en
1107560,2011-07-22 20:18:05,analytics,Is anyone not using Google Analytics and why? ,iwyqh,Gustomaximus,1275914485.0,https://www.reddit.com/r/analytics/comments/iwyqh/is_anyone_not_using_google_analytics_and_why/,1.0,0.0,,en
1107561,2011-07-23 01:04:37,statistics,Variance explained PER variable...,ix6y5,suckeggmule,1250642053.0,https://www.reddit.com/r/statistics/comments/ix6y5/variance_explained_per_variable/,7.0,13.0,"Multiple regression texts always describe R^2 as percent of variance explained by the *model*.  I've seen news articles, etc., where the researcher states that a single variable explains ""only 40%"", then a second cause was investigated , and that ""explains another 20%"" and so on.  How are these ""per variable"" variance-explained results arrived at?  Are they building models where first one variable is in, then a second model with another variable, and so on?  Then each change in R^2 is that variable's ""portion"" of R^2?   Or is this information available in the output when running the full model?   (If it is the former, won't the results change depending on which variable goes into the model first?)

Thanks
",en
1107562,2011-07-23 15:58:46,datasets,Does anyone have access to a dataset of clinical trials (RCTs) and their outcomes? Even just cardiology would be useful.,ixmht,GUNLION,1308960791.0,https://www.reddit.com/r/datasets/comments/ixmht/does_anyone_have_access_to_a_dataset_of_clinical/,11.0,8.0,I'm a junior doctor - event a sample would be so useful,en
1107563,2011-07-23 21:35:23,statistics,Anyone hiring a fresh-out-of-college statistics major in Chicago?,ixt74,benjeye,1292457824.0,https://www.reddit.com/r/statistics/comments/ixt74/anyone_hiring_a_freshoutofcollege_statistics/,7.0,3.0,"If anyone has any tips for me on how to get a stats job in Chicago, it would be greatly appreciated.

Thanks!",en
1107564,2011-07-24 03:09:14,datasets,"Ask r/datasets: Does anyone have a dataset of two competing social networks? Specifically where people can join two different groups and either choose to join/leave one or two groups, interact with members in the group or join the opposing group?",iy0mt,zitterbewegung,1149091890.0,https://www.reddit.com/r/datasets/comments/iy0mt/ask_rdatasets_does_anyone_have_a_dataset_of_two/,0.0,0.0,,en
1107565,2011-07-24 05:56:26,artificial,The Dangers of Automated Transactions,iy41z,danhakimi,1285041707.0,https://www.reddit.com/r/artificial/comments/iy41z/the_dangers_of_automated_transactions/,6.0,2.0,,en
1107566,2011-07-24 07:01:06,statistics,Real Time World Stats Counters,iy5cb,Okanekure,1156288446.0,https://www.reddit.com/r/statistics/comments/iy5cb/real_time_world_stats_counters/,0.0,0.0,,en
1107567,2011-07-24 18:33:29,MachineLearning,Freshplum is looking for software engineers interested in Machine Learning.,iyelf,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/iyelf/freshplum_is_looking_for_software_engineers/,8.0,4.0,,en
1107568,2011-07-25 01:51:40,computervision,"CMU Robotics Instititue's vision finds a home at 
Google",iyo5x,jeanlucpikachu,1159634949.0,https://www.reddit.com/r/computervision/comments/iyo5x/cmu_robotics_instititues_vision_finds_a_home_at/,1.0,0.0,,en
1107569,2011-07-25 12:46:28,MachineLearning,How would one go around creating an AI player for magic the gathering?,iz1vq,haffi112,1236798119.0,https://www.reddit.com/r/MachineLearning/comments/iz1vq/how_would_one_go_around_creating_an_ai_player_for/,18.0,17.0,"I was wondering about this yesterday. Magic the Gathering is a very broad game and a new set of cards is released every year so the cards people are playing with are always changing. There are some big competitions and money to be earned in it if you're good.

Magic got a pretty solid [rules](http://www.wizards.com/Magic/TCG/Article.aspx?x=magic/rules) so implementing the rules in code shouldn't be that much of a hassle (taking Hofstadter's law into account...) but hammering all the cards in would take some time.

My question now is, what method would you use to train an AI player? I was thinking about using monte carlo but then it occurred to me that I can't let the deck of cards be in a predefined state because then I would be allowing the player to ""see"" the future when he's drawing the cards. Some cards however allow the player to change the order of e.g. the top three cards of his deck so I can't let the player choose a random card from it.

Training a neural network for each possible deck seems out of hand because of the immense range of cards out there.

Would it also be possible to let two AI players play against each other to find the best possible deck?

This is merely just a speculation for me at the moment, my assumptions might be naive. I've only taken one undergraduate course in Machine Learning but also a few in optimization so I was wondering if I could use this idea to expand my knowledge.",en
1107570,2011-07-25 16:55:25,statistics,Rappin' 'bout stats,iz682,sumdawg,1311540098.0,https://www.reddit.com/r/statistics/comments/iz682/rappin_bout_stats/,0.0,0.0,,en
1107571,2011-07-25 22:22:48,MachineLearning,"Running high-performance neural networks on a ""gamer"" GPU",izfw4,reidhoch,1209656850.0,https://www.reddit.com/r/MachineLearning/comments/izfw4/running_highperformance_neural_networks_on_a/,20.0,26.0,,en
1107572,2011-07-26 02:07:48,statistics,"So as a stats student, should I learn SQL? Why is it useful?",izmfg,cbrunos,,https://www.reddit.com/r/statistics/comments/izmfg/so_as_a_stats_student_should_i_learn_sql_why_is/,17.0,16.0,"Are there some online resources with exercises I could use to learn SQL? What software should I install for this (on ubuntu)? I heard of MariaDB, is that ok?

EDIT: I study econometrics by the way.",en
1107573,2011-07-26 05:36:19,MachineLearning,"Is anyone headed to Bellingham (or just northward) tomorrow (the 25th)?  My friend is flying into Seattle, but can't make his connecting flight to Vancouver due to a passport mixup.",izs55,[deleted],,https://www.reddit.com/r/MachineLearning/comments/izs55/is_anyone_headed_to_bellingham_or_just_northward/,1.0,0.0,"He's trying to get to the border to be picked up by family, but would be happy to be brought as far northward as possible.  His grandmother will be coming to pick him up, but can't make the trip all the way down to Seattle.  He's trying very hard to go visit his elderly grandmother, and would be very grateful for anyone's help.

If this is something you're willing to help with, thank you, and please private message me a contact number.",en
1107574,2011-07-26 11:12:23,statistics,Beyond reasonable doubt,izztn,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/izztn/beyond_reasonable_doubt/,7.0,2.0,,en
1107575,2011-07-26 14:56:05,statistics,linear algebra tutorials for statisticians ,j02xz,Wonnk13,1287022683.0,https://www.reddit.com/r/statistics/comments/j02xz/linear_algebra_tutorials_for_statisticians/,19.0,7.0,"hey everybody. I'm starting to take more advanced stats classes this fall (multi-level, times series, generalized linear models) and a data mining class. All of the syllabi i've seen don't require, but strongly recommend some familiarity with lin. algebra. 

I've started watching Strang's MIT OCW videos and browsed Khan academy. Are there any resources targeted specifically at statisticians?

sorry for such a broad question, any advice is appreciated!! ",en
1107576,2011-07-26 15:53:52,statistics,"What is the difference between this and ""regular"" statistics?  ",j0418,Dangger,1242487715.0,https://www.reddit.com/r/statistics/comments/j0418/what_is_the_difference_between_this_and_regular/,6.0,1.0,,en
1107577,2011-07-26 15:56:55,statistics,The Obama Campaign is hiring quantitative analysts,j0435,fstorino,1199712288.0,https://www.reddit.com/r/statistics/comments/j0435/the_obama_campaign_is_hiring_quantitative_analysts/,31.0,11.0,,en
1107578,2011-07-26 19:38:56,statistics,Creating R Packages: A Tutorial [PDF].,j0a9h,dearsomething,1210808677.0,https://www.reddit.com/r/statistics/comments/j0a9h/creating_r_packages_a_tutorial_pdf/,15.0,13.0,,en
1107579,2011-07-27 02:51:43,statistics,X-Post from r/visualization: A little help with R + ggplot2,j0n9p,WillieWampum,1249176349.0,https://www.reddit.com/r/statistics/comments/j0n9p/xpost_from_rvisualization_a_little_help_with_r/,2.0,0.0,,en
1107580,2011-07-27 03:26:16,statistics,I was wondering if r/Statistics can help me with some significance testing concepts?,j0o6x,Pyro636,1273769767.0,https://www.reddit.com/r/statistics/comments/j0o6x/i_was_wondering_if_rstatistics_can_help_me_with/,0.0,1.0,"So I have this problem for my homework, and while this might not be the point of this sub, I need help understanding some things.

In order to get this problem right, I need to have all answers to the following correct, and if one or more is incorrect, I cannot tell which ones are correct and which ones are wrong.

In order to save everyone from a formatting nightmare, here is a screenshot of the questions and my answers so far: http://i.imgur.com/ZutqM.jpg

I'm pretty confident about the first three.
The 4th I'm slightly unsure of. We don't *actually* know what mu is right? 

5 6 and 7 I think are right as well, but again these questions are mostly meant to be tricky so I could be wrong.

Number 8 (sorry they aren't actually numbered) I am unsure of for the same reasons as number 4.

9 I'm reasonably sure that since the mean given is of the sample it is a sample statistic only

And the last one I am unsure of. True, the standard deviation is not given, however technically it is not needed in order to perform a significance test correct? In which case which of the other three are not given?

I thank you for any and all insight you can give me, because the format of this question has me banging my head up against the wall.",en
1107581,2011-07-27 06:00:41,computervision,DirectShow USB Video Class Capture Library,j0scd,[deleted],,https://www.reddit.com/r/computervision/comments/j0scd/directshow_usb_video_class_capture_library/,4.0,1.0,,en
1107582,2011-07-27 10:32:53,MachineLearning,"What is your opinion on Probabilistic Boosting Trees? In particular, how do they compare with SVM?",j0yss,joelthelion,1146260183.0,https://www.reddit.com/r/MachineLearning/comments/j0yss/what_is_your_opinion_on_probabilistic_boosting/,19.0,5.0,,en
1107583,2011-07-27 15:19:20,artificial,"I'm annoyed by votekarma, should I write a votebot?",j13et,gettingmyselfanigga,1311769022.0,https://www.reddit.com/r/artificial/comments/j13et/im_annoyed_by_votekarma_should_i_write_a_votebot/,0.0,1.0,That will downvote comments with positive votes and upvote the negative ones. Then deploy 30-50 instances? Discuss.,en
1107584,2011-07-27 20:44:38,statistics,Does anyone have experience with Apophenia?,j1c5s,dirkdd,,https://www.reddit.com/r/statistics/comments/j1c5s/does_anyone_have_experience_with_apophenia/,1.0,1.0,"Curious if anyone has used [Apophenia](http://apophenia.sourceforge.net/)  for statistics.  I usually use **R** but was thinking about checking out Apophenia.  Experiences?  Good, bad?

Thanks.",en
1107585,2011-07-27 22:49:09,statistics,Bootstrapping Approaches for Inference,j1g7f,dearsomething,1210808677.0,https://www.reddit.com/r/statistics/comments/j1g7f/bootstrapping_approaches_for_inference/,11.0,0.0,,en
1107586,2011-07-28 03:02:11,MachineLearning,TechTalks.TV : ICML 2011,j1nsn,leonoel,1284436447.0,https://www.reddit.com/r/MachineLearning/comments/j1nsn/techtalkstv_icml_2011/,28.0,0.0,,en
1107587,2011-07-28 05:43:48,statistics,Comparing categorization methods,j1sca,thegabeman,1239690590.0,https://www.reddit.com/r/statistics/comments/j1sca/comparing_categorization_methods/,1.0,0.0,"I'm working on a project where I'm using two different methods to classify a large group of text documents, but I think my question holds more generally for comparing any two different approaches to classify any N observations into M categories. In method A, I use hierarchical clustering (of which there are many sub-varieties of approaches) and in method B I use a [latent dirichlet allocation](http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) approach. Both methods allow me to classify my N documents into any integer M categories, so there is some flexibility in how to apply these methods. My question is what to do once I have these categories to compare how different the methods are working. I would be interested in developing some sort of statistic that could help me summarize and understand the ""quality"" of the within-group closeness. One initial idea I had would be to use a distance metric and look at the distribution of distance metrics across groupings under the two methods. Then I'd be able to pick out quantities of interest like minimax within-group distances, lowest average within-group distances, etc.",en
1107588,2011-07-28 09:44:04,artificial,Artificial life 1 cpu per creature,j1yga,learc83,1175899960.0,https://www.reddit.com/r/artificial/comments/j1yga/artificial_life_1_cpu_per_creature/,3.0,5.0,"So I had an idea, and I wanted to run it by r/artificial to see if anyone has already done this.

I've always been fascinated with emergence. I'm a programmer and played building artificial lifeforms that interact using simple rules that create a complex system.

My idea is to create a network of microprocessors (ATmegas or maybe inexpensive ARMs) where each processor controls just 1 creature. They would connect to a display that handled graphics and facilitated the network. Each processor would have attached memory that stored the state of the creature, so you could remove the modules, swap them around with other networks etc...

I'm sure a system like this doesn't have many practical advantages, but I'm thinking about building it as a side project just for fun and would welcome any input (I'm also handy with a soldering iron, so I can handle the hardware side as well).",en
1107589,2011-07-28 09:51:36,MachineLearning,2 Economists reckon that our current exponential model discounts the future /far/ too much (thinking about Reinforcement Learning).,j1ylx,great-pumpkin,1212943450.0,https://www.reddit.com/r/MachineLearning/comments/j1ylx/2_economists_reckon_that_our_current_exponential/,25.0,6.0,,en
1107590,2011-07-28 15:37:07,MachineLearning,Open-source business/marketing data,j24cp,mathsuu,1197264006.0,https://www.reddit.com/r/MachineLearning/comments/j24cp/opensource_businessmarketing_data/,3.0,1.0,Does any one know of any open-source data useful to marketing/business analytics folks? I'm trying to juice up a project I'm working on that involves analyzing spend behavior.,en
1107591,2011-07-29 04:54:06,MachineLearning,"Gender Prediction with Python
        ",j2sg0,rhnet,1299174902.0,https://www.reddit.com/r/MachineLearning/comments/j2sg0/gender_prediction_with_python/,16.0,2.0,,en
1107592,2011-07-29 07:33:08,AskStatistics,Question about statistical significance of this test?,j2wpi,Memento1979,,https://www.reddit.com/r/AskStatistics/comments/j2wpi/question_about_statistical_significance_of_this/,4.0,2.0,,en
1107593,2011-07-29 18:10:40,MachineLearning,The Hitchhiker’s Guide to A Kaggle Competition,j39jc,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/j39jc/the_hitchhikers_guide_to_a_kaggle_competition/,15.0,0.0,,en
1107594,2011-07-29 21:33:43,statistics,Opinion on this book - Probability: a graduate course By Allan Gut,j3g85,anonemouse2010,1262468532.0,https://www.reddit.com/r/statistics/comments/j3g85/opinion_on_this_book_probability_a_graduate/,5.0,10.0,I was browsing and it seemed to be much more clear than \*my\* old book.  Does anyone have any experience with this book?  Is it worth buying?,en
1107595,2011-07-29 23:01:08,statistics,Question about generating random numbers from distributions. ,j3j42,[deleted],,https://www.reddit.com/r/statistics/comments/j3j42/question_about_generating_random_numbers_from/,8.0,13.0,"I currently have a numerical PDF generated by a Kernel Density Estimator, and I need to use that distribution to generate random numbers. I am using the inversion method that involves taking the inverse of the cumulative distribution function, and generating a uniformly distributed random number from 0 to 1 and evaluating the inverse function at that number. 

My question is how would I extend that to a 2 dimensional case? I have a numerical 2D cumulative distribution function that is unique, so it can be inverted without dealing with infinite derivatives, im just not sure how I would map the uniform distributed random number to a sample space vector.",en
1107596,2011-07-29 23:11:54,statistics,Stat vs. Biostat vs. Job,j3jfl,[deleted],,https://www.reddit.com/r/statistics/comments/j3jfl/stat_vs_biostat_vs_job/,12.0,17.0,"I'm entering my last year in undergrad and trying to decide what to do after. I'll graduate from a top 10 school with a BS in math and an MS in stat. I'm trying to figure out the optimal process for figuring out what I want to do next year (and for the rest of my life). I definitely want to do something statistical. I have some, but not much, exposure to various biostat stuff, though I have little interest in strictly biology. The main advantage to biostat seems to me that I'd be able to get into top grad school or get a good job. Any advice on how to figure out what I want?",en
1107597,2011-07-30 10:48:37,analytics,Android Analytics referral tracking,j3ztq,devarticles,1303668316.0,https://www.reddit.com/r/analytics/comments/j3ztq/android_analytics_referral_tracking/,1.0,0.0,,en
1107598,2011-07-30 12:27:18,MachineLearning,Ask reddit: what Machine Learning package(s) do you use under linux?,j4127,joelthelion,1146260183.0,https://www.reddit.com/r/MachineLearning/comments/j4127/ask_reddit_what_machine_learning_packages_do_you/,21.0,31.0,"I'm getting started with machine learning and more precisely, classification, and I'd like to know what tools more experienced people used. If possible, I'd prefer to work from python, but really, any tool is fine, as long as it is good :)",en
1107599,2011-07-30 16:51:06,statistics,"If the SD is only really appropriate for describing normally distributed data, what alternatives are there for skewed distributions?",j4452,jjrs,1170375384.0,https://www.reddit.com/r/statistics/comments/j4452/if_the_sd_is_only_really_appropriate_for/,1.0,15.0,"So I'm analyzing a test with a skewed distribution (say, mean = 80%). I want to give benchmarks for scores based on the SD, but due to the shape of the distribution, it doesn't work very well. say the SD is 15 points. This works fine for SD's of -1 and -2, but if you're 2 SD away from the norm, your score is 106%. The SD seems appropriate for low scores, but not for high where a ceiling effect kicks in.

Would it make sense to calculate an asymmetrical SD for this kind of situation?. You could mirror the left and right sides of the distribution along the mean to create 2 symmetrical normal distributions. You could use one for under-mean scores, then switch to the other for higher.

What do you think? Stupid idea?",en
1107600,2011-07-31 18:28:47,statistics,Modelling level-1 variance as a function of a level-2 variable (heteroskedastic models),j4v66,[deleted],,https://www.reddit.com/r/statistics/comments/j4v66/modelling_level1_variance_as_a_function_of_a/,1.0,0.0,"Hey everyone!
I would like to know if there are software which allow you to  model level-1 variance as a function of two continuous level-2 variable and their interaction.

From what I've seen, only MLwiN can do that. Neither Mplus nor Nlmixed could fit heteroskedastic models and I've looked into a couple of R packages and they don't seem to be able to model the interaction between two variables (like MCMCglmm) and lme4 couldn't do that I think.

Any thoughts?",en
1107601,2011-07-31 22:01:25,statistics,"Question about determining the frequency of ""unpredictable"" events.",j4zs4,wanderingjew,1263111279.0,https://www.reddit.com/r/statistics/comments/j4zs4/question_about_determining_the_frequency_of/,2.0,8.0,"Ok, I'm just going to use this as an example (it's not what I'm actually doing, but it gets the point across): I'd like to determine the frequency of unpredictable events, like drunk driving accidents.

These events are unpredictable. It's impossible to know if there's going to be a drunk driving accident *tonight*, but you can assign a probability to it. I'm trying to figure out how to determine the frequency of events given previous data on these accidents.

*Here's what I'm thinking*: I do a Fourier analysis on my dataset (it has a magnitude, like x number of people killed by a drunk driving accident) to get the frequency of these events. *This sounds so completely wrong*, and I probably don't know what I'm talking about. How is this done in the real world and how do I get a confidence interval?",en
1107602,2011-07-31 23:20:55,statistics,Can I use t tests in very large (&gt;1000) sample sizes even if they are not normally distributed?,j51o8,mutabilis,1284179253.0,https://www.reddit.com/r/statistics/comments/j51o8/can_i_use_t_tests_in_very_large_1000_sample_sizes/,12.0,19.0,"The title says it all, I've some sources which say ([here](http://www.statsoft.com/textbook/nonparametric-statistics/?button=2)) yes while others say no. 
Thank you for your answers/replies.

Edit: replaced who with which.",en
1107603,2011-08-01 17:46:08,artificial,"It's AI Week at the World Science Festival! Video clips, articles, and more all week long",j5pro,sciencecomic,1296694019.0,https://www.reddit.com/r/artificial/comments/j5pro/its_ai_week_at_the_world_science_festival_video/,1.0,0.0,,en
1107604,2011-08-01 19:29:04,statistics,A plea for accuracy (joke),j5sqr,B-Con,1174169387.0,https://www.reddit.com/r/statistics/comments/j5sqr/a_plea_for_accuracy_joke/,5.0,0.0,,en
1107605,2011-08-01 20:48:41,statistics,"Reddit, I need some help with analysis for statistical significant; more in the post.",j5v38,rogue417,1238384789.0,https://www.reddit.com/r/statistics/comments/j5v38/reddit_i_need_some_help_with_analysis_for/,8.0,3.0,"**EDIT**
I don't think I did a very good job describing the data I have to work with.


I have about 154 log entries which contain the host and the magnitude of a delay (in milliseconds) in each entry.  These log entries were generated over about 40 days.  So essentially I have data points that look something like this:

* 7/13/2011 14:55 - HOST_A - 266ms Delay
* 7/13/2011 04:25 - HOST_B - 587ms Delay
* 6/22/2011 19:17 - HOST_E - 100ms Delay


The total population of hosts has been broken down into two groups. Group 1 has 8 hosts which have a special type of caching enabled on them.  Group 2 has 23 hosts which do not have this caching enabled.  I'm trying to determine if the cache applied to group 1 is actually helping at all.  


I started by turning the log entries into a table which contains the total number of delays for each host (some hosts did not experience any delay during the 40 days of log gathering though they were included in this data and simply had 0 values) and the average magnitude of delay for each host.  I then calculated group averages as well as the standard deviation of each group though neither of these seem to be terribly useful data points.

**/EDIT**

Hello Reddit, I need some help analyzing a data set I've generated.  The data set consists of hosts (servers), the frequency of an event per host and the magnitude of each event.  


This data set is split into two groups, a control group and a test group.  I'd like to determine if there is any statistically significant difference between the two groups in terms of both the frequency of the events and the magnitude of the events but am at a loss as how to proceed (it's been a long time since I did any statistics).

Details about the groups:

* The control group consists of 23 hosts
* The test group consists of 8 hosts
* The data set was collected over 40 days


As previously stated it has been sometime since I've studied statistics or performed any analysis so please do let me know if I've neglected to provide any necessary information.  Any help this great community can provide would be very much appreciated!",en
1107606,2011-08-01 22:13:53,MachineLearning,Looking for a guest blogger for a high profile Big Data analytics company's website (cross-post from /r/bigdata),j5xvw,ohsnaaap,1270647929.0,https://www.reddit.com/r/MachineLearning/comments/j5xvw/looking_for_a_guest_blogger_for_a_high_profile/,2.0,0.0,"We are looking for someone to write a blog about anything 'big data' related, whether it's visualization, hadoop, mapreduce, data reduction techniques, machine learning, etc. 

We'd be happy to link back to your own blog/linkedin/twitter. 

Please PM me for details! Hoping to find someone by Friday.",en
1107607,2011-08-02 05:11:50,statistics,Test to determine the width of a peak in a distribution.,j6a2b,[deleted],,https://www.reddit.com/r/statistics/comments/j6a2b/test_to_determine_the_width_of_a_peak_in_a/,0.0,0.0,"What should I be looking for to find this?

I've been using [TSpectrum](http://root.cern.ch/root/html/TSpectrum.html) in ROOT to find peaks in some histograms I've collected (scroll down and you'll see the type of thing I'm working on).  From what I understand this command's functionality is based off of markov chains.  I am willing to find the peaks in a different way if necessary, this was just the fastest, and I linked the info since there are a bunch of plots that look similar to my work.

So, statistically what sort of criterea should I be looking at for what range constitutes ""in"" the peak?  Any help much appreciated, even just a point in the right direction.",en
1107608,2011-08-02 05:38:40,MachineLearning,Dear NIPS reviewers,j6avk,[deleted],,https://www.reddit.com/r/MachineLearning/comments/j6avk/dear_nips_reviewers/,1.0,0.0,Seriously? did you even read the paper? fuck.,en
1107609,2011-08-02 09:37:03,statistics,"Free Access (for now!) to ""Statistics In Biological and Medical Sciences"" - Elsevier.",j6h2y,dearsomething,1210808677.0,https://www.reddit.com/r/statistics/comments/j6h2y/free_access_for_now_to_statistics_in_biological/,0.0,0.0,,en
1107610,2011-08-02 11:35:43,computervision,What open source handwriting identification programs exist?,j6j6l,cavedave,1128052800.0,https://www.reddit.com/r/computervision/comments/j6j6l/what_open_source_handwriting_identification/,3.0,5.0,,en
1107611,2011-08-02 11:37:30,MachineLearning,What open source handwriting identification programs exist?,j6j7u,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/j6j7u/what_open_source_handwriting_identification/,15.0,10.0,,en
1107612,2011-08-02 20:14:23,artificial,IBM's David Ferucci on the challenges of building WATSON,j6usc,sciencecomic,1296694019.0,https://www.reddit.com/r/artificial/comments/j6usc/ibms_david_ferucci_on_the_challenges_of_building/,10.0,0.0,,en
1107613,2011-08-03 00:42:24,statistics,Can anyone give me the REAL reason SAS is the standard software for federal statistics?,j73he,[deleted],,https://www.reddit.com/r/statistics/comments/j73he/can_anyone_give_me_the_real_reason_sas_is_the/,26.0,62.0,"I recently took a job where our primary language is SAS. As a program, is so full of inconsitencies and unintuitive issues, I am ready to break my computer. I have complained to no end, but I always hear the same excuse: ""It's a *validated* piece of software."" I disagree! What arbitrary criteria make software ""validated""? You can't read or understand source code and the documentation never mentions the routines or methods for fitting models, unlike R. 

*The SAS license is crushingly expensive* but that doesn't necessarily mean it's any better or worse than R. In fact, the core development team for R has some of this generation's best statisticians collaborating on its development: Duncan Murdoch, Robert Gentleman, Frank Harrell Jr., etc. I can download the source code for R, compile the binaries myself, and read help files which tell me exactly how models were fit. For instance, all GLMs are fit using the Newton-Raphson algorithm. That sounds like validated software to me!

Lastly, *SAS hinders the understanding of statistics*. If you call proc glm and supply a binary adjustment variable, the glm procedure inherits an ANOVA procedure and prints the usual ANOVA output, sum of squares stuff. Why not a t-test? It completely masks the estimate of interest: a mean difference comparing the Yes group to Nos. SAS additionally gives pages and pages of worthless output. So when there's one thing you want, it's buried in a haystack. This wastes paper! Worse still, the eager statistician has more output in front of him or her to overinterpret. Sometimes, you just fit a linear model and report a coefficient and a confidence interval and you're done. I don't need ANOVA or likelihood information (if I did, I'd be willing to do extra work to extract them).

*SAS syntax and layout is terrible*! There is no equivalence equality distinction. You write code like:

    if time = 1 then time = 0;

The assignment operator is *reversed* for ODS output. Who ever had this terrible idea? I'd rate it with the system lag Windows does every time there's a minor security issue: worst ideas in programming, ever! Their bizarre data / proc this and that causes programs to become hideously bloated every time a simple sort or parse is called. SAS documentation doesn't have examples! Their syntax is so goddamn confusing and weird, this is inexcusible! Online help resources, except for UCLA, are all extremely deprecated! The SAS community is a private, unhelpful bunch. And SAS's GUI looks like Win 3.1, am I right?

*R is free as in beer and speech* and incorporation could greatly improve the regulatory research field. User questions are very well documented and it's easy to just Google solutions for simple problems.  It is by far MORE capable than SAS and I just don't believe the rumor that SAS is better with large datasets. I have worked with millions of rows of data with hundreds of variables and both are equally slow. As for ""validated"" software: Validation is a matter of documentation and independent reproduction of results using a blinded 3rd party agent without conflict of interest. Validation is not guaranteed by a single statistical software. I can generate the wrong results in either program! 

Anyone else working in the regulatory/governmental field and agree with me? Is change on the horizon? Or will SAS continue to prey off this monopoly?",en
1107614,2011-08-03 00:45:25,statistics,Stats analysis from SurveyMonkey,j73ko,Neurologica_Diabolic,1300622754.0,https://www.reddit.com/r/statistics/comments/j73ko/stats_analysis_from_surveymonkey/,2.0,4.0,"I am planning a questionnaire on Survey Monkey.  It includes some drop down option questions and some ""rate of a scale"" questions and some free text boxes.

Is there an easy way to do stats analysis on this data, without having a degree in statistics?

I have a vague knowledge of stats, to an Allied Health Professional level......",en
1107615,2011-08-03 02:34:13,statistics,What's your favorite free source for basic demographic data?,j76qj,[deleted],,https://www.reddit.com/r/statistics/comments/j76qj/whats_your_favorite_free_source_for_basic/,2.0,5.0,"I'm looking for a good free source for the following (searchable by city and possibly including surrounding areas):
Population
Median Income
Median Age (or preferably % of population in each age range)

Wikipedia and the Census website seem to have what I'm looking for, but curious if anybody has another source they like better?
Trying to do very preliminary market research on different cities in Minnesota for the fist time.

Thanks Reddit!",en
1107616,2011-08-03 04:54:17,artificial,X-post from /r/science - Researchers give robot ability to learn (w/ Video),j7apw,DocAwkward,1296534950.0,https://www.reddit.com/r/artificial/comments/j7apw/xpost_from_rscience_researchers_give_robot/,9.0,1.0,,en
1107617,2011-08-03 05:19:49,statistics,Furthering quest of knowledge in statistics,j7bfo,ukraineisnotweak,1255901832.0,https://www.reddit.com/r/statistics/comments/j7bfo/furthering_quest_of_knowledge_in_statistics/,1.0,7.0,"I've developed quite an interest in stats ever since grad school, and my current job. The extent of my knowledge of stats is a Masters degree in psychology, as well as my the expertise I've gained in my job. The thing is, I feel like I know just enough stats to get myself into trouble. I want to pursue more education in this field and would love to understand stats better.

The thing is, I was always ok at math, and was never good in the one computer science class I took in college. Got through advanced calc II and never went any further. Now I write syntax through SPSS for automation, report production, data mgmt, and analyses for research. My interest has peaked. For example, I took a free seminar on random slope/random intercept modeling and found it fascinating.

Question is, with a full time job, how can I get a better education in stats? What type of programs/classes should I look for?",en
1107618,2011-08-03 10:32:59,MachineLearning,I tried to explain hidden markov models after learning them recently. Feel free to criticize or comment!,j7jdt,mish4,1286697468.0,https://www.reddit.com/r/MachineLearning/comments/j7jdt/i_tried_to_explain_hidden_markov_models_after/,27.0,11.0,,en
1107619,2011-08-03 18:52:49,statistics,Microbiologist looking for some direction with principal component analysis.,j7tlx,dinasaur,1292054117.0,https://www.reddit.com/r/statistics/comments/j7tlx/microbiologist_looking_for_some_direction_with/,9.0,16.0,"Hey Guys,
I think this is the right place. I am a microbiology masters student looking to compare diversity spatially in a passive treatment system treating mining impacted water. I have read some papers that use PCoA to do this, but I do not understand what it really means (in simple terms), or more importantly how I can use it to interpret my data. Any help? Or can you point me towards some helpful resources? 
Thanks!",en
1107620,2011-08-03 20:17:18,statistics,Dimensionality Reduction for Binomial Variables,j7wiq,CosinQuaNon,1270804031.0,https://www.reddit.com/r/statistics/comments/j7wiq/dimensionality_reduction_for_binomial_variables/,3.0,4.0,"Does anyone know of a method of dimensionality reduction, such as PCA, that is specific for discrete binomial data? I know there are other forms of PCA for discrete data using NMF but I am unable to find any methods specific to data in the range of $\{0,1\}$. I am trying to do some cluster analysis on a data set of 50+ features but I think that the underlying assumptions for PCA (e.g. that the data is normal) are limiting the usefulness.",en
1107621,2011-08-04 02:21:56,MachineLearning,RTextTools: an R machine learning package for text classification,j88lw,tymekpavel,1190535699.0,https://www.reddit.com/r/MachineLearning/comments/j88lw/rtexttools_an_r_machine_learning_package_for_text/,14.0,0.0,,en
1107622,2011-08-04 08:12:10,MachineLearning,How Google makes online advertising more effective using R,j8ikg,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/j8ikg/how_google_makes_online_advertising_more/,34.0,0.0,,en
1107623,2011-08-04 18:17:04,artificial,Hod Lipson at the World Science Festival talking about self-learning and robot proprioception,j8v5y,sciencecomic,1296694019.0,https://www.reddit.com/r/artificial/comments/j8v5y/hod_lipson_at_the_world_science_festival_talking/,6.0,0.0,,en
1107624,2011-08-04 18:55:37,statistics,"Rusty data analysis skills, any help choosing which tests to run in SPSS or similar program?",j8whb,LetsDOone,1282787065.0,https://www.reddit.com/r/statistics/comments/j8whb/rusty_data_analysis_skills_any_help_choosing/,2.0,6.0,"I took a boatload of stats in college while getting my psych degree, but you guessed it, years of not using it rendered all those hours of studying nearly useless. Here I am, years later scratching my head about how to analyze this data. 

Through my work at a non-profit, we showed specific images to a group of about 25-30 individuals, they rated all the images on a 1-9 scale. I also have the order in which the images were shown to them, which I think could contain valuable insight I also have basic demographic data such as sex, location. Can someone please help me figure out which tests would best help identify outliers and significant trends in our data? Thanks :)",en
1107625,2011-08-04 20:40:34,analytics,Help! User deleted google analytics profile,j903o,ubrkifix,1301519506.0,https://www.reddit.com/r/analytics/comments/j903o/help_user_deleted_google_analytics_profile/,1.0,0.0,"Is there anything I can do? The answers I'm seeing on google is no... c
Does any one know anything otherwise",en
1107626,2011-08-04 21:21:22,statistics,How do you measure smart phone market success?,j91k9,[deleted],,https://www.reddit.com/r/statistics/comments/j91k9/how_do_you_measure_smart_phone_market_success/,1.0,0.0,,en
1107627,2011-08-05 04:24:19,statistics,"So r/statistics, what are the odds of me landing on r/statistics twice in three clicks of the Random subreddit link ('cause that ironicness just happened)?",j9epx,w24x192,1273378063.0,https://www.reddit.com/r/statistics/comments/j9epx/so_rstatistics_what_are_the_odds_of_me_landing_on/,0.0,0.0,"In between, I landed on r/mycrazyfamily.  There wasn't much there...",en
1107628,2011-08-05 16:19:07,statistics,Best laptop computer chip for statistical analysis?,j9ttq,RA_Fisher,1299707119.0,https://www.reddit.com/r/statistics/comments/j9ttq/best_laptop_computer_chip_for_statistical_analysis/,1.0,0.0,"Does anyone have advice on computer chips for speedy statistical analysis on a laptop?

Is Intel i5? i7?

Will a faster dual core i5 do better than a slower quad core i7, etc?

Edit: I use both R and SAS primarily.",en
1107629,2011-08-05 16:38:47,MachineLearning,FREE Online Introduction to Artificial Intelligence class - Fall 2011 - taught by real Stanford Professors,j9uau,dylanger,,https://www.reddit.com/r/MachineLearning/comments/j9uau/free_online_introduction_to_artificial/,2.0,0.0,,en
1107630,2011-08-05 17:32:38,artificial,Any redditors heading to ECAL (European Conference on Artificial Life) this year?,j9vtf,jmborg,1228697088.0,https://www.reddit.com/r/artificial/comments/j9vtf/any_redditors_heading_to_ecal_european_conference/,4.0,2.0,,en
1107631,2011-08-05 20:30:18,statistics,Outlier Detection with DPM (Slides from JSM 2011),ja1io,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/ja1io/outlier_detection_with_dpm_slides_from_jsm_2011/,7.0,0.0,,en
1107632,2011-08-05 21:15:38,statistics,Survival models for counting processes,ja2zm,[deleted],,https://www.reddit.com/r/statistics/comments/ja2zm/survival_models_for_counting_processes/,7.0,8.0,"I was asked to analyze some data for a program effectiveness evaluation. The idea was that a hospital could have longer hours and more staff which would accelerate the triage time for people, getting them in and out of the hospital faster, more satisfied.

We have historical data from two consecutive months in the year previous to the intervention. Then in next year, the first month remained with standard hours, and the second month, extra staff were brought on and the hours were increased. We have individual level data on the people who were admitted to the hospital, what their reported and diagnosed sickness was, and when they entered and left the hospital.

The sponsor has suggested using Cox models to analyze this data. There's very little censoring. They want to separate analyses by accuity (a type of severity of disease), and they also want to ""adjust for patient volume"".

I'm confused by this idea of adjusting for patient volume. Clearly, individuals will wait longer if there are more people in the hospital so it's tempting to adjust and find the individual level effect. However, if the hypothesis holds, then patient volume should be a function of the experimental intervention, so adjusting for it would invalidate the effect. Has anyone worked on anything similar or has thoughts about the matter?",en
1107633,2011-08-05 23:27:05,computervision,Eye Tracking Study: where do users look first on a website?,ja74k,hersheyzombie,1308945658.0,https://www.reddit.com/r/computervision/comments/ja74k/eye_tracking_study_where_do_users_look_first_on_a/,4.0,5.0,,en
1107634,2011-08-05 23:45:03,statistics,How many copies of shredded text are needed for getting 99.9% reconstruction of mutated text? (NASA scientist interested in this),ja7ok,inquilinekea,1274654696.0,https://www.reddit.com/r/statistics/comments/ja7ok/how_many_copies_of_shredded_text_are_needed_for/,1.0,0.0,"This is a question posed by NASA astrobiologist Chris McKay (who wants to put life together back from its pieces), who really wants an answer to this question since it could lead us to reconstruct ancient genomes that are billions of years old (but mutated through years of internal radioactivity)

- A text of length N composed of an alphabet of m distinct units (eg Hamlet has 29551 words and 60 distinct units - 52 letters, space, plus 7 punctuation marks)

- Shred the text in pieces of average length X

- How many copies C of the shredded text are needed to give 99.9% reconstruction?",en
1107635,2011-08-06 12:09:09,statistics,Anyone knows this book? Good for a novice?,jaodq,[deleted],,https://www.reddit.com/r/statistics/comments/jaodq/anyone_knows_this_book_good_for_a_novice/,1.0,0.0,,en
1107636,2011-08-07 00:05:48,statistics,Book on getting the most out of large non-normal multidimensional data sets,jb1sj,SpaceWizard,1294385608.0,https://www.reddit.com/r/statistics/comments/jb1sj/book_on_getting_the_most_out_of_large_nonnormal/,11.0,2.0,"I work with a lot of neuroimaging data, and was hoping that I could find a book (not necessarily a neuroscience/fmri one) that covers the stats useful for non-normal multidimensional time series data (~600 time points per location) with many (10s of thousands) of locations per brain/subject. We also have structural measurement data which doesn't have the time element that could be included. So, at each location, there are different kinds of data, e.g. thickness of brain, overall amplitude, and power in many frequencies. The goal is to explore the relationships between many of these variables/dimensions with data reduction, clustering, resampling, multi-dimensional correlation, subject/group differences, and any other statistical tools available. Is there a good book out there that covers the most useful tools for this kind of data? Is there perhaps an awesome biostats, finance, geography, climate science, genetics or computer vision book that clearly explains the stats best for getting the most out of these large multidimensional data sets?",en
1107637,2011-08-07 09:47:09,datasets,"SimpleGeo releases 21 million geographical ""points of interest""  (CC0 license)",jbebq,dgryski,1242396284.0,https://www.reddit.com/r/datasets/comments/jbebq/simplegeo_releases_21_million_geographical_points/,9.0,2.0,,en
1107638,2011-08-07 19:47:26,MachineLearning,Looking for an algorithm that projects data *onto* a high dimensional plane,jbmhy,[deleted],,https://www.reddit.com/r/MachineLearning/comments/jbmhy/looking_for_an_algorithm_that_projects_data_onto/,1.0,0.0,"I suspect this is already out there, but I don't know the term.  Following an SVM like approach, I'd like to project an n dimensional dataset into a higher dimensional (m) space such that all the data fall onto a hyperplane.   

Similar to SVM, the projection to the hyperplane could be approximate and robust (say as measured via cross validation).

The application is to gene expression data.   If I get many many measurements for a single cell type (say yeast), then all of these gene expression values should be constrained by the cell's physiology (+ noise).   One way to express this cellular constraint is by a projection onto a hyperplane, or better yet a nonlinear variant.

Does anyone know of an algorithm that already does this? ",en
1107639,2011-08-08 02:15:43,statistics,Russ Roberts of Econ Talk was Driving Me Nuts,jbvkf,SoPoOneO,1148260694.0,https://www.reddit.com/r/statistics/comments/jbvkf/russ_roberts_of_econ_talk_was_driving_me_nuts/,7.0,7.0,,en
1107640,2011-08-08 02:47:55,MachineLearning,Andrew Ng is looking for non programmers for an experimental course.,jbwba,leonoel,1284436447.0,https://www.reddit.com/r/MachineLearning/comments/jbwba/andrew_ng_is_looking_for_non_programmers_for_an/,25.0,5.0,,en
1107641,2011-08-08 04:28:32,datasets,US Census 2010 Demographic Profiles,jbyti,unquietwiki,1263108455.0,https://www.reddit.com/r/datasets/comments/jbyti/us_census_2010_demographic_profiles/,10.0,1.0,,en
1107642,2011-08-08 07:32:05,statistics,AskStatistics: What tests are there for ratios?,jc3nj,chewxy,1215510344.0,https://www.reddit.com/r/statistics/comments/jc3nj/askstatistics_what_tests_are_there_for_ratios/,2.0,1.0,"I would like to blame this on the massive amounts of coffee I have imbibed, but it's really because I didn't really pay attention in uni.

So I have a bunch of ratios - I recall a test of independence that can be used on ratios the way pearson's chi square can be used on proper counts. I can't for my life remember what the test is though. 

Any help?",en
1107643,2011-08-08 07:45:59,artificial,"Hello /r/artificial, my attempt at explaining the basic structure of a human mind, what do you think?",jc40g,jpvoid,,https://www.reddit.com/r/artificial/comments/jc40g/hello_rartificial_my_attempt_at_explaining_the/,0.0,2.0,,en
1107644,2011-08-08 09:08:25,MachineLearning,Introduction to Artificial Intelligence - Fall 2011,jc5x9,leonoel,1284436447.0,https://www.reddit.com/r/MachineLearning/comments/jc5x9/introduction_to_artificial_intelligence_fall_2011/,23.0,8.0,An online course by Thrun and Norvig. You'll even will be scored. No certificate though.,en
1107645,2011-08-08 13:44:30,datascience,How Social Network Analysis Solves Real World Problems,jcaj9,mhermans,1169219262.0,https://www.reddit.com/r/datascience/comments/jcaj9/how_social_network_analysis_solves_real_world/,3.0,0.0,,en
1107646,2011-08-08 19:04:25,statistics,Help finding significance.,jci14,EndlessDaze,1266038831.0,https://www.reddit.com/r/statistics/comments/jci14/help_finding_significance/,3.0,6.0,"I'm a research scientist who's statistical skills have always been weak, but could use a little help getting pointed in the right direction.  I have results from an experiment - I treated two groups of mice identically except for certain hormone treatments.  My results are strikingly different: For one group my readouts are just above the baseline of detection, and for the second they are at minimum 5x higher, and at maximum 15x higher.  Obviously the error for my first group is very tight, and for the second is relatively large because of the variability on the high end.  Intuitively speaking there is no question that these samples are different, but I can't reach significance because of the variability on the top end.  

What's the proper way to analyze these data in a situation like this?  Should I do some sort of transformation?  These data are for publication, so I want to be rigorously honest, but I'm not sure how to interpret them statistically.

Any help would be greatly appreciated.

Edit: Addendum: I have another restriction in that this experiment is very costly to do and my n is limited to a paltry 4. Its frustrating, but I can't do much about it.",en
1107647,2011-08-08 20:27:43,statistics,Statistics ≠ Checking Your Brain at the Door,jckyv,FieldofScience,1237390073.0,https://www.reddit.com/r/statistics/comments/jckyv/statistics_checking_your_brain_at_the_door/,0.0,0.0,,en
1107648,2011-08-09 00:32:00,statistics,Recommended reads for statistics masters student?,jct9v,denacioust,1294348977.0,https://www.reddit.com/r/statistics/comments/jct9v/recommended_reads_for_statistics_masters_student/,2.0,3.0,"I'm about to start my masters in statistics this coming Autumn and want some books or articles to read which might help me in this.

I'm coming from a background in economics and finance with an intermediate knowledge of statistics.

Just wonder if anyone has any relatively light reading for me or any other tips before I start?",en
1107649,2011-08-09 06:14:35,statistics,"I have been drooling over this for the past hour. Statistics using real world pics, yes please.",jd354,statguy,1271026910.0,https://www.reddit.com/r/statistics/comments/jd354/i_have_been_drooling_over_this_for_the_past_hour/,28.0,3.0,,en
1107650,2011-08-09 07:14:07,statistics,"Could someone explain to me the mathematics behind heavy-tail distributions, like the Cauchy and others?",jd51j,jollyjiyant,1305524084.0,https://www.reddit.com/r/statistics/comments/jd51j/could_someone_explain_to_me_the_mathematics/,0.0,2.0,I just read black swan and i want to see the difference between these and a normal distribution.,en
1107651,2011-08-09 15:24:42,datasets,UK riots: every verified incident. Download the full list,jdev3,cavedave,1128052800.0,https://www.reddit.com/r/datasets/comments/jdev3/uk_riots_every_verified_incident_download_the/,10.0,0.0,,en
1107652,2011-08-09 16:12:12,statistics,Dissertation Statistics: Hire a PhD Statistician,jdfz6,eduguru167,1307043175.0,https://www.reddit.com/r/statistics/comments/jdfz6/dissertation_statistics_hire_a_phd_statistician/,2.0,0.0,,en
1107653,2011-08-09 18:23:57,computervision,Question: Best choice for implementing convolutions on the gpu?,jdjsg,parthenogenesis,1295392302.0,https://www.reddit.com/r/computervision/comments/jdjsg/question_best_choice_for_implementing/,3.0,2.0,"Hi there reddit!

I'm currently writing an automated inspection system that uses scale-space representation for ridge and edge detection. 

However my software implementation is too slow and i want to speed it up somehow. Every operation i do can be interpreted as a convolution on 3Megapixel images: A Gaussian blur followed by various differential operators.
And no, there's no order-of-magnitude improvement to be had with my implementation.

Would HLSL be able to manage that? What would be the best choice for such a system? We're going to need assurances that whatever option we go with will be supported in 10 years. We're a microsoft shop, btw.

Additionally, on an Intel Core 2 Duo or similair, what is the speed difference between byte math on an array compared to double math on a similar array?",en
1107654,2011-08-09 19:47:55,statistics,More help with survival curves using ecdf function,jdmoh,eberndl,1200962642.0,https://www.reddit.com/r/statistics/comments/jdmoh/more_help_with_survival_curves_using_ecdf_function/,2.0,8.0,"Hi,

[This](http://www.reddit.com/r/statistics/comments/ir00n/help_with_plotting_empirical_survival_function_in/) thread has already been HUGELY helpful to me, and has allowed me to change from a standard ecdf to a survival curve.

But I don't know how to put in the lines. Using a similar example to the previous thread:

    x = rexp(10,1)
    f = ecdf(x)
    plot (-1,1,xlim=c(0,5), ylim=c(0,1))
    lines(f, verticals=TRUE)

This gives me a step like function with solid steps.

I can then do
    plot(x,1-f(x))

Which gives me the points that I want.  Can anyone help me fill in the lines? I'd like to use the lines function because I have to over lay multiple sets of data.

Thanks!


",en
1107655,2011-08-09 19:56:09,analytics,How can I find high ranking non-US websites?,jdmyx,jeremiahwarren,1299431719.0,https://www.reddit.com/r/analytics/comments/jdmyx/how_can_i_find_high_ranking_nonus_websites/,0.0,2.0,"I've noticed that there are a lot of high ranking Russian, Spanish, etc sites that have a lot of followers. How can I find these sites, other than the few that I already know about? With Technorati I can search by popular, but they don't include non-English sites. ",en
1107656,2011-08-09 20:21:40,statistics,"Help with one-tailed t-test, mean hypothesis",jdntt,peanutman,1230831813.0,https://www.reddit.com/r/statistics/comments/jdntt/help_with_onetailed_ttest_mean_hypothesis/,0.0,3.0,"I'm having trouble with testing a mean hypothesis. 

I have 2 independent samples (group E and group F), so I'm doing a Independent Samples T-Test. I want to prove that F's mean is significantly (95%) larger than E's mean.

So, I did the test in SPSS, and this is the output:
[http://i.imgur.com/q07xQ.png](http://i.imgur.com/q07xQ.png)

The trouble I'm having, is that this is a one-tailed test instead of the 2-tailed tests that I'm used to. Since SPSS only does two-tailed tests I'm a bit lost.

I know I have two options for using p for a one-tailed test:

- p = 0.027 / 2
- p = 1 - (0.027 / 2)

I'm unsure as to which one to use, and why. I know it has something to do with the side of the tail, and the value of t, but at the moment I'm clueless.

Is there anyone willing to point me in the right direction? 

Edit: 

This is were I got the impression that I can't just halve p for all cases:
[https://laulima.hawaii.edu/access/content/user/hallston/341website/17c_SPSS.pdf](https://laulima.hawaii.edu/access/content/user/hallston/341website/17c_SPSS.pdf)

My old statistics course also states (translated) ""use p / 2, but be sure to check if t is on the correct side of the middle of the confidence interval"".

So I feel that I'm missing some sort of condition here, but I have no clue.


",en
1107657,2011-08-10 04:29:54,MachineLearning,Maybe you'll find this cool too,je41c,the_cat_kittles,1287171936.0,https://www.reddit.com/r/MachineLearning/comments/je41c/maybe_youll_find_this_cool_too/,11.0,1.0,kaggle has a wikipedia challenge [here](http://www.kaggle.com/c/wikichallenge) to help predict participation. I bet wikipedia helped alot of the participants improve their models. Wikipedia is making itself smarter!,en
1107658,2011-08-10 04:31:37,statistics,"Simple explanation for a simple person, please: Why transform data sets from non-normal to normal?",je430,halasjackson,1289490537.0,https://www.reddit.com/r/statistics/comments/je430/simple_explanation_for_a_simple_person_please_why/,2.0,5.0,"Particularly, what I'm trying to understand is, I think, Stats 101-level stuff. If I am measuring something and find that my data distribution of 50 points is not normal by an Anderson Darling test, I can not use z-scores and the like to make accurate inferences about things in that data set, right?

Secondly, if I run a BoxCox transformation and I get a new, normal dataset with a desirable lambda value, well, what things can I do with the new data set that ""conveys"" or infers back to the original data set?

For example, if all I have to do is square my data points to make a normal data set out of a non-normal set, and then look for ""how much of my new distribution is less than 60"" by using a z-score and get a proportion of 68%... well that doesn't mean that 68% of my *original, non-normal* data set is less than 60. Does that mean that *about* 68% of my original data set is less than the **square root** of 60 (i.e., 7.75) ???

If you know the answer, please use small words and speak slowly :) Thank you in advance!",en
1107659,2011-08-10 12:34:40,computervision,Google Group Members to Use Facial Recognition to Identify London Rioters,jefp3,cavedave,1128052800.0,https://www.reddit.com/r/computervision/comments/jefp3/google_group_members_to_use_facial_recognition_to/,7.0,0.0,,en
1107660,2011-08-10 13:45:22,statistics,Stats heads!! Have to design and index and could use some help..,jegtb,dav1b,1275393727.0,https://www.reddit.com/r/statistics/comments/jegtb/stats_heads_have_to_design_and_index_and_could/,2.0,7.0,"Hi all... I've been charged with designing this index to measure the wellbeing of a country. We want to take into consideration all kinds of dimensions, from economic and political, to health, education, ecological etc. 

My original idea was to simply collect data across a bunch of countries and regress it on GDP per capita at PPP. Then using the coef estimates, my index would simply be b1*x1 + b2*x2 +... etc. where the b's are the coef estimates and the x's are the data for that independent variable for some particular country. 

Then I was reading something about principle component analysis and I'm not sure now what to use. Does anybody have any ideas to help me out? Much appreciated.",en
1107661,2011-08-10 17:49:50,MachineLearning,"Ask /r/MachineLearning: Predicting with Markov Chains, and text classification?",jembt,someone13,1281015435.0,https://www.reddit.com/r/MachineLearning/comments/jembt/ask_rmachinelearning_predicting_with_markov/,0.0,0.0,"Hello all, two questions for you!

I remember reading something about this earlier this year, but I can't seem to find where I read it, so I figured that I'd ask here.  It was about predicting the randomness of some amount of text using Markov chains.  You'd enter a word, and the program would say whether it thought it was random or not (possibly implemented in Python).  Does anyone remember reading about this?  Or, if not, any suggestions on how to implement?

My second question involved classifying text.  Say I have some text strings that are of certain classes (A and B):

A) SomeRandomTextString  
A) SomeRandomLongerString  
A) This.One.Has.Periods  
A) And_This_One_Has_Underscores

B) sh0rt  
B) bb  
B) 12*b$q)x  
B) arijwijqwoqqkf

Now, given an input string, I would like to see which class of strings it's closer to.  A simple approach would be to calculate the Levenshtein distance for the input compared to each string, but that's rather inefficient.  Does anyone know of an alternative method?

Thanks!",en
1107662,2011-08-10 18:29:09,statistics,I need some help creating a continuous distribution...,jenkm,OhDannyBoy,1275060169.0,https://www.reddit.com/r/statistics/comments/jenkm/i_need_some_help_creating_a_continuous/,1.0,5.0,"... and I'm not sure how to even go about it.  It should have the following properties:

* Non-negative, positively skewed
* Mean of around 8
* Standard deviation of around 6
* The mean of the lower 50% should be around 2
* The mean of the upper 50% should be around 14
* It can be truncated at the high end, but doesn't have to be.  There should be only a very small amount of probability exceeding 25.  

Any ideas? 
",en
1107663,2011-08-10 19:18:00,datasets,Question: How do I find out how many Architects there are in Japan?,jepaf,upload333,1292863385.0,https://www.reddit.com/r/datasets/comments/jepaf/question_how_do_i_find_out_how_many_architects/,6.0,4.0,"I have a list of 7 different professions(Contractors,Engineers, Distributors, etc) of which I need to find out how many there are in China, South Korea and Japan. I've used ReferenceUSA, International Marketing Data and Statistics, CIA.Gov, International Labor Organization's website and I just can't find this information. I've even just tried googleing but that comes up with non-relevant articles about architects in Japan, China or Korea. I've also talked with reference librarians at 3 universities and 2 of my local libraries, all of whom have been looking for the passed 4 days and haven't found much either.

How or where can I go to find this information?",en
1107664,2011-08-11 04:11:17,statistics,Can anyone help out with an intro to statistics?,jf78l,Abyssal6,1313024866.0,https://www.reddit.com/r/statistics/comments/jf78l/can_anyone_help_out_with_an_intro_to_statistics/,1.0,0.0,"Im preparing for a statistics course in the Fall semester at my college, and need all the help I can get. I find math to be very difficult due to my learning style. Im not looking for anything real heavy, just a light intro with some practice problems relating to unions, intersections, ect.

Can anyone point me in the right direction?",en
1107665,2011-08-11 04:54:20,statistics,Why epidemiology refuses to reduce a ~80% false positive rate,jf8kt,gwern,1161738385.0,https://www.reddit.com/r/statistics/comments/jf8kt/why_epidemiology_refuses_to_reduce_a_80_false/,20.0,6.0,,en
1107666,2011-08-11 21:14:36,MachineLearning,Performance gains when using Cloud Computing,jfwyo,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/jfwyo/performance_gains_when_using_cloud_computing/,0.0,0.0,,en
1107667,2011-08-11 21:58:01,statistics,Modelling Data - Simple places to start?,jfygs,1ddqd,1284514551.0,https://www.reddit.com/r/statistics/comments/jfygs/modelling_data_simple_places_to_start/,4.0,23.0,"I've been running ad hoc informational queries for a couple years now and I want to start creating models of data.

I don't really know where to start, other than knowing that Excel can do some basic modelling.  I also have a problem in just plain starting: I don't know the first thing about modelling (in terms of academic knowledge).  I took Stats 101 in college but we didn't cover this.

My role right now is analyzing the results of Marketing pieces mailed.  We need to target more specifically and I understand modelling could help.

Where should I start?",en
1107668,2011-08-11 22:55:17,computervision,How do I go from still photos to a 3d scene?,jg0jm,inquisitive_idgit,1312673232.0,https://www.reddit.com/r/computervision/comments/jg0jm/how_do_i_go_from_still_photos_to_a_3d_scene/,5.0,8.0,"I've got like a dozen pictures of a mural from my childhood school.   It covered two walls, some of a ceiling.   Basically when we moved, we took lots of pictures of the whole thing.   

I have the pictures, and they kinda recreate the experience, but i'd prefer to render a movie of a camera moving around a 3d scene, or as two 'flat' wallpapers-- one for each wall.   

Surely this is possible, but I can't imagine how to do it. ",en
1107669,2011-08-12 02:54:59,statistics,Is it possible to solve a problem like this?,jg8cn,enkrypt0r,1265929749.0,https://www.reddit.com/r/statistics/comments/jg8cn/is_it_possible_to_solve_a_problem_like_this/,0.0,6.0,"Hey guys.  I have the option to enter a raffle, and I would like to know if it is possible to make a statistically wise decision **without** knowing how many people are entering the raffle.  I've taken one stats class before, and I know that this is definitely a critical piece of information, but a lot of people around here know much more than I do.  Hopefully you can help sort out some kind of risk/reward tradeoff.  This is for my college, and if I win I get my room and board paid for, which would be incredible.  Here's the info:

* The prize is $3,000 USD

* I can get 1 ticket for $10 or 6 tickets for $50

* The maximum number of tickets I can buy is 6.

Now, I suppose that I could make an estimate regarding how many tickets are sold, but odds are it would be off by thousands and probably not even worth estimating.  Is it possible to draw any conclusions without knowing how many tickets are entered, or am I going into this completely blind?  

Thanks guys, sorry if this is a dumb question.",en
1107670,2011-08-12 07:42:56,statistics,"Crap!  I've forgotten something from one of the courses I took last year, I can't find it in Casella-Berger, I don't know where that class's notes are, and wikipedia has come up short.  Help me remember?",jggig,Crotchfirefly,1289629047.0,https://www.reddit.com/r/statistics/comments/jggig/crap_ive_forgotten_something_from_one_of_the/,5.0,4.0,"Hey fellers!

I remember only vaguely what it means to say that a quantity relating to a linear model is *estimable*; I kinda know how it relates to a two-way anova model... but I've forgotten the specifics, such as how one goes about showing that a quantity is estimable/what theorems tend to be relevant in establishing estimability/what's a precise definition of the concept.

Can someone point me in the direction of a good resource to refresh my memory?  Many thanks!",en
1107671,2011-08-12 08:00:02,AskStatistics,What do I need to learn in order to be able to follow this analysis?,jggxh,rpglover64,1282753801.0,https://www.reddit.com/r/AskStatistics/comments/jggxh/what_do_i_need_to_learn_in_order_to_be_able_to/,5.0,2.0,"A friend of mine linked to [this](https://epianalysis.wordpress.com/2011/07/14/sdhcalculations/) blog entry (which happens to cite several papers), and I tried to follow the math but couldn't.

I took AP Stat and took college level probability and statistics courses (although I didn't do very well in them).

What do I need to understand to follow the analysis the post (and the papers) makes, and where might I learn it?

Thanks.",en
1107672,2011-08-12 15:54:57,statistics,Can anyone recommend a decent (and free) basic statistics reference book?,jgpuq,rhennigan,1247435768.0,https://www.reddit.com/r/statistics/comments/jgpuq/can_anyone_recommend_a_decent_and_free_basic/,1.0,0.0,"So this fall, I'll be a ""supplemental instructor"" for a research methods and statistics course, which I guess is sort of like being a TA as an undergraduate.  The problem is, with statistics, I generally rely heavily on the internet for quick reference to formulas, etc.  I'm wondering if anyone has any good suggestions for a text that is:

 * Free (I'm super poor)
 * In a decent ebook format (preferably epub or mobi)
 * Aimed more towards being a quick reference sources as opposed to a text book.

I'm hoping to have something handy on my Kindle that I can use as a quick source for formulas, definitions, etc.  I don't really want to waste any student's time by having to look for this stuff since I don't often memorize much.  Thanks very much fellow stat-nerds!",en
1107673,2011-08-12 19:14:00,statistics,Question about using the binomial distribution for pass-fail testing if the probability of success is not constant,jgvc5,darylb,1273089488.0,https://www.reddit.com/r/statistics/comments/jgvc5/question_about_using_the_binomial_distribution/,4.0,4.0,"Smart statisticians of reddit, is it possible to use the binomial test to determine a confidence level associated with the number of successes (*m*) in a series of pass-fail trials (*n*) with a given probability of success (*PD*) if the probability of success is not constant? Also, what if there is an uncertainty associated with the determination of the number of success (*m*)?

Here is an example to try and articulate my question. Say you have device on an assembly line that counts the number of cookies that pass. The machine is stated to have a probability of detecting the cookies of 90% if they are 1mm tall. If they are taller it is higher, if they are shorter, it is lower. So to verify the performance machine, you take a sample of 20 cookies and see that the machine found 17 of them. This seems like less than 90% however now you manually measure the height of each cookie to ensure that they were all taller than 1mm. It appears they are but this measurement has an error of +/- 0.25mm. How do you account for the uncertainty in the manual measurement of cookie? There is a chance that a cookie could be less than the 1mm threshold.

",en
1107674,2011-08-12 19:34:37,statistics,web-based data collection software? (x-post from askreddit),jgw1r,cixeltree,1301522643.0,https://www.reddit.com/r/statistics/comments/jgw1r/webbased_data_collection_software_xpost_from/,1.0,0.0,,en
1107675,2011-08-12 23:05:08,MachineLearning,Infer.NET - a Framework for Running Bayesian Inference in Graphical Models,jh34e,swiz0r,1235971035.0,https://www.reddit.com/r/MachineLearning/comments/jh34e/infernet_a_framework_for_running_bayesian/,23.0,6.0,,en
1107676,2011-08-13 01:30:08,statistics,What's the best way to go about calculating the parameters (mu and sigma) of a truncated normal from data?,jh7dp,Crotchfirefly,1289629047.0,https://www.reddit.com/r/statistics/comments/jh7dp/whats_the_best_way_to_go_about_calculating_the/,5.0,10.0,"This isn't a homework problem, by the way.

So I've got a bunch of data, and the prevailing assumption is that they come from a univariate truncated normal distribution (I'm skeptical of that, so I'm trying to test this).  Obviously, if it were just a regular normal, then calculating the MLE would be stupid-easy.  Taking the derivative of a log-likelihood of an indicator function is something that confuses me, though so... yeah.

I DO have the cutoff point already, so don't worry about that.  The question is, if you had to estimate (preferably without bias; I have a large number of data points and it need not be THAT efficient) the parameters of a truncated normal, how would you go about doing that?  ",en
1107677,2011-08-13 17:18:23,computervision,"Ensemble of Exemplar-SVMs for Object Detection and 
Beyond",jhovn,jeanlucpikachu,1159634949.0,https://www.reddit.com/r/computervision/comments/jhovn/ensemble_of_exemplarsvms_for_object_detection_and/,1.0,0.0,,en
1107678,2011-08-13 20:56:02,statistics,What's your method to calculate quartile?,jhthn,[deleted],,https://www.reddit.com/r/statistics/comments/jhthn/whats_your_method_to_calculate_quartile/,1.0,0.0,"Hi all,

I'm confused. I understand that different way of calculating the quartiles can lead to different results. I'm reading a basic introduction to statistic where the author use the ""ideal fourths"" to calculate the values. However, when i use R, it gives me different results, and i don't really understand how R compute them. What should i do? Should i use R or another methods? What's yours? I'm sorry if this sound as a noobie question, because i'm one!",en
1107679,2011-08-14 02:13:15,statistics,Coin Flip Question with Uncertainty,ji1as,[deleted],,https://www.reddit.com/r/statistics/comments/ji1as/coin_flip_question_with_uncertainty/,8.0,4.0,"I was thinking about coin flip problems with weighted coins, and I realized I didn't have an answer to a simple question:

Say you have a weighted coin, that lands on heads h% of the time, tails t% of the time, but you don't know the weights in advance. If you flip a number of times, say 1000, you gain some information. Obviously, heads/flips gives you an h estimate (and t).

But how accurate is that estimate? Without knowing if there's any formal way of doing this, I'd assume you could check if the distribution converged as n approached 1000. Or you could bootstrap and test the distribution of h%. But either of these tests limit the information and provide an approximation; does a formal solution exist?

Thanks!",en
1107680,2011-08-14 18:29:17,statistics,"Paul Meier, Statistician Who Revolutionized Medical Trials, Dies at 87",jihvi,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/jihvi/paul_meier_statistician_who_revolutionized/,13.0,0.0,,en
1107681,2011-08-14 23:30:35,statistics,"Dear r/statistics, I know you get a lot of these asking for help, it probably pisses you off a bit, but I am so jealous you understand the hieroglyphic language that is statistics. Need a bit of SPSS help, please. ",jip0a,ROFLMFAO,1265982705.0,https://www.reddit.com/r/statistics/comments/jip0a/dear_rstatistics_i_know_you_get_a_lot_of_these/,7.0,17.0,"Sorry again for being one of those people asking, I do have someone at Uni to ask, but it is the summer and she is away.

Anyway, I undertook a simple questionnaire, Likert scale, 9 questions, 2 different locations where these were undertaken (main variable I'm interested in), 46 questionnaires at each so 92 in total.

I've done the easy descriptive stats, but I want to do a Mann-Witney U test, the onlines tutorials make it look SO easy. 

What I'm doing wrong I think is labelling the variables appropriately. How do I do this? I want to compare two columns, one called 'Project one' the second called 'project 2'. I just wanna know if they are statistically significantly different. Is that so much to ask??? I can't make it work.

Also, I want to do a cronbach's alpha correlation coefficient for internal consistency in the data. Again, it's being a fuck. I don't know if I should whack all the data into one big spreadsheet and compare the lot, or consistency in the data individually per question. I just don't get it. It keeps telling me there is a negative coefficient which means exactly fuck all to me. (I am a biologist by the way, we are generally not natural statisticians, I'm jealous of you). 

If anyone has the time to help me, I would be so grateful. This is my masters dissertation and the data is so simple! If I can actually be helped, and find the solution to these (bare in mind I'm as thick as shit and do not find this logical) then I will include you in my acknowledgements (Seriously).

Love Mike",en
1107682,2011-08-14 23:44:52,statistics,Good SAS references coming from a heavy R background,jipeu,asjohnson,1288157377.0,https://www.reddit.com/r/statistics/comments/jipeu/good_sas_references_coming_from_a_heavy_r/,18.0,7.0,"Hey Everyone,

I am just finishing up a MS in statistics and going out to get a real job. I have been working mostly in R, but most workplaces I am looking at seem to use SAS. I have done a bit of SAS here and there, but I am certainly rusty. I was wondering if you had any links to some of your favorite SAS resources that I could trouble you for. 

Things like this for R come to mind: http://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf

I know there have to be great resources out there for SAS and I figure reddit might be able to make finding them easier.

Thanks.",en
1107683,2011-08-15 04:21:57,statistics,good news everyone... i need help figuring out which statistical test to use,jiwkx,[deleted],,https://www.reddit.com/r/statistics/comments/jiwkx/good_news_everyone_i_need_help_figuring_out_which/,0.0,12.0,"So here is the 'sitch' as they say; them being people that I don't know,  but I assume some people say sitch instead of situation. As part of a college course we gave a class of students a pre and post test (with similar content) to judge how well the students knew the material before and after the class. We also told the students not to study for the test so it would accurately portray their long term knowledge. 

Anyway, I figured reddit would be a good stats consult... I would like to know what type of test to run and what I can justifiably infer from the data (maybe something a little creative). We have 30 students who took the test twice (before and after). Each test was 50 problems and the students received a number grade (with a corresponding percentage score). 

I'm not familiar with the rules of /r/statistics and I apologize if this post is off topic. 

Thanks!


edit1 Thanks everyone for your help; you have all contributed to helping advance mankind!",en
1107684,2011-08-15 10:39:52,statistics,"Question: Combining different frequencies in time series (""converting"" annual to monthly)?",jj65q,latortilla,1299276404.0,https://www.reddit.com/r/statistics/comments/jj65q/question_combining_different_frequencies_in_time/,3.0,5.0,"Hi, I am working with time series data, in particular the variable of interest is measured monthly, but some of my independent variables are measured annually. Does anyone know what procedures exist to ""convert"" the annual data into monthly data?

In particular I am faced with 2 types of annual data:
- aggregates: here I could possibly divide the value by 12 and then have it monthly, though that seems too basic.
- non-aggregates, a value that would not be the sum of the months, probably more difficult to handle.

In my quick lit review I couldn't really find papers that deal with moving from annual to monthly (most papers seem to focus on monthly/quarterly to annual which of course is easier). Can anyone help?",en
1107685,2011-08-15 20:05:25,MachineLearning,C library for SVD,jjil1,qwsazxerfdcv,1287517243.0,https://www.reddit.com/r/MachineLearning/comments/jjil1/c_library_for_svd/,10.0,9.0,"hi , 
For a particular part of a project , we need to do some singular value decomposition on some data, the rest of the code is in c++ , i dont want to bring in matlab , is there any library in c which can  help me ",en
1107686,2011-08-15 20:30:57,statistics,You might want a tolerance interval,jjjiz,kmjn,1292111209.0,https://www.reddit.com/r/statistics/comments/jjjiz/you_might_want_a_tolerance_interval/,5.0,1.0,,en
1107687,2011-08-16 00:05:55,statistics,Teaching mixed models to non statisticians ,jjr4f,Case_Control,1263273609.0,https://www.reddit.com/r/statistics/comments/jjr4f/teaching_mixed_models_to_non_statisticians/,5.0,6.0,"So I'm in the process of preparing a lecture for a group of PhD epidemiology students about the analysis of correlated data (specifically focusing on group randomized trials).  I've been trying to track down some reading for the students and have come up against some trouble.  First article I intend for them to read is a big cursory examination of some of the options for analyzing group randomized trials (focusing on GEE, mixed effects, and methods adapted from complex surveys).  They already have a good handle on using survey methods (applied only, show them any real math and they flee in terror), so I'm not super concerned about them getting the concepts there. I've found what I think is a great intro article for GEE:

http://aje.oxfordjournals.org/content/157/4/364.full

The problem I'm having is finding an article about mixed models that is at more or less the same level of sophistication. The closest I've come is:

http://www.ncbi.nlm.nih.gov/pubmed/20013937

but from previous experience with this PhD program, I am afraid that it might still be a little to much.  I'd like them to come into the lecture with enough of a handle on the concepts that we can go through some examples in SAS as a class (sorry no R, it seems to scare them worse than matrix math for some reason...). Any ideas /r/stats?
",en
1107688,2011-08-16 00:09:43,MachineLearning,What schools have machine learning programs?,jjr9d,lifesavergummies,1313442276.0,https://www.reddit.com/r/MachineLearning/comments/jjr9d/what_schools_have_machine_learning_programs/,12.0,10.0,"I am an undergrad who is extremely interested in studying machine learning in grad school. It seems like most programs are in computer science departments and there is no way I would ever get into a computer science graduate program, as I have no formal computer science background (though I often program for fun on my own). 

I have noticed that only a few schools seem to offer an explicit degree in machine learning (such as Carnegie Mellon, which also offers a joint degree in ML/statistics). The problem is I don't think it is realistic for me to get into CMU or other elite schools like that. 

I've been watching Andrew Ng's video lectures  and reading ML texts on my own and this is something I'm definitely interested in, but it seems like my options are quite slim. Does anyone have any suggestions or advice regarding this matter? ",en
1107689,2011-08-16 05:13:52,MachineLearning,"Machine Learning Class, this coming Fall 2011, by Andrew Ng",jk0on,leonoel,1284436447.0,https://www.reddit.com/r/MachineLearning/comments/jk0on/machine_learning_class_this_coming_fall_2011_by/,52.0,12.0,,en
1107690,2011-08-16 10:15:35,artificial,Stanford's Machine Learning class 2011 - Online,jk90s,[deleted],,https://www.reddit.com/r/artificial/comments/jk90s/stanfords_machine_learning_class_2011_online/,1.0,0.0,,en
1107691,2011-08-16 10:53:06,MachineLearning,brief intro to the expectation-maximization algorithm,jk9rb,mish4,1286697468.0,https://www.reddit.com/r/MachineLearning/comments/jk9rb/brief_intro_to_the_expectationmaximization/,12.0,0.0,,en
1107692,2011-08-16 17:04:24,statistics,Statistics of Making Ca$h Online...,jkgpa,[deleted],,https://www.reddit.com/r/statistics/comments/jkgpa/statistics_of_making_cah_online/,1.0,0.0,,en
1107693,2011-08-16 21:59:49,statistics,"Upgrading computer, recommendations?",jkqxf,cookie_partie,1285036674.0,https://www.reddit.com/r/statistics/comments/jkqxf/upgrading_computer_recommendations/,5.0,21.0,"Hi r/statistics.  I am in need of some advice for my work computer where SAS will be my workhorse program.  I'm a junior faculty at a pharmacy school where I frequently work with large data sets (5 million patients per year, hundreds of rows per patient, data extracts sent to me for analysis).

I'm ""upgrading"" from a laptop where I access data and use SAS via a server to a desktop where I will run SAS locally.  I have two options for the desktop and **would appreciate input on if the desktop server is worth the extra cost**.  The true costs are lower than what I see, but I don't have access to the true prices.  Marked major differences in bold.

Option 1 ($1000) - desktop

* **Intel Core i5 2400 3.1 GHz, 6M Cache **
* **4GB, Non-ECC, 1333MHz DDR3 (2 Dimms) **

&gt; (will upgrade to 8 GB)

* 250GB SATA 6.0GB/s and 8MB DataBurst Cache
* 8X DVD+/-RW
* 512MB AMD RadeonHD 6350 Graphics Dual DVI, LP
* Gigabit Ethernet, No Modem
* Windows 7 Professional 64-Bit

-

Option 2: desktop server ($1900)

* **Quad Core Intel Xeon W3550 3.0GHz, 8M L3, 4.8 GT/s Turbo**

* **6GB ECC DDR3 @1333MHz (3 DIMMS) SDRAM** 

&gt; upgrade to 12 GB is $400...worth it?

* 300GB 10K RPM SATA

&gt; Or 1 TB 7200K

* 16X DVD+/-RW
* 768MB PCIe x16 nVIDIA Quadro FX1800, Dual Monitor 2 DP + 1 DVI
* Gigabit Ethernet, No Modem

I realize this isn't a hardware forum, but I thought this still might be the best spot for this recommendation.  My dept's IT person initially recommended the desktop server, then immediately backtracked and won't give a recommendation one way or the other any more (perhaps for cost savings).

Thanks for any advice you can offer.",en
1107694,2011-08-16 22:07:25,statistics,Inverse probability weighting with clustered data,jkr79,NY_epigenes,1308108331.0,https://www.reddit.com/r/statistics/comments/jkr79/inverse_probability_weighting_with_clustered_data/,6.0,10.0,"Hi! I have a quick question regarding IPWs and clustered data. My data is clustered by village level (people are my unit of analysis but they cluster into villages). 

I am using proc genmod in SAS to generate the probabilities (outcome is binary). My question is whether I need specify clustering when I generate the probabilities AND when I apply the IPWs to the final model or ONLY when I apply the IPWs to estimate the final model. 

Clustering does not affect the beta coefficients (and thus should not affect the probabilities), only the standard errors, so I would presume that the latter is the best approach but I can't seem to find a definitive answer on this. If anyone has suggestions for an article that address this, that would be helpful!",en
1107695,2011-08-16 22:11:32,datasets,Database of megachurches in the U.S.,jkrc4,Quintote,1269890504.0,https://www.reddit.com/r/datasets/comments/jkrc4/database_of_megachurches_in_the_us/,10.0,2.0,,en
1107696,2011-08-17 03:41:33,analytics,Need some quick advice!!!,jl24p,Joobei,1295418245.0,https://www.reddit.com/r/analytics/comments/jl24p/need_some_quick_advice/,1.0,0.0,"I applied for an analytic job and they sent me some homework to complete. I have been zooming through it until I reached a part that asked me to just analyze a document. I don't have access to SPSS and was wondering what I should do.

I am trying to start in the analytic world and could really use some help!",en
1107697,2011-08-17 04:17:59,AskStatistics,A priori effect size calculation for an ANCOVA? (x-post r/askscience),jl3bf,GloriousGoldenPants,1295762391.0,https://www.reddit.com/r/AskStatistics/comments/jl3bf/a_priori_effect_size_calculation_for_an_ancova/,1.0,0.0,"Hi,

I'm designing a study, and I need to do an a priori calculation to figure out what sample size I need to have a moderate eta-squared in an ANCOVA study. Does anyone out there know an online calculator or a simple equation I could use?

Much Thanks,
 A Brain Dead Grad Student",en
1107698,2011-08-17 04:42:53,MachineLearning,Question: What is the state of the art machine learning strategy for adserving (yield optimization),jl42h,lerchmo,1141331929.0,https://www.reddit.com/r/MachineLearning/comments/jl42h/question_what_is_the_state_of_the_art_machine/,2.0,1.0,"What is an efficient / flexible way to optimize an adserver?

Assuming you have a number of data points for each user:

* Current Webpage
* User Session History
* Browser / Operating System
* Time of day

And you have performance data per-ad across these data points.

Assuming you are starting with random distribution of Ads per Webpage.

How do you effectively start exploiting performance information to weight ads for maximum yield?",en
1107699,2011-08-17 23:30:51,statistics,"I have a SPSS module coming up, what basic features could I learn first to be a step ahead!?",jlz1k,[deleted],,https://www.reddit.com/r/statistics/comments/jlz1k/i_have_a_spss_module_coming_up_what_basic/,1.0,10.0,,en
1107700,2011-08-18 00:14:55,MachineLearning,"The first ""AI and Intelligent Systems Hall of Fame"" Inductees",jm0oj,prince_nerd,1284796043.0,https://www.reddit.com/r/MachineLearning/comments/jm0oj/the_first_ai_and_intelligent_systems_hall_of_fame/,11.0,18.0,,en
1107701,2011-08-18 06:29:28,statistics,Geometric Interpretation of the Markov/Chebyshev Inequality,jmcm1,[deleted],,https://www.reddit.com/r/statistics/comments/jmcm1/geometric_interpretation_of_the_markovchebyshev/,2.0,1.0,"Hi,

is there a common geometric interpretation of the Markov and Chebyshev Inequality?",en
1107702,2011-08-18 15:05:32,statistics,The Tribes of Science - The Statisticians,jmnsq,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/jmnsq/the_tribes_of_science_the_statisticians/,25.0,0.0,,en
1107703,2011-08-18 16:15:12,MachineLearning,Sending the Police Before There’s a Crime,jmpcu,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/jmpcu/sending_the_police_before_theres_a_crime/,27.0,11.0,,en
1107704,2011-08-18 16:37:34,statistics,Through the Eyes of a Statistician,jmpwq,dontstalkmebro,1269461443.0,https://www.reddit.com/r/statistics/comments/jmpwq/through_the_eyes_of_a_statistician/,12.0,9.0,,en
1107705,2011-08-19 01:09:17,computervision,Face recognition technology fails to find UK rioters,jn815,cavedave,1128052800.0,https://www.reddit.com/r/computervision/comments/jn815/face_recognition_technology_fails_to_find_uk/,7.0,0.0,,en
1107706,2011-08-19 01:09:37,MachineLearning,Face recognition technology fails to find UK rioters,jn81g,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/jn81g/face_recognition_technology_fails_to_find_uk/,29.0,5.0,,en
1107707,2011-08-19 02:04:38,artificial,"Football analysis leads to advance in artificial 
intelligence",jn9t7,Sciencehipster,1313518679.0,https://www.reddit.com/r/artificial/comments/jn9t7/football_analysis_leads_to_advance_in_artificial/,10.0,2.0,,en
1107708,2011-08-19 03:01:45,artificial,"IBM produces first 'brain chips' - The system is capable of ""rewiring"" its connections as it encounters new information, similar to the way biological synapses work.",jnbi6,r721,1258928336.0,https://www.reddit.com/r/artificial/comments/jnbi6/ibm_produces_first_brain_chips_the_system_is/,16.0,2.0,,en
1107709,2011-08-19 03:54:43,MachineLearning,"Halcyon is hiring a machine learning expert  (and more) in Redwood City, CA. Shared by Randal Koene to Facebook. ",jnd32,VCavallo,1170738698.0,https://www.reddit.com/r/MachineLearning/comments/jnd32/halcyon_is_hiring_a_machine_learning_expert_and/,0.0,0.0,,en
1107710,2011-08-19 07:00:32,statistics,"statisticians of reddit, how important is it that i know programming?",jnin0,odles_44,1287850176.0,https://www.reddit.com/r/statistics/comments/jnin0/statisticians_of_reddit_how_important_is_it_that/,3.0,14.0,"Hello reddit. I am currently a punk ass undergraduate who chose to major in statistics. i took a programming class last fall, and it was easily the least favorite class i've ever taken. very difficult course work, a professor who was exactly what i expected from a programming professor (awkward, confusing and super smart but not good at explaining anything). i had my engineering buddies do all of my projects for me and i obviously learned very little from the class. my question is, what can i expect when i get out in the ""real world""? I realize that computers and programming are definitely going to be a part of the deal. i have a basic understanding of excel and spss, which as far as i understand seem to be essential for statisticians. for further context, i had an internship the past two summers with a cpa and he had me monitor his stock/bond portfolio which was intriguing, so i'm thinking the financial sector is a possible future. sorry for the long post, i'm just a freaking out a little bit about my future and not sure who to talk to about it.",en
1107711,2011-08-19 16:31:40,artificial,Are there still jobs in AI? What are they like?,jnu9k,[deleted],,https://www.reddit.com/r/artificial/comments/jnu9k/are_there_still_jobs_in_ai_what_are_they_like/,21.0,8.0,"I'm extremely interested in AI, and I'm considering going to graduate school for it. I've heard a lot of conflicting information about the job market, though. I've heard from reliable sources that there are lots of great AI jobs, and from other reliable sources that there aren't any good AI jobs anymore. Any light you could shed on this subject would be extremely helpful.",en
1107712,2011-08-19 18:47:10,statistics,"Cross post from r/Excel: Asking for Excel help - how do you calculate streaks (win streak, who scores first streak) from sports data",jny8t,randude,1286197010.0,https://www.reddit.com/r/statistics/comments/jny8t/cross_post_from_rexcel_asking_for_excel_help_how/,2.0,2.0,"My buddy and i play PS3 hockey and i've kept stats of our rivalry...i'd like to have stats automatically calculated out for winning streaks and who scored the 1st goal in each game (current data shows Tim on a 2 game win streak and he now has a 1 game scored first streak)

there's multiple tabs to the sheet, the current relevant data is on the 'NHL10 Games' tab and i'd like to place the new stats on the 'NHL10 Summary' tab (starting in cell A94)

I have the sheet posted up at:

http://randude.com/ps3/2_buds_hockey_stats.xlsx

Thank you for your help
",en
1107713,2011-08-19 20:10:04,MachineLearning,Groupon Explains Why It Wants To Constantly Track Customers' Whereabouts - Forbes,jo13t,imbenzene,1253279624.0,https://www.reddit.com/r/MachineLearning/comments/jo13t/groupon_explains_why_it_wants_to_constantly_track/,14.0,2.0,,en
1107714,2011-08-19 22:19:18,statistics,Suggestions on how I can teach myself SAS? and SQL?,jo5qr,benjeye,1292457824.0,https://www.reddit.com/r/statistics/comments/jo5qr/suggestions_on_how_i_can_teach_myself_sas_and_sql/,2.0,8.0,,en
1107715,2011-08-19 23:24:05,statistics,What things should I absolutely know when I get a job in Statistics?,jo7yu,HughManatee,1271624440.0,https://www.reddit.com/r/statistics/comments/jo7yu/what_things_should_i_absolutely_know_when_i_get_a/,9.0,18.0,"Sorry if the question is vague at all, but I'm a grad student currently studying Statistics, and I'm just curious if there are a lot of issues you run into, or skills you've learned that would be useful to read up on and practice before I get a job as a Statistician. ",en
1107716,2011-08-19 23:47:48,MachineLearning,Power of Data - Short (14min) talk on how Google uses machine learning,jo8p2,pastr,1196202081.0,https://www.reddit.com/r/MachineLearning/comments/jo8p2/power_of_data_short_14min_talk_on_how_google_uses/,1.0,3.0,,en
1107717,2011-08-20 03:54:41,MachineLearning,ML+Crime = Minority Report ,jofnm,eugenium,1302886540.0,https://www.reddit.com/r/MachineLearning/comments/jofnm/mlcrime_minority_report/,12.0,6.0,,en
1107718,2011-08-20 12:54:22,artificial,New IBM microchip to 'rival' the human brain,joqxq,soccer,1190247728.0,https://www.reddit.com/r/artificial/comments/joqxq/new_ibm_microchip_to_rival_the_human_brain/,11.0,4.0,,en
1107719,2011-08-20 18:34:49,AskStatistics,How do you compare the significance of different p-values that have reached significance?,jow5i,inquilinekea,1274654696.0,https://www.reddit.com/r/AskStatistics/comments/jow5i/how_do_you_compare_the_significance_of_different/,4.0,15.0,"What value is stronger? 

Risk of Parkinson's Disease from Herbicides?

Odds ratio of 3.36, p-value of 0.034, and 95% CI of 1.09 to 10.33

vs.

Risk of Parkinson's Disease from Pesticides?

Odds ratio of 3.15, p-value of 0.002, and 95% CI of 1.54 to 6.49

==

Also, for fungicides, there's an OR of 1.50, p-value of 0.526, and a 95% CI of 0.43 to 5.26. Does that mean that fungicides might have a small effect? I don't think we should rule it out entirely - see the book ""The Cult of Statistical Significance"" at http://www.amazon.com/Cult-Statistical-Significance-Economics-Cognition/dp/0472050079",en
1107720,2011-08-21 07:48:47,statistics,Applying to grad schools this fall...,jpf1b,kwux,1301077062.0,https://www.reddit.com/r/statistics/comments/jpf1b/applying_to_grad_schools_this_fall/,4.0,16.0,"I'm graduating this December with two B.S. degrees (Mathematics, Statistics) and I'm looking to apply to Masters and/or PhD programs in Statistics this fall for the following academic year beginning fall 2012.  

I have what I believe to be a fairly respectable GPA (3.84), I'm gaining some research experience this summer with two separate projects in the Mathematics and Statistics departments at my undergraduate university, and I'll be taking the GRE here in the next month or so.

What has been your experience with the graduate school application process, or graduate school in general?  Where did you apply?  Where did you end up?  Do you have any words of encouragement or advice?

",en
1107721,2011-08-21 13:20:11,artificial,"Girl, 15, is youngest to receive bionic fingers",jpk70,soccer,1190247728.0,https://www.reddit.com/r/artificial/comments/jpk70/girl_15_is_youngest_to_receive_bionic_fingers/,26.0,13.0,,en
1107722,2011-08-21 15:18:49,AskStatistics,Averaging unpredictable timing,jplhs,[deleted],,https://www.reddit.com/r/AskStatistics/comments/jplhs/averaging_unpredictable_timing/,1.0,0.0,"Suppose there is a phenomenon that occurs once per day, and I wish to set my watch by it. I’m going to pretend that the phenomenon is a groundhog waking up and coming out of its den.

Unfortunately, the groundhog isn’t very punctual. It never comes out of its den before 6 a.m., but after that it has a certain, constant probability of coming out its den per unit time. It’s not strictly guaranteed that the groundhog will come out of its den by the end of the day, but the probability is high enough that that’s not a concern.

Luckily, several groundhogs live in the same field. They have identical behaviour, but they act independently of each other. I know I can take advantage of their numbers to help me set my watch more accurately than I could with one groundhog.

One obvious method of exploitation is to wait for first groundhog to wake up and record at what time that occurs (there’s an accurate and independent timepiece for testing purposes). Do this for several days to find out at what time the first groundhog wakes, on average, and then set the watch to that time.

Then I thought about counting the groundhogs as they wake, recording the time at which half of them are up, finding what that time is on average, and setting the watch accordingly. On first glance, this seems like a better option—would individual groundhogs’ wake‐up times not average out better this way?


So I wrote a program to test the two methods, and low and behold the first one yields superior results. However, with no background in statistics, I have no idea why.

So, statisticians, why is the first method superior? And is there a third method that is superior to either of mine?",en
1107723,2011-08-22 03:58:04,MachineLearning,Fake reviews on Amazon detected by algorithm,jq3ra,cygn,1308147132.0,https://www.reddit.com/r/MachineLearning/comments/jq3ra/fake_reviews_on_amazon_detected_by_algorithm/,2.0,0.0,,en
1107724,2011-08-22 11:07:56,statistics,"Time Series: Inductive proof of the PACF for a MA(1) process, using the Durbin-Levinson algorithm. How?!",jqeq0,a_baby,1310495789.0,https://www.reddit.com/r/statistics/comments/jqeq0/time_series_inductive_proof_of_the_pacf_for_a_ma1/,5.0,0.0,"I'm trying to work to prove that the PACF for a MA(1) process, X_t = epsilon_t + theta*epsilon_t-1, is given by: alpha(h) = -(-theta)^h / (1 + theta^2 + ... + theta^2h), using an inductive argument based on the Durbin-Levinson algorithm.

I have spent hours and hours trying to get this to work. Any time series masters willing to give me some guidance?",en
1107725,2011-08-22 18:12:54,MachineLearning,The best way to detect dark matter is to practice on white matter,jqn3f,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/jqn3f/the_best_way_to_detect_dark_matter_is_to_practice/,4.0,0.0,,en
1107726,2011-08-22 20:06:39,statistics,"Breathing Earth - Births, Deaths, Co2 - In real time",jqqof,recklessjudge,1308241421.0,https://www.reddit.com/r/statistics/comments/jqqof/breathing_earth_births_deaths_co2_in_real_time/,6.0,1.0,,en
1107727,2011-08-22 22:59:35,MachineLearning,Subreddit for the Stanford Machine Learning Course,jqwsg,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/jqwsg/subreddit_for_the_stanford_machine_learning_course/,10.0,2.0,,en
1107728,2011-08-23 01:13:14,statistics,Don’t kill America’s databook,jr1b9,castman10,1294175326.0,https://www.reddit.com/r/statistics/comments/jr1b9/dont_kill_americas_databook/,44.0,3.0,,en
1107729,2011-08-23 03:09:55,MachineLearning,"""Introduction to Octave"" for those starting CS 229/MLclass",jr4ri,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/jr4ri/introduction_to_octave_for_those_starting_cs/,11.0,1.0,,en
1107730,2011-08-23 03:15:52,statistics,Creating a Consumer Price Index?,jr4wf,daseinphil,1178737982.0,https://www.reddit.com/r/statistics/comments/jr4wf/creating_a_consumer_price_index/,2.0,4.0,"I work for a small startup, focusing on electronic grocery coupons, who has generated a fair amount of transactional data across the US. A novitiate in statistics, I've been tasked with using our dataset to create a  price index of goods that we offer coupons for. I've been going though the relevant wikipedia links, and found the (rather imposing) CPI manual from the Int'l Labor Organization. I'd like to produce something both easy to understand and statistically accurate - is there a more accessible resource /r/statistics would recommend I start with? ",en
1107731,2011-08-23 10:34:01,statistics,"Bayes, regression to the mean and baseball",jrhfs,ccutler69,1202258299.0,https://www.reddit.com/r/statistics/comments/jrhfs/bayes_regression_to_the_mean_and_baseball/,7.0,1.0,,en
1107732,2011-08-23 13:35:24,datasets,US census records (&gt;72 years old) released for genealogical research,jrkgi,utcursch,1175517803.0,https://www.reddit.com/r/datasets/comments/jrkgi/us_census_records_72_years_old_released_for/,18.0,2.0,,en
1107733,2011-08-23 21:27:19,datasets,"Help us build the RezScore Skills Explorer, to help visualize job market demand for your skills.  What additional data would you like?",jry8l,theorpheus,1152547926.0,https://www.reddit.com/r/datasets/comments/jry8l/help_us_build_the_rezscore_skills_explorer_to/,3.0,0.0,,en
1107734,2011-08-24 00:21:50,MachineLearning,Map of Magnitudes from 2011 East Coast Earthquake using ggplot2/R! (cross-post from /r/bigdata),js5f5,ohsnaaap,1270647929.0,https://www.reddit.com/r/MachineLearning/comments/js5f5/map_of_magnitudes_from_2011_east_coast_earthquake/,5.0,7.0,,en
1107735,2011-08-24 01:20:37,statistics,Help. meeting english prof tomorrow. ,js7hg,[deleted],,https://www.reddit.com/r/statistics/comments/js7hg/help_meeting_english_prof_tomorrow/,10.0,8.0,"I need some help in what to expect tomorrow.

I get an email from an english prof looking for someone who can do stats. I reply and ask what kind of analysis that they will be using.

she replies with this ""We have some stats on continuation, retention, grades, etc. But we do not know how to account for sample size.  If you have time to meet with me, I would be happy to go over all of this with you.""

Anyway, what do you think this could be about?
 ",en
1107736,2011-08-24 12:58:11,statistics,Making Penn Tables and World Bank Data Comparable...,jsq2h,dav1b,1275393727.0,https://www.reddit.com/r/statistics/comments/jsq2h/making_penn_tables_and_world_bank_data_comparable/,0.0,0.0,"Because of the way they calculate GDP at PPP are different, the Penn World Tables and the World Banks World Development Indicators data cannot be compared. I've been trying to rebase the data to the same year but between deflating to real LCU then up to reflating to 2000 US$ PPP I think I've confused myself and am wondering if perhaps one of ye gentlemen have come across and resolved this problem before? Any help would be much appreciated.",en
1107737,2011-08-24 16:35:21,statistics,GoogleDocs and Statistics,jsubr,privy_,1301886522.0,https://www.reddit.com/r/statistics/comments/jsubr/googledocs_and_statistics/,22.0,6.0,"Did you know in google's spreadsheet you can enter something like this: 

=GoogleLookup(""entity"", ""atribute"") 

For Instance, =GoogleLookup(""United States of America"", ""Population"") 

would return 308798000 in the cell. It gives you a choice as to what source it picks from. Obviously not the most accurate peer reviewed info, but cool nonetheless. ",en
1107738,2011-08-24 23:39:11,statistics,What can I do with C and statistical programming in my spare time....thoughts?,jt96z,irisblackwater,1302140061.0,https://www.reddit.com/r/statistics/comments/jt96z/what_can_i_do_with_c_and_statistical_programming/,4.0,6.0,"Hi Reddit,

I am trying to learn C. I know a good bit already and have previous OO-prog experience with Java (upto the data structures level).

I also like statistics, econometrics, time-series and number-crunching(big fan of R!)

What are some fun things for me to do in my spare time that involves C programming and statistical modelling/econometrics?

(made a post to ask.reddit about this too)",en
1107739,2011-08-25 00:08:32,statistics,"After spending hours and hours trying various (and ineffective) techniques, I realize I need some SPSS help",jtabu,[deleted],,https://www.reddit.com/r/statistics/comments/jtabu/after_spending_hours_and_hours_trying_various_and/,2.0,2.0,"Here's the skinny:

My thesis is on the association between personality and marital status in later life.

I did a MANOVA using SPSS (with an LSD post hoc) using three marital groups as my IVs (married, never married, and divorced), and 30 facets of a personality assessment called the NEO as my DVs. I found some really interesting results.

However, the marital status groups are formed by what the participant self-reports at baseline. I do have some people in my ""married"" group who have been divorced up to five times. I want to run another MANOVA, but this time using four marital groups (married, never married, divorced, and people who are married now but have been divorced before), just to see if the inclusion of these formally married people in my married group is sussing up my results at all.

I have tried 'select cases' in every way I can think of, but can't get this done correctly. I have one variable for marital status, and I have another for # of divorces. If I do ""select cases"" and just remove all participants who have been divorced before, I'm taking some people out of the divorced group who have been divorced several times, and I want to keep them in that group. I only want the previously divorced people out of the married group.

Thanks for your help...and I'd ask my advisor, but he tends to be not so great at stats, soooo...here I am.",en
1107740,2011-08-25 04:06:31,statistics,Video of talk on experiences with using SAS and R in insurance and banking,jtikb,mycatharsis,1294046118.0,https://www.reddit.com/r/statistics/comments/jtikb/video_of_talk_on_experiences_with_using_sas_and_r/,12.0,0.0,,en
1107741,2011-08-25 20:14:54,statistics,Stanford offering free CS and probability courses online,ju768,Case_Control,1263273609.0,https://www.reddit.com/r/statistics/comments/ju768/stanford_offering_free_cs_and_probability_courses/,29.0,4.0,,en
1107742,2011-08-25 21:48:28,analytics,I run a small deal aggregator. This is what the HP Touchpad deal did to my traffic.,juaik,kbrower,1154524324.0,https://www.reddit.com/r/analytics/comments/juaik/i_run_a_small_deal_aggregator_this_is_what_the_hp/,0.0,0.0,,en
1107743,2011-08-26 17:45:37,datasets,This show.,jv5jf,[deleted],,https://www.reddit.com/r/datasets/comments/jv5jf/this_show/,1.0,0.0,,en
1107744,2011-08-26 22:18:05,statistics,Logistic regression or data transformation,jvet1,MonsieurB,1240608341.0,https://www.reddit.com/r/statistics/comments/jvet1/logistic_regression_or_data_transformation/,2.0,14.0,"Hi Reddit,

I had a phone call from my friend today and she talked to me about a problem she had. She has two datasets, both ratios of sound frequencies ranging from 0 to 1. She wants to see if there's a correlation between the two. In her own words: ""she wants a kind of R^2 between ratios"".

I haven't done statistics in a while, but my first impression was to use logistic regression and see if the model fit better than the null model.

She said she was told to do a Arcsine transformation (i.e. take the arcsine of the square root of the dependent variable) and then, I guess, use linear regression with the other set of (untransformed) ratios as the dependent variable (Big X) to obtain a R^2.

As I said I haven't done these kinds of stats in a while, so even though initially I thought her approach was strange, I couldn't exactly pinpoint why. So, was I in the wrong? What's your opinion?

Thanks!",en
1107745,2011-08-27 01:00:02,statistics,P-value fallacy on More or Less,jvk5q,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/jvk5q/pvalue_fallacy_on_more_or_less/,2.0,0.0,,en
1107746,2011-08-27 01:05:37,statistics,Quickly build interactive graphs in R - using ‘manipulate’ in RStudio,jvkc1,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/jvkc1/quickly_build_interactive_graphs_in_r_using/,22.0,2.0,,en
1107747,2011-08-27 18:25:51,datasets,So how useful is Data.gov anyway,jw5f7,[deleted],,https://www.reddit.com/r/datasets/comments/jw5f7/so_how_useful_is_datagov_anyway/,1.0,0.0,,en
1107748,2011-08-28 01:12:43,datasets,USDA Food Nutrient Database: clutter-free JSON edition,jwgwv,ashleyw,1216784454.0,https://www.reddit.com/r/datasets/comments/jwgwv/usda_food_nutrient_database_clutterfree_json/,17.0,1.0,,en
1107749,2011-08-28 01:24:17,MachineLearning,SIGKDD 2011 Conference — Summary of days 2-4 ,jwh8q,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/jwh8q/sigkdd_2011_conference_summary_of_days_24/,9.0,0.0,,en
1107750,2011-08-28 02:28:55,MachineLearning,Supervised learning of text data with 250 labels?,jwj3e,[deleted],,https://www.reddit.com/r/MachineLearning/comments/jwj3e/supervised_learning_of_text_data_with_250_labels/,0.0,0.0,Would it be possible to achieve greater than 70% accuracy using supervised learning to classify text data with 250 unique labels?,en
1107751,2011-08-28 06:54:25,artificial,"New to AI, no coursework at college. What should I read/learn on my own. Studying Computer Science atm.",jwq96,Accerbus,1266814566.0,https://www.reddit.com/r/artificial/comments/jwq96/new_to_ai_no_coursework_at_college_what_should_i/,10.0,15.0,"I was planning on attending a college with courses in AI for undergrad but it didn't pan out and now I am at a college that only offers psychology or computer science. I chose to major in computer science, but it is just a general computer science degree. I was wondering what books or things I could/should learn on my own so I can have an edge on anyone when applying for graduate level studies or even in the job world. I think that self-learning is the best method for me and most people who want to be successful or at least provide groundbreaking work to the field. I have hit a financial speedbump in my college career so I am going to have a lot of free time after work to do whatever. I am taking one course in java at the moment and will be learning c# with the school. I am trying to teach myself python/c++ as well. Beyond programming languages I don't know what else I can really learn. SO this is where I need your help.

Any suggestions would be awesome!(Free stuff is preferred, but will pay money for things that are of good quality)",en
1107752,2011-08-28 19:47:07,statistics,Considering career in statistics... I have a few questions for the Statisticians here!,jx39c,Distance_Runner,1293552667.0,https://www.reddit.com/r/statistics/comments/jx39c/considering_career_in_statistics_i_have_a_few/,12.0,20.0,"First off, a little about where I'm at. I'm a Junior in college, majoring in Biology at my university. I started off with the plan to go into medicine, but after research and shadowing, I'm reconsidering... I've always been fascinated by statistics and have always enjoyed the classes. I took biostatistics a year ago and got a 98% in the class, highest grade overall. I loved that class and it was really easy for me. Because of that I declared a minor in statistics, and so far I've taken a few classes other than biostat, one was a ""computer applications of statistics class"" and the other was the second ""statistical methods"" class. I've used excel, minitab, and SAS. I plan to finish my degree in Biology, but will have taken calcII and linear algebra before I graduate. I'm considering on going to grad school in Statistics or Biostat.

My questions:

- what are the work hours like for the already established statisticians here?
- do you guys still like your jobs? Any regrets about the statistics path you took?
- What types of companies are in most demand for statisticians right now? (like pharmaceuticals, etc.)
- How's the pay after roughly 10 years in the field?
- what else can I do to prepare myself for grad school in stat.
- any other advice you'd like to offer?

Thanks! ( and I realize I could simply google this, and I have, but I'm just looking for further advice!)",en
1107753,2011-08-29 00:02:36,statistics,Time Series Problem,jxagl,[deleted],,https://www.reddit.com/r/statistics/comments/jxagl/time_series_problem/,1.0,0.0,"I am having a really tough time grasping the class I am taking this semester. If anyone can help explain on what to do for this I would be extremely thankful. 

Link to the actual question

http://i.imgur.com/wvbm0.png

",en
1107754,2011-08-29 07:26:19,MachineLearning,Best order to go through these Data Mining tutorials?,jxna7,[deleted],,https://www.reddit.com/r/MachineLearning/comments/jxna7/best_order_to_go_through_these_data_mining/,21.0,3.0,,en
1107755,2011-08-29 18:34:31,statistics,"Could someone explain the Independent Component 
Analysis method? (crosspost from r/askscience)",jy1c0,whambamthankyoumam,1287938237.0,https://www.reddit.com/r/statistics/comments/jy1c0/could_someone_explain_the_independent_component/,9.0,14.0,I am aware of the complex stuff but could someone explain in simple layman terms?,en
1107756,2011-08-30 00:24:00,MachineLearning,Introduction to Latent Dirichlet Allocation,jycwb,jasonrosen,1304619652.0,https://www.reddit.com/r/MachineLearning/comments/jycwb/introduction_to_latent_dirichlet_allocation/,23.0,15.0,,en
1107757,2011-08-30 03:16:36,statistics,Anyone have any experience coding C extensions for R?,jyi58,hep_th,1303620608.0,https://www.reddit.com/r/statistics/comments/jyi58/anyone_have_any_experience_coding_c_extensions/,11.0,8.0,"background: I'm a new convert to biostatistics. I have a lot of experience with programming.

I've found that a lot of the experienced scientists I work with don't have much experience with just good programming methods, and I'm often able to speed up things they do in R. Now, I'm trying to understand the interface between C and R, and I'm curious if anyone has had experience coding libraries in R with mostly C. In particular, is it generally possible to beat the performance of R functions such as apply() by coding an implementation specific version in C? I definitely don't think I can improve on the BLAS calls that R makes...",en
1107758,2011-08-30 04:05:08,statistics,Any good ideas for an undegrad honors paper?,jyjmz,adizzan5220,1313616651.0,https://www.reddit.com/r/statistics/comments/jyjmz/any_good_ideas_for_an_undegrad_honors_paper/,2.0,4.0,"I'm double majoring in Statistics and Biology (specialized in Microbiology) at my University and I am planning to write a thesis paper for my final year in the STATS department. However, I don't want to write a theoretical MATH-STAT paper and would prefer to write an epidemiology/biostat paper.  At this point I have a couple of ideas about what to write about, but nothing that is concrete. Any ideas? Suggestions?",en
1107759,2011-08-30 09:38:41,datascience,What is Data Science?,jytjm,[deleted],,https://www.reddit.com/r/datascience/comments/jytjm/what_is_data_science/,1.0,0.0,,en
1107760,2011-08-30 15:30:25,statistics,Interesting statistics questions for statistical analysis,jyzpv,rsamrat,1306639444.0,https://www.reddit.com/r/statistics/comments/jyzpv/interesting_statistics_questions_for_statistical/,1.0,0.0,I'm studying Think Stats which is based on the case study that addresses the question of whether **first babies arrive late**. What are other such questions to explore? ,en
1107761,2011-08-30 16:27:57,MachineLearning,Two Analytics Success Stories,jz18j,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/jz18j/two_analytics_success_stories/,18.0,2.0,,en
1107762,2011-08-30 20:22:04,MachineLearning,Machine Learning for Knowledge Extraction from Wikipedia and Other Semantically Weak Sources,jz8fc,sm1000,1312473200.0,https://www.reddit.com/r/MachineLearning/comments/jz8fc/machine_learning_for_knowledge_extraction_from/,14.0,1.0,,en
1107763,2011-08-30 20:37:22,statistics,Data mining for fantasy football,jz8yj,agconway,1228006618.0,https://www.reddit.com/r/statistics/comments/jz8yj/data_mining_for_fantasy_football/,19.0,8.0,,en
1107764,2011-08-30 22:30:39,statistics,Watch as r/TheoryOfReddit fails at data collection,jzcv0,shaggorama,1233555004.0,https://www.reddit.com/r/statistics/comments/jzcv0/watch_as_rtheoryofreddit_fails_at_data_collection/,2.0,2.0,,en
1107765,2011-08-31 03:32:51,statistics,"If anyone can help me with this, I will buy you reddit gold. Due tomorrow",jzmr3,[deleted],,https://www.reddit.com/r/statistics/comments/jzmr3/if_anyone_can_help_me_with_this_i_will_buy_you/,0.0,12.0,,en
1107766,2011-08-31 05:15:54,statistics,Stats help. Please? ,jzqem,bfhancock,1294107695.0,https://www.reddit.com/r/statistics/comments/jzqem/stats_help_please/,1.0,0.0,"I know you will probably think, ""Why is he in this class if he doesn't understand something this simple?""... long story. Anyways, can anyone help me with this?


""Suppose at one extreme a negotiation results in a tax rate a = 10 % and at the other extreme a high tax rate of b = 35%. It is assumed that the result of the negotiation will be somewhere in between with all possibilities equally likely.


a) What is the pdf f(x), cumulative distribution F(x), E[X], and Var[X]?


b) What is the probability the tax rate will fall between 12 % and 18 %?


Thank you very much for any help.


This is not homework. This is a general review my professor handed out. He said these are the sorts of things we should already be able to do... so I need some help being able to do this. It is not to turn in. 


 I was in two semesters of stats before this but my professor was obsessed with R (which I realize is very powerful) but did not ever explain much about the distributions, random variables, expected value, etc... ",en
1107767,2011-08-31 08:38:36,statistics,"For a multiple regression, is there a way to estimate a minimum n?",jzwoe,benjeye,1292457824.0,https://www.reddit.com/r/statistics/comments/jzwoe/for_a_multiple_regression_is_there_a_way_to/,6.0,12.0,"If I'm making a multiple regression model with *k* variables, is there a way to find the minimum number of samples I need (*n*)?",en
1107768,2011-08-31 11:28:12,MachineLearning,Man vs. Machine on Wall Street: How Computers Beat the Market,k00b2,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/k00b2/man_vs_machine_on_wall_street_how_computers_beat/,31.0,2.0,,en
1107769,2011-08-31 18:39:50,analytics,Analytics Multi-Channel Funnels Overview,k09xs,insite,1238258385.0,https://www.reddit.com/r/analytics/comments/k09xs/analytics_multichannel_funnels_overview/,1.0,0.0,,en
1107770,2011-08-31 20:44:40,analytics,I have a second interview lined up tomorrow for a Web Analytics Position but I need help preparing.,k0eg6,ClarenceCW,1249835779.0,https://www.reddit.com/r/analytics/comments/k0eg6/i_have_a_second_interview_lined_up_tomorrow_for_a/,1.0,0.0,"They want me to bring a sample Analytics report showing funnels and goals and provide observations and actionable insights on how I'd increase the conversion rate.

The problem is, I've only used Google Analytics on my own content based websites and have never had the need to setup any goals.

I am scouring the web to find some sample reports that I can analyze, but I am having no luck.

Does anybody have any advice on how I can properly prepare for this given my current situation?",en
1107771,2011-09-01 04:49:51,MachineLearning,In Its Image,k0uf2,RockofStrength,1269879812.0,https://www.reddit.com/r/MachineLearning/comments/k0uf2/in_its_image/,6.0,8.0,"[In Its Image](http://video.google.com/videoplay?docid=-6464697696665901632)

I'm trying to decide if this is for real. The part that really wowed me was at 19:30, as it reminded me of that scene in Transformers where the Mini-con possesses a gadget.

The central idea seems to be some sort of a 'conscious' computer network modeled on the human brain called the ""Creativity Machine"", which works through the strengthening/weakening of synapse-like connections between nodes, using things called ""perceptrons"" (critics) and ""imagitrons"" (idea generators). New ideas are generated by introducing noise into the system.

The machine has general intelligence rather than domain-specific intelligence. For example, it writes music, designs superconductors, invents commercial products, etc.

I failed to find any good discussions on it from Google, and I am not an expert in the field, so I was hoping for some insights. The video is from 2006, which is admittedly a bad sign, and the discussion moves to ""cosmic consciousness""-type stuff towards the end.

Overall, I found the video to be fascinating but extremely vague, and was disappointed by the dearth of available online literature.

EDIT: (from http://newmedia.wikia.com/wiki/Stephen_Thaler)  
&gt;Stephen Thaler, of Imagination Engines, Inc., in St. Charles, MO, is a pioneer in the area of artificial intelligence and the inventor of the Creativity Machine Paradigm. This system consists of an artificial neural network that is perturbed by noise so as to seed the generation of new ideas and strategies. Another neural network acts as a critic selecting good from bad results and steering the perturbed network in the most promising directions. According to Tina Hesman the Creativity Machine was used to design a wealth of commercially available products. She also reports that the device has been mainly used by the US military to design new weapons. Dennis Bushnell, NASA's leading visionary has called the Creativity Machine ""AI's Best Bet"" at creating human to trans-human intelligence in machines.

I guess there is some substance here :)

EDIT 2: [A more balanced article.](http://www.sramanamitra.com/2010/05/26/deal-radar-2010-imagination-engines/)",en
1107772,2011-09-01 10:40:39,artificial,An awesomely weird conversation between two AI Cleverbots - Shareables,k13jk,purno,1311685925.0,https://www.reddit.com/r/artificial/comments/k13jk/an_awesomely_weird_conversation_between_two_ai/,22.0,4.0,,en
1107773,2011-09-01 15:45:46,statistics,Might verifying the theory of astrological influence require a more sophisticated statistical method than searching for similarities between sign types?,k18to,skepticofskeptics,,https://www.reddit.com/r/statistics/comments/k18to/might_verifying_the_theory_of_astrological/,0.0,19.0,,en
1107774,2011-09-01 19:21:56,statistics,"""LSMEANS"" option in SAS proc glm -- is there an equivalent in R?",k1fk3,nucleusaccumbens,1304493444.0,https://www.reddit.com/r/statistics/comments/k1fk3/lsmeans_option_in_sas_proc_glm_is_there_an/,5.0,8.0,"I'm moving from SAS to R and so far I love it, with one fairly major exception:

For unbalanced ANOVAs in SAS, I could specify the LSMEANS option in proc glm followed by my variable names and would get just what I need.

I've done quite a bit of googling and have encountered a few potential solutions for reproducing these results in R, but am still running into dead ends.

Using the car package in R, I request a type-III sum of squares table for car's Anova, but keep getting the error that there are ""aliased coefficients"", which apparently means that my X matrix is singular...even though the same model seems to run in SAS for my friend. Not sure if there is an issue in my code I'm overlooking, or if SAS just handles this kind of thing more naturally via LSMEANS.

I've also seen suggestions to specify:
&gt; options(""contrast""=c(""contr.sum"",""contr.poly""))
then run:
&gt; drop1(model, .~., test=""F"")

this completes an output, but my degrees of freedom for the categorical variables are all wrong (example: ""SchoolID"" has 160 levels, but using ""drop1"" it only shows up as having 131 df instead of 159).

Finally, just for clarification -- this data set (over 7000 observations so there are df to burn) is for an assignment in a stats class, which is why I have a friend's SAS outputs to compare with mine in R.

Any help from those fluent in both SAS and R would be greatly appreciated!",en
1107775,2011-09-02 00:07:48,MachineLearning,"Just a little exposure, if you like vast fields of data  with semantic data in it, you might want to check out /r/SemanticWeb",k1pyu,verifex,1164931980.0,https://www.reddit.com/r/MachineLearning/comments/k1pyu/just_a_little_exposure_if_you_like_vast_fields_of/,17.0,0.0,,en
1107776,2011-09-02 04:13:23,statistics,Power analysis questions,k1xw6,Spamicles,1181078716.0,https://www.reddit.com/r/statistics/comments/k1xw6/power_analysis_questions/,1.0,23.0,"My group would like to compare the protein levels of several different proteins in healthy patients and patients with cancer. I used the freely available GPower software (http://www.psycho.uni-duesseldorf.de/abteilungen/aap/gpower3/download-and-register) to calculate the power using the default parameters:

t tests - Correlation: Point biserial model Analysis: A priori: Compute required sample size
Input: Tail(s) = One Effect size |ρ| = 0.3 α err prob = 0.05 Power (1-β err prob) = 0.95
Output: Noncentrality parameter δ = 3.3133098 Critical t = 1.6589535 Df = 109 Total sample size = 111 Actual power = 0.9503016


This gives me a total sample size of n=111. Does this mean that half of this sample size should be n/2 control patients and the other half should be n/2 patients with cancer, or does this mean that the control and the cancer group should both have 111 patients?
Additionally, we'd like to look at the differences of these protein levels in African American and Caucasian patients. Would I have to compute the power again for this? I'm a little confused if there would even be a control group for this since we just want to compare the differences of two populations. I know how to look for statistically significant differences between two populations (ie looking at cells treated with one chemical vs cells treated with another chemical), but I'm not sure how or if this is relevant when talking about power analysis. Thanks!",en
1107777,2011-09-02 09:32:35,datasets,Tweets2011 Twitter Collection,k27g7,chime,1134104400.0,https://www.reddit.com/r/datasets/comments/k27g7/tweets2011_twitter_collection/,12.0,2.0,,en
1107778,2011-09-02 15:29:56,statistics,Statistics Notes in the British Medical Journal,k2dtj,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/k2dtj/statistics_notes_in_the_british_medical_journal/,18.0,0.0,,en
1107779,2011-09-02 16:06:42,MachineLearning,"Inaugural Data Without Borders Datadive: NYC, Oct. 14-16",k2epa,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/k2epa/inaugural_data_without_borders_datadive_nyc_oct/,22.0,5.0,,en
1107780,2011-09-02 19:58:27,statistics,help on a simple stats question,k2mj7,makingnosmallplan,1292961757.0,https://www.reddit.com/r/statistics/comments/k2mj7/help_on_a_simple_stats_question/,1.0,9.0,"""You are examining three datasets, each consisting of 10 letters mailed on March 9th each year for three years: 2009, 2010, and 2011.""

*What is the N?*

I'm thinking it is either 10, or 30, and that each answer is correct when you clarify that depending on whether you run an analysis on a given year, or for the entire 3 year sample. Am I right?

",en
1107781,2011-09-02 21:48:00,statistics,"/r/statistics, I've seen a lot of posts about programming questions in R lately so I created this subreddit.",k2qns,[deleted],,https://www.reddit.com/r/statistics/comments/k2qns/rstatistics_ive_seen_a_lot_of_posts_about/,1.0,1.0,,en
1107782,2011-09-03 00:32:10,statistics,"Any one know how to do an ""averageifs"" in R",k2wlp,krs28,1241158999.0,https://www.reddit.com/r/statistics/comments/k2wlp/any_one_know_how_to_do_an_averageifs_in_r/,3.0,26.0,"This is something I could do in in about 2 minutes in excel, but my boss wants me to write something in R with which I have only a decent amount of experience.  Below is a simplified example of what Im trying to do.  I would like to be able to average X only when column Y equals 1 and Month =2 (and other variations).  I already have all of the data in a Matrix and I have been able to make the calculations using subsets, 

i.e. 

FebPeak1 &lt;- mean(subset(Profile, Bucket ==1&amp; Month == 2 &amp;  X500  == 1,select=c(capacity)))

but this is terribly inefficient.

I have looked at statmethods and some other sites and I can't seem to find a better solution.  Would anyone be able to point me in the right direction?


| Capacity          | Month      | Y |  N|
|:--------------:|:----------:|:-:|:-:|
|0.847176411	|1		 |0|	1|
|0.615039795	|1		 |0|	0|
|0.120085029	|1		 |1|	1|
|0.116997561	|1		 |1|	1|
|0.774328804	|1		 |0|	0|
|0.360060134	|2		 |1|	1|
|0.376074672	|2		 |0|	0|
|0.366286074	|2		 |0|	0|
|0.302895166	|2		 |1|	1|
|0.728768316	|2		 |1|	1|
|0.058530614	|3		 |0|	0|
|0.75197139	|3		 |0|	1|
|0.765492128	|3		 |0|	0|
|0.399371452	|3		 |1|	1|
|0.576376026	|3		 |1|	1|
|...                    | ...           |..|  ..


Edit: Formatting",en
1107783,2011-09-03 21:33:05,artificial,What kind of jobs are there in A.I?,k3ov9,[deleted],,https://www.reddit.com/r/artificial/comments/k3ov9/what_kind_of_jobs_are_there_in_ai/,20.0,31.0,"I'm interested in working in the A.I department somewhere in the future, but what kind of areas are there? Where is a good place to start looking for what area would be best suited for me? Thanks in advance for any help!",en
1107784,2011-09-03 22:44:03,AskStatistics,Can you explain the why a little better regarding variance and standard deviation for normal distributions?,k3qr8,jason-samfield,1248855567.0,https://www.reddit.com/r/AskStatistics/comments/k3qr8/can_you_explain_the_why_a_little_better_regarding/,3.0,3.0,"Essentially, why is the variance actually the square root of the mean of the squared differences from the mean of the population rather than just the mean of the absolute value of the differences from the mean of the population?

I hope I stated that correctly.

Why ((a-mean)^2 + (b-mean)^2 + (c-mean)^2) / 3) instead of (abs(a-mean) + abs(b-mean) + abs(c-mean) / 3?",en
1107785,2011-09-03 22:48:21,statistics,Evidence in support of the null hypothesis?,k3qvd,swiz0r,1235971035.0,https://www.reddit.com/r/statistics/comments/k3qvd/evidence_in_support_of_the_null_hypothesis/,4.0,11.0,"Quick question:

I have a set of paired data which I suspect came from different distributions.  The null hypothesis, then, is that they didn't.  I can take a paired t-test, and get back a p value, which is a measure of the evidence against that null hypothesis.

A low p-value means that there is a lot of evidence against the null hypothesis, but a high p-value does not mean that there is a lot of evidence *for* the null hypothesis.  How do I measure the evidence for the null hypothesis?

I understand that this is a different question, but how do I find P(null hypothesis | Data)?",en
1107786,2011-09-04 09:52:04,MachineLearning,Cool ideas for a graduate level ML/pattern recognition project?,k46xa,[deleted],,https://www.reddit.com/r/MachineLearning/comments/k46xa/cool_ideas_for_a_graduate_level_mlpattern/,17.0,19.0,"I'm taking a graduate level pattern recognition class and long story short the class changed from a lecture style class to a project class.  This is unfortunate for me as I don't have much experience in ML or pattern recognition and was hoping to learn from the lectures.

What are some good project ideas that are not too difficult to implement/manageable for someone with no experience in ML?  I'm working primarily in Matlab, have a decent handle of linear algebra, probability theory and some statistics.

Edit: even though this is a graduate level class, I should emphasize that I have zero experience in ML.  I'm hoping for absolute beginner type of projects!

Edit 2: Thanks for the ideas so far.  I really liked the LSA suggestion by urish.  If you're following this thread, I've found a pretty simple applied introduction to LSA: http://www.miislita.com/information-retrieval-tutorial/svd-lsi-tutorial-4-lsi-how-to-calculations.html",en
1107787,2011-09-04 15:43:38,statistics,Countries With the Highest Crime Rates - according to Ranker.com,k4bnf,[deleted],,https://www.reddit.com/r/statistics/comments/k4bnf/countries_with_the_highest_crime_rates_according/,0.0,6.0,,en
1107788,2011-09-05 00:40:24,statistics,The most interesting statistician in the world,k4p94,zubrin,1304383407.0,https://www.reddit.com/r/statistics/comments/k4p94/the_most_interesting_statistician_in_the_world/,2.0,0.0,,en
1107789,2011-09-05 05:36:38,statistics,Probit Models,k4xgv,Lord_Talon,1313340629.0,https://www.reddit.com/r/statistics/comments/k4xgv/probit_models/,11.0,11.0,"Can someone explain 'Probit Models' in a way I might understand better? I'm looking at a paper written by Kirk Monteverde where he looks at data from the automobile industry using 'probit techniques' and even after reading up on them, I'm still a little lost.",en
1107790,2011-09-05 07:20:17,computervision,"Transform Invariant Low-Rank Features, or how I learned to stop relying on known patterns and love CVPR.",k50hj,pmugowsky,1286477995.0,https://www.reddit.com/r/computervision/comments/k50hj/transform_invariant_lowrank_features_or_how_i/,13.0,0.0,,en
1107791,2011-09-05 08:34:09,statistics,Are you a kurtosis measures peakedness or heavy tail kind of person?,k52ep,[deleted],,https://www.reddit.com/r/statistics/comments/k52ep/are_you_a_kurtosis_measures_peakedness_or_heavy/,0.0,2.0,,en
1107792,2011-09-05 11:31:54,artificial,Ideas for small AI project?,k568p,the_phet,1297521165.0,https://www.reddit.com/r/artificial/comments/k568p/ideas_for_small_ai_project/,18.0,25.0,"I am a student and I need to do a small project about AI. It is 10 ECTS, which is like 1/3 of a master thesis. I've been thinking for weeks and I cannot come up with a good idea, let's say I am in a wall.

My areas of interest are mainly machine learning, robotics, vision and biology.

I am open to any idea and obviously I will ack you in the report.",en
1107793,2011-09-05 15:57:05,statistics,A question about coherence,k5are,whambamthankyoumam,1287938237.0,https://www.reddit.com/r/statistics/comments/k5are/a_question_about_coherence/,5.0,2.0,"I was reading these papers - 

1. http://www.sciencedirect.com/science/article/pii/S0013469497000667 - Nunez 

2. http://sccn.ucsd.edu/eeglab/download/eeglab_jnm03.pdf - Makeig

They define coherence between signals. I understand the phase synchrony part. But does amplitude coherence exist? Can any formula of coherence tell us how amplitudes are correlated? For ex. if one signal is x, the other is 2x (pair1) and one signal is x, the other is 3x(pair 2), will the coherence of pair 1 signals and pair2 signals be same? 

Sorry if the questions are kind of stupid, but I am trying to understand these things. Thank you for your time.",en
1107794,2011-09-05 19:37:32,statistics,Are you a kurtosis measures peakedness or heavy tail kind of person?,k5gdu,jason-samfield,1248855567.0,https://www.reddit.com/r/statistics/comments/k5gdu/are_you_a_kurtosis_measures_peakedness_or_heavy/,0.0,7.0,,en
1107795,2011-09-05 23:08:24,statistics,"Need help, binomial distribution.  ",k5nb1,dudethechad,1314409210.0,https://www.reddit.com/r/statistics/comments/k5nb1/need_help_binomial_distribution/,0.0,1.0,"Pleas help me, i have no idea how to do this problem. 

In 1998, a baseball player hit 65 home runs, a new major league record. Was this feat as surprising as most of us thought? In the three seasons before 1998, this champion hit a home run in 0.112% of his times at bat. He went to bat 512 times in 1998. His home run count in 512 times at bat has approximately the binomial distribution with n = 512 and p = 0.112.

What is the mean number (±0.001) of home runs he will hit in 512 times at bat?

What is the probability (±0.0001) of 65 or more home runs? (Use the Normal approximation.)   


",en
1107796,2011-09-06 00:44:34,artificial,Artificial human brain to be built on a supercomputer in 10 years,k5qd8,ecatrossi,1315257292.0,https://www.reddit.com/r/artificial/comments/k5qd8/artificial_human_brain_to_be_built_on_a/,0.0,2.0,"Henry Markram: Simulating the Brain — The Next Decisive Years [1/3]
http://www.youtube.com/watch?v=_rPH1Abuu9M

Henry Markram: Simulating the Brain — The Next Decisive Years [2/3]
http://www.youtube.com/watch?v=wDY4cFJauls

Henry Markram: Simulating the Brain — The Next Decisive Years [3/3]
http://www.youtube.com/watch?v=h06lgyES6Oc

Henry Markram, Ph.D., Director of the Blue Brain Project at École Polytechnique Fédérale de Lausanne, speaks at the International Supercomputing Conference 

2011.",en
1107797,2011-09-06 02:25:27,artificial,Jeff Hawkins (author of On Intelligence) on Understanding the Brain,k5tig,[deleted],,https://www.reddit.com/r/artificial/comments/k5tig/jeff_hawkins_author_of_on_intelligence_on/,7.0,4.0,"http://www.youtube.com/watch?v=WE6NnlCkrfM

MIT recently held a symposium on Brain, Minds and Machines as part of their 150th anniversary celebration. Jeff Hawkins was invited to participate on a panel that addressed the question: Is it time to try again to understand the brain and engineer the mind? Jeff presents his answer to this question in this 10-minute video. This talk was given on May 4, 2011.",en
1107798,2011-09-06 06:45:48,datasets,Weblog and Social Media crawler API with free/cheap researcher access...,k61mb,[deleted],,https://www.reddit.com/r/datasets/comments/k61mb/weblog_and_social_media_crawler_api_with/,10.0,0.0,,en
1107799,2011-09-06 06:55:53,MachineLearning,Anyone from Trinidad and Tobago doing this course? ,k61wy,[deleted],,https://www.reddit.com/r/MachineLearning/comments/k61wy/anyone_from_trinidad_and_tobago_doing_this_course/,1.0,0.0,Wanna start a study group?,en
1107800,2011-09-06 10:37:30,statistics,"Hi /r/stats, a question if you will",k67fu,GentLemonArtist,1312106168.0,https://www.reddit.com/r/statistics/comments/k67fu/hi_rstats_a_question_if_you_will/,0.0,1.0,"Caveat: I may have read this on /r/stats and merely forgotten, like an eager puppy.  Hence the self-post.

Situation:  I am going to flip 100 coins.  I win as long as there are cumulatively more heads than tails.  You win as long as the opposite.

Example: HH (im currently winning).  3 more flips HHTTT, now you're winning.  Over the course of 100 flips, how could I model how many 'changes in leadership' I could expect to occur on average? ",en
1107801,2011-09-06 12:04:28,MachineLearning,Philosophy and Machine Learning: abstract of a workshop in the upcoming NIPS2011 ,k68zx,urish,1221689900.0,https://www.reddit.com/r/MachineLearning/comments/k68zx/philosophy_and_machine_learning_abstract_of_a/,8.0,10.0,,en
1107802,2011-09-06 13:02:53,artificial,Survey of early work on automating design,k69xj,kmjn,1292111209.0,https://www.reddit.com/r/artificial/comments/k69xj/survey_of_early_work_on_automating_design/,1.0,0.0,,en
1107803,2011-09-06 20:08:39,artificial,WHAT IS ARTIFICIAL INTELLIGENCE? - John McCarthy,k6ksh,[deleted],,https://www.reddit.com/r/artificial/comments/k6ksh/what_is_artificial_intelligence_john_mccarthy/,20.0,1.0,,en
1107804,2011-09-07 04:20:33,MachineLearning,ML/DM Jobs,k71tm,lol_fps_newbie,1294289929.0,https://www.reddit.com/r/MachineLearning/comments/k71tm/mldm_jobs/,9.0,10.0,"I'm about to graduate soon, and as such I've started searching for jobs. While I've found quite a few places which seem to have nice advertisements for jobs in the States, there seems to be a distinct lack of places to find jobs in Europe. Does anyone know of any websites which collect such data and/or European companies looking to hire Ph.D.s?",en
1107805,2011-09-07 04:58:46,MachineLearning,Getting Started with Latent Dirichlet Allocation using RTextTools + topicmodels,k737i,tymekpavel,1190535699.0,https://www.reddit.com/r/MachineLearning/comments/k737i/getting_started_with_latent_dirichlet_allocation/,5.0,0.0,,en
1107806,2011-09-07 05:49:53,MachineLearning,Automated Design of Trading Strategies: Intro to Automated Design and Demo [Video],k751o,pgroves,1306191110.0,https://www.reddit.com/r/MachineLearning/comments/k751o/automated_design_of_trading_strategies_intro_to/,17.0,4.0,,en
1107807,2011-09-07 12:36:27,artificial,Error estimation from Hidden Markov Model,k7fe3,Dummas,1298123807.0,https://www.reddit.com/r/artificial/comments/k7fe3/error_estimation_from_hidden_markov_model/,13.0,2.0,"Dear reddits,

I'm currently in search for a nice approach in order to derive an error of classification from Hidden Markov Model. Besides simple
    miss_classification
    ------------------------ * 100%
    total_number_of_classes

Is there any more *truth saying* approaches ?

Thank You",en
1107808,2011-09-07 16:51:55,artificial,Do you know any good AI forums?,k7kxl,[deleted],,https://www.reddit.com/r/artificial/comments/k7kxl/do_you_know_any_good_ai_forums/,12.0,5.0,"Please? I hope to find an AI forum

**edit** response seems to be ""no"" which is what I sort of guessed. Ah well",en
1107809,2011-09-07 18:17:05,MachineLearning,Building the largest Chess AI ever,k7nlr,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/k7nlr/building_the_largest_chess_ai_ever/,24.0,4.0,,en
1107810,2011-09-08 03:41:08,statistics,Guys I need your help....I don't know how to do this. ,k8861,iwannabeamagician,1315429112.0,https://www.reddit.com/r/statistics/comments/k8861/guys_i_need_your_helpi_dont_know_how_to_do_this/,0.0,5.0,"Here's the problem:

The Army reports that the distribution of head circumference among soldiers is approximately normal with mean 22.6 inches and standard deviation 1.2 inches. Helmets are mass-produced for all except the smallest 5% and the largest 5% of head sizes. Soldiers in the smallest or largest 5% get custom-made helmets. What head sizes get custom-made helmets?

Head sizes less than BLANK in and head sizes greater than BLANK  in will get the custom-made helmets.


I have no idea where to start or how to go about solving this problem....stat geniuses please help me!",en
1107811,2011-09-08 09:19:38,MachineLearning,Analyzing big data in R (2 presentations from useR! 2011),k8jo0,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/k8jo0/analyzing_big_data_in_r_2_presentations_from_user/,17.0,0.0,,en
1107812,2011-09-08 17:10:59,statistics,Statistics: The “Half-Life” of an Internet Link,k8toa,jeffyablon,1283280097.0,https://www.reddit.com/r/statistics/comments/k8toa/statistics_the_halflife_of_an_internet_link/,1.0,0.0,,en
1107813,2011-09-08 19:46:01,AskStatistics,A good resource for a layman?,k8z3e,nonethewiser,,https://www.reddit.com/r/AskStatistics/comments/k8z3e/a_good_resource_for_a_layman/,3.0,2.0,"I'm starting a job in insurance soon as a programmer, and while I don't need to know most of the statistical aspects of insurance, I've been told that I will pick some of it up on the job and that it will help me determine what someone is really asking for when a report is run.

I have a background in mathematics and CS, but I never liked probability or statistics because I prefer to do things without a calculator.

With that said, I was wondering if any of you could recommend a book or text that I could use to lean some basic college-level statistics. Should I just go to a used book store and pick up a statistics book? Is there a distinction between applied vs theoretical statistics? Is there even such a thing as theoretical statistics?",en
1107814,2011-09-09 00:07:14,statistics,Is there a free version of SAS 9.2? Their website is pretty hard to navigate,k995h,theloniousnole,1315515927.0,https://www.reddit.com/r/statistics/comments/k995h/is_there_a_free_version_of_sas_92_their_website/,12.0,31.0,,en
1107815,2011-09-09 00:19:58,statistics,Master thesis t-test problem,k99mz,oehoe,1309556553.0,https://www.reddit.com/r/statistics/comments/k99mz/master_thesis_ttest_problem/,2.0,13.0,"So i estimated about a hundred betas (and their st. errors etc.) with time-series. That allows me to test the statistical significance of the individual betas, but now i have to test whether the mean of the whole sample of estimated betas is different from zero. How do i do this properly? 

In eviews you can do a simple hypothesis test for the list of betas, but this does not take into account the corresponding standard errors. 

help is much appreciated",en
1107816,2011-09-09 02:29:12,statistics,"R: ""missing value where T/F needed"" error msg",k9ecj,[deleted],,https://www.reddit.com/r/statistics/comments/k9ecj/r_missing_value_where_tf_needed_error_msg/,1.0,3.0,"    &gt; myloop
    function(a,b){
    q=0
    i=1
    w=0
    while (q&lt;1){
    j&lt;-a[i]
    k&lt;-b[i]
    if (j!=k) q = 2
    if (i&gt;4) q = 1
    i = i+1
    }
    if (q==1) w=1
    return(w)}
    &gt; x = myloop(a,b)
Error in if (j != k) q = 2 : missing value where TRUE/FALSE needed
&gt; 
",en
1107817,2011-09-09 05:34:34,statistics,Markov chains help (xposts),k9krx,MyNameCouldntBeAsLon,1311784142.0,https://www.reddit.com/r/statistics/comments/k9krx/markov_chains_help_xposts/,6.0,12.0,"I posted this in r/math, and although this isn't my homework, i got downvoted without reasoning (as of this posting), so I thought I would get better luck here:


I would like to build a Markov Chain with the following characteristics:

P(x(t)=1)=1-p, P(x(t)=0)=p, which in every state t is just a Bernoulli (a success, try again distribution), as you go 'forward' in time (t+1,t+2, ...), the probabilities are adjusted as follows P(x(t+1)=1)=p^2 , P(x(t+1)=0)=(1-p^2 ).

I am currently reading ""A Markov-Binomial Distribution"" by Omey, Santos and van Gulck (2008), and I have a pretty clear idea of the 'behavior' of a chain with these characteristics of the chain I am triying to build. A basic computational approach of the previously stated probabilities shows that at t=6, P(x=1)~1, so this should be a very fairly small chain, with desirable characteristics. Whether the probabilities do not match the convention [P(x(t)=1)=1-p instead of the usual P(x(t)=1)=p], is currently irrelevant, I want the success to be marked when x=1, and the probability of that outcome being ""forced"" by (1-p^2 ).

Ozekici (1997) ""Markov Modulated Bernoulli Process"" discusses a Bernoulli process with ""adaptative"" probabilities, that followMarkov chain's formulations. I am pretty sure this is the equivalent of what I'm looking for. Also, would a 'success' trial in any state 'reset' the probabilities to their original setting ((p,1-p) instead of (p^n , 1 - p^n )?



I x-posted to r/learnmath and r/cheatatmathhomework as well...",en
1107818,2011-09-09 06:28:34,statistics,"Truncated, non-parametric estimation: what would you do?",k9mo0,tel,1145237966.0,https://www.reddit.com/r/statistics/comments/k9mo0/truncated_nonparametric_estimation_what_would_you/,5.0,6.0,"Inspired by [this recent thread](http://www.reddit.com/r/statistics/comments/k8yyr/any_commentssuggestions_on_improving_the/c2ig5o9) here's an interesting problem, inspired by my research.

Suppose I have a graph encoding truncated distances between N objects. The truncation process is known: we don't observe any distances less than a given T. 

Further suppose that these nodes are labeled with M &lt;&lt; N labels inducing natural clusters. If I want to consolidate the graph above such that it now expresses truncated distances between labels, how should I proceed? I want estimates both of the distance between two labels and whether or not that edge should be observed in the consolidated graph.

*Notes*

1. Don't assume a simple distribution of distances between two label clusters: it's not necessarily parametric

2. Because we know the numbers m_i and m_j which are the numbers of nodes having labels i and j, we know how many edge distances were potentially observable (m_i * m_j)

3. Methods don't have to have deep justification, but that sort of defeats the purpose of thinking about it, right?

4. There is a fair amount of noise in the distance estimation such that even two labels which are truly very distance might have one or two low distance edges. You'll have to be robust to that.

*Edit:* This problem has shown up a few times for me recently, but here's a particular example. Pick a vocabulary of words and then hire a bunch of people to pronounce those words for you. Now you have a set of utterances each with the particular word label. Compute the acoustic distances between word utterances (in a very high dimensional space on spectral features) and truncate. This creates the labeled, truncated distance graph.",en
1107819,2011-09-09 19:19:14,statistics,Measure theory for dummies...,ka5w9,GutterBaby69,1309625171.0,https://www.reddit.com/r/statistics/comments/ka5w9/measure_theory_for_dummies/,39.0,2.0,,en
1107820,2011-09-09 19:32:56,statistics,Crime vs. Time - when is it safe to walk the streets of your city?,ka6du,misnamed,1264321314.0,https://www.reddit.com/r/statistics/comments/ka6du/crime_vs_time_when_is_it_safe_to_walk_the_streets/,8.0,3.0,,en
1107821,2011-09-10 04:09:25,statistics,"Reddit, I am having trouble wrapping my head around some lottery odds here. Can someone help me explain this to myself? Mucho thanks!",kaoo6,[deleted],,https://www.reddit.com/r/statistics/comments/kaoo6/reddit_i_am_having_trouble_wrapping_my_head/,3.0,7.0,"[So if you look at this image](http://i.imgur.com/gu81Y.png), it explains the odds of winning a bunch of prizes for the Florida Lottery scratch off. It also shows an overall odds of winning.

[Here is a link to the actual page](http://www.flalottery.com/scrathoff$GamesDetailMain.do?gameNumber=1118)

And here is the data contained in the page: 

Ticket Price: $5.00 
Overall Odds: **1-in-4.03**

Odds of Winning

 	 	$250,000.00    1-in-1,200,000	 
 	 	$5,000.00	    1-in-60,000	 
 	 	$1,000.00      1-in-12,000	 
 	 	$500.00        1-in-4,000	 
 	 	$150.00        1-in-2,400	 
 	 	$100.00        1-in-285.71	 
 	 	$50.00         1-in-240	 
 	 	$30.00         1-in-480	 
 	 	$25.00         1-in-240	 
 	 	$20.00         1-in-60	 
 	 	$15.00         1-in-60	 
 	 	$10.00         1-in-10	 
 	 	$5.00	        1-in-10

---

So my question is, if even at the lowest prize amount, the odds are 1 in 10, how can the overall odds of winning possibly be 1 in 4.03? It seems to me that in order for the overall odds of winning to be 1 in 4.03, at least some of the prize levels would have to have better odds than that in order for the overall to average out. 

Am I wrong? Im not totally math illiterate, but it has been a while since I have messed with it. I figure you guys would be the best people to ask. If you could explain it in lamens terms that would help a lot.

Thanks in advance! ",en
1107822,2011-09-10 16:06:01,datascience,A programmatic data processing API on top of Google Refine,kb3hb,mhermans,1169219262.0,https://www.reddit.com/r/datascience/comments/kb3hb/a_programmatic_data_processing_api_on_top_of/,1.0,0.0,,en
1107823,2011-09-11 09:36:49,statistics,Polynomial Interpolation with R,kbv42,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/kbv42/polynomial_interpolation_with_r/,5.0,0.0,,en
1107824,2011-09-11 19:46:33,statistics,Finding similar sequence in two dimensional data?,kc5l9,[deleted],,https://www.reddit.com/r/statistics/comments/kc5l9/finding_similar_sequence_in_two_dimensional_data/,9.0,4.0,"Forgive me for not knowing how to word this, as I haven't taken a statistics course in 20 years and have forgotten the language and material entirely. As such I'm having trouble even formulating useful search terms to find an answer myself.

Given a sequence of two dimensional data, with (if it matters) one dimension being a measure of performance and the other being time, what kind(s) of analysis would be useful for finding roughly similar patterns of performance over time within other sequences / subsequences? Just the name of something I can look into further would be really helpful.

Thank you for any assistance you can provide getting me pointed in the right direction.",en
1107825,2011-09-12 08:07:08,artificial,Nature Publishing Group announces a ban on human authorship.,kcrsd,wakkaplakka,1294432863.0,https://www.reddit.com/r/artificial/comments/kcrsd/nature_publishing_group_announces_a_ban_on_human/,0.0,0.0,,en
1107826,2011-09-12 11:36:51,statistics,Need help with ANCOVA,kcw54,djent_illini,1312414426.0,https://www.reddit.com/r/statistics/comments/kcw54/need_help_with_ancova/,6.0,9.0,"I am currently working on a research to understand the mating behaviors among Primates in logged and unlogged areas. I have 2 groups in the unlogged areas and 3 groups logged areas.  I have the number of copulations within a time period and the number of days. Since one group's time period is shorter than the rest, I included the number of copulations per day. Also in the data set, each group had different number of males present, for e.g. group 1 has 8 males, group 2 has 4 males, group 3 has 8 males, group 4 has 11 males, and group 5 has 4 males. We suspected that it could have an effect of the number of copulations per day.

I was told to standardize the number of males so I did that in SAS using proc standard command, because we are interested in the difference in the logged and unlogged areas in terms of number of copulations per day.

So since I have a independent categorical variable (area) and continuous variable (standardized number of males), I decided to use ANCOVA. I had the interaction term Area*standardized. Before I ran the test, I did some diagnostics and saw that the distribution for both areas were normal and the qq-plot looked linear. So I continued with the test and removed the interaction term because it was not significant at the 0.05 level.

I ran the test without the interaction and saw the the covariate (standardized number of males) is not significant. So if I remove this covariate term, was doing ANCOVA justified? The slope is not significantly different from zero. I think that variable has no effect on the response variable. All I have in the final model is this

unlogged:
Copulations/day = (Intercept + unloggedestimate)

logged:
Copulations/day = Intercept + logged*estimate (zero) = 0


**TL;DR** So I want to know what to do if the covariate in the ANCOVA is not significant after removing the interaction. What other method is there? Do I use ANOVA??

",en
1107827,2011-09-12 11:54:59,rstats,Need help with ANCOVA,kcwfx,djent_illini,1312414426.0,https://www.reddit.com/r/rstats/comments/kcwfx/need_help_with_ancova/,1.0,1.0,"I am currently working on a research to understand the mating behaviors among Primates in logged and unlogged areas. I have 2 groups in the unlogged areas and 3 groups logged areas. I have the number of copulations within a time period and the number of days. Since one group's time period is shorter than the rest, I included the number of copulations per day. Also in the data set, each group had different number of males present, for e.g. group 1 has 8 males, group 2 has 4 males, group 3 has 8 males, group 4 has 11 males, and group 5 has 4 males. We suspected that it could have an effect of the number of copulations per day.

I was told to standardize the number of males so I did that in SAS using proc standard command, because we are interested in the difference in the logged and unlogged areas in terms of number of copulations per day.

So since I have a independent categorical variable (area) and continuous variable (standardized number of males), I decided to use ANCOVA. I had the interaction term Area*standardized. Before I ran the test, I did some diagnostics and saw that the distribution for both areas were normal and the qq-plot looked linear. So I continued with the test and removed the interaction term because it was not significant at the 0.05 level.

I ran the test without the interaction and saw the the covariate (standardized number of males) is not significant. So if I remove this covariate term, was doing ANCOVA justified? The slope is not significantly different from zero. I think that variable has no effect on the response variable. All I have in the final model is this

unlogged: Copulations/day = (Intercept + unloggedestimate)

logged: Copulations/day = Intercept + logged*estimate (zero) = 0

TL;DR So I want to know what to do if the covariate in the ANCOVA is not significant after removing the interaction. What other method is there? Do I use ANOVA?
",en
1107828,2011-09-12 14:28:35,statistics,Limit behaviour of the mean in a non-identical sequence,kcywu,[deleted],,https://www.reddit.com/r/statistics/comments/kcywu/limit_behaviour_of_the_mean_in_a_nonidentical/,1.0,0.0,"**EDIT:** Oops, turns out you can prove it directly from the limit result, which is straightforward (and should have been obvious, I was thinking about it the wrong way). My bad!


Hi all. This is a homework question, but I'm not looking for anyone to do it for me - just assure me that it can be done! The problem is 2.7.7 in Lehmann's *Elements of Large Sample Theory*. The course is a big step up from previous ones I've taken and I'm essentially taking it as a reading course, so I'm stuck for people to ask.

We are given that Xj (j = 1,...,n) are independent and can take on values ± j with probability 1/2 each. The question asks you to prove a CLT result involving the sample mean (which was easy enough), but then asks you to show that the mean -&gt; ± infinity with probability 1/2 each. This seems to me to be a false statement! My doubts are reinforced by the fact that he provides the hint:
P(Xbar &gt; M) --&gt; 0 for any M&gt;0
which contradicts the statement we are trying to prove!

The reason is seems an unlikely result is because the n'th value is not large enough to 'determine' the mean. In this way, it is different from other situations I've encountered (earlier in this chapter a problem got us to prove that if Xj = ±3^i then the mean --&gt; ±infinity because the n'th value was large enough to determine the sign of the mean. This is certainly not the case here).

The only evidence I have of the result is that the mean is obviously symmetric about zero and has a variance which tends to infinity. But this doesn't imply the result, does it? Am I missing something here?

Any help whatsoever would be very much appreciated!",en
1107829,2011-09-12 20:02:02,statistics,I want to take a thanksgiving vacation where I experience as much rain as possible. What steps would I take to maximize my chances of getting rained on?,kd8ah,daddyjackpot,1151350918.0,https://www.reddit.com/r/statistics/comments/kd8ah/i_want_to_take_a_thanksgiving_vacation_where_i/,0.0,3.0,,en
1107830,2011-09-12 22:00:06,statistics,Data presentation tools?,kdcjj,taciturnbob,1304227801.0,https://www.reddit.com/r/statistics/comments/kdcjj/data_presentation_tools/,6.0,22.0,Anyone know of any neat data presentation tools? I tried looking around for private desktop versions of Gapminder or Google Data Explorer platforms but came up empty.,en
1107831,2011-09-13 00:11:52,artificial,Just a notice about censorship in r/aiclass,kdha9,[deleted],,https://www.reddit.com/r/artificial/comments/kdha9/just_a_notice_about_censorship_in_raiclass/,0.0,2.0,,en
1107832,2011-09-13 08:45:45,datasets,Put the team on my back- NFL dataset 2000-2010,kdy7k,pc1618,1254421610.0,https://www.reddit.com/r/datasets/comments/kdy7k/put_the_team_on_my_back_nfl_dataset_20002010/,12.0,0.0,,en
1107833,2011-09-13 11:28:20,artificial,Do we expect too much from AI too quickly?,ke1ex,Boomdabower,1291980458.0,https://www.reddit.com/r/artificial/comments/ke1ex/do_we_expect_too_much_from_ai_too_quickly/,16.0,47.0,"I heard recently, on a psychology podcast about children's development, a small anecdote about asking children which glass had more liquid, a short stocky glass or a tall thin glass. Apparently the child, if aged younger than 7 (obviously with some variation based on their own rate of development), will answer the tall thin glass because they can not yet reason that although the shorter glass appears smaller it may contain a larger volume.

Now in humans we accept that it takes 7 years for such seemingly trivial skills of logic to develop, yet in AI we expect to develop a system that immediately has such skills. Why do we not allow for such a long developmental period? I regularly hear people complain in AI research about how long their experiments take to show good results, sometimes even up to a week (my own included) but by comparison a child of one week (or perhaps the fairer comparison is a fetus of one week) could not learn such concepts at such a rate. 

TL;DR: Could an AI given even just todays methods and equipment become generally intelligent (hard AI) given a similar 15-20 years that we allow children to become adults and so begin to show signs of complex intelligence?",en
1107834,2011-09-13 17:56:16,MachineLearning,Machine Learning in Science and Engineering,ke99k,manning10,1309041336.0,https://www.reddit.com/r/MachineLearning/comments/ke99k/machine_learning_in_science_and_engineering/,9.0,1.0,,en
1107835,2011-09-13 18:17:01,datasets,New open source tool Superfastmatch finds overlapping text between documents in huge corpora ,ke9ye,kev097,1286915567.0,https://www.reddit.com/r/datasets/comments/ke9ye/new_open_source_tool_superfastmatch_finds/,16.0,0.0,,en
1107836,2011-09-13 19:53:17,statistics,Are you a woman that wants a chance at graduating? Better head to Estonia. ,ked9m,[deleted],,https://www.reddit.com/r/statistics/comments/ked9m/are_you_a_woman_that_wants_a_chance_at_graduating/,0.0,3.0,,en
1107837,2011-09-13 20:38:43,rstats,Is there a gui debugger for R?,keews,SmoothB1983,1314230667.0,https://www.reddit.com/r/rstats/comments/keews/is_there_a_gui_debugger_for_r/,5.0,5.0,"In IDE's for .NET, Java, and many other classical languages we have GUI debuggers. Does such a thing exist for R or is there one being tested? I think such a feature would help some budding R programmers develop into stronger ones. With the recent addition of reference classes this would go a long way to making R more accessible to us Oop folks.",en
1107838,2011-09-13 23:08:28,analytics,HBX inventor &amp; Entrepreneur Blaise Barrelet blogs about how far website analytics has come since HitBox,keka3,kaswing,1289244510.0,https://www.reddit.com/r/analytics/comments/keka3/hbx_inventor_entrepreneur_blaise_barrelet_blogs/,1.0,0.0,,en
1107839,2011-09-14 00:45:55,statistics,Unbiased decision using a biased coin,kens8,statguy,1271026910.0,https://www.reddit.com/r/statistics/comments/kens8/unbiased_decision_using_a_biased_coin/,9.0,6.0,"So this question just popped into my head. Is there is a way to make an unbiased decision that does not assume that a coin is fair? Lets say P(H) = p and P(T) = q , p+q = 1 and p,q need not be 0.5. The objective is to pick a winner between team A and team B with equal probability.

One solution I came up with was to toss the coins twice. The possible out comes and their probabilities would be  
HH = pp  
HT = pq  
TH = qp  
TT = qq  

Now if we define a process where two coins are tossed and if the first is head and second is tail team A wins (HT) and if the first is tail and second is head team B wins (TH). If the head or tail repeats (HH or TT) we discard the toss and toss again. This will of course be very inefficient since we will be on average doing 4 tosses to make 1 decision (this depends on p,q), but the decision is then independent of the distribution from which the toss comes. 

To take it further you can draw a random sample from any distribution and use any arbitrary cutoff to define head or tail and still be able to use this approach to make a fair decision.

What do you guys think of this, you think we should use this to decide which team should bat first or computer simulations which are very sensitive to biases. Is there a more efficient way to do this?

PS: This is my original thought, but its too simple to be new. If there is any article or literature around this please let me know.",en
1107840,2011-09-14 04:34:21,datasets,Australian Public Toilet Locations in XML,kev6o,alphabeat,1189059497.0,https://www.reddit.com/r/datasets/comments/kev6o/australian_public_toilet_locations_in_xml/,13.0,2.0,,en
1107841,2011-09-14 08:10:44,statistics,Assigning values to categorical variables for logistic regression,kf22g,aguyfromucdavis,1279525151.0,https://www.reddit.com/r/statistics/comments/kf22g/assigning_values_to_categorical_variables_for/,5.0,10.0,"Hi all, I'm currently working on a project using logistic regression in R. Most of my independent variables are categorical. For example, one column shows the father's highest educational attainment and those surveyed were given the choices of Junior High, High School Diploma, College/beyond, and Unknown. However, some chose not to answer it and thus my data shows Select (which is the default option for the user to select the education level).

What's the best way to assign values to this variable? I'm thinking something like -1 for Select, 0 for Unknown, 1 for Junior High, so on and so forth. As long as I render this variable as a factor (using the as.factor() function), should it work out fine?

Thanks! :)",en
1107842,2011-09-14 10:39:13,MachineLearning,"
	China Zhongwang Extends Deep Processing Technology and Targets at High-end Industrial Chain
",kf5nn,alcircle,1315985437.0,https://www.reddit.com/r/MachineLearning/comments/kf5nn/china_zhongwang_extends_deep_processing/,0.0,0.0,,en
1107843,2011-09-14 17:26:19,statistics,Simple SPSS logic confuses me. Help,kfdo9,chaoticneutral,1258587986.0,https://www.reddit.com/r/statistics/comments/kfdo9/simple_spss_logic_confuses_me_help/,6.0,4.0,"I am trying to flag survey responses that don't follow a logical pattern for cleaning.

I want my variable ""LogicCheck_1"" to equal ""0"" when, A1 equals 2, 98 or 99 and there is any response in A2.

So create this:
 
IF  (A1= 2 or A1 = 98 or A1 = 99) And (A2 NE $SYSMIS)
LogicCheck_1=0.				
EXECUTE.		
 
 
Edit: 
So for example if A1=2 and A2= 99.. I run this syntax and it does seem to catch this combination. What is going on?",en
1107844,2011-09-14 19:05:44,analytics,"Website Key Performance Indicators, KPIs, You Need to Know",kfgve,Gaylias24,1309551974.0,https://www.reddit.com/r/analytics/comments/kfgve/website_key_performance_indicators_kpis_you_need/,3.0,0.0,,en
1107845,2011-09-14 19:47:37,statistics,Is there any situation where a frequentist approach is preferable to a Bayesian one?,kfib1,MurrayBozinski,1281197925.0,https://www.reddit.com/r/statistics/comments/kfib1/is_there_any_situation_where_a_frequentist/,0.0,17.0,"The Bayesian approach has a lot to go for it:

* it's easier to understand (no slippery p-values and textbooks that look like recipe books)

* philosophically it's more appealing (although it's the engineering approach by Jaynes which really convinced me.)

* it forces you to be explicit about your model and assumptions - better for everyone and science in general.

I appreciate that the frequentist approach was a good shortcut when easy computing and simulation was not available but this only (to my mind) advantage is gone now. As I see it, the frequentists are still in the majority but only due to a generational effect, not to its better value.

So to go back to my question, is there any situation where a frequentist approach is preferable to a Bayesian one? If not, can we please do away with it and stop torturing people with this convoluted way of thinking?

*Bayes: Statistics for human beings!*",en
1107846,2011-09-14 21:46:40,rstats,How can I justify using R to a boss that is used to SAS?,kfmnf,RA_Fisher,1299707119.0,https://www.reddit.com/r/rstats/comments/kfmnf/how_can_i_justify_using_r_to_a_boss_that_is_used/,10.0,14.0,"I'm not really sure where to start.  He's skeptical of this ""free"" statistics software that he has ""never heard of"". Any ideas on where to start?",en
1107847,2011-09-15 00:17:49,datasets,Ask DataSets: Can I get a reddit dataset?,kfs4a,data_vcat,1316034793.0,https://www.reddit.com/r/datasets/comments/kfs4a/ask_datasets_can_i_get_a_reddit_dataset/,21.0,4.0,"Hi Reddit,
  A research project I am currently involved in is in trying to process blogs to mine memes from them. I was wondering if anyone knew if the entire reddit comment/post corpus was available because I figure that a ton of memes come up here? If so, can someone point me to this?",en
1107848,2011-09-15 04:00:15,statistics,Go Figure: Why we think rituals can influence results (or a simple introduction to Type I and Type II errors),kfzet,geek_barbie,1311979289.0,https://www.reddit.com/r/statistics/comments/kfzet/go_figure_why_we_think_rituals_can_influence/,11.0,0.0,,en
1107849,2011-09-15 04:40:38,statistics,"Quick question on a homework problem, both 16 and 17 if you can... thanks in advanced r/statistics!",kg0r5,[deleted],,https://www.reddit.com/r/statistics/comments/kg0r5/quick_question_on_a_homework_problem_both_16_and/,0.0,4.0,,en
1107850,2011-09-15 08:50:58,MachineLearning,Structured prediction problems,kg8n9,Howarth,1230851204.0,https://www.reddit.com/r/MachineLearning/comments/kg8n9/structured_prediction_problems/,11.0,3.0,"Hello all, I'm currently in a structured prediction class and am interested in seeing what problems SP can solve. All the examples of SP I've seen in class so far are NLP problems",en
1107851,2011-09-15 08:51:16,artificial,Nostradamus in silico: can supercomputers really predict revolutions?,kg8nm,jason-samfield,1248855567.0,https://www.reddit.com/r/artificial/comments/kg8nm/nostradamus_in_silico_can_supercomputers_really/,9.0,0.0,,en
1107852,2011-09-15 14:11:35,statistics,GAE Bingo the A/B testing software lets you pick a winner ,kgeic,[deleted],,https://www.reddit.com/r/statistics/comments/kgeic/gae_bingo_the_ab_testing_software_lets_you_pick_a/,1.0,0.0,,en
1107853,2011-09-15 21:54:28,statistics,First-time poster with a question on stats.,kgsh2,[deleted],,https://www.reddit.com/r/statistics/comments/kgsh2/firsttime_poster_with_a_question_on_stats/,4.0,5.0,Does anyone have a list of things to look for in a study that will show they used good methods and are not just using stats to support their bias?,en
1107854,2011-09-15 23:18:41,statistics,Shortest paths to/from nodes of a certain type,kgvie,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/kgvie/shortest_paths_tofrom_nodes_of_a_certain_type/,2.0,0.0,,en
1107855,2011-09-16 00:28:17,computervision,Resources for Self-guided Study in Computer Vision,kgy0t,colincsl,1210302386.0,https://www.reddit.com/r/computervision/comments/kgy0t/resources_for_selfguided_study_in_computer_vision/,23.0,1.0,,en
1107856,2011-09-16 02:55:27,rstats,IAE chronically confused by the abundance of subtly different data structures in R?,kh2u4,stetson9,1212548163.0,https://www.reddit.com/r/rstats/comments/kh2u4/iae_chronically_confused_by_the_abundance_of/,9.0,6.0,"I'm a fairly competent programmer, in R as well as a few other languages.  I learned R in a class.  I've been using it pretty intensively for nearly two years now.  But I'm still continuously hung up by the differences between objects, lists, data frames, vectors, matrices, arrays.  I'd guess 80% of the bugs in my scripts are because I did something like

    mydata[i]

instead of

    mydata[[i]]

or used

    matrix(x)

instead of

    as.matrix(x).

and something down the line needed the data in a different structure.  I assume all these subtleties have some reason for existing...right?  Can anyone enlighten me?",en
1107857,2011-09-16 03:29:04,statistics,Can the standard deviation be based on the median?,kh3wq,bucknuggets,1245166255.0,https://www.reddit.com/r/statistics/comments/kh3wq/can_the_standard_deviation_be_based_on_the_median/,2.0,9.0,"I've got a great deal of sensors that generate data automatically.  Some generate a lot of data, others very little.  And the data varies considerably over time.    My job is to figure out which ones are broken - and only barely limping along.

I can identify the daily mean count of events over a period of 90 days and easily identify all of those sensors that produced less than three standard deviations below the mean on any given day.   And this does an ok job.  Except the mean is heavily influenced by unusual high-volume, bursty, days.  I'd like to eliminate this problem.

Can you help me understand why nobody seems to use standard deviations with medians?  It seems like the logical next step to improve the accuracy of my numbers.  ",en
1107858,2011-09-16 06:28:03,MachineLearning,Is it possible to feed an AI like Cleverbot autobiographies or literary works of people and have that person emulated through the program?,kh9bj,[deleted],,https://www.reddit.com/r/MachineLearning/comments/kh9bj/is_it_possible_to_feed_an_ai_like_cleverbot/,3.0,4.0,"Just had this thought, not sure if this is the right place. I was just imagining if all the writings of a philosopher were input it would very interesting to have a conversation with the AI as if it were that person. And how it would differ for different people",en
1107859,2011-09-16 13:29:37,statistics,"Using an ANCOVA for two continuous independent variables, instead of splitting the participants into four groups and using contrasts",khidr,Calpa,1255275322.0,https://www.reddit.com/r/statistics/comments/khidr/using_an_ancova_for_two_continuous_independent/,2.0,7.0,"I have a question about a specific analysis. I have one continuous dependent variable, and two continuous independent variables. I want to test for main effects and an interaction effect. 

Someone else did this by splitting the participants into four groups (high on both scales, high on one and low on the other and vice versa, en low on both scales) and used a factorial analysis with contrasts to test for main effects and an interaction.

I'm wondering though why you can't just do a univariate analysis with the two IV's as covariates, and specify (in this case in SPSS) you want to test for main effects and an interaction between the two IV's.   

First time I took the time to try and type out a coherent question here, so I hope it's clear enough.. *So in short I'm not sure about the difference between these two methods when dealing with one DV, two IV's and you're testing for main effects and an interaction.* ",en
1107860,2011-09-16 14:22:22,MachineLearning,Tutorial: Introduction to Bandits,khj73,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/khj73/tutorial_introduction_to_bandits/,10.0,0.0,,en
1107861,2011-09-16 19:46:41,statistics,"As we suspected, people would rather go to a comedy night than a club night. Email statistics from the live events industry.",khsl3,megfitz,1267637475.0,https://www.reddit.com/r/statistics/comments/khsl3/as_we_suspected_people_would_rather_go_to_a/,1.0,0.0,,en
1107862,2011-09-16 22:25:11,statistics,An infographic of the most popular infographics on the internet,khyjj,thegabeman,1239690590.0,https://www.reddit.com/r/statistics/comments/khyjj/an_infographic_of_the_most_popular_infographics/,21.0,3.0,,en
1107863,2011-09-16 22:53:52,MachineLearning,Lawyers Object As Computer Program Does Job Better  | Singularity Hub,khzlx,drcross,1239806264.0,https://www.reddit.com/r/MachineLearning/comments/khzlx/lawyers_object_as_computer_program_does_job/,47.0,0.0,,en
1107864,2011-09-17 01:25:23,MachineLearning,To: Machine Learning Researchers Interested in Social Science Applications (NIPS Workshop),ki4oo,nipcssworkshop,1316211581.0,https://www.reddit.com/r/MachineLearning/comments/ki4oo/to_machine_learning_researchers_interested_in/,3.0,0.0,,en
1107865,2011-09-17 03:03:06,AskStatistics,What are good intermediate books on statistics?,ki7iw,godless_communism,1249752556.0,https://www.reddit.com/r/AskStatistics/comments/ki7iw/what_are_good_intermediate_books_on_statistics/,3.0,6.0,"I love statistics!  But most books I've seen in bookstores and even on-line are primarily designed for beginners.  In fact, it mostly seems that they're designed to be extra-gentle for newbies.  I have no arguments with being gentle, but it's really hard to find a book that covers the next steps.  Most introductory books end at multi-linear regression or ANOVA and don't go farther.  Are there any good intermediate books on statistics (even gentle ones)?",en
1107866,2011-09-17 05:17:37,statistics,Bias-Variance Tradeoff,kib5l,Fordperfect42,1300552031.0,https://www.reddit.com/r/statistics/comments/kib5l/biasvariance_tradeoff/,3.0,10.0,"I've been reading papers and The Elements of Statistical Learning - Hastie. I still can't wrap my head around it which I'm finding very frustrating at this point. I understand the basics of it but don't feel I have a strong grasp on it. 

If I was looking at the Elastic Net and Lasso, would the measurement be the same? How would it affect these two differently? 

If anyone has any papers or resources that I should read, would be greatly appreciated. ",en
1107867,2011-09-17 05:48:22,MachineLearning,Support Vector Machine with GPU,kibxi,gaussin,1316225103.0,https://www.reddit.com/r/MachineLearning/comments/kibxi/support_vector_machine_with_gpu/,6.0,0.0,,en
1107868,2011-09-17 07:11:45,MachineLearning,I will be doing my Master's thesis on a topic in Machine Learning. Where do I start? ,kie4u,marshmallowsOnFire,1311921521.0,https://www.reddit.com/r/MachineLearning/comments/kie4u/i_will_be_doing_my_masters_thesis_on_a_topic_in/,3.0,17.0,"I am working on identifying certain features in certain images. I know I need to train the computer on a data set and then proceed with using the larger data set. But I have not done any courses in Machine Learning. I looked through some FAQs here, but it seems like such an overwhelming field. Is there one book I can learn the basics from? I don't want to be going about without a proper direction. thank you so much!  ",en
1107869,2011-09-17 09:25:56,statistics,BBC - Podcasts - More or Less: Behind the Stats,kih35,mnm9999,1286957126.0,https://www.reddit.com/r/statistics/comments/kih35/bbc_podcasts_more_or_less_behind_the_stats/,9.0,0.0,,en
1107870,2011-09-17 13:38:24,artificial,Physics major going into AI: any good papers you would recommend?,kikr0,AnimalRidens,1275165744.0,https://www.reddit.com/r/artificial/comments/kikr0/physics_major_going_into_ai_any_good_papers_you/,18.0,9.0,"I am about to complete an undergraduate degree in theoretical physics, but I would like to continue my studies in the field of artificial intelligence.

I have some background in theoretical computer science, solid mathematical skills, but I have never had a formal education in the field.

Are there any good papers about AI that you would recommend for a ""beginner""? I want to get a good feel of the subject and the kind of research that is done in the field before I commit myself to it.

For instance, I recently came across this blog post: [10 Technical Papers Every Programmer Should Read (At Least Twice)](http://blog.fogus.me/2011/09/08/10-technical-papers-every-programmer-should-read-at-least-twice/). It's been an extremely interesting read and I've learned a lot, I was hoping to do the same for AI.

Thanks!",en
1107871,2011-09-17 15:57:31,computervision,Best way to detect hand features?,kimif,Ilidur,1311176017.0,https://www.reddit.com/r/computervision/comments/kimif/best_way_to_detect_hand_features/,4.0,2.0,"I'm doing virtual button press detection using cv and I need to detect features such as the tip of the finger, and even multiple fingers. I was looking at www.andol.info and he was using a convex hull technique to do his detection but I don't understand how it actually works.
(I understand how it should work but I can't find some good references for his algorithms)

I'd be really grateful for some help.",en
1107872,2011-09-17 19:17:43,statistics,Quick question about which statistical test to use.,kiqiw,[deleted],,https://www.reddit.com/r/statistics/comments/kiqiw/quick_question_about_which_statistical_test_to_use/,2.0,17.0,"Hi r/statistics.  I was wondering whether someone could impart their wisdom to me.  I have a very basic grasp of statistics and wasn't sure what test to use to see if my results are significant or not.

What I am looking at is whether hospital patients going to the ED have to wait longer for analgesia depending on their age.  

N=48.  
27 are less than 84 years old and they wait an average of 106 minutes.
21 are 85+ and they wait an average of 136 minutes.  

How would I test if that 30 minute difference is 'statistically significant' (i doubt it is due to the small sample size but a confirmation would be ideal.)",en
1107873,2011-09-18 11:11:18,datasets,The Data Hub by Comprehensive Knowledge Archive Network,kjdrq,utcursch,1175517803.0,https://www.reddit.com/r/datasets/comments/kjdrq/the_data_hub_by_comprehensive_knowledge_archive/,8.0,0.0,,en
1107874,2011-09-18 12:49:21,MachineLearning,Is machine learning a saturated field? ,kjf1r,marshmallowsOnFire,1311921521.0,https://www.reddit.com/r/MachineLearning/comments/kjf1r/is_machine_learning_a_saturated_field/,11.0,29.0,"I had a friend tell me that everything that could have been done in ML has been done; that there are libraries in OpenCV that do EVERYTHING one could possibly do with an image. Is it true? Have we now reached a stage where any 'innovation' would mean assembling together already existing stuff and making cooler stuff? 


EDIT: Thank you EVERYBODY for giving me an idea of the real picture. Must go and tell that idiot how wrong he is. ",en
1107875,2011-09-18 14:55:31,MachineLearning,Learn Python The Hard Way,kjgi3,prodatalab,1315999416.0,https://www.reddit.com/r/MachineLearning/comments/kjgi3/learn_python_the_hard_way/,0.0,0.0,,en
1107876,2011-09-18 22:21:29,statistics,Trying to parse the effects of two highly collinear variables,kjpzp,casualfactors,1285978895.0,https://www.reddit.com/r/statistics/comments/kjpzp/trying_to_parse_the_effects_of_two_highly/,5.0,3.0,"Running a simple linear regression trying to reveal, among other things, the contribution of race to voting behavior. So my independent variables include the following electorate data: % of Precinct that's Black, % of Precinct that turned out to vote, and a control for Precinct population. The dep is Democratic share of the 2way.

Send off the regression output. Client comes back and says, ""Okay, but what's the effect of being *white?*"" I'm trying to point out that this is a useless piece of data because ""% white"" is basically ""1 - %Black"" but I want to give him something. Any ideas on how to parse these two?",en
1107877,2011-09-19 09:30:19,datasets,Mining of Massive Datasets,kk8yb,utcursch,1175517803.0,https://www.reddit.com/r/datasets/comments/kk8yb/mining_of_massive_datasets/,22.0,0.0,,en
1107878,2011-09-19 14:08:12,statistics,Familywise error rate,kkdjy,MD89,1316429577.0,https://www.reddit.com/r/statistics/comments/kkdjy/familywise_error_rate/,0.0,6.0,"I have a question about the calculation of the familywise error rate by using the formula: 1 – (.95)n. My question concerns a specific example.

That is: I have two categorical independent variables, one covariate and one dependent variable. The following statistical tests are conducted: 

1) A 2 x 3 ANCOVA, to test the significance of both main effects and the interaction between the two IV's
 
2) 6 seperate t-tests to further examine the above mentioned main effects and interaction.

My question is: What is ""n"" in the above mentioned formula? Can I count the two main effect and the interactions as separate comparisons, which leads to a total n of: 3 (2 main effects + interaction) + 6 (separate t-tests) = 9? Or should I only look at the separate t-tests (thus n=6).

This is the first time I post a question here, so I hope I make sense. ",en
1107879,2011-09-19 18:00:40,statistics,The special trick that helps identify dodgy stats,kkisk,UOAH,1310786547.0,https://www.reddit.com/r/statistics/comments/kkisk/the_special_trick_that_helps_identify_dodgy_stats/,25.0,1.0,,en
1107880,2011-09-19 20:00:54,statistics,Grab the most recent FBI crime data for yourself (third link down),kkmrm,therealprotonk,,https://www.reddit.com/r/statistics/comments/kkmrm/grab_the_most_recent_fbi_crime_data_for_yourself/,7.0,0.0,,en
1107881,2011-09-19 21:28:28,statistics,The most random dataset I've ever encountered in an R package.,kkpt0,mathguymike,1289137122.0,https://www.reddit.com/r/statistics/comments/kkpt0/the_most_random_dataset_ive_ever_encountered_in/,74.0,9.0,,en
1107882,2011-09-20 00:50:46,computervision,Best stereo correspondence algorithm?,kkwvg,rsaborio,1154538563.0,https://www.reddit.com/r/computervision/comments/kkwvg/best_stereo_correspondence_algorithm/,1.0,0.0,"I'm doing some research about stereo correspondence and I would like to know ""how best"" is the best stereo correspondence algorithm (for stereo vision).
I'd be really grateful for some article references if you have them.",en
1107883,2011-09-20 00:53:27,statistics,What are you calling terrorism?,kkwz3,[deleted],,https://www.reddit.com/r/statistics/comments/kkwz3/what_are_you_calling_terrorism/,0.0,0.0,,en
1107884,2011-09-20 01:03:01,computervision,Best dense stereo correspondence algorithm?,kkx9z,rsaborio,1154538563.0,https://www.reddit.com/r/computervision/comments/kkx9z/best_dense_stereo_correspondence_algorithm/,11.0,5.0,"I'm doing some research about dense stereo correspondence (try to match as many pixels as possible) and I would like to know what algorithm is the state of the art in solving this.
I'd be really grateful for some article references if you have them.",en
1107885,2011-09-20 05:19:56,MachineLearning,Sho: data-analysis library from Microsoft Research,kl5i8,unquietwiki,1263108455.0,https://www.reddit.com/r/MachineLearning/comments/kl5i8/sho_dataanalysis_library_from_microsoft_research/,3.0,1.0,,en
1107886,2011-09-20 05:45:02,statistics,Conditional Probability Proof Help?,kl6au,[deleted],,https://www.reddit.com/r/statistics/comments/kl6au/conditional_probability_proof_help/,2.0,4.0,"I was wondering if anyone had any thoughts on how to prove this:

P(M|R) = 1 - P(COMPLEMENT[M]|R)

My initial thoughts were:  
P(M|R) = P(M and R)/P(R)  
= (1 - COMPLEMENT[P(M and R)])/P(R)  
= (1 - P(COMPLEMENT[M] or COMPLEMENT[R]))/P(R)  
= (1 - P(COMPLEMENT[M] - P(COMPLEMENT[R]) + P(COMPLEMENT[M] and COMPLEMENT[R]))/P(R)  


  Then I kinda just... cannot figure it out.",en
1107887,2011-09-20 15:02:46,artificial,Which AI technique to use for picking the most effective performance measure black-box?,kli7s,keije,1313364927.0,https://www.reddit.com/r/artificial/comments/kli7s/which_ai_technique_to_use_for_picking_the_most/,10.0,7.0,"I am faced with a problem, where I have a known algorithm that processes input data, it has a known objective function and it can be guided by multiple rules. The rules are black-boxes of the system. The rules are consistent, so black-box-1 will always be the same rule, the rule is some mathematical function (unknown to me, nor do I care to find out), and given the same input, the rule always returns the same result. The objective function can only be evaluated some time after the algorithm has made a decision based on the rule, so GA algorithms are a poor fit, since they require immediate comparison of fitness of the different decisions/chromosomes.

What I wish to achieve is to use some AI technique (Neural Networks seem most appropriate, but I'm no expert) to find out based on input-data, which rule should be guiding the algorithm to maximize the objective function. It needs to both pick the rule, and pick the meaning of that rule (i.e. I do not know beforehand if higher or lower values are better as output of black-box-1)

The more immediate interest is to have a system that based on input data of the algorithms would know which is the best rule to use and how to interpret it's result (is higher or lower rule values better), next step would be trying to see if a combination of rules would be better overall.

I am working with C#, but can also rely on C++, Matlab or R for the purposes of building this system. I just do not wish to start from scratch, I'm probably not the first one to want something of the sort.

Any direction you can give me would be greatly appreciated. I'm specifically looking for practical examples (and frameworks/libraries to use), not so much theoretical/mathematical papers (of which I've read many, but I do not wish to re-invent square wheels trying to build what they describe in theory).

**/EDIT: I'll try to provide a better example here of how the whole thing is supposed to work:**

There is an algorithm that makes Start and Stop decisions. There are certain statistical/algorithmic/heuristic (Rules) measures that can tell the algorithm how confident it should be in its decision. Which rule is more effective depends on input data, in a non-trivial way (i.e. I can't just eye-ball it or express it as a simple if/else check), but the difference is significant so I am motivated to pick the best (or at least a better) rule.

The problem is that Start and Stop decisions can be separated by a rather significant time interval, and the quality (objective function) of the overall decision pair can only be evaluated at Stop. So GA algorithm is not useful, because it picks parents to create offspring based on their fitness and the algorithm may decide to crate hundreds of Start points before it picks a Stop point.

In other words my Fitness function can only be used to verify fit, not to guide variable or rule selection.


*Thanks /r/tificial*",en
1107888,2011-09-20 15:47:29,datasets,CC-licensed data dump of the user-generated content on Stack Overflow and other Stack Exchange sites,klj3j,utcursch,1175517803.0,https://www.reddit.com/r/datasets/comments/klj3j/cclicensed_data_dump_of_the_usergenerated_content/,14.0,0.0,,en
1107889,2011-09-20 17:24:19,statistics,What distribution is this?,kllq8,evilmaus,1278545501.0,https://www.reddit.com/r/statistics/comments/kllq8/what_distribution_is_this/,9.0,7.0,"Imagine an urn containing a finite but unknown number of tokens.  Of those tokens, there is a finite but unknown number of red tokens.  Each time you take a sample, you remove the red tokens and replace the rest.  Each sample is much like a Poisson sample, wherein it is more of the number of red tokens discovered in a continuous interval.  What distribution out there most closely models either the number of red tokens discovered or the number of red tokens remaining over time?

I do have a specific case that I'd like to model, but don't necessarily want to contaminate thinking with any preconceived associations or conceptions of the problem.  

[The Specific Case](/s ""I'm trying to model software defects.  There is a finite but unknowable quantity that is slowly discovered over time.  We can theoretically never find and remove them all, since they also become ever more difficult to find, but it'd be nice to be able to estimate a remaining defect level and to show a cost/benefit trade-off in continuing to test."")

Thanks!",en
1107890,2011-09-20 18:09:02,statistics,Three Must-Have Books on Data Visualization (Updated 2011-09-11) » Borasky Research Journal,kln2r,[deleted],,https://www.reddit.com/r/statistics/comments/kln2r/three_musthave_books_on_data_visualization/,9.0,0.0,,en
1107891,2011-09-20 20:46:49,statistics,Q: How does someone become a data scientist? - Forbes,klshm,[deleted],,https://www.reddit.com/r/statistics/comments/klshm/q_how_does_someone_become_a_data_scientist_forbes/,14.0,12.0,,en
1107892,2011-09-20 21:09:21,computervision,"Good Review of Convex Optimization (broadly applicable to CV algorithms, particularly model-fitting, image registration, etc.)",klt95,pmugowsky,1286477995.0,https://www.reddit.com/r/computervision/comments/klt95/good_review_of_convex_optimization_broadly/,5.0,0.0,,en
1107893,2011-09-20 21:27:58,MachineLearning,On cluster validation,kltwu,mosavian,1316543161.0,https://www.reddit.com/r/MachineLearning/comments/kltwu/on_cluster_validation/,8.0,4.0,,en
1107894,2011-09-20 23:20:39,statistics,Looking for an Applied Statistics webcast,klxud,SarenAid,1297792031.0,https://www.reddit.com/r/statistics/comments/klxud/looking_for_an_applied_statistics_webcast/,9.0,2.0,"I would like to find a webcast course that mirrors this one offered by MIT:

http://ocw.mit.edu/courses/sloan-school-of-management/15-075-applied-statistics-spring-2003/

Unfortunately, this course doesn't offer a lecture series online.

I've checked out the webcasts offered by Berkley but have not found an applied stats course. Ideally, I would like to find a course that covers the theories and components of simple and multiple linear regression and ANOVA while using SAS or SPLUS.

I do not have a background in Math or Statistics but am working in the Biostatisics field and have some catching up to do. Any resources would be very helpful. ",en
1107895,2011-09-21 00:58:12,AskStatistics,Possible to differentiate set of random numbers from set of psuedorandom numbers through statistics?,km1bo,Mason11987,1239548171.0,https://www.reddit.com/r/AskStatistics/comments/km1bo/possible_to_differentiate_set_of_random_numbers/,3.0,6.0,"Cross post from my other question [here](http://www.reddit.com/r/askscience/comments/klk52/what_is_random_with_respect_to_the_physical_world/).

It may be that there is such a thing as truly random acts, in physics a good candidate might be nuclear decay.  If this is truly random, and hooked up to a computer to pump out numbers from 0-1, would there be something fundamentally and provably different from that set then say a set of numbers from a computers psuedo random number generator, which are necessarily deterministic but at a shallow glance *look* random?
",en
1107896,2011-09-21 01:40:05,computervision,My MS research project: Detecting 'V' shaped structures within an image taken from a microscope. What are good algorithms/strategies for implementing functionality besides Hough Transform?,km2sd,[deleted],,https://www.reddit.com/r/computervision/comments/km2sd/my_ms_research_project_detecting_v_shaped/,1.0,0.0,"I have just started research into this topic about three weeks ago and my instructor has told me to research the traditional, generalized, and randomized Hough Transform. I have started to understand these algorithms fairly well, and was wondering what other algorithms/strategies I could benefit from understanding as well. This is my first semester really looking into and researching image detection, and all advice would be helpful.",en
1107897,2011-09-21 04:18:58,MachineLearning,Mining of Massive Datasets,km87q,requiemderseele,1262134570.0,https://www.reddit.com/r/MachineLearning/comments/km87q/mining_of_massive_datasets/,8.0,0.0,,en
1107898,2011-09-21 05:07:19,artificial,practical scaling in indexing and re-indexing multi-document algorithms (i.e. TF-IDF)?,km9uf,unstoppable-force,1297432953.0,https://www.reddit.com/r/artificial/comments/km9uf/practical_scaling_in_indexing_and_reindexing/,2.0,1.0,"i've been researching and developing some NLP, and I'm interested in finding out what methods of dataset organization/management scale.  every resource i find generally only talks about high level math (matrices or vectors) if at all.  i have solutions for the following questions, but in some cases it's easy to see in a simple glance that the method won't scale well as the document count goes up.

1) how do you organize your document/corpus spaces?  do you use separate files for each document or a database (if so, which db engine)?  where/how do you store tags (i.e. NER/POS)?

2) let's say you've just run a multi-document algorithm (i.e. TF-IDF) on one space of x documents.  now you increase the document-space by y.  obviously, you don't have to mine the documents in x again, but what do you do now that the analytical value is no longer correct?  do you re-index the analytical algorithm over the entire space (x + y), or is there a less computationally intensive method?

3) as your document space gets large, meta-tables (i.e. n-grams) explode in size.  even if you prune stop-words and low occurrence phrases, you can still get into billions of phrases in a heartbeat.  what do you use to determine your ""low occurrence"" threshold?  when the table starts to get massive, what do you do to help it scale besides simply using a faster server?  what are some best practices for tabling this data?  is there any kind of sharding software designed for distributing load at this level?",en
1107899,2011-09-21 05:24:16,statistics,Pattern Recognition (Bayesian Statistics) :D Perhaps someone could help! ,kmaed,SeaworthXIII,1310410276.0,https://www.reddit.com/r/statistics/comments/kmaed/pattern_recognition_bayesian_statistics_d_perhaps/,1.0,5.0,":D Greetings fellow Redditors!

I am hoping one or many of you guys could help my friend with a statistics question he has for his Patter Recognition Course. He is a great friend and is always helping me out - so I told him I would come to Reddit to see if anyone could help.

:B I would be very thankful and appreciative for your time, patience, and knowledge.  This community is amazing so I have my hopes up.

The question is: 

Its on bayesian statistics, specifically error.
  So the question first states that Total error = the integral from  negative infinity to infinity of P(error|x)*p(x)*dx
  The questions asks to show that for arbitrary densities, you can replace P(error|x) with 2*P(w1|x)*P(w2|x) in the integral and receive an upper bound on the full error.
__________________________________________________________

That looks like a foreign language haha - once again any help would be awesome.  Feel free to respond or message me.  Thanks for taking a look! 
 ",en
1107900,2011-09-21 07:35:59,AskStatistics,iPad *significantly* better at transference learning than pen-and-paper?,kmem0,Imxset21,1297473272.0,https://www.reddit.com/r/AskStatistics/comments/kmem0/ipad_significantly_better_at_transference/,1.0,2.0,"So I was on Google Reader and read this result of a ""study"" about how iPads [""students who annotated text on their iPads scored 25% higher on questions regarding information transfer than their paper-based peers.""](http://www.appleinsider.com/articles/11/09/19/university_study_finds_students_with_apples_ipad_perform_better.html).

I was curious to see if this meant anything, so I Google Scholar'd the paper, but couldn't find anything published. I did find a .pdf off the school's website, with this [graph on page 29 relating the transference scores](http://www.acu.edu/technology/mobilelearning/documents/research/effects-of-technology-on-learning.pdf).

I couldn't find any correlation analysis or significance value.

Is this finding statistically significant, or is this paper bullpoo?
",en
1107901,2011-09-21 08:46:37,artificial,"Google uploaded a bunch of talks from ""The Fourth Conference on Artificial General Intelligence""",kmgk4,pastr,1196202081.0,https://www.reddit.com/r/artificial/comments/kmgk4/google_uploaded_a_bunch_of_talks_from_the_fourth/,49.0,7.0,,en
1107902,2011-09-21 11:35:34,MachineLearning,Site showing Stages of Data Classification / Machine Learning,kmjy7,teedex,1289547489.0,https://www.reddit.com/r/MachineLearning/comments/kmjy7/site_showing_stages_of_data_classification/,6.0,2.0,"Recently (~3 months ago) I had come across a blog post/ website where the author had neatly presented the different stages of data classification, each stage in a column with information on that stage. 

I seem to have lost the site, does anyone else know what I might be talking about or have come across something like this?

I believe the site was light blue and white (reddit like) in color scheme and had a neat (js+css) effect of showing more information on a stage on mouse rollover ...  ",en
1107903,2011-09-21 17:41:51,MachineLearning,Machine Learning Generative and Discriminative Models,kmred,sm1000,1312473200.0,https://www.reddit.com/r/MachineLearning/comments/kmred/machine_learning_generative_and_discriminative/,1.0,0.0,,en
1107904,2011-09-21 19:02:27,statistics,Standard Deviation of the Mean - How can these be the same?,kmu5w,[deleted],,https://www.reddit.com/r/statistics/comments/kmu5w/standard_deviation_of_the_mean_how_can_these_be/,0.0,10.0,"Example 1:

1. 5000ppm ; Std. dev. of **50ppm**

2. 6000ppm ; Std. dev. of **60ppm**

Mean = 5500ppm ; **Std. dev. of 707.1**

---

Example 2:

1. 5000ppm ; Std. dev. of **500ppm**

2. 6000ppm ; Std. dev. of **600ppm**

Mean = 5500ppm ; **Std. dev. of 707.1**

---

The std. dev. of both means are the same. Is this correct, or do I need to account for the std. dev. of the data set?",en
1107905,2011-09-21 19:04:34,statistics,Wikiversity entry on SAS programming is coming together nicely. Needs more contributions!,kmu8l,[deleted],,https://www.reddit.com/r/statistics/comments/kmu8l/wikiversity_entry_on_sas_programming_is_coming/,10.0,17.0,,en
1107906,2011-09-21 22:46:02,statistics,Why define standard deviations in terms of the Euclidean metric?,kn2de,[deleted],,https://www.reddit.com/r/statistics/comments/kn2de/why_define_standard_deviations_in_terms_of_the/,1.0,0.0,"Today after years of wondering why we square the distance to the mean in the process of finding standard deviations, I realized that if you have n data points finding the variance is equivalent to finding the distance between two vectors in    $R^n$, namely     $(Y_1 , Y_2, ... , Y_n)$ and     $(\bar(Y), \bar(Y), ... , \bar(Y))$.

So this variance thing implicitly chooses Euclidean distance over infinitely many other choices.  I'm not saying Euclidean is a bad choice, I'm just saying that in particular for the subject of regression analysis, maybe we can get a better fit if we use a different metric for measuring variance.

Anyone know anything about this for sure?",en
1107907,2011-09-22 00:08:26,statistics,"/r/statistics, can you help me come up with some creative queries on my data?",kn5iy,shnuffy,1262802406.0,https://www.reddit.com/r/statistics/comments/kn5iy/rstatistics_can_you_help_me_come_up_with_some/,1.0,2.0,"I hope I have come to right place!

I have a dataset *logs*. *logs* contains server download logs tracking millions of downloads of forms by customers. 

For example, *logs* could contain the following rows:

* 2009 form1 customer1
* 2009 form1 customer1
* 2010 form1 customer2
* 2011 form2 customer2
* 2011 form2 customer2
* 2011 form3 customer3
* 2011 form4 customer3
* etc.


These rows imply *customer1* downloaded *form1* twice in 2009, *customer2* downloaded *form1* once in 2010 and *form2* twice in 2011, and *customer3* downloaded *form3* and *form4* each once in 2011.

I'd love to get some help thinking of interesting and creative queries on *logs*. 

So far I have a few simple ideas like:

* Most downloaded forms (in a given year)
* Customers with the most downloads
* Least downloaded forms
* etc.


As you can see these are not overly creative. I'd love to leverage some neato statistics tricks for this just for fun.

Thanks /r/statistics!",en
1107908,2011-09-22 00:09:18,statistics,"Norm Matloff's new book ""Art of R Programming"" released, with 40% discount code",kn5kf,spaceispotent,1272318570.0,https://www.reddit.com/r/statistics/comments/kn5kf/norm_matloffs_new_book_art_of_r_programming/,17.0,5.0,"http://nostarch.com/artofr.htm

Use the code ARTOFR for 40% off, valid until 9/28. If you order from nostarch.com you also get the ebook free.",en
1107909,2011-09-22 01:51:25,datasets,"Data Set for Hospitals, Clinics, Churches, and Gun Stores?",kn98q,Theoretician,1300672455.0,https://www.reddit.com/r/datasets/comments/kn98q/data_set_for_hospitals_clinics_churches_and_gun/,11.0,6.0,"I'm looking for one or more sets of data that give the street address, Latitude/Longitude, and size of the building. If I get only part of the data it is no big deal, but I would like to get all of it. Does anybody know where I could find or scrape this data from?",en
1107910,2011-09-22 02:33:51,statistics,"Help! I just can't get the whole ""Event"" ""If given"" part of probability statistics.",knan1,-RobotDeathSquad-,1315209381.0,https://www.reddit.com/r/statistics/comments/knan1/help_i_just_cant_get_the_whole_event_if_given/,3.0,5.0,"This is probably baby stuff for you guys but I don't know how to do the whole P(B|A) etc. from a word problem. Like how to divide up the data in the problem properly and then what formula to use. It is not ""clicking"". Any tips?

Also, the mutually exclusive stuff, independent or dependent is very confusing.",en
1107911,2011-09-22 06:55:22,statistics,probability distribution of the sum of n random variables,knjg3,enfieldacademy,1302474908.0,https://www.reddit.com/r/statistics/comments/knjg3/probability_distribution_of_the_sum_of_n_random/,2.0,7.0,"I understand one of the ways they're calculated (by taking the convolution of the sum of n-1 random variables with another identical random variable), but I have a hard time working out the explicit equations for n = 3 or higher myself. The integrals and limits are hard to keep straight.

And I couldn't finding them online, which confuses me.
Is this because the formulas are piecewise, and there get to be a lot of pieces?

By the way, I am just wanting to know about the simplest case, summing uniformly distributed random variables on the interval from 0-1.

",en
1107912,2011-09-22 12:33:45,artificial,&lt;the final graduation work related to AI,knr4k,p6v53as,1314699765.0,https://www.reddit.com/r/artificial/comments/knr4k/the_final_graduation_work_related_to_ai/,7.0,4.0,"Hello, I am thinking of doing the final graduation work in the AI field because it is very interesting to me. But i have trouble to actually think of any suitable project for this purpose. Maybe you have any ideas? The best i could think of this far is comparison of the efficiency of different neural network algorithms.",en
1107913,2011-09-22 13:10:31,statistics,Statistics quick reference guides,knrow,kaeserdc,1282713477.0,https://www.reddit.com/r/statistics/comments/knrow/statistics_quick_reference_guides/,11.0,11.0,"Hi /r/statistics. I'm looking for any useful links regarding statistics, specifically reference guides/glossaries. I've started my masters in social policy recently and it's been 6 years since my undergraduate, so my knowledge is a little rusty. Also any guides to reading and interpreting figures/tables etc would be very appreciated. I have a basic knowledge, but it's taking me longer than it should to process some of the information I'm getting.

Thanks in advance!",en
1107914,2011-09-22 15:20:52,datasets,Last words of the offenders executed in Texas,kntw9,utcursch,1175517803.0,https://www.reddit.com/r/datasets/comments/kntw9/last_words_of_the_offenders_executed_in_texas/,25.0,11.0,,en
1107915,2011-09-22 15:52:18,datasets,The World Top Incomes Database,knukb,utcursch,1175517803.0,https://www.reddit.com/r/datasets/comments/knukb/the_world_top_incomes_database/,7.0,0.0,,en
1107916,2011-09-22 16:03:20,statistics,Somethings appears to of happened in the early 1910s that changed our collective perspective. Any thoughts?,knutm,[deleted],,https://www.reddit.com/r/statistics/comments/knutm/somethings_appears_to_of_happened_in_the_early/,1.0,0.0,,en
1107917,2011-09-22 18:59:35,statistics,The best statistic program for multilevel modelling??,ko0c7,Cognitive86,1308913962.0,https://www.reddit.com/r/statistics/comments/ko0c7/the_best_statistic_program_for_multilevel/,1.0,0.0,"Hello all, 

I previously posted about options in SPSS to do multilevel modelling in a different forum and had a very helpful response (I could run a GEE model or a generalized linear mixed model (GLMM)). I'll be attempting these for the time being, but I just wondered, what is the BEST stats program for multilevel modelling (especially for categorical variables - 95% of the data I work with is categorical)?

It could be free or paid - e.g. Stata? R? Some other program? Ideally it would be under £1000. In general, would there be any other benefits to having this program (I currently have SPSS professional)?

Any opinion would be appreciated!
Best 
Paul",en
1107918,2011-09-23 00:52:41,MachineLearning,A new set of awesome intro lectures on machine learning,kod8e,the_cat_kittles,1287171936.0,https://www.reddit.com/r/MachineLearning/comments/kod8e/a_new_set_of_awesome_intro_lectures_on_machine/,41.0,1.0,,en
1107919,2011-09-23 02:18:34,datasets,Looking for free and public data sets ?,kog44,Abigate,1298619314.0,https://www.reddit.com/r/datasets/comments/kog44/looking_for_free_and_public_data_sets/,1.0,0.0,,en
1107920,2011-09-23 15:39:44,statistics,Including more than two non-continuous variables in an analysis.,kozpn,[deleted],,https://www.reddit.com/r/statistics/comments/kozpn/including_more_than_two_noncontinuous_variables/,5.0,8.0,"I need help. I have a data set that I need to analyse. It has one DV (continuous variable). 9 IVs. 1 IV is continuous (age). 1 IV is ordinal (5 categories). 7 IVs are nominal, with 4 of those being dichotomous (i.e. Yes/No). At current one of my nominal (non-dichotmous) IVs is the focus of my analysis, therefore with my limited knowledge of statistics I have performed 7 Two-Way ANOVAs with that important IV as one of the Factors every time and the other variables (not age) as the other Factor on my DV. However, I was wondering if there was a way I could include ALL the IVs in the analysis at once? 

I have heard of higher order factorial ANOVAs, but have no idea how to do them or interpret them and can't seem to find much using Google. Also I was going to use Multiple Regression but I am unsure about the use of Nominal and Ordinal variables in an MR.

I am using SPSS to analyse my data. Any help would be extremely appreciated!! Thankyou.",en
1107921,2011-09-23 17:35:33,datascience,Building data science teams - O'Reilly Radar,kp2rv,mhermans,1169219262.0,https://www.reddit.com/r/datascience/comments/kp2rv/building_data_science_teams_oreilly_radar/,3.0,0.0,,en
1107922,2011-09-23 18:28:11,statistics,Time Series stress. Resources needed. ,kp4j1,bfhancock,1294107695.0,https://www.reddit.com/r/statistics/comments/kp4j1/time_series_stress_resources_needed/,2.0,5.0,"In Time Series. So lost. Straight A, hard working student. Very stressed. Are there any good references ou their for this subject?? Pleeeease! I don't only want a good grade but also want to understand! Thanks so much. ",en
1107923,2011-09-23 19:24:31,statistics,Time series analysis when companies only buy a few months a year,kp6ka,asjohnson,1288157377.0,https://www.reddit.com/r/statistics/comments/kp6ka/time_series_analysis_when_companies_only_buy_a/,5.0,6.0,"Hey Reddit,

I was wondering if anyone had any thoughts on a problem that has me stumped. I am trying to model sales of companies over time, which sounds like a nice time series problem, but the catch is the sales are inconsistent. Company A might buy things Jan-June, while company B might buy things March-Aug, etc. I think that throws my standard Box Jenkins sort of approach though a bit of a loop, though I may be mistaken. Are there any tricks I should be using here or am I just over thinking it?

Thanks for your thoughts!",en
1107924,2011-09-24 00:09:33,statistics,The importance of being median,kpgg8,AndrewKemendo,1190929741.0,https://www.reddit.com/r/statistics/comments/kpgg8/the_importance_of_being_median/,15.0,2.0,,en
1107925,2011-09-24 02:25:08,artificial,What is the current state of artificial intelligence?,kpkm3,kevelev,1316813996.0,https://www.reddit.com/r/artificial/comments/kpkm3/what_is_the_current_state_of_artificial/,35.0,12.0,"I have a bunch of questions about modern AI. What is the current state of AI research? Is entering AI as a field confining you to academia? Is there a sizable private sector in this area? What are private companies trying to do, is it only with robotics?

Those are my career-esque questions, but about the state of AI research in general, is it all now probability and statistics? What are the major problems / strategies being used in AI today?

Sorry for all the questions, just really curious.

This was first posted in /r/askscience and /r/compsci but I was redirected here.",en
1107926,2011-09-24 06:50:59,MachineLearning,A neurobiologically plausible model for how the brain might do inference by minimizing a free energy functional.,kprcu,qkdhfjdjdhd,1211920583.0,https://www.reddit.com/r/MachineLearning/comments/kprcu/a_neurobiologically_plausible_model_for_how_the/,23.0,12.0,,en
1107927,2011-09-24 14:51:58,MachineLearning,Logo/Symbol recognition in a video clip ,kpz3f,ranza,1247172660.0,https://www.reddit.com/r/MachineLearning/comments/kpz3f/logosymbol_recognition_in_a_video_clip/,5.0,3.0,"Hey reddit,
I'm working on a group project which aims to recognise if a given logo/symbol is present in a video clip.
At the beginning we've took SURF for feature recognition, but after first tests it feels like it's a bad idea. The problem is that very often the logos present in the picture don't get much attention and I get a lot of intrest points, but not the one that are really interesting for me.

Could anyone possibly give me some advice, where to look?",en
1107928,2011-09-25 02:38:13,artificial,TIL Santa Claus has an actual address in Canada,kqf4s,[deleted],,https://www.reddit.com/r/artificial/comments/kqf4s/til_santa_claus_has_an_actual_address_in_canada/,1.0,0.0,,en
1107929,2011-09-25 03:13:00,statistics,Need some help programming a likelihood function in R…,kqg0a,cbrunos,,https://www.reddit.com/r/statistics/comments/kqg0a/need_some_help_programming_a_likelihood_function/,8.0,4.0,"Hello /r/statistics, I am trying to program a likelihood function in R (I want to show that the residuals of a poisson regression sum up to zero when there is a constant term in the covariates) but for some reason, my code doesn't work and I really don't see what's wrong. Here it is: [pastebin link](http://pastebin.com/aMrwWbd2)

I get the following error when launching the nlm procedure:

    
    Error in z[i, ] %*% theta : non-conformable arguments


I have looked and looked, tried with z[1,]%*%theta in the prompt (it works) but I can't find the solution. Any ideas?",en
1107930,2011-09-25 06:46:10,statistics,Boxplot vs. Bar graph,kql61,porgia,1315171896.0,https://www.reddit.com/r/statistics/comments/kql61/boxplot_vs_bar_graph/,7.0,13.0,Help me! can anyone explain when it is better to use a boxplot? i thought i knew but the more i ponder the more sense it seems to just always use boxplots...:(,en
1107931,2011-09-25 07:56:56,statistics,"Cuteness and terror, by size: Nature's perfect probability density function",kqmrs,[deleted],,https://www.reddit.com/r/statistics/comments/kqmrs/cuteness_and_terror_by_size_natures_perfect/,1.0,0.0,,en
1107932,2011-09-25 14:47:44,statistics,Expectation and dice rolling,kqshi,_delirium,1246944723.0,https://www.reddit.com/r/statistics/comments/kqshi/expectation_and_dice_rolling/,0.0,0.0,,en
1107933,2011-09-25 23:37:33,statistics,Random errors on chromatography data,kr4bn,64-17-5,1235975730.0,https://www.reddit.com/r/statistics/comments/kr4bn/random_errors_on_chromatography_data/,4.0,8.0,"See [this graph](http://i.imgur.com/x9GWV.png). I am building an open sourced chromatography software ([MionChrom](http://sourceforge.net/p/mionchrom/home/Home/)). In this software we integrate signals over time (just sums up the signal in pre decided time intervals). The problem is that the background is increasing over time. The way we solve it now is to calculate the sums, then subtracting the triangle I have indicated in green and the big rectangular area under the selected interval. We need to know the  height at start and height at end and this is done by calculating the mean in the signal points before and after indicated start and stop points. As you can see there are noise before and after the indicated area. So the mean will have a confidence limit.

So my questions is, that I hope you will answer or point me in the right direction is: 

A) How do I calculate the confidence interval of my background area (The triangle and rectangle)? 

B) I hope this will give me a confidence interval for the entire integration. Or is am I missing something? What is your opinion?

Thank you very much! Feel free to download my source code. Like connect to it with SVN. 

Edit: [Another screencap from my program.](http://i.imgur.com/06wWU.png)",en
1107934,2011-09-26 00:37:34,datasets,World Bank data - Accessing and plotting with R,kr636,[deleted],,https://www.reddit.com/r/datasets/comments/kr636/world_bank_data_accessing_and_plotting_with_r/,0.0,0.0,,en
1107935,2011-09-26 05:27:24,statistics,ARMA model identification,kren1,feeblestats,,https://www.reddit.com/r/statistics/comments/kren1/arma_model_identification/,8.0,9.0,"I currently trying to construct an ARMA time series model using Box-Jenkins. In theory and from my reading, it's fairly straight forward: test for stationarity, use the ACF and PACF to identify the AR and MA terms and usually, the textbooks/reference material will provide an example of some an ACF and a PACF graph and I can clearly see that this suggests an ARMA(1,0) model (for example). 

The problem is, in reality, the ACF and PACF aren't always as nice or as obvious looking. I understand that there's an element of art in this, but I'm wondering if there are any decent text/resources out there with much ""crappy"" looking ACF and PACFs and discussions on what type of ARMA models they look like corresponding to. ",en
1107936,2011-09-26 07:20:19,statistics,Is there an easy formula to compute an n-fold cross validation?,kri3z,Crotchfirefly,1289629047.0,https://www.reddit.com/r/statistics/comments/kri3z/is_there_an_easy_formula_to_compute_an_nfold/,2.0,1.0,"I know that there are GCV statistics to approximate it, but I was wondering for the simplest, OLS case, is there some kind of easy formula to calculate it exactly?",en
1107937,2011-09-26 17:45:21,statistics,"Excluding the constant from a binary logistic regression? I think I can, I think I can...",krurm,postliterate,1315331842.0,https://www.reddit.com/r/statistics/comments/krurm/excluding_the_constant_from_a_binary_logistic/,7.0,5.0,"In binary logistic regression, the ""constant"" term in the equation represents predictive value of all independent terms set to zero. I've searched my own brain, and done a brief scan of the information on the interwebs, and am wondering if it makes sense to exclude the constant from my current analysis - where it is possible, and meaningful, for all of my independent variables to be zero. I know that excluding the constant changes my results - am I right to conclude that the results I get when I exclude the constant are more accurate? Or am I missing something that should be obvious? (Most of my independent variables are themselves binary, none are categorical.)",en
1107938,2011-09-26 22:08:33,statistics,Worst graph of 2010,ks3gj,[deleted],,https://www.reddit.com/r/statistics/comments/ks3gj/worst_graph_of_2010/,0.0,0.0,,en
1107939,2011-09-26 23:47:00,statistics,Digital monkeys with typewriters recreate Shakespeare,ks6nk,jason-samfield,1248855567.0,https://www.reddit.com/r/statistics/comments/ks6nk/digital_monkeys_with_typewriters_recreate/,3.0,7.0,,en
1107940,2011-09-27 00:12:23,statistics,Self-Taught Statistics Advice?,ks7ij,elgandy,1288394661.0,https://www.reddit.com/r/statistics/comments/ks7ij/selftaught_statistics_advice/,20.0,25.0,"Hopefully this is an appropriate topic for this reddit. I've tried to find more specific, relevant subreddits but haven't found an active community.

I'm seeking advice as to how I could work on my own to develop my statistics/econometrics skills. 

I'm two years out of college where I majored in quantitative economics, so I've taken stats, econometrics, linear algebra, used STATA, etc. However, I've done virtually little work relating to these courses since graduating.

In short, I've been largely unemployed in the years since graduating. I'm currently at home and applying to jobs with way too much free time on my hand.

**I don't have many contacts in my network that relate to statistics, so I'm coming to reddit. What can I do to revive the skillset I learned as an undergrad and also develop skills that would be attractive to employers?**

e.g. open courseware, books, software

*if this is an inappropriate reddit for this question, please suggest a different audience rather than just downvote*

",en
1107941,2011-09-27 03:59:21,MachineLearning,"Stanford's online Machine Learning class now open 
for enrollment (ml-class.org)",kseyi,requiemderseele,1262134570.0,https://www.reddit.com/r/MachineLearning/comments/kseyi/stanfords_online_machine_learning_class_now_open/,41.0,12.0,,en
1107942,2011-09-27 09:37:32,analytics,Spend analysis - an overview,kspag,[deleted],,https://www.reddit.com/r/analytics/comments/kspag/spend_analysis_an_overview/,0.0,0.0,,en
1107943,2011-09-27 13:30:48,AskStatistics,Anyone good with MATLAB? I need to test for co-movement with x number of lags (ie using a Vector Autoregressive (VAR) model). Can't find the command or series of commands I need after extensive Googling.,kstdi,Scott_MacGregor,1264057504.0,https://www.reddit.com/r/AskStatistics/comments/kstdi/anyone_good_with_matlab_i_need_to_test_for/,1.0,1.0,"My field is finance, I have 250 companies, and a few years of data on each of their Equity and CDS movements, I'm using a VAR model to detect co-movement patters. My supervisor said it could be done in SPSS, but I've found I would need to programme something in Python, so now she's suggested MATLAB since it's more formula driven.

Any advice, even just suggesting online guides to MATLAB you've found useful, would be very much appreciated",en
1107944,2011-09-27 15:43:47,statistics,"What information is required before I can say ""X occurs at a rate independent of Y""",ksvs4,mechanicalhuman,1298010469.0,https://www.reddit.com/r/statistics/comments/ksvs4/what_information_is_required_before_i_can_say_x/,6.0,9.0,"I've had a hard time understanding what stats work goes into saying something happens independently of something else. I'm working on a review article, and this is the statement I'm trying to make, and the data is below. Does it make sense?

**Leukoaraiosis (LA) occurs 9x more frequently in patients with LACIs, independent of Hypertension**

LA was statistically significant more frequent among LACI(+) than LACI(-) (61% and 14%, respectively; OR, 9.13; 95% CI, 5.48 to 15.21; P_0.001).

Hypertension was more frequent in the LACI(+) group, but this difference was not statistically significant (49% and 39%, respectively; OR, 1.46; 95% CI, 0.88 to 2.41).
",en
1107945,2011-09-27 19:09:21,statistics,Does anyone have documentation on how to use OpenBugs under Ubuntu (or any other linux based OS)?,kt1zt,cbrunos,,https://www.reddit.com/r/statistics/comments/kt1zt/does_anyone_have_documentation_on_how_to_use/,3.0,2.0,"Hello /r/statistics,

we started using Winbugs in my course of bayesian econometrics, but since I use Ubuntu (and Winbugs is no longer maintained anyway) I wanted to try openbugs. I successfully installed it, and ran the model our professor gave us. However, once I got the results, how can I continue to work with it? Display the convergence graphics for example (or [this](http://i.imgur.com/8okY9.png) kind of graphs)?

I looked all over the internet and there doesn't seem to be any documentation available for OpenBugs on Linux systems. Does anyone have experience with openbugs under linux, and could help me?

Thanks! ",en
1107946,2011-09-27 21:40:07,statistics,Dear Reddit! Could you please guide me to the right direction for info on filling a time series data based on existing data?,kt7io,[deleted],,https://www.reddit.com/r/statistics/comments/kt7io/dear_reddit_could_you_please_guide_me_to_the/,1.0,0.0,"So, here is the problem. I am working on a project where I need daily flows from several waste water treatment plants into rivers or diversions from a river.  I need the data spanning couple of decades. Most of the data however I get from the agencies is sporadic. Sometimes it is available daily, sometimes weekly, and sometimes monthly. Sometimes the data is average value and sometimes it is a point value.  

What I want to do is to have a utility that can read my existing time series with whatever missing data I might have, and predict the data for the missing periods based on what it learnt. From what I know this would be a simple problem in Machine learning. (Am I right?)

Can anybody guide me in how to get started in solving this problem? Thanks a lot for reading.",en
1107947,2011-09-27 21:42:48,statistics,STATA help needed...,kt7ms,[deleted],,https://www.reddit.com/r/statistics/comments/kt7ms/stata_help_needed/,1.0,5.0,"So, as the title suggests, I need help with STATA…

How can I incorporate propensity scores as weights in the “nnmatch” command?

How do I save and access selected coefficient estimates for a regression?
",en
1107948,2011-09-28 00:13:19,MachineLearning,"All about ""Information Value""",ktd17,altrego99,1226078897.0,https://www.reddit.com/r/MachineLearning/comments/ktd17/all_about_information_value/,11.0,0.0,,en
1107949,2011-09-28 00:53:31,statistics,The soundtrack for long days of SAS coding.,ktegw,[deleted],,https://www.reddit.com/r/statistics/comments/ktegw/the_soundtrack_for_long_days_of_sas_coding/,0.0,0.0,,en
1107950,2011-09-28 01:41:53,statistics,Need some help with R..,ktg1l,ctfdacow,1283012468.0,https://www.reddit.com/r/statistics/comments/ktg1l/need_some_help_with_r/,0.0,6.0,"I need to plot http://www-958.ibm.com/software/data/cognos/manyeyes/visualizations/cpu-performance-vs-price in R. Then change it to somehow make it better, but I have no idea how to start.. Our professor expects us to just learn it all but barely taught us anything. Any help about how to start plotting would be great.",en
1107951,2011-09-28 07:38:24,statistics,Grad school options,ktri8,tavernkeeper,1284171957.0,https://www.reddit.com/r/statistics/comments/ktri8/grad_school_options/,7.0,8.0,"Right now, I'm working toward a bachelor's degree and looking at my options for master's degree programs. Can anyone here offer any advice as to which universities have a strong statistics program or which do a lot of statistical research in particular fields (eg bioststistics, actuarial science, demography, etc.)? I am not really sure at this point what specific field I want to go into, but I think it would be helpful to know what option are out there. I am especially interested in programs outside of the US.

Also, If you have other advice in choosing and applying to grad schools and selecting a field of study, please share.",en
1107952,2011-09-28 08:05:51,statistics,probability question:,ktsb9,Mr_Norris,1274942231.0,https://www.reddit.com/r/statistics/comments/ktsb9/probability_question/,1.0,2.0,"On a game show, a car is hidden behind one of 5 doors. You randomly select 2 doors. The host knows where the car is and then opens 1 door that you did not choose and shows you that the car is not behind it. What is the probability you win the car if you choose one of the 2 remaining closed doors that you did not choose?",en
1107953,2011-09-28 15:01:54,MachineLearning,The MASH project: A collaborative platform for the design of very large feature sets.,ku033,FrancoisFleuret,1266000345.0,https://www.reddit.com/r/MachineLearning/comments/ku033/the_mash_project_a_collaborative_platform_for_the/,11.0,1.0,"Dear all,

We have started 18 months ago the MASH project, which aims at designing collaboratively large families of image features for pattern recognition.

The rationale behind the project is that increasing the complexity of a feature set almost systematically reduces prediction error rates, and that a collaborative development is the key to massively increase that complexity.

The open platform is accessible at

    http://mash-project.eu/

There you can create a user account and upload image feature extractors written in C++ (that we call ""heuristics""). You can also check the existing feature extractors already in the system to serve as examples for your own.

By default, the code you upload will remain private, and the system will run a few small-scale pattern recognition experiments using Boosting to assess performance of your features. If you decide to make your submissions public, then they will be included in large-scale experiments, and be available to everybody under the GPL2 license.

You can download an SDK which is compatible with Linux, Windows, and OS X, and check the documentation and screen cast tutorials on the web site.

Please contact me for any comment or remark.
",en
1107954,2011-09-28 21:20:25,MachineLearning,Question: Balanced vs Unbalanced Accuracy,kubqn,el_chief,,https://www.reddit.com/r/MachineLearning/comments/kubqn/question_balanced_vs_unbalanced_accuracy/,8.0,22.0,"Used kNN to get a decent accuracy rate classifying two classes (~80%). In this case the classes were balanced 50/50

When I applied the model to the unbalanced dataset (10/90), the accuracy drops substantially.

Is there a way to determine the unbalanced accuracy from the balanced accuracy?

Thanks!",en
1107955,2011-09-28 22:01:28,statistics,help with SAS...,kuda3,[deleted],,https://www.reddit.com/r/statistics/comments/kuda3/help_with_sas/,1.0,0.0,"I was wondering if anyone knows how to do something like this and if they could help me get this started. Thanks

B. Consider the problem of rearranging the numeric values in a given SAS data set by
concatenating the values from every K consecutive observations into a single observa-
tion of a new SAS data set. An example is shown below. Write a single DATA step
that uses arrays to perform such a rearrangement for K = 5, then apply it to a data
set that you create with 4 numeric variables and 11 observations.
Hint. To handle cases where K doesn't evenly divide the number of observations, the
DATA step may need to test whether it's on the last observation and act accordingly.

Example of concatenating K observations as in Problem B:

3 7 9
2 1 4
0 5 3
2 2 7
8 3 0
3 1 4
Combine K = 2 observations of DS to get:

3 7 9 2 1 4 
0 5 3 2 2 7 
8 3 0 3 1 4 
Combine K = 3 observations to get:

3 7 9 2 1 4 0 5 3
2 2 7 8 3 0 3 1 4
Combine K = 4 observations to get:

3 7 9 2 1 4 0 5 3 2 2 7
8 3 0 3 1 4 ....",en
1107956,2011-09-28 22:34:18,statistics,SAS problem help,kueje,[deleted],,https://www.reddit.com/r/statistics/comments/kueje/sas_problem_help/,1.0,0.0,"I was wondering if anyone knows how to do something like this and if they could help me get it started. Thanks.

Consider the problem of rearranging the numeric values in a given SAS data set by
concatenating the values from every K consecutive observations into a single observa-
tion of a new SAS data set. An example is shown below. Write a single DATA step
that uses arrays to perform such a rearrangement for K = 5, then apply it to a data
set that you create with 4 numeric variables and 11 observations.
Hint. To handle cases where K doesn't evenly divide the number of observations, the
DATA step may need to test whether it's on the last observation and act accordingly.
Example of concatenating K observations as in Problem B:
Original data set DS with 6 observations is:

*3 7 9
*2 1 4
*0 5 3
*2 2 7
*8 3 0
*3 1 4
Combine K = 2 observations to get

*3 7 9 2 1 4 
*0 5 3 2 2 7 
*8 3 0 3 1 4 
Combine K = 3 observations of to get

*3 7 9 2 1 4 0 5 3
*2 2 7 8 3 0 3 1 4

Combine K = 4 observations to get:

*3 7 9 2 1 4 0 5 3 2 2 7
*8 3 0 3 1 4 : : : : : :",en
1107957,2011-09-28 23:27:36,statistics,A Bayesian view of Amazon Resellers,kugkz,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/kugkz/a_bayesian_view_of_amazon_resellers/,6.0,0.0,,en
1107958,2011-09-28 23:40:58,statistics,SAS help,kuh2g,thuggy_d,1315442979.0,https://www.reddit.com/r/statistics/comments/kuh2g/sas_help/,0.0,5.0,"Does anybody know how to do something like this and could help me get started? Thanks.

Consider the problem of rearranging the numeric values in a given SAS data set by
concatenating the values from every K consecutive observations into a single observa-
tion of a new SAS data set. An example is shown below. Write a single DATA step
that uses arrays to perform such a rearrangement for K = 5, then apply it to a data
set that you create with 4 numeric variables and 11 observations.
Hint. To handle cases where K doesn't evenly divide the number of observations, the
DATA step may need to test whether it's on the last observation and act accordingly.
",en
1107959,2011-09-29 00:01:57,MachineLearning,An Algorithm that Can Predict Weather a Year in Advance,kuhue,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/kuhue/an_algorithm_that_can_predict_weather_a_year_in/,14.0,15.0,,en
1107960,2011-09-29 01:17:57,statistics,Help with Statistics??,kukkx,[deleted],,https://www.reddit.com/r/statistics/comments/kukkx/help_with_statistics/,1.0,0.0,"Angus Reid conducted a survey of adult Canadians finding that, overall, 56% supported the legalization of marijuana. The percentage of support was different among voters for the three major political parties as indicated in the table, below. For instance 17% of people both supported legalization of marijuana and also were Conservative voters.

Conservative - Support 17%, Oppose 26%, Not sure 1%

Liberal - 21%, 9%, 1%	
NDP - 18%, 7%, 0%


(3) (c) Suppose I talk to 5 people about support for the legalization of marijuana at a meeting of the Liberal party where everyone is a Liberal voter. What is the probability that none of the 5 people supports the legalization of marijuana?

I have 2 different theories about how to do this..

1 involves taking the following:

0.10 / 0.31 = 0.32 ^ 5 = 0.0034...

That doesn't look right.

Another involves using the complement rule:

P(A) = 21% or 0.21
P(A) = 1 – P(Ac)
0.21 = 1 – 0.79
P(Ac) = 0.79

(P(Ac) = 0.79)5
0.79^5 = 0.3077056399
Rounding…
= 0.3077

Any help on my ideas would be greatly appreciated!

",en
1107961,2011-09-29 03:27:36,statistics,"I've proved X and Y are related, but now I have to prove Y and X are related...what?!",kuozv,ScarbDerp,1309160987.0,https://www.reddit.com/r/statistics/comments/kuozv/ive_proved_x_and_y_are_related_but_now_i_have_to/,4.0,5.0,"How can i prove that Y and X are related iff X and Y are related??

I proved X and Y are related by showing that the conditional distribution of Y given X changes as X changes.

Any clues, ideas, epiphanies, etc... ? ",en
1107962,2011-09-29 04:44:04,statistics,Question on Poisson Distributions,kurq3,ender0887,1308269248.0,https://www.reddit.com/r/statistics/comments/kurq3/question_on_poisson_distributions/,1.0,3.0,"I have to answer this question for homework but I don't exactly understand it, can anyone help me?


Show that the variance of the Poisson distribution, given by P(x,m) = e-x xm/m!,  is:

Var(m) = E((m-x)2) = E(m2 - x2) =  Σm=0∞ (m2 - x2) e-x xm/m! = x.  (prove just the last step, i.e. Var(m) =x)",en
1107963,2011-09-29 06:31:06,statistics,"Dear Reddit, I am in intro physics and had a question about skew on histograms.",kuvff,[deleted],,https://www.reddit.com/r/statistics/comments/kuvff/dear_reddit_i_am_in_intro_physics_and_had_a/,2.0,3.0,"The question is: Is the distribution of education symmetric or skewed? If it is skewed, in what direction is the skew? The data are: 9,9,10,10,10,11,11,12,12,12,12,12,12,12,13,13,14,14,16,16

My professor told me that it should be obvious from histograms which side the graph is skewed towards by seeing which ""tail was longer"" and that the mean of the data would usually be pulled towards the skew . But when I drew the histogram on Excel, I thought it was left skewed because it seems like the left tail is longer...Also, the average is also the median....I definitely feel in my gut that this will by on my midterm on Tuesday. Is there anybody who could explain skew to me better? Show me a fail proof way? On the two questions that the professor asked for practice on skew, I got them both wrong :( I know that skew is to show which side of the graph has the most outliers and to point out where they are.....but I think I am still missing some concept? Thanks redditers..",en
1107964,2011-09-29 07:51:40,statistics,SAS: Logistic regression shows all tied,kuy53,altrego99,1226078897.0,https://www.reddit.com/r/statistics/comments/kuy53/sas_logistic_regression_shows_all_tied/,1.0,1.0,,en
1107965,2011-09-29 18:12:59,statistics,Question on Wilcoxon Mean Rank,kvc7l,aubreya24,1297201872.0,https://www.reddit.com/r/statistics/comments/kvc7l/question_on_wilcoxon_mean_rank/,1.0,7.0,"I am looking at a length in time, in years, by gender. The distributions are not normal, so I can't run an ANOVA on the means and had to resort to the Wilcoxon-Mann-Whitney median method. I've never used this test before and I'm trying to grasp how to understand the output. (If it matters, I ran this in SPSS)

This is what I got:
MALE: N=5346  MEAN RANK=3950.82   P=.001
FEMALE: N=2673  MEAN RANK=4128.35   P=.001

Does this mean that, on average, the medians are lower for males than females? Is there a way I can tell by *how much*, or can I only conclude that male average medians are lower? I'm not sure how the 3950.82 &amp; 4128.35 translate back to my original measures.
",en
1107966,2011-09-30 03:05:33,MachineLearning,k-means learning,kvvpc,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/kvvpc/kmeans_learning/,14.0,2.0,,en
1107967,2011-09-30 12:28:43,analytics,Google Analytics gets premium version,kwb50,Gustomaximus,1275914485.0,https://www.reddit.com/r/analytics/comments/kwb50/google_analytics_gets_premium_version/,1.0,1.0,,en
1107968,2011-09-30 17:35:36,computervision,plenoptica theoretica: fields vs. particles,kwhia,jeanlucpikachu,1159634949.0,https://www.reddit.com/r/computervision/comments/kwhia/plenoptica_theoretica_fields_vs_particles/,1.0,0.0,,en
1107969,2011-09-30 20:47:53,AskStatistics,"Hey Reddit, where's the best place to find statistics on energy use by state?",kwocw,jjkmk,1258493838.0,https://www.reddit.com/r/AskStatistics/comments/kwocw/hey_reddit_wheres_the_best_place_to_find/,2.0,4.0,"Hi guys,

I'm having trouble gathering some statistics I need for a report on solar energy.

Can someone direct me to a resource that can tell me, state by state, how many kilowatts/hour are used on an average farm?  I also need to know, state by state, how much an average farm pays for water per month (not with solar energy).  

Thanks!
",en
1107970,2011-10-01 00:42:22,statistics,SAS Crash Course website,kwwvd,djent_illini,1312414426.0,https://www.reddit.com/r/statistics/comments/kwwvd/sas_crash_course_website/,9.0,18.0,"Can anyone recommend me any website for SAS crash course? I have not used SAS intensively since last May and I have a job interview dealing with mostly SAS programming. Thank you

[EDIT] Thanks for the suggestion. My undergrad stats advisor told me I could get a free SAS E-Learning software through the university. Problem solved.",en
1107971,2011-10-01 03:21:42,statistics,"If var(x) = var(y), can we prove x = y??",kx1iz,ScarbDerp,1309160987.0,https://www.reddit.com/r/statistics/comments/kx1iz/if_varx_vary_can_we_prove_x_y/,1.0,32.0,"Can we prove x and y are equal if they have the same variance?
Any conditions or 'if-and-only-if's' to make this true?




",en
1107972,2011-10-01 21:37:42,statistics,question on quantitative interval data,kxnuq,[deleted],,https://www.reddit.com/r/statistics/comments/kxnuq/question_on_quantitative_interval_data/,7.0,1.0,"I started taking statistics last week, and we have our first project... We have to survey people on a set of questions.

My questions are as followed:

On the average day, how much time do you spend doing stuff online?

What is your main use of the internet?

What is your main method of access online?

How many emails do you get per day?

Is the internet your main source of news?

How many devices do you have that can access the internet?

The teacher told me I need to get 'one quantitative interval data that is not an option.'

I thaught that my first question would be one?  I guess im not quite sure on the meaning of quantitative interval data.  Could someone help me out in figuring out the meaning?",en
1107973,2011-10-02 04:42:36,artificial,Can Anyone Recommend an Interdisciplinary Graduate Program in AI? ,kxzhu,w4r3z,1213725093.0,https://www.reddit.com/r/artificial/comments/kxzhu/can_anyone_recommend_an_interdisciplinary/,13.0,19.0,"I know this is asking a lot, but I'm trying to find a graduate program that studies intelligence using a mix of CS, Neuroscience, Philosophy, and other disciplines. Does anything like this even exist? I know that, taken individually, these are all fields that one could devote a lifetime to, but I'm holding out hope that somewhere there's a program that tries to study intelligence using a more interdisciplinary approach.

",en
1107974,2011-10-02 06:31:56,artificial,Reform Wall Street good suggestions.,ky25k,[deleted],,https://www.reddit.com/r/artificial/comments/ky25k/reform_wall_street_good_suggestions/,1.0,0.0,,en
1107975,2011-10-02 21:26:27,statistics,Question on Honest Statistical Reporting,kyitp,bingbutt,1293556902.0,https://www.reddit.com/r/statistics/comments/kyitp/question_on_honest_statistical_reporting/,9.0,13.0,"I'm working on a project for a statistics class where we must compare the relationship between quantitative variables and decide if they are linear or bent. The problem is, the more I look at the plots, the more unclear this is becoming. I have plotted the variables on SAS and Excel, and it seems like my opinion of the graphs keeps changing depending on how I format my axis, which data points I consider outliers, etc. A major part of our grade is honest reporting of the statistics, so I could really use some advice on how to ensure that the graphs in my report are displayed honestly....",en
1107976,2011-10-02 23:41:09,MachineLearning,A technique for me is a task for you,kymxr,urish,1221689900.0,https://www.reddit.com/r/MachineLearning/comments/kymxr/a_technique_for_me_is_a_task_for_you/,14.0,0.0,,en
1107977,2011-10-03 02:49:00,artificial,RoboRat: Israelis Create Rodent With Robot Brain,kysrd,nassauhedron,1317573676.0,https://www.reddit.com/r/artificial/comments/kysrd/roborat_israelis_create_rodent_with_robot_brain/,16.0,11.0,,en
1107978,2011-10-03 05:15:18,statistics,Help with this proof please :),kyx89,galata44,1317607618.0,https://www.reddit.com/r/statistics/comments/kyx89/help_with_this_proof_please/,0.0,4.0,E(SS) = (n-1)σ^2,en
1107979,2011-10-03 09:46:58,datasets,Featured Kasabi Dataset: UK Government Art Collection,kz4x0,mhermans,1169219262.0,https://www.reddit.com/r/datasets/comments/kz4x0/featured_kasabi_dataset_uk_government_art/,4.0,0.0,,en
1107980,2011-10-03 10:02:46,datascience,Data journalism: a new level of playing field,kz58a,mhermans,1169219262.0,https://www.reddit.com/r/datascience/comments/kz58a/data_journalism_a_new_level_of_playing_field/,1.0,0.0,,en
1107981,2011-10-03 17:45:15,statistics,The will to understand power.,kzdv9,Professor_IR,1310512226.0,https://www.reddit.com/r/statistics/comments/kzdv9/the_will_to_understand_power/,1.0,0.0,,en
1107982,2011-10-03 20:19:37,statistics,Webcast on Tuesday (Oct 4) about hypothesis testing using simulation.,kzj21,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/kzj21/webcast_on_tuesday_oct_4_about_hypothesis_testing/,1.0,4.0,,en
1107983,2011-10-03 20:27:53,statistics,Need help with incomplete gamma function and regularized incomplete beta function.,kzjcd,evilmaus,1278545501.0,https://www.reddit.com/r/statistics/comments/kzjcd/need_help_with_incomplete_gamma_function_and/,3.0,4.0,"Hi r/statistics,

I'm working on writing a statistics library for PHP, since the language seems to be so very lacking on it.  And yes, I've looked at the PECL library, but the thing has been sitting without much progress for years now.  So, I'm trying to build this in native PHP code and release it for free (LGPL) download.  Trouble is, I'm absolutely stuck trying to implement the incomplete gamma function and the regularized incomplete beta function.  I haven't found anything on the former, though I do have Stirling's approximation in for the complete gamma function.  I found [a scholarly article](http://www.ams.org/journals/mcom/1967-21-100/S0025-5718-1967-0221730-X/S0025-5718-1967-0221730-X.pdf) on implementing the latter, but I simply cannot understand case C in there.

I'm not so much looking for a mathematical understanding of these functions, though that would be cool, as I'm looking for help in implementing a numeric approximation to the functions.

Many thanks!",en
1107984,2011-10-03 21:46:05,MachineLearning,Text feature extraction (tf-idf) - Part II,kzm77,perone,1232625557.0,https://www.reddit.com/r/MachineLearning/comments/kzm77/text_feature_extraction_tfidf_part_ii/,20.0,8.0,,en
1107985,2011-10-03 21:48:55,statistics,Question about a t-test (it's been awhile).,kzmaw,benjeye,1292457824.0,https://www.reddit.com/r/statistics/comments/kzmaw/question_about_a_ttest_its_been_awhile/,1.0,1.0,"I'm helping a friend who has her data in excel:

In excel I put tdist(tStat, DF, 2) the 2 is for two-tailed. To get my determine if I reject or not, do compare the results of the above to the alpha level (.05) or to half the alpha level (.025)?

Thanks!",en
1107986,2011-10-03 23:22:41,statistics,Sufficiency versus minimal sufficiency (Math Stat Question),kzpt5,Gradstudentinperil,1317166349.0,https://www.reddit.com/r/statistics/comments/kzpt5/sufficiency_versus_minimal_sufficiency_math_stat/,1.0,3.0,"Hi r/stats, you've been extraordinarily helpful to me in the past, so I thought I might try here for a bit of help with some of the mechanics of my math stat homework this week.

So I understand the concept of sufficiency, and I've found a sufficient statistic by playing with the joint pdf of (x1, x2, .. , xn) taken from a distribution with a given pdf.  (i.e. Exponential(lambda)).  However, the problem says to find the minimally sufficient statistic T, i.e. T such that for all S in the class of sufficient estimators, the S can be written as a function of T.  

As it is impractical, nay impossible, to write out all S; how would one show that T is minimally sufficient?",en
1107987,2011-10-04 00:38:46,statistics,"R/statistics, I'm interested in learning SAS, R, STATA, and/or SQL to complement my degree in economics..",kzsk1,PabloBablo,1287961322.0,https://www.reddit.com/r/statistics/comments/kzsk1/rstatistics_im_interested_in_learning_sas_r_stata/,12.0,32.0,"I'm looking for advice as to which of the above languages would be the best to learn for data analysis. How similar are these statistical analysis programs? If I learn R because it is free, would I also be learning SAS to a degree? Would learning SQL give me a good baseline to work with in learning the others? Is one far and away superior to the others that I should focus on or am I missing something completely?

Edit** Thanks for the responses so far.  I should have specified but I am asking this as it pertains to getting a job. Im not asking so much as to what is easiest, but what is most practical(in the public and/or private sector) 
All things equal, i'd want to learn SAS as it seems to be the most widely used

**EDIT2** **Is there any way to learn SAS for free/cheap?**


",en
1107988,2011-10-04 02:27:58,statistics,A podcast from Frank Harrell on nonparametric statistics,kzw7l,[deleted],,https://www.reddit.com/r/statistics/comments/kzw7l/a_podcast_from_frank_harrell_on_nonparametric/,1.0,0.0,,en
1107989,2011-10-04 03:02:08,MachineLearning,"Looking for strategies for encoding input data with categorical fields that have a very large number of possible values, suitable for a Neural Net or SVM",kzxdy,[deleted],,https://www.reddit.com/r/MachineLearning/comments/kzxdy/looking_for_strategies_for_encoding_input_data/,0.0,10.0,"So I'm looking at situations where I have very large quantities of data, perhaps hundreds of millions of entries, where much of the input data is categorical in nature, for example zipcodes, or first names.

I know the recommended way to encode categorical data is to transform it to N fields, where there are N possible input values, and set the field corresponding to the value present in any given record to 1, while setting all the others to 0.

I'm concerned that where there might be hundreds of thousands of possible categorical values, that this will not scale.

Questions:

1. Will this scale?
* If not, how can I encode this type of data such that it will be suitable for a SVM or Neural Net?",en
1107990,2011-10-04 05:56:06,statistics,Can someone explain the difference between cluster sampling and stratified sampling to me?,l03f6,[deleted],,https://www.reddit.com/r/statistics/comments/l03f6/can_someone_explain_the_difference_between/,3.0,3.0,"I have googled the difference but everytime I answer a sample question to answer whether it is stratified or cluster, I get it wrong 50% of the time! Maybe if you could give me some examples with the answers provided?",en
1107991,2011-10-04 08:04:29,statistics,Data Without Borders,l07q1,Robin_Banx,1315607858.0,https://www.reddit.com/r/statistics/comments/l07q1/data_without_borders/,7.0,0.0,,en
1107992,2011-10-04 10:18:28,rstats,Oracle's Big Data Appliance to include R,l0aux,foxops,1289930223.0,https://www.reddit.com/r/rstats/comments/l0aux/oracles_big_data_appliance_to_include_r/,7.0,1.0,,en
1107993,2011-10-05 04:59:33,statistics,Help with TI-84 Plus Programming,l17wh,mhsnhspres,1314516483.0,https://www.reddit.com/r/statistics/comments/l17wh/help_with_ti84_plus_programming/,5.0,7.0,"I'm currently in AP Statistics and was wondering if anyone had a link to a good Statistics program I could download. I'm in the middle of writing one (really simple stuff, linear correlation, etc.) but wanted to make it more advanced to find R^2 values for nonlinear correlation as well as standard deviation of a line, etc. I actually have no idea how to do this with a calculator or without. I know this is a lot to ask and I know it's fairly basic for this subreddit, but anything can help. Thanks!

TL;DR- I need AP Statistics help on the TI-84",en
1107994,2011-10-05 13:23:38,statistics,Solution manual for Introduction to probability and statistics for engineers and scientists ,l1kql,Don_Ozwald,1302045302.0,https://www.reddit.com/r/statistics/comments/l1kql/solution_manual_for_introduction_to_probability/,1.0,3.0,"We're are using this book in statistics in my school and the problem is... the exercises do not have the solutions in the book(which is very annoying).
Does anyone know if and then where i can find a solution manual for the 4th edition?",en
1107995,2011-10-05 17:00:09,MachineLearning,Oracle’s Big Data Appliance to include R,l1pfc,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/l1pfc/oracles_big_data_appliance_to_include_r/,7.0,1.0,,en
1107996,2011-10-05 17:24:12,statistics,What's the best way to pool standardized mean differences from multiple measurement scales?,l1q3c,krnv,1306266742.0,https://www.reddit.com/r/statistics/comments/l1q3c/whats_the_best_way_to_pool_standardized_mean/,3.0,3.0,"I'm doing a meta-analysis and a lot of the included studies use multiple measurement scales for the same construct. I want to pool them, but I do not know the covariance between them. Many authors seem to just take the mean of the different dependent effect sizes. Which seems fine, but what do to with the variance? Taking the average variance would be like computing the pooled variance with r = 1, which is obviously too high. 

Another option would be to just use one measurement per study, either randomly or based one some criteria. 

Does anyone here have experience with this kind of computations? ",en
1107997,2011-10-05 18:18:52,datasets,54GB of web filtering logs from seven of fifteen Blue Coat servers used by the Syrian Telecommunications Establishment,l1ryr,Quintote,1269890504.0,https://www.reddit.com/r/datasets/comments/l1ryr/54gb_of_web_filtering_logs_from_seven_of_fifteen/,21.0,4.0,,en
1107998,2011-10-05 18:40:31,statistics,If anyone here plays the Florida Fantasy 5 lottery game - here's lots of stats and frequency distributions i've created,l1sp7,randude,1286197010.0,https://www.reddit.com/r/statistics/comments/l1sp7/if_anyone_here_plays_the_florida_fantasy_5/,4.0,1.0,"http://randude.com/lottery/Fantasy_five.xlsx

check out the ""frequencies"" tab....it should open to that....",en
1107999,2011-10-05 19:10:17,statistics,question re: GLM and MANOVAs,l1tqn,PseudoSane00,1299294967.0,https://www.reddit.com/r/statistics/comments/l1tqn/question_re_glm_and_manovas/,2.0,5.0,"So I'm working on some research, looking at the effect BMI has on 4 cardiovascular measures, where I recoded the continuous variable BMI into a categorical variable with 4 levels so that I could do a MANOVA.

Let’s say I want to see if another categorical variable is having an effect, like gender. Would I want to toss gender into the MANOVA as another fixed factor (independent variable)? I was thinking maybe as a covariate, but from what I understand that’s typically only for other continuous variables…

Suggestions / advice?
",en
1108000,2011-10-05 19:18:03,statistics,Block resampling. How does it work?,l1u15,cbrunos,,https://www.reddit.com/r/statistics/comments/l1u15/block_resampling_how_does_it_work/,6.0,5.0,"I am reading ""Bootstrap Methods and their Applications"" by Davison and in the part about Time Series he talks about different resampling methods for dependent data. Model-based resampling is pretty intuitive, but I don't really see how Block resampling (useful to avoid problems of misspecification of the model) can work. How can assigning the same observations to different places  produce interesting results? Magnets?

If someone would be kind enough to explain that would be awesome, thanks.",en
1108001,2011-10-05 19:49:12,MachineLearning,"What do you guys think of the Siri demo, professionally?",l1v7v,xamdam,1129780800.0,https://www.reddit.com/r/MachineLearning/comments/l1v7v/what_do_you_guys_think_of_the_siri_demo/,13.0,24.0,"from what I saw the NLP was pretty good, but I did not see signs of deep intelligence that CALO project suggested. Also, having Siri serve up WolframAlpha is a cheat :) (but kudos to Wolfram for putting his logo on every answer)",en
1108002,2011-10-05 21:32:49,statistics,"Help with breaking down a variable into more manageable units, please! ",l1z8t,something_different,,https://www.reddit.com/r/statistics/comments/l1z8t/help_with_breaking_down_a_variable_into_more/,2.0,11.0,"I am working on my final paper for my Statistics class, and I've been staring at this for at least an hour feeling like I know the answer, but I can't get it down. My paper isn't due til Friday afternoon, so I have a little bit of time. 

I have a data set in SPSS with which I am to write a faux-report, hypothesis, method, and all. I've got my hypothesis figured out, but what I need to do is break down a variable from a sample of ~16,000 cases into more manageable and understandable numbers. The variable is a Family Poverty Ratio; mean of 2.90, sd of 2.64. Range is 0 to 30.65. 

What is the best way to break this down in SPSS so that the differences are visible when using this variable as my independent variable? 

Thanks for your help! ",en
1108003,2011-10-06 03:23:51,datasets,IRC archives - any datasets?,l2din,[deleted],,https://www.reddit.com/r/datasets/comments/l2din/irc_archives_any_datasets/,1.0,0.0,"I'm looking for medium sized archives (10-100 users, hundreds of posts each) of IRC rooms, specifically relating to programming or a technical subject.

I've had a look around, but Google turns little up and its surprisingly difficult to find keywords that don't return irrelevant results.

Does anyone know of any?",en
1108004,2011-10-06 06:13:02,artificial,Proof that resolution is complete?,l2l3w,ImNoYankee,1296667153.0,https://www.reddit.com/r/artificial/comments/l2l3w/proof_that_resolution_is_complete/,1.0,1.0,"I have read a couple of proofs on completeness of [resolution](http://en.wikipedia.org/wiki/Resolution_(logic\)), but I can't seem to get any through my thick skull.  Can anyone clearly explain why it is complete?",en
1108005,2011-10-06 11:54:44,datasets,DSPL datasets for the programming barely literate?,l2u3l,jsweezy,,https://www.reddit.com/r/datasets/comments/l2u3l/dspl_datasets_for_the_programming_barely_literate/,3.0,0.0,"Anyone have tips or suggestions (or any luck) creating DSPL datasets for Google's Public Data Explorer? In particular, folks with limited to no programming experience?  I've tried to find a decent guide but have had no luck so far.",en
1108006,2011-10-06 12:12:39,statistics,Breusch Pagan test on constant variance,l2ucb,Hiolpe,1294958205.0,https://www.reddit.com/r/statistics/comments/l2ucb/breusch_pagan_test_on_constant_variance/,1.0,4.0,"The formula for this test is log(σ^2) = y(0) + y(1)X(i)
I understand this test attempts to see if the error variance σ^2 can be expressed on a log scale linearly related to the value of the X value.  What I don't get is what y(0) and y(1) mean.  I'm only familiar with b(0) the y-intercept or b(1) the slope.  I also don't understand why y(1) being zero would mean there is constant variance.  

Any help would be appreciated.  I know that this question isn't as advanced as most questions here, so please try to keep the wording simple.  Thanks!",en
1108007,2011-10-06 14:34:28,statistics,How to recode in SPSS (slightly more complicated than usual!),l2wdp,Cognitive86,1308913962.0,https://www.reddit.com/r/statistics/comments/l2wdp/how_to_recode_in_spss_slightly_more_complicated/,6.0,3.0,"Hello

I'm hoping to recode a string variable into a grouped numeric variable, but I want to do it based on what is in the string. The string variable has around 500 levels/categories, and I'd like to reduce this down to either '1' if the string includes the word 'board', '2' if it includes 'organisation' and '3' if it includes business. 

I know this will be some type of conditional coding, and have searched the web, but can't seem to get started. Does anyone have any ideas? Obviously I could go through and code it manually, but it will take a long time. 

As always, many thanks. 

C86",en
1108008,2011-10-06 16:14:48,statistics,Bogus with a capital B: A blog post following up on the discussion of repeated A/B tests.,l2yg9,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/l2yg9/bogus_with_a_capital_b_a_blog_post_following_up/,3.0,3.0,,en
1108009,2011-10-06 16:55:58,datascience,Interview with Drew Conway on Data Without Borders,l2zjz,mhermans,1169219262.0,https://www.reddit.com/r/datascience/comments/l2zjz/interview_with_drew_conway_on_data_without_borders/,1.0,0.0,,en
1108010,2011-10-06 17:08:39,MachineLearning,I need to cluster images with kmeans.,l2zw6,linus_rules,1206286095.0,https://www.reddit.com/r/MachineLearning/comments/l2zw6/i_need_to_cluster_images_with_kmeans/,5.0,19.0,"I need to cluster images with [kmeans](http://en.wikipedia.org/wiki/K-means_clustering) and [NCD](http://www.complearn.org/ncd.html) for comparison with other clustering algorithm. 
I am stuck with the centroid calculation. Why? If we have two images X and Y the average (X + Y)/2 makes sense only if size(X) is equal to size(Y). This fact negates the utility of the NCD (It can compares two images of different sizes).
My question is : does it make sense to assume the centroid is equal to the concatenation of X and Y?
",en
1108011,2011-10-06 21:10:11,statistics,Variable in regression model that is based off of dependent variable,l383m,asjohnson,1288157377.0,https://www.reddit.com/r/statistics/comments/l383m/variable_in_regression_model_that_is_based_off_of/,1.0,9.0,"I've come across a problem that I keep going back and forth on, so some input from other folks would be awesome.

I have a bunch of numeric data and a bunch of variables that explain it, but the spread of the dependent variable is quite large and my independent variables end up over predicting the low end and under predicting the high end, so I decided to create a categorical variable that divides the dependent variable into different bins and include it in the model. The idea being that I don't want to compare locations with really low values to locations with really high values, because they are in some way fundamentally different, yet my independent variables are not catching that.

While doing it, I kept thinking back to dealing with reverse causality and all of the problems associated with it and it feels a bit shady to use a variable that is so directly explained by the dependent variable in the regression model. At the same time, I know it is there and I have it there to limit comparisons between really low things with really high things. 

Is this reasonable? Am I going to statistics hell? Any thoughts would be appreciated.",en
1108012,2011-10-07 01:19:02,statistics,What are part and partial correlations?,l3hbo,McFace82,1294450675.0,https://www.reddit.com/r/statistics/comments/l3hbo/what_are_part_and_partial_correlations/,4.0,1.0,"I'm writing the results of a few regressions and I'm not quite sure what part and partial correlations tell me (or if I need to report them for that matter). I've looked it up and just can't seem to understand or interpret them. 

Can someone shed some light or point me in the right direction? ",en
1108013,2011-10-07 05:35:29,statistics,A redditor posted a complete online statistics course. [x-post from r/UniversityofReddit],l3pq5,joltin_josh,1291239790.0,https://www.reddit.com/r/statistics/comments/l3pq5/a_redditor_posted_a_complete_online_statistics/,42.0,0.0,,en
1108014,2011-10-07 20:23:32,MachineLearning,Help with Predicting or Generating Sequences,l4b5o,deusfaber,1206722209.0,https://www.reddit.com/r/MachineLearning/comments/l4b5o/help_with_predicting_or_generating_sequences/,4.0,7.0,"I have a data set with a target concept. 
For example: with states (s)

s^1, s^2, s^3, s^4, s^5, target
s^2, s^3, s^6, s^7, target
s^1, s^4, s^5, s^6, s^7, s^8, target


The sequences are of **unequal** length, but the target concept is the same. What would be a good starting point to explore some off-the-shelf learning algorithms for the above data set?

",en
1108015,2011-10-07 20:52:53,datasets,With Big Data Comes Big Responsibilities - research paper discusses the ease of overestimating what sort of conclusions can be drawn from large datasets,l4c6d,Quintote,1269890504.0,https://www.reddit.com/r/datasets/comments/l4c6d/with_big_data_comes_big_responsibilities_research/,14.0,4.0,,en
1108016,2011-10-07 22:30:13,statistics,Can someone explain Q-Q Plots in simple terms?,l4fmj,[deleted],,https://www.reddit.com/r/statistics/comments/l4fmj/can_someone_explain_qq_plots_in_simple_terms/,1.0,0.0,I'm a college sophomore (Psychology). Statistics is not my strong point. I've searched for hours and no website (including Wikipedia) explains it in simple terms. Can anyone shed some light on these godawful things?,en
1108017,2011-10-08 08:43:45,MachineLearning,Mining Twitter,l4wiy,[deleted],,https://www.reddit.com/r/MachineLearning/comments/l4wiy/mining_twitter/,13.0,0.0,,en
1108018,2011-10-08 12:01:58,statistics,"Any stats for ""Children of Divorce"" over time.  -- We need good data on percentage of children with divorced parents.  ",l4zow,ProfessorBaylock,1309886956.0,https://www.reddit.com/r/statistics/comments/l4zow/any_stats_for_children_of_divorce_over_time_we/,1.0,3.0,"We want **per-child** data, not per family or per capita.  Focus on 1920s-2000 US
 
* Divorce rates won't work-- if they got married but divorced without kids, they don't count.
* Single-parent-family rates won't work-- if one parent is deceased, not divorced, then they don't count.
* Per-household measures won't work-- divorced parents might tend to have only one child while married parents might have many.
* If a child's parents were never married (and not cohabitating/deceased) , that functionally counts as 'divorced' for the purposes of the study.

Any statistics wizards know where we could obtain such this data (or calculate a good estimate from other existing datasources)? 

For a good cause.",en
1108019,2011-10-08 12:09:33,statistics,"Reviews of ""A Million Random Digits""",l4zso,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/l4zso/reviews_of_a_million_random_digits/,17.0,2.0,,en
1108020,2011-10-09 20:24:52,MachineLearning,How well do Support Vector Machines scale to very large training datasets?,l65dm,sanity,1149365629.0,https://www.reddit.com/r/MachineLearning/comments/l65dm/how_well_do_support_vector_machines_scale_to_very/,20.0,56.0,"I'm considering using a Support Vector Machine on a very large training set consisting of millions of rows of data, where each row will have hundreds of attributes (once the data is converted to a form suitable for a SVM).

Is it practical to train a SVM on such a large training set?  How long is it likely to take on a modern consumer-grade laptop?

Oh, also, I've been looking for a good pure-Java SVM library.  I found [SVM-JAVA](https://sites.google.com/site/postechdm/research/implementation/svm-java) but looking through its source code it isn't well written.

Can anyone recommend a good Java SVM library?",en
1108021,2011-10-10 03:08:28,statistics,How Obama's data-crunching prowess may get him re-elected,l6hmj,Hiolpe,1294958205.0,https://www.reddit.com/r/statistics/comments/l6hmj/how_obamas_datacrunching_prowess_may_get_him/,26.0,15.0,,en
1108022,2011-10-10 03:10:45,computervision,"Kinect Object Datasets: Berkeley's B3DO, UW's RGB-
D, and NYU's Depth Dataset",l6hpa,jeanlucpikachu,1159634949.0,https://www.reddit.com/r/computervision/comments/l6hpa/kinect_object_datasets_berkeleys_b3do_uws_rgb_d/,6.0,0.0,,en
1108023,2011-10-10 12:05:15,statistics,when is multiple comparison correction necessary?,l6wf6,thrope,1275749746.0,https://www.reddit.com/r/statistics/comments/l6wf6/when_is_multiple_comparison_correction_necessary/,4.0,5.0,"I have a sort of philosophical question about when multiple comparison correction is necessary.

I am measuring a continuous time varying signal (at discrete time points). Seperate events take place from time to time and I would like to establish if these events have a significant effect on the measured signal.

So I can take the mean signal that follows an event, and usually I can see some effect there with a certain peak. If I choose the time of that peak and do say a t-test to determine if it is significant vs when the event doesn't occur do I need to do multiple comparison correction?

Although I only ever performed one t-test (calculated 1 value), in my initial visual inspection I selected for the one with the largest potential effect from the (say) 15 different post delay time points I plotted. So do I need to do multiple comparison correction for those 15 tests I never performed?

If I didn't use visual inspection, but just did the test at each event lag and choose the highest one, I surely would need to correct. I am just a little confused as to whether I do need to or not if the 'best delay' selection is made by some other criterion than the test itself (eg visual selection, highest mean etc.). Or if in the visual selection if there was no clear peak then I just choose a delay at random, no correction would be required?

Any help on this issue appreciated!
",en
1108024,2011-10-10 12:19:45,statistics,1-sample vs 2-sample test question,l6woc,thrope,1275749746.0,https://www.reddit.com/r/statistics/comments/l6woc/1sample_vs_2sample_test_question/,1.0,5.0,"I have a situation where I measure something for a long time, and occasionally a seperate event of interest will occur. I want to determine if these events have a significant effect on the signal I am measuring. So I am thinking of taking the signal value at each event time, and comparing it to times when there was no event.

But as I see it there are a couple of ways to do this. First, I could take the whole population mean over all the data I have collected (ignoring presence or absence of an event), then do a 1-sample t-test on the event present data set against the whole data mean. Or I could take the mean over all the data, but excluding the times the event happens but still do a 1-sample test against the measured no-event mean.

Or I could take my set of event responses and create a surrogate 'no-event' data set of the same size from points chosen at random when there was no event and do a 2-sample t-test between these two. This surrogate set could have the same number of points as the event set, or could have many more (I have much more no-event data than event data).

So I am in a bit of a muddle as to which of these would be the best in terms of statistical power and avoiding any mistakes. 
I am leaning towards non-balanced 2 sample test with as much non-event data as possible, but any advice is appreciated. I would especially be interested in understanding conceptually the reason for any preference of one method over another.
",en
1108025,2011-10-10 16:00:19,datasets,A little help with a stats assignment please guys,l70ge,JamacanPenguin,1287427380.0,https://www.reddit.com/r/datasets/comments/l70ge/a_little_help_with_a_stats_assignment_please_guys/,0.0,0.0,"Can you please simply state your gender and age, then state, in the form of a number between 1 and 10, how mature you like your cheese. Appreciate it, cheers.",en
1108026,2011-10-10 16:52:53,statistics,"Statistics community members, where do you guys fall on this?",l71qc,ntlaxboy,1299697302.0,https://www.reddit.com/r/statistics/comments/l71qc/statistics_community_members_where_do_you_guys/,9.0,6.0,,en
1108027,2011-10-10 17:48:34,analytics,Tracking Impressions,l73cf,teamcolab,1293815814.0,https://www.reddit.com/r/analytics/comments/l73cf/tracking_impressions/,2.0,0.0,"What are your suggestions on tracking listing impressions on a page?  I have a category that displays various listings depending on a users geographic location.  I would like to track how many times a listing is shown.  Currently we are using Google analytics across the site and would like to use the Google Analytics API to track these listings.  Do you recommend custom event, page tracking, or creating a custom solution?  Thanks!",en
1108028,2011-10-10 20:32:39,artificial,Researchers create the first artificial neural network out of DNA,l79f4,black_or_white,1318255889.0,https://www.reddit.com/r/artificial/comments/l79f4/researchers_create_the_first_artificial_neural/,24.0,0.0,,en
1108029,2011-10-10 20:53:48,statistics,Monte Carlo Simulation Analysis Question,l7aax,Secret_Identity_,1250274504.0,https://www.reddit.com/r/statistics/comments/l7aax/monte_carlo_simulation_analysis_question/,6.0,18.0,"I am running somewhat large Monte Carlo simulation and I need some help with confidence bands.

td:lr If I run 1000-2000 Monte Carlo simulations, how do I exclude outlier runs?

Back story: Every month x number of widgets enter the simulation and y number of widgets exit the simulation by failing. I am trying to get an idea of what the mean and median timeline is for each cohort of widgets; with the assumption being that the likelihood of failing is determined by when the widget entered the simulation.

I have the life-cycle of each widget from 1000 runs of the simulation (10 million widgets, segregated by simulation). What I have been doing to date is looking at each run of the simulation independently and getting the mean and median age of each cohort, i.e., the mean and median age of every widget that entered the simulation in 01/2011 and failed during the run of the sim. I can then compare the mean and median of each sim against every other simulation. Obviously some runs of the simulation are going to be outlier runs, with widgets failing too soon, or surviving for too long. How do I exclude these?

My background is more math than statistics. I tried to read this [paper](http://wrap.warwick.ac.uk/32145/), but I don't have enough stochastic calculus. 

Could anyone point me in the right direction?",en
1108030,2011-10-10 22:34:53,datasets,Is it legal to datamine an entire website?,l7e9s,ponchedeburro,1318261531.0,https://www.reddit.com/r/datasets/comments/l7e9s/is_it_legal_to_datamine_an_entire_website/,0.0,1.0,"Hi,

I found this website containing which music was played on the given radio station a given date and time internal in hours (i.e 1-2 PM, 2-3 PM and ...). I would like to download and analyze this data, however I am considering the legality of mining the site. It has data from now back to January 2010. So that's 24*(days from January 2010 till now) queries to the server from my computer alone. 

I have two cases in mind. 1) Is it legal for me to datamine a website in that way and 2) could me querying the server that many time be considered spam and therefore maybe harmful? 

Thanks",en
1108031,2011-10-11 00:36:56,computervision,"Adobe Photoshop releases ""sneak-preview"" of 3 year old, publicly available technology. Audience is amazed!",l7izm,hersheyzombie,1308945658.0,https://www.reddit.com/r/computervision/comments/l7izm/adobe_photoshop_releases_sneakpreview_of_3_year/,20.0,10.0,,en
1108032,2011-10-11 01:41:54,statistics,Advice on TAing Statistics &amp; Research Methods for Sports Science?,l7lcr,RobMagus,1292332875.0,https://www.reddit.com/r/statistics/comments/l7lcr/advice_on_taing_statistics_research_methods_for/,2.0,13.0,"Hi Stattit,

I've just started my PhD at Brunel university, in psychology research. My second supervisor is in Sports Science, and I'm going to TA for the statistics and research methods modules that the year 3s and masters students will be taking (and possibly the year 1s as well).
They'll be focusing in the basics of stats (descriptives, distributions, t-tests, anova) and using SPSS, and I'll be roaming around in labs to help out during workshops and holding office hours so that dissertation students can ask me for help.

As professors or students, what are some things you'd want me to do? I'm not really familiar with sports science, as my background is in experimental and cognitive psychology, but I do know stats pretty well and can navigate around SPSS (though the way they keep changing the interface every year that's probably mutating).

Should I read up on as many different views of stats as I should, or should I just stick to the Andy Field book and quote passages from it at students who haven't read it? What sorts of things is it reasonable for me to do, and what should I tell my supervisor ""that's not something I can/should be doing""? Do you have any good resources for helping to organize this sort of thing, or places to bone up on teaching stats/research methods?

Thanks for any advice/tips/help you can provide.

*edited to fix grammar and spelling",en
1108033,2011-10-11 01:46:30,artificial,How does AI research feed back into neuroscience? ,l7lir,unicornLulz,1316314850.0,https://www.reddit.com/r/artificial/comments/l7lir/how_does_ai_research_feed_back_into_neuroscience/,14.0,10.0,,en
1108034,2011-10-11 02:02:30,statistics,Help with simple linear regression--conceptual questions.,l7m26,ama1016,1318282108.0,https://www.reddit.com/r/statistics/comments/l7m26/help_with_simple_linear_regressionconceptual/,0.0,5.0,"Please help! I'm trying to answer some simple conceptual questions for my homework, and I'm not sure of the answers. I've scoured the book and notes, but I'm not confident in my responses. Can you help?
Question 1: Refer to regression model ( Yi =β0 +β1Xi +εi ). Assume that X = 0 is within the scope of the model. What is the implication for the regression function if β0 = 0 so that the model is Yi = β1 X i + εi ? How would the regression function plot on a graph?
Where: Yi is the value of the response variable in the ith trial β0 and β1 are parameters Xi is a known constant, namely, the value of the predictor variable in the ith trial εi is a random error term with mean E{εi} = 0 and variance σ2{εi} = σ2; εi and εj are uncorrelated so that their covariance is zero (i.e., σ{εi, εj} = 0 for all i, j; i ̸= j) i = 1, . . . , n
Question 2:
Refer to regression model ( Yi =β0 +β1Xi +εi ). What is the implication for the regression function if β1 = 0 so that the model is Yi = β0 + εi ? How would the regression function plot on a graph?",en
1108035,2011-10-11 04:19:45,statistics,Help with 3x4 ANOVA design matrix?,l7qqp,holesinthinair,1318291753.0,https://www.reddit.com/r/statistics/comments/l7qqp/help_with_3x4_anova_design_matrix/,3.0,1.0,"Hi all.

Here's another person asking for help with linear regression.  In this case, it's an ANOVA using a linear model: a 3x4 ANOVA with interactions.  There are 48 observations: 2 groups with 3 and 4 members each, and then 4 observations for each subgroup.

I'm to make a design matrix that is not overdetermined.

Uploaded are two images that I think are the right answer (what I came up with).  Alpha is the first group, beta is the second, and gamma is interaction terms.

The first image is the full (overdetermined) design matrix, and the second is the 'sigma-restricted coding.'  Have I got it right?

http://img72.imageshack.us/img72/8821/overdetermined.jpg


http://img51.imageshack.us/img51/764/sigmamethod.jpg

Thanks!",en
1108036,2011-10-11 10:08:13,statistics,Can someone translate this statistical hand-waving for the general populace?,l81m6,waitwhut,1318316332.0,https://www.reddit.com/r/statistics/comments/l81m6/can_someone_translate_this_statistical_handwaving/,2.0,9.0,""" Because our primary hypothesis concerning supplement use and total mortality rate with covariate adjustment included 15 separate tests, a conservative Bonferroni approach would require a P value of .05/15.00 = .003. However, many of the additional statistical tests were confirmatory, strengthening confidence that findings were not explainable by chance."" 

Does this basically mean that they didn't get the p-value that they were looking for?

Here's the news article that got my interest: 
[Supplements Offer Risks, Little Benefit, Study Says](http://online.wsj.com/article/SB10001424052970203499704576623203360213200.html?mod=googlenews_wsj)

Here's the article: [Dietary Supplements and Mortality Rate in Older Women: The Iowa Women's Health Study](archinte.ama-assn.org/cgi/content/short/171/18/1625)
",en
1108037,2011-10-11 15:33:05,statistics,Looking for a statistical algorithm,l87nb,Damark81,1274898720.0,https://www.reddit.com/r/statistics/comments/l87nb/looking_for_a_statistical_algorithm/,8.0,8.0,"Hi reddit statistics, 

Can you guys give me a suggestion for a forecasting algorithm that would:
- require complex calculation
- generate a large amount of temporary data in process

I am preparing for a lecture in data intensive computing, and I want to be able to generate a real life case where raw cpu power cannot handle everything (efficiently). 

Thanks",en
1108038,2011-10-11 20:06:59,statistics,"Before performing a T-test, you test for normality right? So, do you test both sets of data for normality independently or combine them and then test for normality?",l8gpt,floatingorb,1217264610.0,https://www.reddit.com/r/statistics/comments/l8gpt/before_performing_a_ttest_you_test_for_normality/,10.0,13.0,"I should also say this is an independent T-test for two samples. I know this is a beginner question, but I could not find the answer anywhere. Thanks.",en
1108039,2011-10-11 21:52:03,statistics,"If I have a 1% chance of success, and I try 100 times, what is the probability that I will fail all 100 times?",l8l31,BetterFutures,1292656862.0,https://www.reddit.com/r/statistics/comments/l8l31/if_i_have_a_1_chance_of_success_and_i_try_100/,16.0,30.0,"Hi, sorry, I'm sure this is an easy one for you guys and I'm sure I learned this at some point in college, but I forget.  If you don't mind explaining the math in addition to the answer, that'd be awesome.  Thank you in advance for satisfying my curiosity :)",en
1108040,2011-10-11 22:09:04,statistics,2 question survey for a statistics project. Can you help  me out?,l8lsw,[deleted],,https://www.reddit.com/r/statistics/comments/l8lsw/2_question_survey_for_a_statistics_project_can/,0.0,4.0,"I'm doing a survey for a statistics project. I need about 30 samples to make this work. Please help! :)

Qualitative question:
What genre of TV shows do you like to watch?

Comedy

Drama

Sports

Reality

Also, if there is another genre that you would like me to add, just let me know.

Quantitative question:

How many hours a day do you watch that genre of TV shows?

Any hour amount will suffice.

Thanks in advance for your time! Hopefully I'll get enough information.",en
1108041,2011-10-11 23:24:25,statistics,Very (basic) open-ended statistics problem about baseball attendance (x-post from r/homeworkhelp),l8ouu,stanleyhudson,1261967179.0,https://www.reddit.com/r/statistics/comments/l8ouu/very_basic_openended_statistics_problem_about/,1.0,2.0,,en
1108042,2011-10-11 23:27:53,statistics,10 cool data visualisations of news events [pics],l8ozu,[deleted],,https://www.reddit.com/r/statistics/comments/l8ozu/10_cool_data_visualisations_of_news_events_pics/,1.0,0.0,,en
1108043,2011-10-12 00:22:15,statistics,SPSS graph for HEXACO results,l8r1c,TempusFrangit,1306005811.0,https://www.reddit.com/r/statistics/comments/l8r1c/spss_graph_for_hexaco_results/,1.0,1.0,"Hello,

I have created 12 variables in SPSS that contain the t-scores for each HEXACO trait for both a participant and his observer. These are:

* Honesty and Honestyo
* Emotionality and Emotionalityo
* Extraversion and Extraversiono
* Agreeableness and Agreeablenesso
* Conscientiousness and Conscientiousnesso
* Openness and Opennesso (Openness to Experience)

One trait is for the participant, the other trait is for the observer (-o suffix). In [this file](http://i.imgur.com/DvIWX.png) you'll see a nice HEXACO graph, with on the horizontal line every T-score for each trait and the vertical line contains the height of the score, ranging from 20 to 80. The grey areas represent the range for average, high, very high, low and very low scores. The line/dots represent the scores of the participant, but I'd also like the scores of the observer in there.

My question is if creating this (or something equally informative, with or without the grey areas) is possible in SPSS, and if it is how can I do it? 

Thanks in advance!",en
1108044,2011-10-12 06:58:10,statistics,Help with my probability homework?,l95x3,bingbutt,1293556902.0,https://www.reddit.com/r/statistics/comments/l95x3/help_with_my_probability_homework/,0.0,12.0,"An urn contains 2 white beans and one black bean. Three dinner partners decide to draw one bean each (without looking), and that whoever draws the black bean must buy dinner for all three. If you were one of these three people, would you prefer to draw first, second, or third? You must defend your answer with probability calculations.

Any help is appreciated",en
1108045,2011-10-12 13:55:34,statistics,R related books: Traditional vs online publishing,l9fh2,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/l9fh2/r_related_books_traditional_vs_online_publishing/,5.0,0.0,,en
1108046,2011-10-12 14:33:52,AskStatistics,What are the odds that there are two Jessicas in a group of 11?,l9g5w,Monster888888888,1310829633.0,https://www.reddit.com/r/AskStatistics/comments/l9g5w/what_are_the_odds_that_there_are_two_jessicas_in/,0.0,7.0,"Rephrasing the question: What is the probability that there are two people with the same name, Jessica, in a group of 11 people? 
(not sure if this is the correct place to post but it is related to statistics)

edit...defining the parameters: random selection from a western college/university population
edit: English Speaking western country.",en
1108047,2011-10-12 17:30:15,artificial,What happened with generation5?,l9kqn,Gumbo72,1309381668.0,https://www.reddit.com/r/artificial/comments/l9kqn/what_happened_with_generation5/,16.0,0.0,,en
1108048,2011-10-12 18:11:04,datasets,"Where to find data to use with R
",l9m7p,talgalili,1271226645.0,https://www.reddit.com/r/datasets/comments/l9m7p/where_to_find_data_to_use_with_r/,3.0,0.0,,en
1108049,2011-10-12 18:11:12,MachineLearning,"Where to find data to use with R
",l9m7y,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/l9m7y/where_to_find_data_to_use_with_r/,2.0,0.0,,en
1108050,2011-10-12 20:16:43,statistics,"Please /r/statistics, I'm lost with my data and I need help.",l9r2u,tourm,1298859091.0,https://www.reddit.com/r/statistics/comments/l9r2u/please_rstatistics_im_lost_with_my_data_and_i/,4.0,5.0,"To cut a long sob story short, for whatever reason I can't seem to find a way to get any sense out of my [dataset](http://www.mediafire.com/?d5m4glu060voiva). I'm trying to find any relationship between both temp. and time of day on the activity of some birds, as well as any covariance(?) between time and temperature. I was told that a 2 way ANOVA would be perfect for this, but I can't get one to calculate even after basically exhausting google of tutorials. Am I missing something or using the wrong statistic? I'm trying to work it out manually right now but my book may as well be speaking Klingon.

I'm sorry if I've made a crap post, but reddit has never failed to give a helping hand before, and you seemed like the place to ask.",en
1108051,2011-10-12 20:27:30,statistics,What's there to like about R?,l9rih,xyzzyrz,1171513719.0,https://www.reddit.com/r/statistics/comments/l9rih/whats_there_to_like_about_r/,31.0,77.0,,en
1108052,2011-10-12 20:59:37,rstats,"in arima models, what are the proper ways to estimate the coefficients? Also using R, how can I validate and calibrate my ARIMA or any time series model?",l9stw,dassouki,1215439790.0,https://www.reddit.com/r/rstats/comments/l9stw/in_arima_models_what_are_the_proper_ways_to/,4.0,7.0,"let's take this data set for example. I modified it as the data itself is proprietary: the years are also different: 

    1967	2,033,407
    1968	2,162,275
    1969	2,159,640
    1970	2,312,352
    1971	2,554,449
    1972	2,548,425
    1973	2,101,225
    1974	1,951,944
    1975	2,106,250
    1976	1,687,625
    1977	1,636,496
    1978	1,494,525
    1979	1,606,825
    1980	1,460,937
    1981	1,310,494
    1982	1,319,750
    1983	1,263,643
    1984	1,171,656
    1985	1,194,950

What is the correct way to forecast this to 5 future years or 10 or even 15? 

What I usually do, is do a linear regression, some form of polynomial, moving average and double moving average, then arima using p = 1, q = 0. I calculate the errors for all these as well. Then I average all the forecasts out and the error to have my final result.

I'm an engineer that wants to get into statistics, so please provide me with your insight. 
Thanks

**EDIT**

I posted this question on [stats.stackexchange](http://stats.stackexchange.com/questions/16915/proper-ways-to-perform-time-series-and-arima)

",en
1108053,2011-10-12 21:29:23,MachineLearning,"Data Reveals That ""Occupying"" Twitter Trending Topics is Harder Than it Looks!",l9u4h,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/l9u4h/data_reveals_that_occupying_twitter_trending/,11.0,2.0,,en
1108054,2011-10-12 22:10:37,statistics,Papers demonstrating Bayesian inference,l9vtm,DwightKurtSchrute,1236585845.0,https://www.reddit.com/r/statistics/comments/l9vtm/papers_demonstrating_bayesian_inference/,5.0,4.0,"I'm writing a paper on applied Bayesian inference, and am hoping to make my own application. I have access to all the databases I could want, but searches for papers like ""applied bayesian"" return 10k+ results, when what I really need is one or two papers that do a good job of demonstrating how to use Bayesian inference. I'm thinking things like choosing priors and likelihoods. Any recommendations?",en
1108055,2011-10-13 00:16:51,AskStatistics,Is there a reason behind the Empirical Rule?,la0mi,Yuforic,1306855053.0,https://www.reddit.com/r/AskStatistics/comments/la0mi/is_there_a_reason_behind_the_empirical_rule/,3.0,6.0,"Or is it like that just because it is, my professor says he doesn't know why, but he just accepts it. I'm speaking of the 68%, 95%, 99.7%.

Edit: More specifically, why is it that those particular numbers are what we use for this? 
For example: [The bottom percentages](http://rchsbowman.files.wordpress.com/2009/01/010309-1504-statisticsn2.png)",en
1108056,2011-10-13 02:04:08,AskStatistics,Is there a reason behind the Empirical Rule?,la4rd,[deleted],,https://www.reddit.com/r/AskStatistics/comments/la4rd/is_there_a_reason_behind_the_empirical_rule/,1.0,0.0,"Or is it like that just because it is, my professor says he doesn't know why, but he just accepts it. I'm speaking of the 68%, 95%, 99.7%.",en
1108057,2011-10-13 03:07:14,computervision,Simple use of the Sobel Edge Detector,la76g,RGKaizen,1307556599.0,https://www.reddit.com/r/computervision/comments/la76g/simple_use_of_the_sobel_edge_detector/,3.0,0.0,"http://imgur.com/7V46A
So I saw this pop up on reddit recently, and I thought of a way to make it easier to see. Basically, I rotate the image by 45 degrees, convolve with a x-oriented Sobel edge detector, and then I rotate it back to normal.",en
1108058,2011-10-13 06:10:27,statistics,Millennium Development Goals Data (by Country) -- 1990-2010.  ,laduq,Gradstudentinperil,1317166349.0,https://www.reddit.com/r/statistics/comments/laduq/millennium_development_goals_data_by_country/,4.0,1.0,,en
1108059,2011-10-13 13:45:11,datascience,Tinkering with the Guardian Platform API ,laoyp,mhermans,1169219262.0,https://www.reddit.com/r/datascience/comments/laoyp/tinkering_with_the_guardian_platform_api/,1.0,0.0,,en
1108060,2011-10-13 15:28:46,statistics,The 50 Best Statistics Blogs ,laqyt,jda06,1162050289.0,https://www.reddit.com/r/statistics/comments/laqyt/the_50_best_statistics_blogs/,34.0,3.0,,en
1108061,2011-10-13 17:29:52,MachineLearning,What's there to like about R?,lauc9,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/lauc9/whats_there_to_like_about_r/,14.0,4.0,,en
1108062,2011-10-13 20:34:37,statistics,Great tip from BillWeld: using 'nws' to parallelize R,lb1do,apathy,1183407755.0,https://www.reddit.com/r/statistics/comments/lb1do/great_tip_from_billweld_using_nws_to_parallelize_r/,7.0,1.0,,en
1108063,2011-10-13 22:39:44,statistics,Clarity on an Infinite Moving Average,lb68m,mcdougan,1201712621.0,https://www.reddit.com/r/statistics/comments/lb68m/clarity_on_an_infinite_moving_average/,2.0,5.0,In one of my courses my professor does not really do notes just pure lecture. I either missed something or dont understand my notes so I was wondering if anyone had a link to a pdf or something that would explain the concept of an infinite moving average. Wikipedia didnt really cover it deep enough,en
1108064,2011-10-14 00:49:19,rstats,How to train neuralnet without overfitting - any advice? ,lbba2,randombozo,1266016617.0,https://www.reddit.com/r/rstats/comments/lbba2/how_to_train_neuralnet_without_overfitting_any/,1.0,1.0,"The formula is the typical y ~ x1 + x2... Any rules of the thumb to tuning learning rate, threshold, etc. for maximum accuracy and minimal overfitting?

I'm new to this and have been unlucky with my google searches.",en
1108065,2011-10-14 03:59:38,MachineLearning,Is this possible (x-post from computervision),lbhy0,j_lyf,1290734799.0,https://www.reddit.com/r/MachineLearning/comments/lbhy0/is_this_possible_xpost_from_computervision/,5.0,7.0,,en
1108066,2011-10-14 05:41:15,statistics,Interpret an R value of 0.92... wait what?,lblif,[deleted],,https://www.reddit.com/r/statistics/comments/lblif/interpret_an_r_value_of_092_wait_what/,2.0,0.0,,en
1108067,2011-10-14 08:15:25,statistics,Need help with unequal sample size (contingency table for 2 x &gt;2),lbqit,djent_illini,1312414426.0,https://www.reddit.com/r/statistics/comments/lbqit/need_help_with_unequal_sample_size_contingency/,1.0,1.0,"I am currently working on a research comparing 2 groups with different categories.  I have some categories with different levels and group A has 21 observations and group B has 27 observations. The level can range from 1-5.

So Category 1 has 3 levels from 1-3, and they are ordinal.

The table is like this. The first letter is the group. The three numbers are the levels, 1 is low and 3 is high.The last number is the sample size

* Group    1    2  3  Total
* A          8   15  4  27
* B          5   11  5  21
* Total    13  26  9  48

I am mainly interested in how the groups compare. How do you go about finding the significance of difference between the two groups? I know how to do it a binary case (yes or no). Does this also apply for groups? If I can figure out this 2x3 table, I can figure out the other 2x4 table. 

Another question, can I use Cochran-Armitage Trend Test here?",en
1108068,2011-10-14 10:44:25,statistics,principles of uncertainty,lbu71,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/lbu71/principles_of_uncertainty/,14.0,1.0,,en
1108069,2011-10-14 14:50:43,datascience,A Picture's Worth A Thousand Senators: Staring Into The Gaping Ideological Chasm That Divides Congress,lbykh,mhermans,1169219262.0,https://www.reddit.com/r/datascience/comments/lbykh/a_pictures_worth_a_thousand_senators_staring_into/,2.0,0.0,,en
1108070,2011-10-14 14:50:46,artificial,"Fun with modern AI: A subreddit for sreenshots of 
interactions with Siri (the iPhone 4 smart assistant)",lbyki,cocoon56,1200778338.0,https://www.reddit.com/r/artificial/comments/lbyki/fun_with_modern_ai_a_subreddit_for_sreenshots_of/,9.0,1.0,,en
1108071,2011-10-14 21:03:31,MachineLearning,Mining Lending Club’s Goldmine of Loan Data (Visualizations by State),lcawx,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/lcawx/mining_lending_clubs_goldmine_of_loan_data/,9.0,0.0,,en
1108072,2011-10-14 23:24:43,MachineLearning,Who wants to throw around ideas on a regression model for upvotes of a submission?,lcgdu,the_cat_kittles,1287171936.0,https://www.reddit.com/r/MachineLearning/comments/lcgdu/who_wants_to_throw_around_ideas_on_a_regression/,17.0,16.0,"Wouldn't it be cool (and maybe dumb) to provide reddit with a predictive model that would tell them how many upvotes their story might get? We could be reddit stars for a day. So far, I can think of the following relevant features:

continuous: Submission time, ~~subreddit~~ (meant to put that in discrete), submitter's karma, how long submitter has been a redditor, length of title, number of non alphanumeric characters in title, number of capital letters

discrete: is nsfw, maybe use pca to figure out which words are buzzwords or something like that and have them as binary variables (those of you with more nlp experience... suggestions?), is text, is link, is video, is image

i was thinking of trying to hit up .json's based on active learning principles to build a training set, anyone have suggestions on this? 

EDIT: news... Kickstarter said no, so maybe there is some other way to pool money together for this? Also, I have the json data in a sqlite db of 1,000,000 submissions culled from 30,000 users (i had to go from users because that seems like you will get a more representative distribution of all types of posts (except maybe people delete their bad posts...)) that im throwing up on infochimps, they have to approve it so it may take a while. pm me if you have better ideas about where to host it, or if you want me to send it to you directly",en
1108073,2011-10-14 23:59:22,MachineLearning,"Dataspora is hiring! Chief Analytics Officer and Chief Executive Officer. San Francisco, CA or Boston, MA. Some travel required. See post for details. ",lchnu,ohsnaaap,1270647929.0,https://www.reddit.com/r/MachineLearning/comments/lchnu/dataspora_is_hiring_chief_analytics_officer_and/,3.0,0.0,"http://www.dataspora.com

Chief Analytics Officer:
http://www.crunchboard.com/opening/detailjob.php?jid=12448

Chief Executive Officer:
http://www.crunchboard.com/opening/detailjob.php?jid=12449",en
1108074,2011-10-15 11:43:04,datascience,Graphs &amp; Politics: Federal Election Commission Campaign Data Analysis with Neo4j,ld0gg,mhermans,1169219262.0,https://www.reddit.com/r/datascience/comments/ld0gg/graphs_politics_federal_election_commission/,2.0,0.0,,en
1108075,2011-10-15 17:01:45,MachineLearning,K-means clustering for Hadoop in R and Java,ld577,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/ld577/kmeans_clustering_for_hadoop_in_r_and_java/,10.0,0.0,,en
1108076,2011-10-15 19:47:54,analytics,"Renegade Economist Interviews Steve Keen .. ( Economics Prof, Neoclassical Economics critic, &amp; Aussie)",ld9dd,xoday,1275490096.0,https://www.reddit.com/r/analytics/comments/ld9dd/renegade_economist_interviews_steve_keen/,1.0,0.0,,en
1108077,2011-10-16 01:14:46,MachineLearning,Best way to get experience in ML?,ldj0b,epios,1286976021.0,https://www.reddit.com/r/MachineLearning/comments/ldj0b/best_way_to_get_experience_in_ml/,12.0,12.0,"I know a lot of you are working professionally with machine learning, and I am interested in gaining some experience with an eye towards being qualified for an engineering job in the field.  I have started studying statistics and some of the basic technologies, and working with Mahout and Lucene in my spare time.  I am currently a software engineer in data warehousing and have a BS in Computer Science.  How you recommend I proceed?  There are Master's programs in the area in Computer Science and another in Applied Statistics.  Is a Master's required? Would Applied Statistics benefit me?  I am also curious about what credentials some of you guys have that are working in the field.",en
1108078,2011-10-16 01:17:02,statistics,Help with Graphpad Prism survival curves?,ldj2m,honger,1297491802.0,https://www.reddit.com/r/statistics/comments/ldj2m/help_with_graphpad_prism_survival_curves/,3.0,5.0,I'm generating survival curves in Graphpad. Below each figure I'd like to put a little text blurb with # of patients at risk at different times like I see in many papers. How can I do this? I've pored over the help files and Google without too much success. Thanks!,en
1108079,2011-10-16 01:32:14,artificial,Some research directions for automatically analyzing videogames,ldjid,kmjn,1292111209.0,https://www.reddit.com/r/artificial/comments/ldjid/some_research_directions_for_automatically/,10.0,1.0,,en
1108080,2011-10-16 08:11:18,statistics,Probability question: Optimizing your bathroom design,ldu9z,[deleted],,https://www.reddit.com/r/statistics/comments/ldu9z/probability_question_optimizing_your_bathroom/,13.0,19.0,"I've had this idea for a while: to occasionally post simple to difficult (imo) probability questions to sharpen the skills of myself and others here. The added benefit is that maybe some of us could learn something interesting as well. It could be fun or a waste of time so let me gauge your level of interest, /r/statistics. So here is my first of such questions:

Occasionally, I go to a bar or a restaurant which is plush enough to have individual, lockable bathrooms. They seemingly always have ""Men"" and ""Women"" written on the door. I guess I can understand that some men may not want to use the same public restroom that a woman used and *vice versa*. The trade-off is that there may be, randomly, many men lined-up to use a men's room while the women's room(s) are completely free. And lines during peak hours can be atrocious. 

The thought is: how much could we gain by allowing men and women to use the same bathrooms if they can overcome their reservations in doing so? By removing the labels, any available bathroom can be occupied, so if there is randomly a large number of men in line, they can use bathrooms which otherwise would have been for women.

Here are just the ""special case"" details / assumptions:

* 20 men and 20 women in the restaurant
* 2 bathrooms (in one scenario unlabeled, in another open)
* each person uses the bathroom at a rate of once every hour
* each bathroom user spends 5 minutes in the loo
* the inter-arrival times between bathroom visitors are independent, and exponentially distributed.

What is the mean difference in expected wait times comparing unlabeled bathrooms to labeled ones?",en
1108081,2011-10-16 11:00:07,statistics,Ask Stats: Bootstrapping logistic regression coefficients,ldxo6,internet_badass,1222930236.0,https://www.reddit.com/r/statistics/comments/ldxo6/ask_stats_bootstrapping_logistic_regression/,3.0,3.0,"Hi all,

I'm using L1-regularized logistic regression to predict disease state from medical images. Currently, I'm trying to analyze which predictor variables (imaging features) are most informative by seeing which ones have the largest magnitude weights in the regression model. I wanted to get a 95% confidence interval for the regression weights, so I am using bootstrapping, but I am getting some strange results.

My largest mean weight value is around 3, but the 95% confidence interval is from -200 to 200. This extremely high variance is worrisome. I was wondering if it was possible that sometimes this particular predictor has either extremely high or extremely low values, and it's mean just happens to be 0. In which case, it would make more sense to use the absolute value of the weight for analysis, but I don't know if that is ""cheating"". Could somebody provide some input as to whether using the absolute value of a regression weight is valid for analysis?

thanks

**Edit**: So I know this is lame, but there was a slight bug in my code. Upon correction, I still get pretty large variance, but at the values will stay either positive or negative. I.e. They don't switch correlation. After reading for a while, I think my method to confidence intervals (just using empirical percentile method), is messed up because the coefficient distributions are not symmetric. I will report back after implementing Bias-Corrected and accelerated confidence intervals.


Edit2: So I managed to get ahold of Tibshirani (it was like talking to God). He essentially told me that confidence intervals on L1-regularization are not informative. You get a bi-modal distribution (many times the weight is 0, and many others it is not), which does not lend itself well to interpretable confidence intervals. He then told me the best way to measure a feature's importance in this model is to simply count the number of times a weight is non-zero.
",en
1108082,2011-10-16 13:15:42,statistics,Clinicaltrials.gov vs. Pharmaprojects datasets,ldzdq,adizzan1212,1299404562.0,https://www.reddit.com/r/statistics/comments/ldzdq/clinicaltrialsgov_vs_pharmaprojects_datasets/,1.0,1.0,"Hey r/statistics,
Was wondering if any of you guys have used either of these datasets/engines before. I'm working on a paper focusing on clinical trials so I need as much information about clinical trials, both specific and broad. on the composition of clinical trials
investigators. Key data items include who are the investigators on the
trial (including name, academic/corporate/govt affiliation, rank/title
within that organization), the phase (I-II-III-IV, and possibly
subgradations of those 4 types) of the trials, the type of information
sought in the trial if it is a phase III or phase IV (for instance, is
it a test of a drug vs. a placebo? Drug vs. another drug (new drug or an established drug in the market)? etc etc.

I'm trying to decide which one of these is better to retrieve all of this information. Any thoughts?

Also, does anyone know of another other similar datasets/ engines that are perhaps better?",en
1108083,2011-10-16 17:43:29,statistics,Statistics in PhD Thesis: Research Design + Data Analysis Support,le3a9,eduguru167,1307043175.0,https://www.reddit.com/r/statistics/comments/le3a9/statistics_in_phd_thesis_research_design_data/,1.0,0.0,,en
1108084,2011-10-16 22:44:34,statistics,SAS 9.2 running old code instead of my changes. (also need help with a function),lebu0,[deleted],,https://www.reddit.com/r/statistics/comments/lebu0/sas_92_running_old_code_instead_of_my_changes/,1.0,0.0,"I'm learning SAS for a course and for whatever reason it likes to run the same piece of code over and over instead of letting me run updated code.  this is seriously pissing me off and the only way I've found to fix it is to entirely exit the program and launch it again.  

The program is pretty much broken at this point and I can't do any work if it continues to act like this...I'm more or less brand new to programming and like to test out a lot of ""what if I leave this out../put this in"" and see what happens.  

I also need help with the mean function syntax: 

    do k = 1 to 4;
	Means[k] = mean(of randNos[1]-randNos[k+1]);
    end;

basically I have a 5x1 array called randNos that I'm storing 5 random, uniform numbers once per the overall iteration.  Then I want to use another array (called Means)to store the means of the first 2, then 3, then 4, then all 5.  I'm getting wonky results.  Instead of filling in the range of values between [1] and [k+1], it's simply taking the average of [1] and [k+1].  

Sometimes it also returns this error: 

ERROR 22-322: Syntax error, expecting one of the following: ), ','

when it's showing the code in the log, immediately after the mean function is called.

Please help...I'm so frustrated right now.

Edit:  one more thing I forgot to mention.  It's generating the *same* output every time, even though it should be generating different random numbers each time.  The ""random"" numbers are identical in each output.

It looks like when I * out the offending code above, it generates new output...ok I figured that part out..it won't run when new code if said code contains syntax errors..like I said, I'm new at this :)

**Anyway, I still need help using the loop index to extend my variable range.**",en
1108085,2011-10-17 02:26:07,statistics,Simple SPSS Question: Creating Hundreds of Cases,leibe,[deleted],,https://www.reddit.com/r/statistics/comments/leibe/simple_spss_question_creating_hundreds_of_cases/,1.0,0.0,"I need to create 600 cases, my book says to do this by ""putting a 1 in every 10th line""? How do I create 600 cases of a uniform random variable with values between 0 and 1. I appreciate any help you can offer. Thanks!",en
1108086,2011-10-17 04:59:05,statistics,SAS Question:  is there a way to use a loop index to extend a variable list?  ,len8p,thavi,1264864567.0,https://www.reddit.com/r/statistics/comments/len8p/sas_question_is_there_a_way_to_use_a_loop_index/,3.0,8.0,"for instance:
    data Junk;
    array Stuff x1-x5;
    array Means m1-m5;
        do i = 1 to 10;
            do j = 1 to 5;
                Stuff[j] = rand('normal');
            end;
            do k = 1 to 5;
                Means[k] = mean (of x1-x&lt;k&gt;);    
            end;
            output;
        end;
etc.

I just want to automate the process of taking means from an increasingly larger n.  This is for a homework problem where I'm trying to demonstrate the central limit theorem.

I've tried using the array cell, i.e. Means[k] but SAS rejects it.",en
1108087,2011-10-17 11:47:07,datascience,MongoGraph - MongoDB Meets the Semantic Web [presentation/screencast],ley5o,mhermans,1169219262.0,https://www.reddit.com/r/datascience/comments/ley5o/mongograph_mongodb_meets_the_semantic_web/,2.0,0.0,,en
1108088,2011-10-17 19:32:03,statistics,Advice? Graphing moderation in hierarchical regression,lf92a,hello_kitteh,1313780080.0,https://www.reddit.com/r/statistics/comments/lf92a/advice_graphing_moderation_in_hierarchical/,1.0,3.0,"My thesis is on the moderating effects of parenting, environment, and hormones on the relationship between peer problems and psychopathology. I also have a number of control variables (age, race, gender, etc.).  I have a significant interaction between peer problems and hormones that I need to graph, but I can't figure out how to do it while controlling for everything else.

All of my variables (except for a couple controls) are continuous.  I know that I'll need to split the sample into two groups according to their level of the moderator, but SPSS will only let me graph the interaction.  I need to control for everything that came earlier in the model.  Any suggestions?",en
1108089,2011-10-17 22:27:58,artificial,Search Algorithms with Google Director of Research Peter Norvig (pretty recent interview),lffwx,last_useful_man,1241273287.0,https://www.reddit.com/r/artificial/comments/lffwx/search_algorithms_with_google_director_of/,13.0,0.0,,en
1108090,2011-10-18 01:38:53,statistics,"Probably Overthinking It: A variant of the Monty Hall Problem, now with more Bayesian goodness.",lfmuf,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/lfmuf/probably_overthinking_it_a_variant_of_the_monty/,11.0,2.0,,en
1108091,2011-10-18 02:39:53,statistics,"Question on mean/median, calculating ratios from medians with two somewhat dependent variables from the same data set",lfp0e,eelnosaj,1289081458.0,https://www.reddit.com/r/statistics/comments/lfp0e/question_on_meanmedian_calculating_ratios_from/,2.0,2.0,"Is this valid?  I am in compensation negotiation, in medicine.  Productivity (measured by RVUs--i.e. 1.0 RVU per office visit--an RVU is a widget) and total compensation (dollars) are the variables.  We are negotiating compensation per RVU.  From a survey data set of 2000-4000 physicians (generating different RVUs and paid at different rates per RVU), a median and mean compensation are given in table (Table 1), as well as median and mean RVUs in another table (Table 58).  In another table (Table 62), the publishers of the survey list, on the same data set, have calculated the ""compensation per work RVU"" with the mean, median, and other statistics.  The administrators are calculating the median compensation rate per RVU by dividing the median of the Table 1 and Table 58, as opposed to using the median from Table 62.  

I was a biology major and am a physician--I claim no knowledge of statistics, but their method seems a little fishy to me (of course, it doesn't work in my favor though).  
For example, a physician who is poorly compensated per RVU trying making a median income would need to have above median productivity.  Another physician might be better compensated per RVU (widget) and still have a high productivity. It is hard for me to express this, but the rate of dollar per widget (dollar per RVU) does not necessarily have much to do with how much someone is paid or how many widgets they make--this is just a rate.  I think the median rate should be calculated from the whole data set, not the medians.  


So what do you all think?
Sorry for the applied/personal nature of this, just looking for help--and I don't know any statisticians.

I'll load the tables if I can to a site to look at soon.  On those tables, I am looking at Internal Medicine--General.
Calculation using medians, their method (Table 1/58), is $41.01/work RVU--n=3868 and n=2866 on those tables

Table 62 median (using n=2852) is $42.50.

Just FYI, using means from tables 1/58, gives 42.62, while from Table 62, actual mean on n=2866 mean is 45.60.  

A bigger issue against their proposed argument  for their method of calculation is that the data sets for the two medians are different.  
The data sets have different n's.  Presumably some medical groups provided only total compensation or work RVUs, not both, for their doctors.  
n=3868 for Table 1, physician compensation

n=2866 for Table 58, physician work RVUs

n= 2852 for Table 62, physician compensation to physician work RVUs

edit:  [tables on Imgur](http://imgur.com/pAJ76)  and spelling
",en
1108092,2011-10-18 05:11:15,MachineLearning,Kernel Density at the Checkout: D'yakonov Alexander on Winning the Dunnhumby Shopper Challenge,lfuam,willis77,1188267725.0,https://www.reddit.com/r/MachineLearning/comments/lfuam/kernel_density_at_the_checkout_dyakonov_alexander/,17.0,2.0,,en
1108093,2011-10-18 10:03:07,statistics,Does anyone know the name of the bottom one?,lg2dy,spdrmn55,1293056863.0,https://www.reddit.com/r/statistics/comments/lg2dy/does_anyone_know_the_name_of_the_bottom_one/,1.0,3.0,,en
1108094,2011-10-18 18:46:26,statistics,Truncated Normal Graph Question,lgdgo,MongrelNymph,1294710569.0,https://www.reddit.com/r/statistics/comments/lgdgo/truncated_normal_graph_question/,3.0,10.0,"I have data of incident rates, where 0% is ""good"" and 100% is ""bad"".  One data set has a mean of 8%, std dev of 14%.  So a normal curve of this ""breaks"" through the y-axis, or 0%. A negative incident rate is not possible.  

How do I interpret this? If I wanted to drill down into a data point that has incident rate of 0.2%, how can I describe this in the context of the other data points?  Can I use a z-score to say it's 70% better than the other data points? I'm not looking at the full curve, since its truncated at 0, so this feels problematic.  

I know I'm not looking at this properly.  Any advice? 

",en
1108095,2011-10-18 23:41:20,MachineLearning,"ACM Data Mining Camp 2011: Report
",lgoim,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/lgoim/acm_data_mining_camp_2011_report/,3.0,0.0,,en
1108096,2011-10-19 00:24:03,statistics,"‎""...I’ve seen this pattern across many news organizations: reporters sidestepping their limited knowledge of the subject material by peering for patterns in a word cloud — like reading tea leaves at the bottom of a cup.""",lgq4m,bubbles212,1294796168.0,https://www.reddit.com/r/statistics/comments/lgq4m/ive_seen_this_pattern_across_many_news/,11.0,3.0,,en
1108097,2011-10-19 02:29:03,statistics,Very quick question - Where do you find ∑ (Xi - xbar)²  on SAS output?,lguo7,ama1016,1318282108.0,https://www.reddit.com/r/statistics/comments/lguo7/very_quick_question_where_do_you_find_xi_xbar²_on/,1.0,0.0,"Where do I find ∑ (Xi - xbar)² on SAS output? Is it under the ANOVA table for sum of squares for the model?

I'm sorry, I'm new to SAS and statistics, but I sure appreciate any help. ",en
1108098,2011-10-19 14:50:38,MachineLearning,The Mode is a Deceitful Beast: William Cukierski on finishing fourth in Dunnhumby's Shopper Challenge,lhe14,willis77,1188267725.0,https://www.reddit.com/r/MachineLearning/comments/lhe14/the_mode_is_a_deceitful_beast_william_cukierski/,7.0,0.0,,en
1108099,2011-10-19 17:04:51,statistics,Weighting.. is that all?,lhhcb,[deleted],,https://www.reddit.com/r/statistics/comments/lhhcb/weighting_is_that_all/,2.0,2.0,"At work, I am the guy that weights all of our survey data. I am primarly self taught (I used an excel spread sheet we had that was use to calculate weights by the previous guy and reversed engineered it). Scary right?

Pretty much all it does is this:
&gt; Population count / actual count = unstandardized weight

&gt; Sample total count/ Population total count = sample proportion

&gt; Unstandardized weight * Sample proportion = Standardized weights.

and that is it... I am wonder what else is there to weighting? Are there any sophisticated techniques I can read up on that I can use?

P.S. Can anyone tell when it is appropriate to ""round the cell counts"" when it comes to weights? SPSS crosstabs does it by default, but nothing else does it that way.",en
1108100,2011-10-19 17:05:21,statistics,"Can I get a sanity check?  StatTrek is wrong, right?",lhhcz,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/lhhcz/can_i_get_a_sanity_check_stattrek_is_wrong_right/,3.0,14.0,"If you Google ""proportion test"", the first hit is 

http://stattrek.com/lesson5/proportion.aspx

so I assume this is a well-regarded resource.  But if you scroll down to the second example problem, which they call ""One-sided test,"" I don't think it makes sense.

The null hypothesis as stated, ""P &gt;= 0.80"" is underspecified.  That is, without specifying the hypothetical distribution of values for P, I don't think we can compute a meaningful P-value.

The calculation as presented seems to be using P=0.8 as the null hypothesis and computing a one-sided p-value.  That might be an ok thing to do, but it is not testing the hypothesis they claim to be testing.

So am I missing something, or is this an error?  At any rate, it seems to me that the page is pretty confusing as written.",en
1108101,2011-10-19 18:12:58,computervision,Publicly available optical flow code,lhjjf,WearingAMonocle,1314073826.0,https://www.reddit.com/r/computervision/comments/lhjjf/publicly_available_optical_flow_code/,9.0,4.0,"I searched r/computervision for optical flow and found no results, so I though I'd start a conversation about it. I am interested in finding any publicly available optical flow code, but it is very difficult to find good implementations.

A good resource for comparing the accuracy of optical flow algorithms is [the Middlebury evaluation](http://vision.middlebury.edu/flow/). I have found this to be a good place to start when looking for the current best-performing algorithms.

In 2010, Sun, Roth &amp; Black published [""Secrets of optical flow estimation and their principles"" [pdf]](http://www.cs.brown.edu/~dqsun/pubs/cvpr_2010_flow.pdf) at CVPR, and in this paper they noted:

&gt; Despite recent algorithmic advances, there is a lack of publicly available, easy to use, and accurate flow estimation software. The [GPU4Vision project](http://gpu4vision.icg.tugraz.at/) has made a substantial effort to change this and provides executable files for several accurate methods [32, 33, 34, 35]. The dependence on the GPU and the lack of source code are limitations. We hope that our public MATLAB code will not only help in understanding the “secrets” of optical flow, but also let others exploit optical flow as a useful tool in computer vision and related fields.

At the time, their algorithm ""Classic+NL"" ranked at the top of the Middlebury evaluation (and is currently 5th, with very similar performance to the others at the top). [Their code is available here.](http://www.cs.brown.edu/people/dqsun/research/software.html)

Apart from [Black &amp; Anandan (1996)](http://www.cs.brown.edu/~black/code.html) and several implementations of the classic [Horn-Schunck](http://en.wikipedia.org/wiki/Horn%E2%80%93Schunck_method) and [Lucas-Kanade](http://en.wikipedia.org/wiki/Lucas%E2%80%93Kanade_method) algorithms, it is very difficult to find original source code or implementations online. I would appreciate any resources that the r/computervision community may be aware of.

For my purposes, the ""Classic+NL"" algorithm by Sun/Roth/Black should work just fine, but that's no reason not to start a topic for people to share other resources.

**Updates**: Thanks for the comments so far! You've helped me expand the list to include:

**[OpenCV](http://opencv.willowgarage.com)** has a C++ implementation of [Farneback optical flow, documented here](http://opencv.willowgarage.com/documentation/cpp/video_motion_analysis_and_object_tracking.html). This is an implementation of [""Two-Frame Motion Estimation Based on Polynomial Expansion""](http://dx.doi.org/10.1007/3-540-45103-X_50). It also has [C implementations](http://opencv.willowgarage.com/documentation/motion_analysis_and_object_tracking.html) of Block Matching (BM), Horn and Schunck (HS), Lucas and Kanade (LK) and Pyramid LK.

**[Megawave](http://megawave.cmla.ens-cachan.fr/)** is a C library of image processing algorithms, including [ws_flow](http://megawave.cmla.ens-cachan.fr/stuff/guid3/node244.html#ws_flow) implementing Weickert and Schnoerr optical flow ([reference](http://dx.doi.org/10.1023/A:1011286029287)). It also has Horn &amp; Schunck optical flow.

[A Duality Based Approach for Realtime TV-L^1
Optical Flow](http://www.inf.ethz.ch/personal/chzach/opensource.html) by Zach, Pock and Bischof [GPU based]

[Optical Flow Evaluation](http://sourceforge.net/projects/of-eval) - a library of optical flow algorithms (800MB due to the inclusion of image sequences). It contains these algorithms:

&gt; M. Proesmans, L. Van Gool, E. Pauwels and A. Oosterlinck. Determination of optical flow and its Discontinuities using non-linear diffusion. In 3rd Eurpoean Conference on Computer Vision, ECCV'94, Volume 2, pages 295-304, 1994.

&gt; T. Camus. Real-Time Quantized Optical Flow. Journal of Real-Time Imaging, Volume 3, pages 71-86, 1997.

I'll keep this post updated as I find more.",en
1108102,2011-10-19 18:17:10,statistics,Graphing a Map - Help!,lhjp7,latortilla,1299276404.0,https://www.reddit.com/r/statistics/comments/lhjp7/graphing_a_map_help/,7.0,11.0,"Hi all,

I have data for a given year for values over different regions. In particular, when I view the data in Excel, each cell is a grid point on the world map, e.g. cell A1 contains the value that corresponds to the coordinates of 180 degrees West, 0 degrees North. So in other words, when I zoom out, because the Ocean has 0 values everywhere, my excel file actually looks like a world map. This is great, though now I would like to actually make a useful graph out of this. What i would like is a ""map type graph"" where each dot on the graph represents through color intensity the value of the cell it is in, and it should be placed according to the coordinates which the cell defines. Does anyone have an idea how to do this in STATA/R/Excel?

something like this basically: http://www.knmi.nl/onderzk/applied/sd/img/EU_NDVI.GIF

Anyone have any ideas?
Thanks!",en
1108103,2011-10-19 19:00:48,datascience,Looking at Active Lobbying Through Meetings with UK Government Ministers,lhlcu,mhermans,1169219262.0,https://www.reddit.com/r/datascience/comments/lhlcu/looking_at_active_lobbying_through_meetings_with/,1.0,0.0,,en
1108104,2011-10-19 22:12:10,statistics,Inter rater reliability with missing observations,lhsln,theresidents13,1317066489.0,https://www.reddit.com/r/statistics/comments/lhsln/inter_rater_reliability_with_missing_observations/,6.0,6.0,"Hi r/statistics, I've got a problem that's twofold:

I'm trying to calculate the inter rater reliability of a set of 8 raters with a set of 29 questions, answered after watching a set of 100 videos.  Thing is, not all the raters answered all the questions.  The first issue is that of the eight raters, only two watched all 100 videos, one watched 96, and the other seven watched the same 20 each (those 20 were randomly selected from the set of 100).  The second issue is that a good deal of the questions had ""N/A"" as an answer, but some raters scored an N/A for certain questions on certain videos while other raters gave numerical scores for the same question/video pairs.

I've looked into Fleiss' Kappa for finding inter rater reliability for more than two raters, but I have no idea how to account for the missing and N/A answers.  No idea how I've made it this far without so much as opening a stats book, please help me r/statistics!",en
1108105,2011-10-19 23:58:41,artificial,"""At least the Catholics were consistent: it only took one new pope and one conclave before the Church declared that artificial intelligence also had the same right to life as fetuses and murderers.""
",lhwkw,[deleted],,https://www.reddit.com/r/artificial/comments/lhwkw/at_least_the_catholics_were_consistent_it_only/,0.0,0.0,,en
1108106,2011-10-20 00:13:54,MachineLearning,Support Vector Machines in R (a course by Lutz Hamel),lhx60,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/lhx60/support_vector_machines_in_r_a_course_by_lutz/,0.0,1.0,,en
1108107,2011-10-20 03:14:34,MachineLearning,Hyper-parameter selection by nonparametric noise estimator for RBF kernel least-squares SVM.,li3qs,AlonzoIsOurChurch,1318472096.0,https://www.reddit.com/r/MachineLearning/comments/li3qs/hyperparameter_selection_by_nonparametric_noise/,1.0,4.0,,en
1108108,2011-10-20 05:04:21,artificial,"""At least the Catholics were consistent: it only took one new pope and one conclave before the Church declared that artificial intelligence also had the same right to life as fetuses and murderers.""",li7x8,[deleted],,https://www.reddit.com/r/artificial/comments/li7x8/at_least_the_catholics_were_consistent_it_only/,35.0,21.0,,en
1108109,2011-10-20 07:19:46,statistics,Need help with Discrete Probability Distributions,licq1,osgd,1310233177.0,https://www.reddit.com/r/statistics/comments/licq1/need_help_with_discrete_probability_distributions/,0.0,6.0,,en
1108110,2011-10-20 17:10:54,statistics,Measurement scales and appropriate statistical analysis,lipg8,luserus,1282212781.0,https://www.reddit.com/r/statistics/comments/lipg8/measurement_scales_and_appropriate_statistical/,6.0,5.0,"I am aware of the nominal, ordinal, interval and ratio scales and what statistical tests are appropriate but I am flummoxed by a paper I am reading.

The dependent variable is stereo acuity (how well we can see depth) and is measured in arc-sec (on a scale from 0 to infinity). Tests are only able to measure from 20-800 arc-sec as they just aren't sensitive enough. An author in a study (Fawcett; 2005) therefore declares that as the tests measure on a ""non-continous"" scale (never heard of that, and it makes no sense to think of it as a discrete scale), all statistical analyses should be non-parametric.

Is this correct? My thoughts would be that since the measurement has all of the characteristics of a ratio scale, parametric statistics could be performed, as long as assumptions held true.

Paper is [here](http://www.sciencedirect.com/science/article/pii/S1091853105001722).",en
1108111,2011-10-20 17:26:56,statistics,Probably Overthinking It: My favorite Bayes's Theorem problems.  Anyone have more?,lipzz,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/lipzz/probably_overthinking_it_my_favorite_bayess/,7.0,1.0,,en
1108112,2011-10-20 18:20:14,MachineLearning,What is marginal likelihood?,lirni,nickponline,1285653350.0,https://www.reddit.com/r/MachineLearning/comments/lirni/what_is_marginal_likelihood/,14.0,4.0,I'm trying to gain some intuition as to what marginal likelihood is (in the context of Bayesian inference) but I'm struggling. I understand it to be a measure how well a model fits the data in general (is this correct?) but I don't really get how?,en
1108113,2011-10-20 22:04:44,statistics,Intuition behind P(A) = E[P(A|Y)],lj0lz,Nyamhap,1319137333.0,https://www.reddit.com/r/statistics/comments/lj0lz/intuition_behind_pa_epay/,6.0,9.0,"Hey r/statistics!

I was wondering what is the intuition behind this formula, 
Could you give me an example?",en
1108114,2011-10-20 23:47:18,MachineLearning,"How well do Decision Trees scale 
to very large training datasets?",lj4ol,rylko,1316880581.0,https://www.reddit.com/r/MachineLearning/comments/lj4ol/how_well_do_decision_trees_scale_to_very_large/,24.0,10.0,"I'm going to use Decision Trees for very large datasets (from megabytes to terabytes).  There are any good implementation in Java? And what kind of DT are best scalable? I see that Mahout uses Random forest. GUIDE also seems interesting. Are ensemble methods right way to go? Or can you suggest good and actual paper to read? Point to science case using decision trees on big data also highly appreciated.

Thanks

edit1: There will be many examples with small number of classes.",en
1108115,2011-10-21 01:30:07,statistics,What math should I expect in an intro statistics class with a calc prereq?,lj8ho,a2c4e6g,1319148886.0,https://www.reddit.com/r/statistics/comments/lj8ho/what_math_should_i_expect_in_an_intro_statistics/,5.0,5.0,"I'm taking a Statistics for Engineers class soon and I want to know what math to brush up on. I know Calculus is sometimes put as a class prerequisite as a measure of ability but never used in class. So is there calculus, if so what kind, and what else should I look out for? Thanks.",en
1108116,2011-10-21 05:22:21,MachineLearning,Merging linear classifiers. Would this work?,ljgf8,lars_,1197145867.0,https://www.reddit.com/r/MachineLearning/comments/ljgf8/merging_linear_classifiers_would_this_work/,4.0,9.0,"I had this idea. I'm sure it's either unoriginal, or there are reasons it wouldn't work that I haven't thought of.

Let's say we have two classifiers which create a model of the form:

y= **θ**^⊤ **X**

Where **X** is our feature vector and **θ** is our parameter vector. Linear Discriminant Analysis and Logistic Regression would both create a model of this type. Logistic Regression would classify based on which side of zero y is. For LDA, we could subtract the mean in order to achieve the same effect.

So lets say the best seperating hyperplane is somewhere in between that of LDA and Logistic Regression. Let's call the fitted parameters from LDA **θ**lda, and those from Logistic Regression **θ**lr. We could formalize ""somewhere in between"" with a parameter α.

**θ**between = (**θ**lda * α) + (**θ**lr (1-α))

We can find good values for α through cross validation, and possibly get a better accuracy than either of the original classifiers on their own. What is this called? Alternatively, why wouldn't it work?",en
1108117,2011-10-21 06:26:15,AskStatistics,"Hi AskStatistics, I have a question abuot modelling. (Crosspost from /r/Econometrics)",ljins,chiRal123,1225776827.0,https://www.reddit.com/r/AskStatistics/comments/ljins/hi_askstatistics_i_have_a_question_abuot/,1.0,0.0,"I have a project reddit that I’m quite stuck on, I’ve been doing a lot of reading about econometrics to see how it can assist me with the issue. I’m looking at the Luxury Car Tax and trying to work out a model to forecast it’s revenue forward. I want to develop an econometric model as I believe it’s the correct approach (please correct me if I’m wrong). The data I have available (which I’m going to treat as parameters in my model) would be motor vehicle sales (total, and possibly by segment as well), car registration data (where I can identify and separate luxury cars), a Motor vehicle price indicator and general economic indicators such as GDP or anything else that may affect the sales of Luxury cars.

Is it possible to use an econometric model to approach this problem ? I want to get reliable forecasts of possible revenues I am able to receive up to 5 years from now. What approach would you take?

Any help would be greatly appreciated, even if you could point me to the right direction or refer me to specific material. I’m sorry if this all sounds a bit confusing. Ask my any questions.

My initial thoughts on this are a simultaneous equation of the form

LCT = a + b(MV sales) + u MV sales = a + b(GDP) + c(consumer sentiment) + d(Price indicator) + [any other economic indicators] + u

u is the random error term for this process, everything is indexed by time t (except the constants a,b c. ... d)

",en
1108118,2011-10-21 13:41:56,statistics,Generating sets of permutations (in R),ljsfr,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/ljsfr/generating_sets_of_permutations_in_r/,3.0,0.0,,en
1108119,2011-10-21 17:05:49,statistics,Six Things Your Company Has in Common with the Oakland A's,ljx36,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/ljx36/six_things_your_company_has_in_common_with_the/,1.0,0.0,,en
1108120,2011-10-21 19:47:32,MachineLearning,Scatterplot-binning for hundreds of millions of points (in R),lk2x2,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/lk2x2/scatterplotbinning_for_hundreds_of_millions_of/,7.0,1.0,,en
1108121,2011-10-21 20:03:26,statistics,I need help graphing.,lk3j7,[deleted],,https://www.reddit.com/r/statistics/comments/lk3j7/i_need_help_graphing/,0.0,0.0,"So I have been doing work on summer learning loss. 

in two weeks I am giving my results at a conference. its aimed at teachers and teacher educators so the stat aspect doesn't need to be all inspiring. 

http://www.chartgo.com/share.do?id=c86d2655aa

here is a graph that I want to present. the only problem is that the tick marks are not equal. summer is 3 ~months while the school year is ~8 months. it would be misleading if the ticks are equal. 

Do you know how I can graph that difference?",en
1108122,2011-10-21 20:50:14,statistics,What lessons should be learned from Edward Tufte?,lk5h5,jasonrosen,1304619652.0,https://www.reddit.com/r/statistics/comments/lk5h5/what_lessons_should_be_learned_from_edward_tufte/,13.0,6.0,"I've read Tufte's books (The Visual Display of Quantitative Information, Envisioning Information, and Beautiful Evidence), but I've always been pretty underwhelmed. While the advice certainly isn't bad (avoid chart junk! maximize your data-ink ratio! make the smallest effective difference!), it seems mostly, well, obvious.

Yeah, I know people still make egregious violations of these every day (cf. a lot of modern infographics), so maybe it's not that obvious, but I guess it kind of reminds me of people's obsessive fawning over Strunk and White's ""Elements of Style"" (omit needless words? well, duh, they're needless!).

So am I missing something? What exactly does Tufte deserve heaps of praise for? Is it just that even though these principles may be somewhat obvious, Tufte was the first person to clearly write them down and promote them? Or are these principles only seemingly obvious now, but not before, because Tufte ingrained them into our minds? Or...?",en
1108123,2011-10-21 22:37:45,statistics,"The pdf of a binomial(p1, N1) where N1~binomial(p2, n2)?",lk9q2,Zelc,1281680624.0,https://www.reddit.com/r/statistics/comments/lk9q2/the_pdf_of_a_binomialp1_n1_where_n1binomialp2_n2/,7.0,7.0,"Is there any way to get a closed form equation for the pdf of a binomial(p1, N1) where N1 ~ binomial(p2, n2)?  If not, is there a way to show that f(p1=A, p2=B) = f(p1=B, p2=A)?

I tried pdf(x1) = SUM(f(x1, x2), x2's) = SUM(f(x1|x2)*f(x2), x2's).  I ended up with SUM( n2!/[x1!(x2-x1)!(n2-x2)!] * p1^x1 * (1-p1)^(x2-x1) * (1-p2)^(n2-x2) * p2^n2 , x2=x1 to x2=n2).  That looks vaguely like a multinomial distribution, but I don't know how to proceed from here.

I've built something in Excel that can calculate the pdf given a p1, p2, and n2 (for a certain range of n2's), but I was wondering if you could show this with a formula.

(If you're wondering what this is for, it's for a ""miniatures"" wargame I play called Battleground: Fantasy Warfare.  Damage is determined by first rolling a number of d6's and counting the number of successes, which is dice with a result less than a threshold number.  Then you roll a number of dice equal to the successes and count the number of dice with a result less than a different threshold number.  That's the amount of damage dealt.)",en
1108124,2011-10-22 13:35:17,statistics,"Why is there a ""3"" in the formula for Pearson's Coefficient of Skew? Is it arbitrary, or is it derived from somewhere?",lkwt1,mjk1093,1156005148.0,https://www.reddit.com/r/statistics/comments/lkwt1/why_is_there_a_3_in_the_formula_for_pearsons/,8.0,3.0,,en
1108125,2011-10-22 19:14:41,statistics,How to diagnose linearity prior to a factor analysis?,ll3ag,robotrebellion,,https://www.reddit.com/r/statistics/comments/ll3ag/how_to_diagnose_linearity_prior_to_a_factor/,7.0,2.0,"As I understand it, linearity is tested by looking at deviation from a linear model such as regression. With regression you have a dependent variable, but with exploratory factor analysis you don't. So how do you screen for linearity?",en
1108126,2011-10-22 20:42:51,MachineLearning,Information on tag prediction for texts.,ll5zh,Xochipilli,1199614441.0,https://www.reddit.com/r/MachineLearning/comments/ll5zh/information_on_tag_prediction_for_texts/,3.0,1.0,"Hi,

Can anyone recommend me some (recent) papers on 'tag prediction' for texts (automatic tagging of forum posts, twitter posts, pdf-books, articles, etc.)?",en
1108127,2011-10-22 21:38:39,computervision,"While reading up on facial detection, I stumbled across a student's results. I spit out my coffee laughing.",ll7uw,tsumnia,1222963366.0,https://www.reddit.com/r/computervision/comments/ll7uw/while_reading_up_on_facial_detection_i_stumbled/,34.0,3.0,,en
1108128,2011-10-23 02:53:41,MachineLearning,What is wrong with Gaussian Processes?,lli2t,nickponline,1285653350.0,https://www.reddit.com/r/MachineLearning/comments/lli2t/what_is_wrong_with_gaussian_processes/,17.0,14.0,"They seem like pretty awesome methods, and aside from the time and space issues of the matrix inversion are there any other limitation that they have for general non-parametric regression?

EDIT: Out of interest for research directions.",en
1108129,2011-10-23 17:04:33,statistics,"Suggest a project for my students, get an appropriately-priced statistical consultant.",lm0hc,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/lm0hc/suggest_a_project_for_my_students_get_an/,9.0,6.0,"I teach a project-based statistics class at Olin College, and my students are looking for projects.  You can read about it here

https://sites.google.com/site/thinkstats2011b/project

But the short version is that they are trying to find an interesting question that can be answered with basic statistical exploration of an appropriate dataset.  The tools they will bring to bear include visualization of PMFs and CDFs, relationships between pairs of variables, hypothesis testing, and Bayesian estimation.

So if you have a dataset in need of exploration, or a question you think would be interesting to explore, please let me know!",en
1108130,2011-10-23 21:17:15,statistics,Where can i find data sets? with these requirements,lm7bg,photoinduced,1319216102.0,https://www.reddit.com/r/statistics/comments/lm7bg/where_can_i_find_data_sets_with_these_requirements/,8.0,12.0,"Hi,

i have been looking for data sets which have a dependable variable, 2 covariates, 2 fixed factors.
anybody know where to find some real data. i tried WHO, World bank, a few 10s of universities
",en
1108131,2011-10-23 23:55:23,MachineLearning,How do I pick which data to label- question inspired by active learning techniques,lmco8,the_cat_kittles,1287171936.0,https://www.reddit.com/r/MachineLearning/comments/lmco8/how_do_i_pick_which_data_to_label_question/,9.0,3.0,"I recently read a paper discussing some general ways to think about how to label additional datapoints when its costly to do so- pick ones that are most ambiguous for the current model, pick ones that are representative of the distribution of data etc... got me thinking- how would you go about selecting your *initial* training set from unlabeled data. What kinds of things could help you decide which points to label?",en
1108132,2011-10-24 00:07:45,artificial,3.31 General Bayes Net 2,lmd3b,stevemulligan,1309531516.0,https://www.reddit.com/r/artificial/comments/lmd3b/331_general_bayes_net_2/,5.0,2.0,"Sebastian says there are 8 parameters on node D and my guess was 6.

I'm not sure what I missed, if someone could refer me to the video that explains why it's eight it would be most appreciated.

I thought you would need 2 params to describe each of:

P(D|A) 
P(D|B)
P(D|C)",en
1108133,2011-10-24 01:59:59,statistics,Finding dose-response over time.,lmguu,jcsunag,1284097041.0,https://www.reddit.com/r/statistics/comments/lmguu/finding_doseresponse_over_time/,4.0,7.0,"I am working with a biostatistics data set. It's a study of how different doses affect the increase of the chemical in the plasma. Measurements were taken at baseline, 3 months, and 9 months. 

I am unsure what kind of analysis to use to incorporate time. A simple regression will just measure the relationship between the baseline measurement and the subsequent measurements, not telling how dose is involved.

Any thoughts?
",en
1108134,2011-10-24 06:53:46,statistics,book on numerical methods in statistics,lmqh6,FailedLifeForm,1299165736.0,https://www.reddit.com/r/statistics/comments/lmqh6/book_on_numerical_methods_in_statistics/,7.0,3.0,"does anybody know of a book that has code (or psudocode) for various statistical methods, such as logistic regression or maximum likelihood in C# or C++?",en
1108135,2011-10-24 10:53:54,statistics,Normality tests - they are of virtually no value to the data analyst,lmwvy,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/lmwvy/normality_tests_they_are_of_virtually_no_value_to/,27.0,24.0,,en
1108136,2011-10-24 19:43:17,statistics,Suggestions for applied stats grad schools?,ln9h8,starwarsyeah,1314830773.0,https://www.reddit.com/r/statistics/comments/ln9h8/suggestions_for_applied_stats_grad_schools/,9.0,35.0,"Long story short, I am currently enrolled in the graduate program at Virginia Tech, and I was extremely misled by the recruiter, professors, and even some students as to the nature of the program. I am looking for an extremely APPLIED program, one that will prepare me to work in industry or business after graduation. What I got is a program that is more focused on pumping out PhDs, and therefore most of the courses delve heavily into theory. I'm not too interested in the theory, just how to apply it. At this point, I'm not doing well, and I'm considering either going straight into the job market with my BS or moving to a different graduate program. This time around, I want some advice from people who have been there/done that. Any suggestions?",en
1108137,2011-10-24 20:57:26,MachineLearning,Machine Learning With Hadoop,lnc96,manning10,1309041336.0,https://www.reddit.com/r/MachineLearning/comments/lnc96/machine_learning_with_hadoop/,1.0,0.0,,en
1108138,2011-10-25 00:05:47,MachineLearning,Matlab (and Standalone application) port for the  `Random Forests' algorithm,lnjx4,blur10600,1318590994.0,https://www.reddit.com/r/MachineLearning/comments/lnjx4/matlab_and_standalone_application_port_for_the/,4.0,0.0,,en
1108139,2011-10-25 00:23:11,MachineLearning,AskReddit: Anybody knows of a good C4.5 or C.5 MAtlab implementation?,lnkmh,blur10600,1318590994.0,https://www.reddit.com/r/MachineLearning/comments/lnkmh/askreddit_anybody_knows_of_a_good_c45_or_c5/,2.0,2.0,,en
1108140,2011-10-25 07:27:20,artificial,John McCarthy — Father of AI and Lisp — Dies at 84,lo0x5,1infiniteloop,1315936966.0,https://www.reddit.com/r/artificial/comments/lo0x5/john_mccarthy_father_of_ai_and_lisp_dies_at_84/,48.0,3.0,,en
1108141,2011-10-25 08:22:20,statistics,CS freshman wondering if there are introduction to statistics resources using R,lo2um,[deleted],,https://www.reddit.com/r/statistics/comments/lo2um/cs_freshman_wondering_if_there_are_introduction/,3.0,2.0,"I'm currently pursuing a CS major and would like to find out if there are any nice resources on introducing one to statistics through R (any textbooks with problems teaching statistics with the implementation of R for solving problems?)

Thanks in advance :)",en
1108142,2011-10-25 09:26:07,statistics,Poisson Process question,lo4o2,furiousnerd,1304741474.0,https://www.reddit.com/r/statistics/comments/lo4o2/poisson_process_question/,9.0,18.0,"Not sure how this conditional pdf is derived, 
S1 is defined as the time until the 1st poisson event, and S2 is the time from the beginning until the second poisson event.  
N(1)=2 means 2 poisson events within the first unit of time.  

f(S1,S2|N(1)=2) = 2!/t^n = 2

I believe a poisson divided by a poisson random variable is binomial, is that where the 2!/t^n comes from?  

Thanks ahead of time!  

EDITED:  Made the definition of S2 a little clearer

Solution:  Posted in case anyone was curious, thanks to 2718282.  Basically I got confused by the S's being non-independent.  However, you can express the joint pdf with the intervals of S1 and S2-S1 which are exponential and independent.  Conditioned on a poisson process with lambda = 2 gives the answer.  ",en
1108143,2011-10-25 18:26:04,MachineLearning,Is there any summary of top models for the Netflix prize?,loh00,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/loh00/is_there_any_summary_of_top_models_for_the/,34.0,2.0,,en
1108144,2011-10-25 21:16:00,statistics,can anyone help me with my stats homework? (joint densities),lonuy,[deleted],,https://www.reddit.com/r/statistics/comments/lonuy/can_anyone_help_me_with_my_stats_homework_joint/,0.0,2.0,,en
1108145,2011-10-26 00:28:51,MachineLearning,"What makes Big Data so different from ""not Big Data""?",low4y,fortyninerbruin,1217817943.0,https://www.reddit.com/r/MachineLearning/comments/low4y/what_makes_big_data_so_different_from_not_big_data/,20.0,19.0,"Any time I interview, people make ""Big Data"" seem like it's completely different from data measured in mere megabytes. They also do not provide a good concrete reason of how it changes the analysis. So what changes need to be made to work with a data set measured in GB/TB, aside from worrying about being efficient with the code?",en
1108146,2011-10-26 01:31:36,artificial,John McCarthy obituary,loyn1,praseodymium,,https://www.reddit.com/r/artificial/comments/loyn1/john_mccarthy_obituary/,16.0,0.0,,en
1108147,2011-10-26 05:56:02,statistics,"When is Independence not Independence? The difference between ""statistical independence"" and ""functional independence.""",lp8q1,thegabeman,1239690590.0,https://www.reddit.com/r/statistics/comments/lp8q1/when_is_independence_not_independence_the/,0.0,0.0,,en
1108148,2011-10-26 06:36:47,statistics,statistics homework question...help!!,lpaa4,[deleted],,https://www.reddit.com/r/statistics/comments/lpaa4/statistics_homework_questionhelp/,1.0,5.0,,en
1108149,2011-10-26 07:48:07,statistics,Blinder-Oaxaca decomposition,lpct0,[deleted],,https://www.reddit.com/r/statistics/comments/lpct0/blinderoaxaca_decomposition/,1.0,1.0,"Hi all,
I am looking at differences in hourly wages between migrants and non-migrants in Indonesia, and it was suggested that I use a blinder-oaxaca decomposition. When I run my regression with logged hourly wages as my dependent variable and migrant vs. non migrant as an independent variable among other things, my migrants have higher hourly wages than my non-migrants the difference is signficant.
However when I use oaxaca decomposition it is the non-migrants that have the higher wages and the difference is not significant! Is there anyone that could help me figure out what is going on here?

(I am using STATA for my analysis, and using the oaxaca add-on)
Cheers",en
1108150,2011-10-26 10:40:53,statistics,Any fun suggestions for exam questions that involve basic t-tests or hypothesis testing? (x-posted from /r/DoctorWho),lph9d,feralparakeet,1276044824.0,https://www.reddit.com/r/statistics/comments/lph9d/any_fun_suggestions_for_exam_questions_that/,1.0,3.0,,en
1108151,2011-10-26 12:07:30,statistics,Interview tips,lpitu,statiscianwannabe,1319618798.0,https://www.reddit.com/r/statistics/comments/lpitu/interview_tips/,1.0,0.0,,en
1108152,2011-10-26 15:52:01,AskStatistics,Comparing sodium intake and systolic blood pressure,lpmoa,lhk91,1309107109.0,https://www.reddit.com/r/AskStatistics/comments/lpmoa/comparing_sodium_intake_and_systolic_blood/,0.0,0.0,"Hi, I have two data sets, one for sodium intake, and the other for systolic blood pressure. I want to compare the association between the two variables. What tests/graphs/plots would be the best way to do this?

Thank you",en
1108153,2011-10-26 16:40:04,statistics,How do I decide whether to suppress the intercept in logistic regression or not?,lpnzb,anothernameagain,1254666862.0,https://www.reddit.com/r/statistics/comments/lpnzb/how_do_i_decide_whether_to_suppress_the_intercept/,9.0,5.0,"In my stats classes, they've generally glossed over this. What's the rule here? I can give more info if needed, but I'm looking for rules to live by rather than a direct answer for my particular situation.",en
1108154,2011-10-26 16:51:38,MachineLearning,Want to help reddit build a subreddit recommender? -- A public dump of voting data that our users have donated for research [x-post from /r/redditdev],lpoc3,TedFromTheFuture,,https://www.reddit.com/r/MachineLearning/comments/lpoc3/want_to_help_reddit_build_a_subreddit_recommender/,30.0,8.0,,en
1108155,2011-10-26 19:51:23,statistics,Memoryless Property of the Exponential Distribution,lpv36,[deleted],,https://www.reddit.com/r/statistics/comments/lpv36/memoryless_property_of_the_exponential/,1.0,6.0,"Hey r/stat,
So I'm a bit confused about the implications of this property. I understand the conclusion of it, but I can't see how many we can use this distribution to model a lot of the typical ""waiting time"" problems. For example, I've seen the lifetime of a light bulb be modeled as an exponential random variable X, but it seems that this memoryless property of the exponential distribution doesn't apply. 

Suppose that you want to find the Pr(X&gt;40 months). Suppose also that we know that 30 months have passed by. The memoryless property states that the Pr(X&gt;40)=Pr(X&gt;10). This seems incorrect since light bulbs burn out over time. In our example, we know that 30 months have passed by so it is safe to assume that the amount of time before it decays is less than when the light bulb was first installed. Am I wrong somewhere?",en
1108156,2011-10-26 20:27:16,statistics,Probability transposition: What's going on here?,lpwi2,t1cooper,1294437155.0,https://www.reddit.com/r/statistics/comments/lpwi2/probability_transposition_whats_going_on_here/,2.0,8.0,"Last night my friend was trying to get a result from rolling a 20-sided die, but all we had was a 6-sided die. We devised a method for transposing rolls from a 6-sided die to a 20-sided die by making a decision tree based on each roll of the 6-sided die.  
  
  
If the first die roll was even we would eliminate 1-10, otherwise, 11-20. We would cut the results in half again with the second die roll, so at this point, based on what we rolled, we would have 1-5, 6-10, 11-15, or 16-20. From this point, we would roll the die and choose a number based directly on what we rolled, re-rolling for a result of 6. I wrote a quick program to do this 100 million times and the most it deviated from a normal distribution was 0.0015%, so we were convinced it worked.  
  
  
My real question I guess is what is happening here? Is there an underlying principle that would suggest this ability to transpose? I postulated that the only way one can make a transposition from n to m (assuming m &gt; n) is if some factor of n divides m, and if m divided by any factor of n can eventually be reduced to something less than n. Example, the case we were using, some factor of 6 (2) divides 20. This can be done twice, reducing 20 to 5, which is less than 6, so the transposition is possible. If we were trying to reduce the 20-sided die to a 40sided die, we could break it into the same four subgroups based on the first roll, but then we would have to choose from 1-5, etc. using only a 4-sided die, for which I cannot devise a method. Does this imply something more generalized?  
  
  
tldr; Made uniform distribution about 1/6 equal to uniform distribution about 1/20, is there a more general rule for 1/n -&gt; 1/m (m &gt; n)?",en
1108157,2011-10-26 22:01:50,statistics,"One more Bayes's Theorem Problem, and a last call for additional problems.",lq0dp,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/lq0dp/one_more_bayess_theorem_problem_and_a_last_call/,8.0,9.0,"I just wrote a new problem involving Bayes's Theorem, and I am quite proud of it:

If you meet a man with (naturally) red hair, what is the probability that neither of his parents has red hair?

Hints: About 2% of the world population has red hair.  You can assume that the alleles for red hair are purely recessive.  Also, you can assume that the Red Hair Extinction theory is false, so you can apply the Hardy–Weinberg principle.

I am looking for more good problems.  An ideal problem should meet at least some of these criteria:

1) It should be based on a context that is realistic or at least interesting, and not too contrived. 

2) It should make good use of Bayes's Theorem -- that is, it should be easier to solve with BT than without.

3) It should involve some real data, which the solver might have to find.

4) It might involve a trick, but should not be artificially hard.

If you send me something that is not under copyright, or is usable under fair use, I will include it in the next edition of Think Stats () and add you to the contributors list.
",en
1108158,2011-10-27 09:21:37,statistics,"I have a real world statistics problem that I want to figure out, and I suck at statistics. Please help :)",lqp89,txl8578,1318824786.0,https://www.reddit.com/r/statistics/comments/lqp89/i_have_a_real_world_statistics_problem_that_i/,3.0,6.0,"The problem itself is a little more complicated, but the basics are as follows:

Say I have a 49% chance of winning a coin toss, for the sake of argument lets say I choose heads all the time and heads for some reason only has a 49% chance of coming up. 

Now, in this particular game I lose if tails comes up 7 times in a row. But I win if I hit heads a cumulative total of 25 times. This can happen in a vast number of coin tosses where the maximum is 6 + 1 + 6 + 1 .... etc until I reach 25 tails. 

So, what are the odds of me winning this game? Basically, how likely is it that I'll cumulatively get 25 heads on the coin before it has an unlucky streak of 7 tails?

In summary since I realize the above may be confusing, I have a game and here are the rules:
1) I win if I flip a coin which lands on heads a cumulative total of 25 times.
2) The game ends and I lose if at any point tails comes up 7 times in a row.
3) Heads only has a 49% chance of landing up.

How likely am I to win?",en
1108159,2011-10-27 10:30:26,statistics,Seasonal forecasting based on historical data for several websites,lqqoy,[deleted],,https://www.reddit.com/r/statistics/comments/lqqoy/seasonal_forecasting_based_on_historical_data_for/,3.0,4.0,"If I have multiple websites in the same category with seasonal data for the past 3 years, what method do I use to make an acceptable prediction for any website for a common season? I feel like i know the parts but not how they go together in this case...

So daily data for n sites in a category where traffic volume varies with the season. How do I best make a consistent prediction for what the traffic is going to be next season given that I have data for many cycles of seasons?


",en
1108160,2011-10-27 11:11:53,analytics,Question about three-month comparison in Google Analytics,lqrfl,[deleted],,https://www.reddit.com/r/analytics/comments/lqrfl/question_about_threemonth_comparison_in_google/,1.0,0.0,"First off, I don't claim to be a Google analytics expert. I'm using it to look at our organisations' site's unique visitor traffic over a three month period. When I look at in in a month-to-month view, there are gains in each month. When I look at the combined three-month view, there's a drop. I'm not doubting it's right, I'm just wondering what's going on.

Can anyone offer a 'like I'm 5' explanation?

Absolute Unique Visitors: March - May
1 March - 31 May 2011:	200,365
1 March - 31 May 2010:	837,827
Change:				-76.09


By the month
March:
2011:		104,716
2010: 		89,517
Change:    	+16.98

April:
2011:		60,989
2010: 		60,188
Change:     	+1.33

May:
2011:		99,639
2010: 		87,864
Change:	       +13.40",en
1108161,2011-10-27 16:12:02,MachineLearning,"Trying to implement a nag-bot, but need some data... anyone willing to help?",lqwps,gms8994,1144342274.0,https://www.reddit.com/r/MachineLearning/comments/lqwps/trying_to_implement_a_nagbot_but_need_some_data/,7.0,3.0,,en
1108162,2011-10-27 17:04:58,MachineLearning,Kaggle screws up and the HHP,lqycl,the_dude_abidz,1319721966.0,https://www.reddit.com/r/MachineLearning/comments/lqycl/kaggle_screws_up_and_the_hhp/,32.0,15.0,"Looks like Kaggle doesn't randomize the order of their training data.  Ouch.

http://www.kaggle.com/forums/t/980/wikipedia-participation-challenge-an-unfortunate-ending

On another topic, the Heritage Health Prize.  Who thinks they'll make the .4 benchmark in the end and pay out the 3M?  Right now they're at .455 :

http://www.heritagehealthprize.com/c/hhp/Leaderboard

I remember that with the Netflix prize it took a year or so to eek out that last 1 percent of performance.  They've got over 10 percent more to go here.

Edit:  Looks like Kaggle isn't interested in learning from this mistake.  They moved the original post to an old competition forum.  It's a shame because its a cool site, but they can't be making friends on the participant side with this bizarre behavior.",en
1108163,2011-10-27 17:46:55,statistics,All your Bayes are belong to us!  Solutions to my favorite Bayes's Theorem Problems (posted last week).,lqzrb,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/lqzrb/all_your_bayes_are_belong_to_us_solutions_to_my/,17.0,5.0,,en
1108164,2011-10-27 22:00:31,statistics,"Trying to determine which sample set displays the greatest improvement, but I suck at statistics! :(",lr9nr,cyanide_productions,1319740609.0,https://www.reddit.com/r/statistics/comments/lr9nr/trying_to_determine_which_sample_set_displays_the/,1.0,3.0,"Hello Everyone,

I am working on compiling the data for my Master's Thesis, but feel quite lost. My hypothesis is to determine which system resource, if any, displays the greatest improvement of end user web page load time; measured in HTTP load time where lower is better. 

With this my sample sets include Baseline Resources, Increased CPU, Increased Memory, and Increased Bandwidth. Initially I issued 5 TCP Synchronize attack sessions against a controlled victim server with Baseline Resources. Afterward I doubled the CPU, repeated the same type of attack 5 times, lowered CPU back to the Baseline Resource standard. This increase, attack, and decrease cycle was also done for Increased Memory and Increased Bandwidth. This provided me with four sets of resource data, each measuring HTTP load time, across 5 attacks of the same form of DDoS attack. 

I believe that ANOVA is the right test to determine if there is in fact a difference between each included level of resources (Baseline, Increased CPU, Increased Memory, or Increased Bandwidth). However, I am lost as of how to determine which of the aforementioned resources displays the greatest level of improvement. 

I thought about using a T-Test to test the difference between each resource, but my statistics book states that this will only inflate Type-1 error. Someone else also suggested that I want to use the Kaplan-Meier, but I am really unfamiliar with this methods as it wasn't taught during my statistics class. If Kaplan-Meier is needed, does anyone know what tools are available that may help process my data and help interpret the results? Free is good, but I do not mind paying. 

Thanks for any available assistance,
Cyanide",en
1108165,2011-10-28 00:44:00,statistics,Random Forest,lrg6i,ksnyder86,1308662430.0,https://www.reddit.com/r/statistics/comments/lrg6i/random_forest/,3.0,7.0,"Hi r/stats.

I'm primarily a lurker but an issue has come up at work that I don't fully understand and no one there can adequately explain it to me. I am hoping that anyone familiar with Random Forest may be able to help explain to me some questions I have.

A little background:

Basically we have a contractor, who is knowledgeable in Statistics and Machine Learning, creating a model that will predict the outcomes of human beings. The problem that I have is that the person in charge of this project on our end either does not understand or doesn't care to understand the methodology of the model. Any question directed at this person is answered with him reading Wikipedia pages to page, and obviously this is unsatisfactory in helping me understand the model. I also am too low on the ladder to be authorized to contact the contractor directly, and plus I doubt that it would go over well that I went over the head of the point of contact on our end. 

I've read the 2001 paper by Brieman, but honestly I am not well versed in lingo of graduate+ level statistics and machine learning (I have bachelor's degrees in Math and Econ, but I hope to apply to a Master's in Stats soon) so the paper mainly went over my head and I feel I need to read more background material in order to wrap my head around Random Forests. 

1) The primary thing I am interested in is how each tree determines whether to create a node, or how does it determine that a variable is a predictor? 

2) My understanding is that each tree is essentially a prediction model itself, and Random Forest gives the plurality of ""votes,"" or predictions, of each ""tree,"" or model as the final prediction. Is that correct?

2a) If that is true and there are three categories that each tree can vote for, if 40 trees vote for A, 30 for B, and 30 for C, would it be fair to say that the trees actually voted for B&amp;C over A? The reason I ask is because in our case, B &amp; C could be condensed into the category of D. If they were, would random forest have voted 40-60?

3) Is it beneficial to use the bootstrapping with replacement with an outcome that is extremely unlikely? It appears that if we had a sample of 100 outcomes, and 99 outcomes were of type A and 1 was of type B, bootstrapping with replacement new samples of size 100 makes duplicates of the rare outcome more likely to be represented in the samples. 

4) If someone could provide me with a good textbook on machine learning that would be helpful!

Thanks r/stats!

",en
1108166,2011-10-28 05:16:33,rstats,Recommendations for new computer specs,lrq66,Lambda_Rail,1264557376.0,https://www.reddit.com/r/rstats/comments/lrq66/recommendations_for_new_computer_specs/,1.0,0.0,"I am looking to get a new computer for some statistics work I'll be doing for my job.  I'm VERY new to this sort of thing and don't know what the limitations/requirements of R will be and I need help selecting the right computer.

I will be handling data sets between ~5k and 5 million records if that helps.",en
1108167,2011-10-28 11:41:36,statistics,"Creating an R package, using developer/productivity tools (video and slides)",ls0nd,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/ls0nd/creating_an_r_package_using_developerproductivity/,18.0,1.0,,en
1108168,2011-10-28 15:05:09,statistics,[Advice] Some guidance about regression and time series,ls3su,[deleted],,https://www.reddit.com/r/statistics/comments/ls3su/advice_some_guidance_about_regression_and_time/,2.0,9.0,"I am currently working on a pet project, trying to predict prices based off some databases i have. I think i have a really good model based off of a simple linear regression that does it, but i want to make it more accurate. Looking at the residuals: http://i.imgur.com/wbdVp.jpg, the bottom right chart  residuals vs. order suggests that a time series would help.  (a zoom up view of it [Here](http://i.imgur.com/oZ04y.jpg))

The problem is, I don't know much about time series. I am not sure where to start, any guidance through minitab or SPSS would be helpful. Thanks!

",en
1108169,2011-10-28 17:45:17,statistics,Questions about a PhD position in Statistics in the Stockholm area,ls85d,Tharlinn,1310133472.0,https://www.reddit.com/r/statistics/comments/ls85d/questions_about_a_phd_position_in_statistics_in/,2.0,1.0,"I wasn't sure where to post this so I'll just give it a shot.
My girlfriend has a bachelor Mathematics and is finishing her MSc in Methodology and Statistics of Behavioural and Social Sciences, both in the Netherlands.
We are looking to emigrate to Sweden, but are a bit lost in the forest of universities and institutes in the Stockholm area. My question to you now /r/statistics, besides Stockholm University and Uppsala, do you have any recommendations to look for jobs/PhD positions in statistics/mathematical statistics/social sciences? Much appreciated!",en
1108170,2011-10-28 21:46:28,statistics,Computer Specs,lsgyf,Lambda_Rail,1264557376.0,https://www.reddit.com/r/statistics/comments/lsgyf/computer_specs/,7.0,19.0,"I'm getting into working with R a lot more at work and I need to upgrade my computer in a bad way.  What should I be looking for in a new machine that will run R?  

I will be working with data sets as large as 4-5 million records if that helps.",en
1108171,2011-10-29 04:09:06,datasets,"Presentation on getting web data through R
",lstcp,[deleted],,https://www.reddit.com/r/datasets/comments/lstcp/presentation_on_getting_web_data_through_r/,0.0,0.0,,en
1108172,2011-10-29 08:43:07,statistics,Goldacre: DIY statistical analysis - experience the thrill of touching real data,lt053,maxwellhill,1142142110.0,https://www.reddit.com/r/statistics/comments/lt053/goldacre_diy_statistical_analysis_experience_the/,17.0,1.0,,en
1108173,2011-10-30 01:13:02,artificial,why has nobody posted this yet: art. neural net,ltkhp,[deleted],,https://www.reddit.com/r/artificial/comments/ltkhp/why_has_nobody_posted_this_yet_art_neural_net/,16.0,3.0,,en
1108174,2011-10-30 03:18:43,statistics,The discovery of structural form,ltnzp,suchow,1317143978.0,https://www.reddit.com/r/statistics/comments/ltnzp/the_discovery_of_structural_form/,1.0,0.0,,en
1108175,2011-10-30 08:25:46,statistics,Who Likes What: Social Media By Demographic (Infographic),ltwkv,geekzu,1274277382.0,https://www.reddit.com/r/statistics/comments/ltwkv/who_likes_what_social_media_by_demographic/,0.0,4.0,,en
1108176,2011-10-30 19:13:19,statistics,Mediator question- possibly really dumb n00b question,lu773,[deleted],,https://www.reddit.com/r/statistics/comments/lu773/mediator_question_possibly_really_dumb_n00b/,4.0,5.0,"I'm planning doing some analyses on some archival data and planning out my hypotheses ahead of time.

There are two DVs of interest: A and B.

A is measured during a task and B is measured after.

I think that Z may predict both A and B.

I'm planning on doing 2 linear regressions, to test this.

I also think that B (which is measured both pre and post so I can control for baseline) might mediate the relationship between Z and A. The problem is that I only have measurements of B before and after (given the nature of what is being measured, measuring B during would be impossible), while A is measured during. So really I think that B during the task predicts A, but I only am able to measure B immediately after. Am I correct in thinking that I am not able to do a mediator analysis for this dataset? Is there something else I could do to look at this hypothesized relationship?
",en
1108177,2011-10-30 23:47:06,MachineLearning,The present and future of the R blogosphere (~7 minute video from useR2011),lufvf,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/lufvf/the_present_and_future_of_the_r_blogosphere_7/,3.0,0.0,,en
1108178,2011-10-30 23:47:21,statistics,The present and future of the R blogosphere (~7 minute video from useR2011),lufvt,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/lufvt/the_present_and_future_of_the_r_blogosphere_7/,3.0,0.0,,en
1108179,2011-10-31 05:45:05,statistics,Am I being a moron or what???,lus07,[deleted],,https://www.reddit.com/r/statistics/comments/lus07/am_i_being_a_moron_or_what/,1.0,1.0,,en
1108180,2011-10-31 17:18:03,MachineLearning,Data is the new .com,lv81n,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/lv81n/data_is_the_new_com/,12.0,4.0,,en
1108181,2011-10-31 18:16:53,MachineLearning,Machine Learning and Big Data at Foursquare,lvact,manning10,1309041336.0,https://www.reddit.com/r/MachineLearning/comments/lvact/machine_learning_and_big_data_at_foursquare/,0.0,2.0,,en
1108182,2011-10-31 19:34:44,statistics,Evaluating a monthly performance measure,lvdi1,[deleted],,https://www.reddit.com/r/statistics/comments/lvdi1/evaluating_a_monthly_performance_measure/,1.0,0.0,"I'm not at all an I/O person, but a friend came to me with a question regarding a monthly performance measure. In short, I'm curious about the reliability of the measurement; it's not a field that I know much about, so I'm not too worried about the validity at the moment. I suspect, from at least eyeballing it so far, that there's a lot of month-to-month variability. What is the best way to go about testing the reliability of a performance metric, and get a numerical outcome?

Edit: Some facts about the measure:

* The range is from about 850 to -450, so it doesn't seem to be affected by 0-boundary skew, though it does tend to be left-skewed, as the median is in the 600s and most are in the 400-800 range - there are major point subtractions for certain failures, I would guess. Would it make sense to exclude those super-penalized employees? The heavy point penalties are definitely not regular, so would seem to be less consistent than whatever other factors go into the score. 

* There are many employees (100+), so sample size isn't a problem.

* Some employees don't have a full year of evaluation, so I'm not sure how to count those - especially if I check the reliability of the relative ranking rather than the actual number outcome itself.",en
1108183,2011-11-01 00:34:47,MachineLearning,Scalable Data Analysis in R - useR2011 talk by Lee E. Edlefsen,lvpxf,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/lvpxf/scalable_data_analysis_in_r_user2011_talk_by_lee/,0.0,0.0,,en
1108184,2011-11-01 12:10:59,AskStatistics,Can you solve this?,lwaaq,mehrracct,1265112604.0,https://www.reddit.com/r/AskStatistics/comments/lwaaq/can_you_solve_this/,1.0,5.0,"http://i.imgur.com/qvzU4.jpg

I have a feeling that the circular logic renders this problem unsolvable, but if there is a solution, I'd be interested to know what it is.",en
1108185,2011-11-01 20:03:33,statistics,Help wanted: Choosing a sample from a population,lwnu0,Warchief_T,1259256310.0,https://www.reddit.com/r/statistics/comments/lwnu0/help_wanted_choosing_a_sample_from_a_population/,2.0,14.0,"Hi! I'm about to research and compare documents from different municipalities in a country and since there is not time to do everything I need to pick a sample. Do anyone of you know some rules of thumb or literature on picking a sample from a population to get it representative. Since it's municipalities I could probably use economy and population as variables here. The only book I have on business research didn't really answer anything but saying that one should argument carefully why and how one picked the sample in.

Thanks

Edit: thanks guys for fast replies, very much appreciated!",en
1108186,2011-11-02 01:35:43,statistics,Help please: Nested comparison is not working in SAS.,lx173,[deleted],,https://www.reddit.com/r/statistics/comments/lx173/help_please_nested_comparison_is_not_working_in/,2.0,0.0,"Hey r/stats,
So I am comparing variations in morphology of dog-whelks by tide height. I want to know if the shell patterns on the dog-whelks vary as a function of tide height.
I have high tide and low tide, and 5 patterns ( dark, light, stripped, red, and white). 
My first assumption was that the patterns are nested within the tide height but when I run 
proc glm; 
class tide color;               
model count=tide color(tide);                                                       
run;   
It gives me an error of zero which makes sense because their is only one replicate.

This is where i am stuck. Does anyone have any suggestions?
",en
1108187,2011-11-02 06:14:30,statistics,The 5 Most Critical Statistical Concepts: discuss,lxbwt,thegabeman,1239690590.0,https://www.reddit.com/r/statistics/comments/lxbwt/the_5_most_critical_statistical_concepts_discuss/,4.0,3.0,,en
1108188,2011-11-02 11:36:41,datasets,Raw answers to online personality tests.,lxj6g,[deleted],,https://www.reddit.com/r/datasets/comments/lxj6g/raw_answers_to_online_personality_tests/,6.0,2.0,,en
1108189,2011-11-02 14:14:11,statistics,Pseudo-code for EM or R code for EM,lxlsz,canteloupy,1287091489.0,https://www.reddit.com/r/statistics/comments/lxlsz/pseudocode_for_em_or_r_code_for_em/,4.0,9.0,"Hello! I have always been very confused about expectation maximisation algorithms, and while I understand the goal and principle I am very curious to understand how it is implemented in practice. I have been trying to find an R implementation for EM for gaussian mixture models to see how it is done, or at least some pseudo-code, but all I found was a Matlab code that is very hard for me to understand.

Does anyone have a source where I might see that?",en
1108190,2011-11-02 17:24:18,statistics,Is it worth it to learn SPSS?,lxr8q,asancho,1232322626.0,https://www.reddit.com/r/statistics/comments/lxr8q/is_it_worth_it_to_learn_spss/,11.0,45.0,"So I would love to get into a Business Analyst role in a biotech company (BS in Biology &amp; recent MBA grad) and thought learning SPSS would increase my marketability to employers. Am I right?

Is it worth it to get IBM SPSS Certified? What books would you guys recommend? Currently I am doing tutorials through Lynda.com.

Thanks for any info.

EDIT: You guys have given me loads of info, and it appears as though I will try to learn SPSS first then learn a bit about R (dont have access to SAS yet).",en
1108191,2011-11-02 19:57:20,MachineLearning,Machine Learning Techniques in Khan Academy [xpost r/programming],lxx4k,personanongrata,1200898594.0,https://www.reddit.com/r/MachineLearning/comments/lxx4k/machine_learning_techniques_in_khan_academy_xpost/,28.0,3.0,,en
1108192,2011-11-02 20:59:25,statistics,Can someone verify my test makes sense?,lxzqe,skolor,1270587499.0,https://www.reddit.com/r/statistics/comments/lxzqe/can_someone_verify_my_test_makes_sense/,6.0,16.0,"I've gotten into playing Magic the Gathering recently, and I've heard a lot of complaints about ""bad shuffling"", so I wanted to do some tests on how randomize shuffling makes a deck. I've got some background in statistics, but its been a while since I've done anything like this, so I wanted to run it past someone with a little more experience.

First off, the existing research about deck shuffling is done on a standard 52-card deck, which isn't particularly relevant to a game of Magic. In particular, you use a 60 card deck, and can have multiple copies of any given card, which at the very least throws the calculations off compared to 52 unique cards. In particular, for my simulations I created a 60 card deck, with 10 unique card, one which has 24 copies and 9 which have 4 copies each. The existing research then uses a shuffling algorithm, and measures how random a deck is using one of a few measures of what a random deck looks like. 

For reference, the primary pieces of existing work are a series of studies by Persi Diaconis. Their work, however, seems to be based on the concept of rising sequences, which don't have a particularly good analogue to a card system which doesn't use numbers, although they could be arbitrarily numbered. 

To solve this issue, I decided to calculate the expected Markov Chains for randomized deck, then compute the same thing on the shuffled decks, and do a Chi-Squared Goodness of Fit test on the results to see how if they matched. If Null Hypothesis cannot be rejected, then it would stand to reason that the tested number of shuffles can be considered random, since for any given card the following card is distributed as would be expected in a truly random distribution.

To test this, I set up some python scripts and randomly shuffled an ordered deck 20 times, recording the state in a database for each shuffle[1]. After 1 million sets of this simulation were performed, the shuffled decks were analyzed. 


So, my question is just to ask if this makes sense, or if I'm missed something crucial. If I did my math right, that means I would be able to take my values for Chi-Squared for each shuffle, and your a Pearson's Chi-Squared test with 99 degrees of freedom (10 cards, so 100 Markov Chains for card states in the deck). If someone wants to, I can supply the scripts I used to calculate this, and the data too, but its a little large (on the order of several gigabytes of data, uncompressed). I'm mainly concerned about the validity of the process right now, since I plan on including a few other shuffling algorithms and see how they stack up.

[1] Note, my shuffling algorithm is slightly different from that used by Diaconis, in that his cuts were a binomial distribution around the mid-point of the deck while mine were normally distributed around the same point.",en
1108193,2011-11-02 21:47:51,datasets,Personality Insights: OkCupid Questions &amp; Answers with Demographic Data,ly1rs,winniechimp,1309463088.0,https://www.reddit.com/r/datasets/comments/ly1rs/personality_insights_okcupid_questions_answers/,7.0,0.0,,en
1108194,2011-11-03 00:16:36,statistics,Help! What statistical test to use for my experiment?,ly7pg,mindbanter,1318569251.0,https://www.reddit.com/r/statistics/comments/ly7pg/help_what_statistical_test_to_use_for_my/,2.0,3.0,"I’m trying to figure out what kind of statistical analysis should be run our experiment. The experiment is on the mismatch negativity phenomenon (MMN) - ERP to an odd stimulus in a sequence of stimuli. [more info on the MMN Phenomenon](http://en.wikipedia.org/wiki/Mismatch_negativity)
we basically present participants with tones that are either neutral (2 different neutral affect stimuli), happy (1), or sad (1).  
Now I’m wondering whether I should do a t-test or repeated measures ANOVA.  
I would be comparing the MMN for Emotional tones paired with Neutral tones and compare that against two emotional tones (“Happy” as a standard and “Sad” deviant” ) to see if the emotional pairing would elicit a larger MMN because the two emotional stimuli have a larger deviance.


[MORE INFO LISTED BELOW- skip if don't need additional info]
“Our objective was to see if there was a quantifiable difference between two different emotional stimuli paired in a sequence as compared to an emotional stimulus paired with a neutral stimulus. Furthermore, wanted to know if the amplitude of the MMN could be used as an objective measure for degree of similarity/difference in the stimulus pairing. By observing and analyzing the auditory MMN elicited from happy tones, sad tones, and neutral tones pairings and measuring the mismatch negativity of the pairings. Our hypothesis is that happy and sad tone pairings will elicit larger MMN amplitude because they have special properties tied to emotion resulting in a larger change-detection response”

Methods Details - - “Auditory MMN Method”

“We used 4 different tones, each with a 500ms duration and played at 65db binaurally using insert earphones.  Each of the tones varied in both fundamental frequency (pitch) and frequency modulation (""warble"").  The ""happy"" tone had a fundamental frequency of 225Hz and frequency modulation of 150hz.  The ""sad"" tone had a fundamental frequency of 125hz and a frequency modulation of 20hz.  Neutral tones had fundamental frequencies of 225 and 125hz and frequency modulations of 20 and 150hz respectively.

Tones were presented in 4 blocks of 100 tones.  One of the four tones served as the standard and the remaining three as the deviant for each of the four blocks of tones.  Additionally, each block was preceded by 15 repetitions of the standard tone for that block.  The inter-tone-interval (time between offset and onset of next tone) was between 3500 and 4000ms (random on each tone presentation).  

Participants were given an Apple ipod and allowed to play games during the tone presentation.”


Thank you!!!!!",en
1108195,2011-11-03 00:43:35,statistics,"
If A union B = S (sample space) , P(A and B compliment) = .25, and P(A compliment) = .35, then P(B) = 
",ly8se,[deleted],,https://www.reddit.com/r/statistics/comments/ly8se/if_a_union_b_s_sample_space_pa_and_b_compliment/,1.0,2.0,,en
1108196,2011-11-03 02:34:36,statistics,Confirmatory Test for Detecting a Spike/Jump in Data,lyd4j,itstimeforaskashley,1317956410.0,https://www.reddit.com/r/statistics/comments/lyd4j/confirmatory_test_for_detecting_a_spikejump_in/,3.0,11.0,"Hi r/statistics,

I have some data that's arranged something like this, where date is the day that was measured, and frequency is the frequency of some event (e.g. percentage of inventory sold, percent of click-through on an advertisement, percent of students who get an A, etc.):

    
    Date  | Frequency
    __________________

    10/21 | .56
    10/21 | .55
    10/22 | .45
    10/23 | .75
    10/24 | .51
    10/25 | .52
    10/26 | .88

If I have an prior prediction of where there should be ""spikes"" in the data (e.g. on 10/23 and on 10/26) is there a statistical test that allows me test where the jump is significant? What I mean by ""spike"" is that there is a sudden increase on a certain day and it goes back down after (is there a more technical term for when the data looks like this: __/\\__ ?


In other words, if I have a hypothesis that some date related event should cause an increase in frequency, is there a confirmatory test that allows me see if the data spikes on that day in a significant or out of the ordinary way?

Second, is there a routine that could take a long list of dates and frequencies and determine the largest or most significant ""spikes"" in data

The only thing I can think of is doing a test of proportions comparing the day before to the day of, and then comparing the day of to the day after. There is dependency in the data I would imagine so maybe a planned quadratic contrast on repeated variables?

Those tests seem rather simplistic and I was wondering if there were more methodologically sound tests that anyone could recommend or other insights people could provide.",en
1108197,2011-11-03 10:58:42,MachineLearning,"The next generation of parallel R
",lysuj,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/lysuj/the_next_generation_of_parallel_r/,13.0,0.0,,en
1108198,2011-11-03 11:43:58,artificial,How Khan Academy is using Machine Learning to Assess Student Mastery (x-post from r/programming),lytk0,vinnl,1213866877.0,https://www.reddit.com/r/artificial/comments/lytk0/how_khan_academy_is_using_machine_learning_to/,20.0,0.0,,en
1108199,2011-11-03 11:56:59,rstats,"Code Optimization: One R Problem, Ten Solutions – Now Eleven!",lytqq,ynotlayerb,1309862137.0,https://www.reddit.com/r/rstats/comments/lytqq/code_optimization_one_r_problem_ten_solutions_now/,1.0,0.0,,en
1108200,2011-11-03 13:22:11,MachineLearning,Machine learning methods for time series,lyv63,perone,1232625557.0,https://www.reddit.com/r/MachineLearning/comments/lyv63/machine_learning_methods_for_time_series/,14.0,19.0,Does anyone knows some good material for machine learning techniques applied for time series analysis ? Thanks !,en
1108201,2011-11-03 14:53:26,datasets,"US patents (1790-present) and trademarks (1870-present): 10TB in all, freely downloadable, compliments of Google",lyx2n,Quintote,1269890504.0,https://www.reddit.com/r/datasets/comments/lyx2n/us_patents_1790present_and_trademarks_1870present/,25.0,4.0,,en
1108202,2011-11-03 17:45:58,MachineLearning,Converting images into time series for data mining,lz2wl,[deleted],,https://www.reddit.com/r/MachineLearning/comments/lz2wl/converting_images_into_time_series_for_data_mining/,1.0,0.0,,en
1108203,2011-11-03 19:05:52,datasets,"Got an IP Address? We'll Give You Foursquare Venues, Wikipedia Articles, Census Data and Much More!",lz638,winniechimp,1309463088.0,https://www.reddit.com/r/datasets/comments/lz638/got_an_ip_address_well_give_you_foursquare_venues/,0.0,0.0,,en
1108204,2011-11-04 00:53:47,statistics,Somebody bet on the Bayes: solution to the red-haired probability problem I posted last week.,lzkf8,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/lzkf8/somebody_bet_on_the_bayes_solution_to_the/,3.0,1.0,,en
1108205,2011-11-04 01:55:37,MachineLearning,IAE disappointed with the Stanford ML course?,lzmvz,duckandcover,1196196596.0,https://www.reddit.com/r/MachineLearning/comments/lzmvz/iae_disappointed_with_the_stanford_ml_course/,3.0,15.0,"I feel like an ingrate writing this.  After all, it's free.  Though I'm well experienced in ML, I'm still shocked at how little math/rigour is in this course.  I signed up for the non-homework version but it was my understanding that except for the homework, the course was the same for both.  If that's not the case and the homework version is more rigorous than ignore this post.

It seems like a high school course.  What can you say when you derive the BP update rules and the chain rule is nowhere to be found nor the derivation of a sigmoid gradient etc.  No discussion of Max Lik. or MAP.  If they spent 5 minutes with a hand-wavy explanation of a partial derivative you wouldn't even need calculus as a prereq for it; just basic algebra.

",en
1108206,2011-11-04 07:33:59,computervision,Detect One Image Within another?,lzzef,ciferkey,1259029610.0,https://www.reddit.com/r/computervision/comments/lzzef/detect_one_image_within_another/,1.0,0.0,"How would I go about detecting one smaller sub image within another? (For example [this](http://i.imgur.com/VwKb0.png) in [this](http://i.imgur.com/Xh8PE.png)) Rotation should be the same but scaling will be different.  I have tried using opencv template matching with the python binding, but its not actually detecting anything.",en
1108207,2011-11-04 13:56:39,statistics,"Is there a ""simple"" or ""for dummies"" resource on  determining the required parameters in an ARIMA model as well as analyzing residuals, ACF, and PACF?",m06nx,dassouki,1215439790.0,https://www.reddit.com/r/statistics/comments/m06nx/is_there_a_simple_or_for_dummies_resource_on/,3.0,5.0,"I've seen numerous youtube videos, read a bunch of tutorials; I looked at the book I have that talks a bit about ARIMA (Statistics, Data Analysis, and Decision Modeling by Evans); however, I can't seem to grasp how to build a proper time series ARIMA model.",en
1108208,2011-11-04 15:54:15,statistics,"Kaggle gets $11 million in venture capital. ""A Site for Data Scientists to Prove Their Skills and Make Money"" (NYTimes)",m09m0,RA_Fisher,1299707119.0,https://www.reddit.com/r/statistics/comments/m09m0/kaggle_gets_11_million_in_venture_capital_a_site/,27.0,0.0,,en
1108209,2011-11-04 20:52:04,statistics,Anyone ever played the card game rook?  Need some probability guys/gals to help me out.,m0laq,MathyMcQuestion,1320431581.0,https://www.reddit.com/r/statistics/comments/m0laq/anyone_ever_played_the_card_game_rook_need_some/,3.0,3.0,"OK, let's see if I can explain this easily enough on here.  The variation of the game of rook that I play has 42 cards, 5-14 and two 'boss' cards.  There are four suits in this game, so there are four 5s, four 4s etc.  The two boss cards are above the 14s, so they 'catch' a 14.  I would like to know the odds of having a re-deal hand.  The rules for a re-deal hand are below.  Each person is dealt 9 face down, random cards, there are four players in a game.

1. no point cards.  in this scenario you can have, for instance, 4 13s, 4 12s an an 11.  since only 5, 10, 14 and the boss cards are points, you are eligible for a re-deal.  any combination of 6, 7, 8, 9, 11, 12, 13s fit this criteria.

2. no cards above a 10.  in this scenario you are allowed to any number of point cards 10 or below (5s, or 10s).  however, in this scenario you cannot have 11, 12, 13,14 or the two boss cards. 

",en
1108210,2011-11-04 22:07:06,MachineLearning,looking for ML aficionado in London for great chats and maybe a startup,m0od7,MLstartupLondon,1320436714.0,https://www.reddit.com/r/MachineLearning/comments/m0od7/looking_for_ml_aficionado_in_london_for_great/,18.0,4.0,"TL;DR? Here's the gist:

Me: 3 startups under my belt. Started as a programmer, then trainer, then entrepreneur, now CTO &amp; Board member for a leading customer insight company part of large bank. Large system and infrastructure specialist. Extensive &amp; practical experience in raising funds and successfully managing both startup and established businesses. Fascinated by the power of data. Can't imagine myself spending the rest of my life being a cog in the machine.

You: Machine learning specialist, programmer, analyst, understands how to navigate and crunch large datasets, from BI to predictive analytics. Interested in implementing applications from fraud detection to margin improvements through better clustering regardless of industry. Fascinated by the power of data. Can't imagine himself spending the rest of his or her life being a cog in the machine.


The startup:

The core idea it to build platforms and systems around the progressively larger datasets held by various sized companies, helping them solve big issues - cost reduction, profitability and reducing risk. I’m an infrastructure and software specialist and have access to 1) systems, 2) datasets 3) extensive practical in certain industry segments, namely web-scale companies and tier 1 retailers.

This project is in the very early planning stages. I'm looking forward to discuss the form it could take with like-minded individuals but with complementary skills sets, namely: predictive analytics &amp; AI as it applies to machine learning on large datasets. 

Want more specifics ideas? I have plenty of these, but I’m sure you do to, so let’s meet face to face and discuss them.

Ultimately the goal is to crystallize on a specific concept, develop together a minimum viable product and get the company bootstrapped or angel-funded (something I also have plenty of experience with), all via a lean startup model.


My philosophy on startups:

Startups built in one’s free time often fail because they drag on, ending up as little more than side projects you can’t quite get rid of (due to co-founder guilt, or perhaps the little money they bring in every month). 

The core idea for this project is based on lean, that is, to launch a minimum viable product as early as possible. Getting feedback. Measuring results (important!). Pivot if it’s not working. This helps tremendously in staying motivated, limits the dreaded paralyzing fear of failure, and more importantly, keep the time from inception to first client/funding to a minimum.

If it sounds interesting please message me and we can exchange contact details! Worst that can happen is we have a great chat!",en
1108211,2011-11-05 01:03:19,datasets,Any software recommendations for animating graphs?,m0uqu,kenfar,1239227323.0,https://www.reddit.com/r/datasets/comments/m0uqu/any_software_recommendations_for_animating_graphs/,6.0,5.0,"I've got a number of data sets that I'd like to present present using some kind of animated graphs - like [Google's Public Data Explorer](http://www.google.com/publicdata/home).

Can someone point me to software that ideally runs on linux?
",en
1108212,2011-11-05 14:26:04,statistics,"""Dr Goldacre suggests the difference between the best- and worst-performing authorities falls within a range that could be expected through chance. But this does not change the fact that there is a threefold difference between the best and worst local authorities."" Amazing.",m1cgx,MurrayBozinski,1281197925.0,https://www.reddit.com/r/statistics/comments/m1cgx/dr_goldacre_suggests_the_difference_between_the/,8.0,6.0,,en
1108213,2011-11-05 15:32:40,statistics,Expanding probability distributions (x-post from /r/math),m1dhn,ipu0014,,https://www.reddit.com/r/statistics/comments/m1dhn/expanding_probability_distributions_xpost_from/,1.0,2.0,"It was once said to me that every probability distribution can be written as

    [; P (x_1, x_2, \dots, x_N) = \frac{1}{Z} \exp \{A \sum_i x_i + B \sum_{ij} x_i x_j + C \sum_{ijk} x_i x_j x_k + \dots\} ;]

Is that a fact? Does anyone know where I can read more about it?

Much obliged!",en
1108214,2011-11-05 20:56:29,MachineLearning,"Graduate course on Machine Learning at CMU from 
Tom Mitchell.",m1m7m,kunjaan,1223255404.0,https://www.reddit.com/r/MachineLearning/comments/m1m7m/graduate_course_on_machine_learning_at_cmu_from/,42.0,3.0,,en
1108215,2011-11-06 01:44:18,statistics,Which MS programs do I have a shot at getting into?,m1vd8,[deleted],,https://www.reddit.com/r/statistics/comments/m1vd8/which_ms_programs_do_i_have_a_shot_at_getting_into/,0.0,5.0,"**Tldr; Math major with a 3.99/4.00 over the past 2 years at a school like Virginia Tech/Purdue/UConn (although I'll prob get a B in Real Analysis this semester). I've taken 3 statistics courses and have a decent comp sci background. No research experience. Interested in a terminal MS programs in statistics. Where can I get in?**

I'm a math major at Virginia Tech graduating in Spring 2012. I recently decided that I'm tired of math, so I've been looking into Masters programs (I don't want a PHD) in statistics because it seems like a great field to get into. I have absolutely no idea as to what schools I have a shot at getting into though, so I'll describe my background and hopefully someone that's knows a little bit about the application process can give me an idea.

* *Grades*: I have around a 3.7/4.0 GPA overall and a 3.99/4.00 over the past 2 years (all A's and an A-). But I'm pretty burned out right now so this semester will see a slight decline. I'll probably get a B in Real Analysis (if I worked my ass off I could prob pull a B+ or A- but I'm so tired of theoretical math right now and am not convinced that it's worth the effort, if I'm wrong then convince me otherwise). Everything else should be A and maybe a A- or two though.

* *Courses (outside math)*: I've taken 2 ""Probability &amp; Statistics"" courses and a Bayesian statistics course. Next semester I'm taking time series. I have a decent computer science background and have taken ""Data Structures &amp; Algorithms"" along with ""Artificial Intelligence"". 

* *Research Experience*: None, although I plan to do machine learning research next semester. Since most grad school application deadlines seem to be in Jan/Feb, I guess I won't be able to say much about that in my application though.

* *Work Experience*: IT internship at a large bank. Programming job before that.

* *GRE*: Haven't gotten my scores back yet. The ranges I got were Quant: 750-800 (I know that doesn't say much), Verbal: 570-670, and I prob got like a 4 on the writing (I'm sure I could def boost this up to at least a 4.5, but I'm not convinced it's worth the effort unless I get a shitty quant score and have to retake the GRE anyways). I'm not taking the math subject GRE.

~~~~~~~~~~~~~~~~~~~

So where can I get in? A friend of mine told me that I could probably get into Columbia. Does he have any idea what he's talking about?

Ideally I'd like to go to a school in a fun location like Columbia or UCLA rather than a college town in the middle of nowhere. I like big cities and/or warm weather. At the same time, I'm not exactly rich, so if I can't get funding then I'll have to pay for school through student loans. So I can't afford to go to an expensive program on the other side of the country if it won't provide me the job opportunities to make up for the tuition/plane ticket costs.

I'm def applying to UVA (in-state tuition for me, although I really want to gtfo of Virginia if I can), UCLA, and Columbia. Anybody have any idea as to what my chances are of getting into any of these programs? Where else should I be looking? I'm thinking GWU, UIUC, and UFlorida too.

Is it possible to get funding if you're not planning to do a PHD, or is it extremely difficult/unlikely?

Also, how important is school reputation for grad school in statistics? To put this into perspective, I'm also applying for Masters in Financial Engineering programs, and school reputation is extremely important in that field. They're also very expensive, so I've been told that it's probably not worth paying the $55k tuition or whatever for a tier 2  school. 

But I figure that since statistics jobs aren't as competitive as Wall Street quant jobs, it's not nearly as big a deal how well-known your school is. And statistics is pretty meritocratic rather than pedigree-based. Right?",en
1108216,2011-11-06 07:03:53,statistics,What is the distribution of this phenomena?,m24tj,[deleted],,https://www.reddit.com/r/statistics/comments/m24tj/what_is_the_distribution_of_this_phenomena/,1.0,2.0,"There is a bunch of random variables which may or may-not be identically distributed. Each of them has a distribution based a large number of parameters. Values of each of these parameters are based on a majority vote of previous outcome of these random variables.

I have with me the previous outcomes of these random variables. I do not know the distribution functions of any of these random variables. However I do know how to calculate the parameters these random variables might use.

I want to find the distribution of the next outcome.

Is this possible? How would I go about doing this?",en
1108217,2011-11-06 10:50:32,MachineLearning,"Scikit-learn, Machine Learning in Python - ""This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language""",m296s,urish,1221689900.0,https://www.reddit.com/r/MachineLearning/comments/m296s/scikitlearn_machine_learning_in_python_this/,34.0,5.0,,en
1108218,2011-11-06 20:16:28,MachineLearning,Help with R nnet script! What am I doing wrong?,m2jyq,GutterBaby69,1309625171.0,https://www.reddit.com/r/MachineLearning/comments/m2jyq/help_with_r_nnet_script_what_am_i_doing_wrong/,0.0,6.0,"I train the neural network and it predicts every element of the training set to be of a single type.

http://imgur.com/DW0lp",en
1108219,2011-11-06 21:34:14,statistics,Wilkinson Test,m2mk8,helpme2332,1320607910.0,https://www.reddit.com/r/statistics/comments/m2mk8/wilkinson_test/,1.0,1.0,I have been experimenting with the Wilkinson test for the comparison of several statistical softwares and I was wondering if anyone knew of any other similar tests? Thanks!,en
1108220,2011-11-06 21:54:46,MachineLearning,R code for optimization under constraint,m2n8p,[deleted],,https://www.reddit.com/r/MachineLearning/comments/m2n8p/r_code_for_optimization_under_constraint/,1.0,1.0,"Hi all,

I've used R's excellent nlm() function for unconstrained optimization in the past. Now I'm a bit curious about constraints of the Lagrange type. Does the Lagrange method translate to real constraint abilities? Are there other numerical methods for optimization under constraints?",en
1108221,2011-11-06 23:06:51,statistics,Nice visualization of the relationship between different distributions,m2pq9,blind_swordsman,1318716377.0,https://www.reddit.com/r/statistics/comments/m2pq9/nice_visualization_of_the_relationship_between/,3.0,1.0,,en
1108222,2011-11-06 23:14:05,statistics,"Significance vs. ""significance"", transposed conditionals, meta-analysis, bayesianism, and so on:  the usual casualties when consenting scientists commit statistics",m2pyn,claird,1249749559.0,https://www.reddit.com/r/statistics/comments/m2pyn/significance_vs_significance_transposed/,19.0,9.0,,en
1108223,2011-11-06 23:41:04,MachineLearning,What is the distribution of the next outcome?,m2qzk,[deleted],,https://www.reddit.com/r/MachineLearning/comments/m2qzk/what_is_the_distribution_of_the_next_outcome/,2.0,12.0,"There is a bunch of random variables which may or may-not be identically distributed. Each of them has a distribution based a large number of parameters. Values of each of these parameters are based on a majority vote of previous outcome of these random variables. I have with me the previous outcomes of these random variables. I do not know the distribution functions of any of these random variables. However I do know how to calculate the parameters these random variables might use.

I want to find the distribution of the next outcome.

Is this possible? How would I go about doing this?",en
1108224,2011-11-07 02:41:42,statistics,What tests can I use to analyze the frequency of letters in different texts?,m2x9p,linguistatistics,1320626222.0,https://www.reddit.com/r/statistics/comments/m2x9p/what_tests_can_i_use_to_analyze_the_frequency_of/,2.0,2.0,"I'd like to look at the frequency of letters in texts throughout English history to determine which letters are now used more/less than they once were. Once I have gathered this data, what are some tests that I can use to analyze and discuss this data/prove it's significant? I don't need help with the math or obtaining formulas, I'm just unsure of what tests could be applicable here.",en
1108225,2011-11-07 05:32:06,artificial,"Do you like to make a bold statement, choose giant artificial flower arrangements, huge sale going on now, with free shipping.",m33jr,cchic,1312853008.0,https://www.reddit.com/r/artificial/comments/m33jr/do_you_like_to_make_a_bold_statement_choose_giant/,0.0,0.0,,en
1108226,2011-11-07 06:45:14,statistics,My wife is feeling hopeless about her #OWS statistics homework,m367v,tastypop,1307136615.0,https://www.reddit.com/r/statistics/comments/m367v/my_wife_is_feeling_hopeless_about_her_ows/,0.0,13.0,,en
1108227,2011-11-07 14:45:39,MachineLearning,Summer schools 2012?,m3g72,mt69298,1275570871.0,https://www.reddit.com/r/MachineLearning/comments/m3g72/summer_schools_2012/,11.0,4.0,Though I'd ask and see if people know of any summer schools planned for 2012.,en
1108228,2011-11-07 17:54:29,statistics,Bionmial Distribution situation to apply on excel,m3l7i,cloj63,1320680807.0,https://www.reddit.com/r/statistics/comments/m3l7i/bionmial_distribution_situation_to_apply_on_excel/,1.0,5.0,"
I want to build an excel sheet to calculate some ""probabilities"".
It is a Binomial Situation, but what I want to know is the number of trial ""n"" that is necessary for a CI (probability) of 95%. The point is; instead of the number of trial, I want to have the Probability/Confidence interval as an input.
Of course, I will know the ""p"" value.

I know it is possible to estimate through the normal distribution, however I will mostly come in situation where np or n(1-p) will ultimately be smaller than 5.

Here is one typical scenario I want to resolve.

P(X is greater or equal than x)
p=0.0092
what ""n"" do I need to have so P(...)=95%

For the time being I did a work-around using only P(greater or equal 1) allowing me to do the complementary: P(X=0)=1-0.95

0.05= {n!/[x!(n-x)!]}*0.0092^x * 0.9908^(n-x)

0.05= {n!/[1*n!]} *1 * 0.9908^n

0.05= 0.9908^n

ln(0.05)/ln(0.9908)=n

giving me:
ln(1-CI)/ln(1-p)=n


But I want my excel sheet to be able to solve for any x; what I came with only works for x=1 (and x=0 with some twitching...)

note that here it gives me n=325; so np would be 2.99 &lt; 5; I would not be able to use the normal distribution to estimate this situation.

Is there any way to estimate reliably without this kind of constraint?",en
1108229,2011-11-07 22:56:40,AskStatistics,What percent of statistics are made up?,m3xhl,[deleted],,https://www.reddit.com/r/AskStatistics/comments/m3xhl/what_percent_of_statistics_are_made_up/,0.0,1.0,"I seriously am looking for the answer for this age old joke. 

I'm not talking about what your friend claims on facebook- the parameters for these stat would be that they are written in an academic/journalistic publication.",en
1108230,2011-11-08 01:35:26,statistics," Integrating C or C++ into R, where to start?",m4424,Wonnk13,1287022683.0,https://www.reddit.com/r/statistics/comments/m4424/integrating_c_or_c_into_r_where_to_start/,12.0,15.0,"What is your preferred language for speeding up R code? Coming from a background of R and Python I have absolutely no experience with compiled languages. With the increasing popularity of the Rcpp package I imagine most will recommend C++ but i've noticed it gets a lot of hate on r/programming and i've seen people on r/learnprogramming recommend C before C++ coming from a Python background. I've started K&amp;R for C (really liking it), but will that help me with R?  ",en
1108231,2011-11-08 02:20:44,artificial,Guarantee great prices and free shipping on outdoor Spiral Topiary trees. ,m45vh,cchic,1312853008.0,https://www.reddit.com/r/artificial/comments/m45vh/guarantee_great_prices_and_free_shipping_on/,1.0,0.0,,en
1108232,2011-11-08 04:38:53,statistics,Baye's theorem and help to solve an example.,m4bf5,[deleted],,https://www.reddit.com/r/statistics/comments/m4bf5/bayes_theorem_and_help_to_solve_an_example/,5.0,8.0,"Suppose that a bag has a 0.5 chance of containing 70 green chips and 30 white chips, and a chance of 0.5 of containing 70 white chips and 30 green chips. A green chip is drawn from the bag at random and then replaced. What is the probability now that the bag is predominently green?


Please, explain to me how to solve this, what the answer is and why it isn't 0.5. I can't seem to understand why the chances of 0.5 are now supposed to have changed just because you drew a green chip.

EDIT: Thanks to you guys, this seems to be the solution:

Let's say the first bag is A and the second is B.

The probability that the bag is predominantly green after drawing one green chip at random and putting it back in the bag, is:

Probability that chip is green from green bag = Probability of chip being green times probability of having picked bag A / (probability of drawing a green chip from A times probability of drawing a green chip from B)

P(A|x=G)=(P(x=G|A) x P(A))/(P(x=G)

P(A|x=G)=(0.7x0.5)/((0.7x0.5)(0.3x0.5))=0.7",en
1108233,2011-11-08 08:28:23,statistics,"""Most profound statistics question"".",m4jv4,brews,1263715301.0,https://www.reddit.com/r/statistics/comments/m4jv4/most_profound_statistics_question/,0.0,2.0,,en
1108234,2011-11-08 08:40:41,statistics,A point in the right direction with this probability problem.,m4k6e,[deleted],,https://www.reddit.com/r/statistics/comments/m4k6e/a_point_in_the_right_direction_with_this/,1.0,0.0,"In a class of 30 students, a professor randomly assigns 12 A's and 18 B's to the students. b)	On the class list the names are written alphabetically.  There are 3 students whose names start with A, 5 students whose names start with B, and 2 students whose names start with C.  Starting from the top of the list, what is the probability that the sixth A grade assigned is to one of the students whose name starts with C?. I tried writing a big-ass tree diagram but that fell apart pretty quickly. I'm just looking for some suggestions. Thanks
",en
1108235,2011-11-08 17:57:38,statistics,The world's hardest probability problem: the red-haired girl named Florida.,m4w0s,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/m4w0s/the_worlds_hardest_probability_problem_the/,3.0,24.0,,en
1108236,2011-11-08 19:49:07,statistics,Alarming Home Burglary Statistics,m50bi,mikedutkewych,1320774397.0,https://www.reddit.com/r/statistics/comments/m50bi/alarming_home_burglary_statistics/,0.0,0.0,,en
1108237,2011-11-08 20:51:34,datascience,Looking deeper into Federal Election Commission Data with Gremlin,m52uz,mhermans,1169219262.0,https://www.reddit.com/r/datascience/comments/m52uz/looking_deeper_into_federal_election_commission/,1.0,0.0,,en
1108238,2011-11-08 20:58:09,data,The World According to the Internet,m534u,winniechimp,1309463088.0,https://www.reddit.com/r/data/comments/m534u/the_world_according_to_the_internet/,1.0,0.0,,en
1108239,2011-11-09 06:21:41,statistics,Any ideas for a regression project?,m5ptj,Hiolpe,1294958205.0,https://www.reddit.com/r/statistics/comments/m5ptj/any_ideas_for_a_regression_project/,1.0,2.0,,en
1108240,2011-11-09 10:41:17,statistics,What is the minimum number of people needed to be involved in a story on reddit to assume that one other member of the story is a redditor?,m5x7n,[deleted],,https://www.reddit.com/r/statistics/comments/m5x7n/what_is_the_minimum_number_of_people_needed_to_be/,4.0,11.0,"I'm not sure this is exactly the right place to ask this, but I figured I'd give it a shot.

I'm curious, there are a non-trivial amount of people who visit reddit. At what point, in terms of people directly involved, does it become likely that someone involved will read a story that they're part of? For example, if I tell a story on reddit that features two people, it is unlikely that the other person is a redditor (more on this in a second), whereas if I tell a story with twenty people, it is significantly more likely that one of those twenty will read it. I'm curious if we can approximate that number of people where the probablity becomes greater than 50%.

For arguments sake, let's assume that everyone has an even chance of being a redditor. Obviously if I tell a story about a reddit meetup, the other actors are significantly more likely to be redditors than a story about a ludite convention. I know it's a bit of selection bias, but to satisfy my curiosity, let's just assume it's even.

Obviously in addition to the answer, I'm curious about the methodology.

Thanks.",en
1108241,2011-11-09 11:35:07,MachineLearning,How Dark Sky Works: Neural Networks and Computer Vision,m5y4l,qvadis,1278024806.0,https://www.reddit.com/r/MachineLearning/comments/m5y4l/how_dark_sky_works_neural_networks_and_computer/,32.0,3.0,,en
1108242,2011-11-09 18:44:29,computervision," It would be good to be able to apply similar techniques to produce images in color, but no one I have consulted is quite sure how to do that.",m68kv,[deleted],,https://www.reddit.com/r/computervision/comments/m68kv/it_would_be_good_to_be_able_to_apply_similar/,1.0,0.0,,en
1108243,2011-11-09 19:37:48,statistics,"Working statisticians, tell us about your Code Versioning Systems",m6au9,[deleted],,https://www.reddit.com/r/statistics/comments/m6au9/working_statisticians_tell_us_about_your_code/,16.0,30.0,"In a previous post, readers of this subreddit spoke a little bit about what they do and where they work. The spread was representative, a lot of tech and biotech, industrial stats, and academics. 

Most statisticians, as you know, are intimate with coding as the computer has revolutionized the field. To do the work we do, we are constantly developing code and related documentation for projects large-and-small. Sometimes this is a group effort and there needs to be effective communication between the employees about changes and revisions.

I'm curious to hear what others' experiences have been with versioning their code for projects. At my work, we're a smaller biotech. I gotta say the versioning system is primative at the moment. There's a mix of commented one-liner changes, log files, archived files with dates, initials, or any combination or lack of the two, sometimes back-dated files are forks and sometimes they're completely deprecated so nothing ever gets cleaned up. We have a large number of people who contribute small amounts to each project, so I find it's quite an unstable system. 

Because this is kind of driving me mad, I'm hoping to poll others and find out a bit about what's done elsewhere.

EDIT: It might be useful for you to say whether or not your code is proprietary and the number of people contributing to the project.",en
1108244,2011-11-09 20:02:09,computervision,Segmenting Google map satellite view images,m6buw,[deleted],,https://www.reddit.com/r/computervision/comments/m6buw/segmenting_google_map_satellite_view_images/,0.0,0.0,"Hey guys, I am working on a project where I have to segment and detect houses in the Google map's satellite view. The primary issues are the noise due to shadows, color of the roofs not being distinctive enough from the surroundings, and the variability of the houses itself. Any ideas",en
1108245,2011-11-09 21:40:06,data,Explore hundreds of datasets for your research or work,m6g0v,malcolmtesla,1320246404.0,https://www.reddit.com/r/data/comments/m6g0v/explore_hundreds_of_datasets_for_your_research_or/,6.0,0.0,,en
1108246,2011-11-09 23:07:34,statistics,I have no idea what I'm doing,m6jn1,Skeetronic,1275623465.0,https://www.reddit.com/r/statistics/comments/m6jn1/i_have_no_idea_what_im_doing/,2.0,10.0,Can anyone suggest a good assistance site? It would be a lot to ask for general guiding for an intro stats class but I'm completely lost. The time/curriculum comparison doesn't allow for extensive studying of the subject matter. I'm so lost.,en
1108247,2011-11-09 23:29:34,data,Everything New is Old Again: Mapping the Republic of Letters (Video),m6khw,curthopkins,1292454943.0,https://www.reddit.com/r/data/comments/m6khw/everything_new_is_old_again_mapping_the_republic/,1.0,0.0,,en
1108248,2011-11-10 06:47:38,computervision,How Dark Sky Works,m7200,ciferkey,1259029610.0,https://www.reddit.com/r/computervision/comments/m7200/how_dark_sky_works/,6.0,0.0,,en
1108249,2011-11-10 11:53:49,statistics,[modelling] Can anyone help me to understand this?,m79qc,[deleted],,https://www.reddit.com/r/statistics/comments/m79qc/modelling_can_anyone_help_me_to_understand_this/,2.0,3.0,"The data I am working with is a multinomial distribution. It is essentially a matrix of counts.

I need to assess whether the data represent a monotonic or non-monotonic function. This is relatively simple conceptually, because it simply means that some of those counts need to be bigger, smaller or the same as others. What follows is what I do not understand:

""When comparing nested models, the G2 statistic is distributed as chi-square with N - k degrees of freedom, where N is the number of data points and k is the number of model parameters. This does not apply in the present case, since the models differ in order constraints rather than number of parameters and, in addition, it is not possible to specify the number of such parameters a priori.""",en
1108250,2011-11-10 13:33:06,MachineLearning,Are there any ML software that can look at an image and recognize if it is a certain type of animal?,m7bbg,[deleted],,https://www.reddit.com/r/MachineLearning/comments/m7bbg/are_there_any_ml_software_that_can_look_at_an/,11.0,11.0,"I know Google's image search is pretty good for this, but this seems more color oriented than say something else. I am trying to take images of insects and have a piece of software recognize the insect species. 

Edit:
What I currently have are about 3000 insect images stored and converted into Python array files. I am really not sure what particular algorithm I can use for training. But I can test that algorithm on samples I find in nature. I am trying to use some image recognition to identify a large mass of collected bugs for what species they belong to. Right now I am trying to visualize the eyes only so I can atleast get proper orientation. ",en
1108251,2011-11-10 17:03:49,artificial,"Looking for the best place with reasonable prices for artificial palm trees,silk plants,silk flowers and floral arrangements. Plus, free shipping!",m7g9n,cchic,1312853008.0,https://www.reddit.com/r/artificial/comments/m7g9n/looking_for_the_best_place_with_reasonable_prices/,1.0,0.0,,en
1108252,2011-11-10 17:43:43,MachineLearning,Quick reference for polishing your ggplot2 plots by Hadley Wickham ,m7hob,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/m7hob/quick_reference_for_polishing_your_ggplot2_plots/,11.0,6.0,,en
1108253,2011-11-10 18:42:26,MachineLearning,Who is hiring?,m7jz2,0111001101110000,1298062458.0,https://www.reddit.com/r/MachineLearning/comments/m7jz2/who_is_hiring/,10.0,14.0,,en
1108254,2011-11-10 21:13:14,statistics,Solution to the Red Haired Girl Named Florida problem.,m7qfm,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/m7qfm/solution_to_the_red_haired_girl_named_florida/,11.0,17.0,,en
1108255,2011-11-10 23:00:07,computervision,opencv books with neural nets,m7v23,strategosInfinitum,1279064470.0,https://www.reddit.com/r/computervision/comments/m7v23/opencv_books_with_neural_nets/,2.0,5.0,"Does anyone know of any good opencv books with guides on neural nets? seems the O'reilly book only briefly mentions but doesn't explain them or show any examples of their use?

I want to convert images to matrices for training the NN  but im not even sure what format to use binary blobs?

Edit: pretty much this , explains it http://blog.damiles.com/2008/11/basic-ocr-in-opencv/",en
1108256,2011-11-11 00:57:07,artificial,The cognitive systems paradigm,m7zvf,kmjn,1292111209.0,https://www.reddit.com/r/artificial/comments/m7zvf/the_cognitive_systems_paradigm/,19.0,5.0,,en
1108257,2011-11-11 01:31:13,statistics,"Coin Flips, Sports Betting, and Confidence Intervals",m819x,MozzNJ,1167769396.0,https://www.reddit.com/r/statistics/comments/m819x/coin_flips_sports_betting_and_confidence_intervals/,0.0,1.0,"In a discussion with someone who writes for a sports betting website. He's saying that ""The last 100 games, the home team has covered 59 of them"". I quickly reply that not only is that irrelevant data (which is besides the point of this post), but the sample size is nowhere near enough to think that it's valid going forward.

Sure enough, I go to:

[This online tool](http://src.alionscience.com/toolbox/oneshotcalc.htm)

And put in 0.41 for p, 100 for n, 41 for r, and get a 46% confidence, which is crap. I give him the benefit of the doubt and up the p to 0.473, which is the breakeven point for wagering, IIRC. 

This gets me an 87.8% conf that using this method will be breakeven or better over the next 100 wagers, which is still not the 95% that would signal a valid strategy. I get that at about 140 successes in 240 attempts. 

Question to you guys is: Am I doing this right? As an actuary, I feel mildly derpish about asking, but screw it, I haven't taken this exam in 10 years.

TIA",en
1108258,2011-11-11 01:35:22,AskStatistics,Better forecasting? I'd like recommendations,m81fu,seainhd,1303768263.0,https://www.reddit.com/r/AskStatistics/comments/m81fu/better_forecasting_id_like_recommendations/,0.0,2.0,"my product is new to market, demand is there. my retailer has poor forecasting so i'm wondering if i can create a regression line, then look at the outliers to find acceleration?

Basically i needed to know after week 5 that sales in week 12 would be so high. 


Below are weekly sales. each line is a new week, oldest on top.

48

127	

87	

99	

151	

70	

140	

145	

135	

209	

215	

335	

318

",en
1108259,2011-11-11 02:07:41,statistics,Implementation of Support Vector Clustering in Python ?,m82nw,tobiassp,1273886327.0,https://www.reddit.com/r/statistics/comments/m82nw/implementation_of_support_vector_clustering_in/,3.0,2.0,"I am currently using SVC in RapidMiner, but need to integrate with existing Python code.  Is anyone aware of an implementation of SVC for Python ?",en
1108260,2011-11-11 03:35:31,statistics,"Ask r/statistics: I want to eventually work in business, not teach.  Is a PhD worth it?",m85wo,YaoPau,,https://www.reddit.com/r/statistics/comments/m85wo/ask_rstatistics_i_want_to_eventually_work_in/,14.0,20.0,"I'm six months from a Master's degree in stats and I'm feeling a little lost.   Ever since starting the program I figure I'd eventually go on for a PhD.  But now that I'm about to apply, I have these unknowns creeping in.

What I know for sure is I'm not interested in working in academia.  My professor told me a PhD is really for those who eventually want to teach, as almost all statistics jobs are open to candidates with just a Master's... you just gotta get in, then it's a matter of learning as you go and moving up.

I keep hearing biostats is the subject to study.  I was never good at biology in high school, but if that's where the PhD jobs are I'm sure I could pick it up.  Predictive analytics is what I'm interested in most, but I've heard there's fewer available jobs, and a PhD with a focus on data mining won't give me much of an edge over someone with a Master's degree.  It's been hard for me to find credible information for this online.

If you've been in my position, what choice did you make and what was your experience?  Would you recommend getting a PhD?  What are your thoughts on the value of a PhD with a predictive analytics focus?",en
1108261,2011-11-11 05:28:33,statistics,Expected Salary by Major,m89vz,ntlaxboy,1299697302.0,https://www.reddit.com/r/statistics/comments/m89vz/expected_salary_by_major/,17.0,4.0,,en
1108262,2011-11-11 06:11:03,statistics,What test to use?  Chi Square or Fisher exact?  My non-parametrics stats are rusty...,m8bgr,DoctorHandwaver,1289662885.0,https://www.reddit.com/r/statistics/comments/m8bgr/what_test_to_use_chi_square_or_fisher_exact_my/,4.0,7.0,"I have a data set of neuronal recordings and I want to compare the proportion of neurons which are active in two groups.

I made 8 sets of measurements (each measurement was a brain slice recording).  So I recorded from 8 slices.  Group A had 42 cells total (2 of which were active, 40 were inactive) and Group B had 540 cells total (13 of which were active, 538 were inactive).

I don't think I have to account for the number of cells in each recording, right?

My guess is I use a Fisher exact test (because of small expected values.. and I should get a p value of .371.

Is this correct?  My non-parametric stats are really rusty...


In a separate experiment I had Groups C and D.
8 recordings again totaling:
Group A 18 neurons active,  509 inactive.
Group B 13 neurons active, 520 inactive.

Again the measurements in each individual slice don't matter, right?
Not sure if I use chi square of Fisher's exact, because of size of expected values...




 ",en
1108263,2011-11-11 17:34:44,statistics,Statistics on Lottery.,m8r2k,thegamersfix,1274464757.0,https://www.reddit.com/r/statistics/comments/m8r2k/statistics_on_lottery/,1.0,12.0,"Me and a co-worker were talking about statistics (which I know nothing about) and lottery. On the Florida lotto there is a 1 in almost 30,000,000 million chance. We were discussing about the possibility about purchasing every possible combination number. 

Then I thought that you could exclude some numbers, that have less of a chance of hitting. Say like numbers in sequential numbers. However, this brought up a discussion about if there is any difference statistically between six numbers coming up in sequence and six numbers coming up out of sequence. I am genuinely interested in how statistics work, but don't know where I could find more about this question. So I turned to reddit.",en
1108264,2011-11-11 17:53:56,statistics,Is this game a variant of the Monty Hall Problem?,m8rq6,BKred09,1299986442.0,https://www.reddit.com/r/statistics/comments/m8rq6/is_this_game_a_variant_of_the_monty_hall_problem/,2.0,3.0,"The player is presented with 5 cards, only one of which is an ace.  The player turns over cards one at a time until he finds the ace.  If he finds it on the first flip, he wins $100; on the second, $50; then $20, $10, and $5.  The particular assigned question to this scenario was to create a simulation to determine the average amount won from this game.  Naturally, I could choose random numbers 0-9, ignoring repeats and anything 5-9, stopping when I get a 0 and recording the corresponding prize.

The question that came up in class, though, was about the probability of getting the ace in any pick.  Isn't it 20% at each point?  Or does the probability change by knowing what isn't an ace?",en
1108265,2011-11-11 23:45:29,statistics,I hate misleading statistics,m95gr,statsnazi,1321047769.0,https://www.reddit.com/r/statistics/comments/m95gr/i_hate_misleading_statistics/,29.0,5.0,,en
1108266,2011-11-12 01:49:05,analytics,Just me and my buddy with a huge check after winning the Adobe Web Analytics Competition,m99r7,TylerRiggs,1309136645.0,https://www.reddit.com/r/analytics/comments/m99r7/just_me_and_my_buddy_with_a_huge_check_after/,9.0,9.0,,en
1108267,2011-11-12 02:53:39,AskStatistics,Unfair Coin Flip?,m9bqq,Kelaos,1290045905.0,https://www.reddit.com/r/AskStatistics/comments/m9bqq/unfair_coin_flip/,0.0,4.0,"**EDIT: Solved thanks to rpglover64**

Just doing some studying for my statistics midterm tomorrow and came across an odd question:
&gt; You ﬂip an unfair coin two times. The probability that you get heads both times is 0.36.
&gt;What is the probability that you get tails both times

They say the answer is 0.16, but I'm confused what the correlation is between 0.36 and 0.16...

If someone could give an explanation that'd be great, thanks :)
",en
1108268,2011-11-12 05:40:32,datasets,System specs (dxdiag) from an unusual source,m9gl8,voltagex,1230861112.0,https://www.reddit.com/r/datasets/comments/m9gl8/system_specs_dxdiag_from_an_unusual_source/,3.0,0.0,,en
1108269,2011-11-12 12:12:46,statistics,To all the Bayesians here!,m9omh,kaligotc,1298010519.0,https://www.reddit.com/r/statistics/comments/m9omh/to_all_the_bayesians_here/,0.0,0.0,"Bayesians in the night with exchangeable glances Assessing in the night the prior chances We'd be sharing risks ... before the night was through.

Something in your prior was so exciting Something in your data was so inviting Something in my model told me I must have you.

Bayesians in the night two statisticians We were Bayesians in the night Then came the moment when we walked down to the sea Under a fault tree Our likelihoods were close together and Sir Ronald lost his final feather

And ever since that night we've been adherents Leaders of the fight to have coherence It turned out all right for Bayesians in the night.

*- Natvig (1986), DeGroot (2000 something)
",en
1108270,2011-11-12 12:24:16,statistics,Visualizing Likert Items using a colored table,m9oro,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/m9oro/visualizing_likert_items_using_a_colored_table/,8.0,1.0,,en
1108271,2011-11-12 16:50:46,statistics,How many songs are on my Pandora station?,m9sjv,deejinator,1306824517.0,https://www.reddit.com/r/statistics/comments/m9sjv/how_many_songs_are_on_my_pandora_station/,4.0,11.0,"Assume that there are a certain number of songs, and that each song is selected randomly, with replacement. If I have listened to n distinct songs, and x of them have been played more than once, what is the most likely number for total songs in the pool?

Can we get confidence bounds on that? Does anybody want to refine my assumptions, for example does the answer change if no song can be played twice in a row, or if no song can be played again until m other songs have been subsequently played? Does the answer change significantly if I ignore certain songs, and don't count them at all?

(Assume also that the station has not been modified at all since the count started. On a related note, does anybody know if thumbs-upping a song by the artist who you seeded the station with affects the station at all?)

Please tell me that this is more complex than a simple proportion, as in, songs played at least twice : songs played at least once = songs played at least once : songs played at least 0 times.

(I don't have data on songs played 3 times or more, I just put them on the list the first time I hear them, and then rate them the second time I hear them, and either update the rating or leave them alone if I hear them again.)

[Crossposted from [/r/math](http://www.reddit.com/r/math/comments/m9p5b/how_many_songs_are_on_my_pandora_station/)]",en
1108272,2011-11-12 23:05:45,statistics,A newly added weapon in my unending battle against the use of Microsoft Excel for numerical computing and data preservation:,ma2t9,brews,1263715301.0,https://www.reddit.com/r/statistics/comments/ma2t9/a_newly_added_weapon_in_my_unending_battle/,13.0,39.0,,en
1108273,2011-11-13 02:38:46,statistics,"Help! Can a 95% Confidence Interval contain 0, 1, or -1? What does it mean if it does?",ma986,thrwayaccount152,1320090067.0,https://www.reddit.com/r/statistics/comments/ma986/help_can_a_95_confidence_interval_contain_0_1_or/,1.0,1.0,,en
1108274,2011-11-13 09:27:06,MachineLearning,Google acquires Katango for its Automatic Friend Finder based on ML clustering techniques,majym,vardhan,1169119047.0,https://www.reddit.com/r/MachineLearning/comments/majym/google_acquires_katango_for_its_automatic_friend/,18.0,1.0,,en
1108275,2011-11-13 13:26:50,MachineLearning,Apache Mahout: Scalable machine learning for everyone,mang0,sunng,1283163374.0,https://www.reddit.com/r/MachineLearning/comments/mang0/apache_mahout_scalable_machine_learning_for/,22.0,7.0,,en
1108276,2011-11-13 16:09:29,artificial,Practical AI Question,mapnj,nsomaru,1309738334.0,https://www.reddit.com/r/artificial/comments/mapnj/practical_ai_question/,11.0,6.0,"I am currently involved in the [ai-class](http://www.reddit.com/r/aiclass/) online class run by Stanford. I am also doing [ml-class](http://www.reddit.com/r/mlclass/).

Could anyone provide direction as to the following problem I would like to solve for a non-profit (using AI/ML techniques):

A data set with a large number of very dirty contact information -- names, addresses, phone numbers and email addresses needs to be deduplicated.

What AI/ML techniques should I be thinking about to solve this problem? If there are any programming suggestions, I would prefer examples/libraries in Python, but anything goes really.

Thanks.",en
1108277,2011-11-13 17:46:59,statistics,Multiple regression question,mari3,LampTree,,https://www.reddit.com/r/statistics/comments/mari3/multiple_regression_question/,4.0,12.0,"Simple concept, I know, but I've always struggled with interpreting this for classes. Alas, now I have a data set where this is the case.

Multiple regression:
y=b0+b1x1+b2x2

Both b1 and b2 are significant (standardized coefficients: .134 and -.331 respectively, both p&lt;.01). However, I have a very small r^2 (.03; p=.09).

My interpretation of what the data is telling me is that:
We cannot be sure that we can account for any variance in y, but we can be relatively certain that both b1 and b2 predict y.

I guess I'm a bit stuck on the implications of that and how it is possible for me to be unsure that any relationship exists (low r2) but see two significant relationships. Any guidance that you are able to provide on interpreting/writing that for a scholarly article would be greatly appreciated.

Thank you! ",en
1108278,2011-11-13 23:00:17,statistics,How could I pull in the headlines of subreddits with R,mb12s,gabjuasfijwee,1260073537.0,https://www.reddit.com/r/statistics/comments/mb12s/how_could_i_pull_in_the_headlines_of_subreddits/,7.0,5.0,I'm not very experience with R and I would like to do soem text analysis of reddit headlines. Is there any way this can be done?,en
1108279,2011-11-13 23:12:13,MachineLearning,How can I acquire the text of reddit headlines from R?,mb1ga,gabjuasfijwee,1260073537.0,https://www.reddit.com/r/MachineLearning/comments/mb1ga/how_can_i_acquire_the_text_of_reddit_headlines/,2.0,5.0,I'm not very experience with R and I would like to do soem text analysis of reddit headlines. Is there any way this can be done?,en
1108280,2011-11-13 23:54:18,statistics,I am looking for a book on bayesian methods with BUGS/JAGS that treats panel data with empirical examples.,mb2ya,cbrunos,,https://www.reddit.com/r/statistics/comments/mb2ya/i_am_looking_for_a_book_on_bayesian_methods_with/,2.0,7.0,"Much like Bayesian Modeling Using WinBUGS by Ioannis Ntzoufras. Does anyone know of one similar but that deals with panel data?

Thanks in advance!",en
1108281,2011-11-14 21:05:06,MachineLearning,Mentorship? ,mc1sx,[deleted],,https://www.reddit.com/r/MachineLearning/comments/mc1sx/mentorship/,6.0,12.0,"I am a noob machine learner, just starting on my career in an NLP job in a few months. And I also plan to go to grad school in the next few years
I feel clueless a lot of the times, being mainly self-taught, and don't have anyone to talk to about machine learning, or discuss research topics and papers with. 

Is there any place or website where I can go to find mentors? Is there any redditor here who would be willing to mentor me? I'll be much obliged! ",en
1108282,2011-11-15 01:59:23,statistics,Best place to find free data sets?,mce1k,mrdeeds23,1276490098.0,https://www.reddit.com/r/statistics/comments/mce1k/best_place_to_find_free_data_sets/,18.0,23.0,"Need to obtain a dataset of at least 50 samples and 5 variables for my stat class, having trouble finding a good one online. Any help is appreciated.

edit: I've found a dataset to use. Thanks for all the help r/statistics ",en
1108283,2011-11-15 02:28:16,statistics,Significant rankings? ,mcf51,Ragingsheep,1251077556.0,https://www.reddit.com/r/statistics/comments/mcf51/significant_rankings/,2.0,5.0,"Hi, just wondering if anyone can helpd me on this and if this makes sense: 

Lets say that we have students A, B, C and D and each has a mean mark from their tests over the year. What statstical tests and processes (if any) should I use to rank the four students so that their rankings are statstically valid? ",en
1108284,2011-11-15 05:32:35,statistics,What does r/statistics think about radar plots?,mcmgs,valen089,1288238268.0,https://www.reddit.com/r/statistics/comments/mcmgs/what_does_rstatistics_think_about_radar_plots/,1.0,10.0,"Title says it all, what do you all think about [radar plots](http://en.wikipedia.org/wiki/Radar_chart)? I've never used them before and never came up in any of my stat classes. I don't necessarily have a need or application for them, just curious as to what others think about them.",en
1108285,2011-11-15 05:33:24,statistics,Help with Standard Normal Distribution,mcmhs,[deleted],,https://www.reddit.com/r/statistics/comments/mcmhs/help_with_standard_normal_distribution/,1.0,0.0,"I'm in a pretty basic statistics class and I'm having a really hard time grasping the whole percentile thing.

For example, I've got a question here stating the mean for my normal distribution, and asking to find the standard deviation if for these scores P(subscript 90) = 160.

I realize I should be trying to find the Z score using this, but how do I start?",en
1108286,2011-11-15 05:56:58,statistics,How do you find the mean and standard deviation of a distribution when given percentile values?,mcngk,[deleted],,https://www.reddit.com/r/statistics/comments/mcngk/how_do_you_find_the_mean_and_standard_deviation/,0.0,8.0,"For example, I'm given that Quartile 3 = 32.8 and Percentile 40 = 28.2, what do?",en
1108287,2011-11-15 10:19:24,computervision,William Shakespeare Needs AI,mcw13,cavedave,1128052800.0,https://www.reddit.com/r/computervision/comments/mcw13/william_shakespeare_needs_ai/,3.0,1.0,,en
1108288,2011-11-15 15:30:44,MachineLearning,"All code from ""Machine Learning for Email"" now on Github",md1pa,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/md1pa/all_code_from_machine_learning_for_email_now_on/,15.0,2.0,,en
1108289,2011-11-15 17:25:21,statistics,Sample size?,md50p,[deleted],,https://www.reddit.com/r/statistics/comments/md50p/sample_size/,6.0,16.0,"My data consists of 128 dimensional vectors which can have 256 possible values for each the total space for my data is:

256^128 = 1.78 * 10^308

Given that this the total possible population what is the required sample to plot a reliable frequency distribution for each dimension where error = 5% and confidence = 95%.

These graphs are plotted using 2million vectors (log y axis): http://i.imgur.com/Zv0Sz.png

Each plot contains 8 dimensions and plots the frequency of each of the 256 possible values. The top left graph shows the frequency for dimensions 1-8 across the 2million vector etc. 

2 million seems like a very low amount but the graphs shape fit what I would expect. I know that the space that is actually filled is less than the total possible space given above. How do I know if I am using a large enough sample if I don't know that actual size of the space just that it is less than 1.78 * 10^308.

I don't do much statistics and would like help on how to analyse my data. I want to know the likely distribution of the data for each vector value.



",en
1108290,2011-11-15 18:40:32,statistics,Question for the distribution of OLS estiminator,md7o9,[deleted],,https://www.reddit.com/r/statistics/comments/md7o9/question_for_the_distribution_of_ols_estiminator/,1.0,1.0,"Hi there,

I have a question for my assignment i'm currently writing on:

How does the normality of ε matter for the distribution of the OLS estimator?

Thanks a lot reddit!
",en
1108291,2011-11-15 21:31:14,datasets,Common Crawl Foundation: use their 5 billion page dataset with fairly unrestricted terms of service. ,mdeoe,Quintote,1269890504.0,https://www.reddit.com/r/datasets/comments/mdeoe/common_crawl_foundation_use_their_5_billion_page/,7.0,4.0,,en
1108292,2011-11-15 22:40:37,statistics,"""An intuitive argument ..."" for the Johnson-Lindenstrauss Lemma:  how well can data squish into a lower-dimensional space?",mdhex,claird,1249749559.0,https://www.reddit.com/r/statistics/comments/mdhex/an_intuitive_argument_for_the/,2.0,4.0,,en
1108293,2011-11-16 02:43:38,MachineLearning,Large scale ML,mdre5,ml_noob,1321404088.0,https://www.reddit.com/r/MachineLearning/comments/mdre5/large_scale_ml/,0.0,3.0,"Hi all.

I work for a company who is looking to do large scale data mining. I was wondering if there were any solutions currently available to do large scale distributed data mining. Thanks!",en
1108294,2011-11-16 03:23:08,datasets,LF: dataset for blog comment spam,mdswh,unstoppable-force,1297432953.0,https://www.reddit.com/r/datasets/comments/mdswh/lf_dataset_for_blog_comment_spam/,1.0,0.0,i'm looking for a dataset for blog comment spam.  the latest one i can find is almost 3 years old.,en
1108295,2011-11-16 06:04:12,MachineLearning,How to compare models with different dimensions,mdz8o,iHeartML,1321415572.0,https://www.reddit.com/r/MachineLearning/comments/mdz8o/how_to_compare_models_with_different_dimensions/,8.0,9.0,"I have some data (~200 dim) and not that many examples (~300). I am trying to get the best generative model of the data. I am using GMMs to create the model however I am adding a PCA reduction prior to this and building the GMM over the reduced subspace, the problem arises in comparing distributions with different dimensionality. I attempted to use cross validation of the likelihood however the problem is in one model the input is 100 dim in another model the input is some other dimension size so my understanding is that i cannot compare these likelihoods directly since lower dimensional data will have more approximation error than the higher dimensional data(but less parameters and noise specific to the training data? one would hope) . Any  ideas in how to compare these in order to choose the best dimension to represent the data?",en
1108296,2011-11-16 10:51:55,datasets,Do you know of any DMOZ-sized  dataset of text clusters?,me7mr,visarga,1166994643.0,https://www.reddit.com/r/datasets/comments/me7mr/do_you_know_of_any_dmozsized_dataset_of_text/,7.0,3.0,"I want to cluster a large collection of web pages by topic and was wondering if I could use the results of an already executed clustering as centroids for my clusters. What I am looking for is a dataset with a large list of topics and some examples from each topic. Thx

Edit: Word clusters would also be ok, like shown in [this paper](http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;sqi=2&amp;ved=0CCEQFjAA&amp;url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.164.5520%26rep%3Drep1%26type%3Dpdf&amp;ei=rX3DTofoF4bxsgbMs7WODA&amp;usg=AFQjCNHMzw2CSocGz12BKxAfyiZAq9uPrg&amp;sig2=0jSzRp_dBzkL9FIbjZa6Wg).",en
1108297,2011-11-16 16:23:42,statistics,Basic stata help requested - Creating a new variable,medzw,brokenbreak,1311439590.0,https://www.reddit.com/r/statistics/comments/medzw/basic_stata_help_requested_creating_a_new_variable/,3.0,3.0,"Hi there, 
I'm creating a new variable which must refer to an existing variable and tag the new variable with a value if the existing variable is within a range.

EX: generate newvar = tag1 if -1&gt;oldvar&gt;1 and tag2 if oldvar&gt;1

Any help would be massively appreciated! 
",en
1108298,2011-11-16 16:46:52,MachineLearning,How well does gradient descent work on extremely noisy data?,meeou,sanity,1149365629.0,https://www.reddit.com/r/MachineLearning/comments/meeou/how_well_does_gradient_descent_work_on_extremely/,8.0,35.0,"I'm interested in using gradient descent to predict the probability that a website visitor will click on an ad, given a number of things we know about them (geographic location, browser, operating system, referrer, etc).

Typical click-rates are around 0.1%, and obviously there are a **lot** of factors that play a part in whether or not the user clicks that aren't represented in the input attributes.  This means that from the learning algorithm's perspective the output data is extremely noisy.

How well does gradient descent work on this kind of problem where you're essentially trying to pick out comparatively subtle relationships between the input and output data amidst a *lot* of noise?

Would a data mining approach be more effective here?

*edit:* In response to some comments, yes - I would use a logistic regression of some form because the output must be a probability.

*edit2:* In response to those asking about my cost function - my ultimate goal is, given multiple ads to choose from to show to a user, pick the one they are most likely to click on.",en
1108299,2011-11-16 19:30:07,MachineLearning,This Guy Broke Jeopardy’s All-Time Record… Using ML Techniques To Train Himself,mekrb,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/mekrb/this_guy_broke_jeopardys_alltime_record_using_ml/,91.0,19.0,,en
1108300,2011-11-16 21:08:25,artificial,"Customer appreciation-$10.00 off your first order of $30.00 or more-Fabulous Christmas Wreaths, silk Areca Palm trees, silk plants and Flowers.",meotq,cchic,1312853008.0,https://www.reddit.com/r/artificial/comments/meotq/customer_appreciation1000_off_your_first_order_of/,1.0,0.0,,en
1108301,2011-11-16 21:40:55,statistics,New DS2 Language for DATA Step Programming in Base SAS 9.3,meq7p,zip117,1259367181.0,https://www.reddit.com/r/statistics/comments/meq7p/new_ds2_language_for_data_step_programming_in/,6.0,17.0,,en
1108302,2011-11-17 01:13:06,statistics,"I'm writing a program to distinguish if a sequence of heads/tails are truly random, or if they were created by a human. Is there an easy test for this?",mez0u,AncillaryCorollary,1290969591.0,https://www.reddit.com/r/statistics/comments/mez0u/im_writing_a_program_to_distinguish_if_a_sequence/,9.0,15.0,"Inspired by [this](http://www.youtube.com/watch?v=H2lJLXS3AYM&amp;feature=mfu_in_order&amp;list=UL) video, I decided to attempt to program a ""human detector"", so I'm able to program a method of finding the frequencies of various patterns, but I don't know a precise mathematical way to determine if they're human.
  
For example, it asks the user to enter in a string of ""t""s and ""h""s, and it shows them how many times different patterns showed up, like so:  
  

ttthhhthththththththththththtthththth  
h	18  
t	19  
  
hh	2  
ht	15  
th	16  
tt	3  
  
hhh	1  
hht	1  
hth	14  
htt	1  
thh	1  
tht	14  
tth	2  
ttt	1  
ttthhhthtthtthttttttththtththttttttth  
h	11  
t	26  
  
hh	2  
ht	8  
th	9  
tt	17  
  
hhh	1  
hht	1  
hth	3  
htt	5  
thh	1  
tht	7  
tth	6  
ttt	11  
  
But I don't know a test that I can write, so that my computer could say with a confidence level of, say, 0.99, that what the user submitted is made up, and not actually random.",en
1108303,2011-11-17 03:06:33,artificial,The Genius of Goertzel; Dr. Ben Goetzel lectures in China,mf3e9,moscheles,1316266003.0,https://www.reddit.com/r/artificial/comments/mf3e9/the_genius_of_goertzel_dr_ben_goetzel_lectures_in/,0.0,4.0,,en
1108304,2011-11-17 07:24:13,data,"Western Digital (WD) Data Recovery 
",mfcz6,barrycortez,1321507200.0,https://www.reddit.com/r/data/comments/mfcz6/western_digital_wd_data_recovery/,1.0,0.0,"Western digital hard drive &amp; disk data recovery, external and internal WD hard drive data recovery.
",en
1108305,2011-11-17 14:56:25,datascience,Visualizing FRBR Worksets,mfmfq,mhermans,1169219262.0,https://www.reddit.com/r/datascience/comments/mfmfq/visualizing_frbr_worksets/,1.0,0.0,,en
1108306,2011-11-17 14:57:03,datascience,Getting Started With Twitter Analysis in R,mfmfz,mhermans,1169219262.0,https://www.reddit.com/r/datascience/comments/mfmfz/getting_started_with_twitter_analysis_in_r/,5.0,0.0,,en
1108307,2011-11-17 14:57:49,datascience,Gephi adds Neo4j graph database support [info + screencast],mfmgi,mhermans,1169219262.0,https://www.reddit.com/r/datascience/comments/mfmgi/gephi_adds_neo4j_graph_database_support_info/,1.0,0.0,,en
1108308,2011-11-17 16:44:18,statistics,I'm sorry for this...,mfp3i,[deleted],,https://www.reddit.com/r/statistics/comments/mfp3i/im_sorry_for_this/,0.0,4.0,,en
1108309,2011-11-17 17:52:51,MachineLearning,More courses from Stanford,mfrfc,amair,,https://www.reddit.com/r/MachineLearning/comments/mfrfc/more_courses_from_stanford/,43.0,17.0,"Link to [Probabilistic Graphical Models](http://www.pgm-class.org/) - starting Jan.  More courses at the bottom of the page (ML, NLP, Game Theory, et al).",en
1108310,2011-11-17 18:27:36,datasets,Main Street Public Library Database,mfsk3,cavedave,1128052800.0,https://www.reddit.com/r/datasets/comments/mfsk3/main_street_public_library_database/,6.0,1.0,,en
1108311,2011-11-17 19:41:50,statistics,Top 10 Beauty Pageant Statistics,mfve0,souljoel,1310837355.0,https://www.reddit.com/r/statistics/comments/mfve0/top_10_beauty_pageant_statistics/,1.0,0.0,,en
1108312,2011-11-17 20:03:21,MachineLearning,Using fmin from scipy optimize,mfw6u,marshallp,1239903633.0,https://www.reddit.com/r/MachineLearning/comments/mfw6u/using_fmin_from_scipy_optimize/,3.0,11.0,"is using the fmin functions from scipy optimize a
good way to train neural nets. And in neural nets,
why are multipliers used in each weight of the perceptron - has anyone tried powers or more
complicated thing on each weight of the perceptron?",en
1108313,2011-11-17 20:58:51,statistics,Simple R question,mfyeu,taciturnbob,1304227801.0,https://www.reddit.com/r/statistics/comments/mfyeu/simple_r_question/,4.0,9.0,"This is a very simple question, but I can't seem to find the right search terms to google the answer:

I am creating a program that reads in SAS output, runs R for plotting, and outputs a report to LaTeX. My problem is generalizing the code is that I don't know how to refer to the output I read in. What I'm doing now is something like mydata$race or mydata$gender, where I really need to do mydata$VAR1 or mydata$VAR2, so I can read in any variable I want.

I know this is not an R forum, but you guys know your shit. Thanks!

EDIT: Calling the column indices work, I suppose I thought I couldn't do that in the data.frame class. PARSLEYsage and nblarsen brought up good points though, so I might go ahead and do that for a more robust looking code. Thanks!",en
1108314,2011-11-18 00:43:27,analytics,GA - How can I track clicks on a single link on a transition page hosted on another server/domain?,mg7py,gerbs,1288081043.0,https://www.reddit.com/r/analytics/comments/mg7py/ga_how_can_i_track_clicks_on_a_single_link_on_a/,2.0,4.0,"I have a client that wants to know the CTR on one of the ads they have running my mailing. I've told them it would be simplest if they just monitored all incoming traffic from my domain in their analytics, but I think they're technologically retarded, so they've asked us to provide the number of clicks for them.

Here's my problem: the ad/transition page only shows up for clicks from my e-newsletter, and each link is built in HTML with a different URL (they all begin the same, but link to different articles or sites, so the code is different for each one). I pasted the GA code into each transition page, but, I can't seem to find the number of times a link is clicked on those pages. 

And then it leads to a second problem: I would have to check each piece of content each day to check for links.

There has to be an easier way! The internet proved no luck, affiliate marketer friends had no knowledge, so I'm bringing the question to you.


**Is there a way to track clicks of a single URL leading away from multiple pages that my code is in?** And if so, tell me tell me tell me.",en
1108315,2011-11-18 03:47:57,statistics,"A Radical Centrist View on Election Forecasting - NYTimes.com [A ""soft"" article, but I found it interesting]",mgeh9,Sieyes314,1300056679.0,https://www.reddit.com/r/statistics/comments/mgeh9/a_radical_centrist_view_on_election_forecasting/,6.0,0.0,,en
1108316,2011-11-18 04:17:37,computervision,Is the computervision subreddit the right place to find someone who can create a digital image of a product?,mgfig,tennis87,1265946515.0,https://www.reddit.com/r/computervision/comments/mgfig/is_the_computervision_subreddit_the_right_place/,0.0,2.0,...because that's what I'm looking for. Like the bottles on www.drinkarizona.com.,en
1108317,2011-11-18 07:22:32,statistics,multicollinearity: is it ok to recursively remove variables until all variance inflation factors are below 10?,mglyv,FailedLifeForm,1299165736.0,https://www.reddit.com/r/statistics/comments/mglyv/multicollinearity_is_it_ok_to_recursively_remove/,2.0,1.0,,en
1108318,2011-11-18 09:49:31,MachineLearning,ML for making code recommendations (people that called X also called Y),mgpz9,microbiotic,1320511094.0,https://www.reddit.com/r/MachineLearning/comments/mgpz9/ml_for_making_code_recommendations_people_that/,12.0,0.0,Interesting research project at http://eclipse.org/recommenders that leverages static analysis and ML to create nice tools around it in the Eclipse IDE.,en
1108319,2011-11-18 20:02:32,statistics,simple question about R,mh50e,oddlytoddly,1303968411.0,https://www.reddit.com/r/statistics/comments/mh50e/simple_question_about_r/,2.0,7.0,"I’m just learning how to use R and I was wondering how I would use R to generate 100 random samples of size four, Zsub1, Zsub2, Zsub3, and Zsub4, from an N(0,1) distribution and calculate X = Z2sub1+ Z2sub2+ Z3sub2+ Z4sub2. 

My first thought was to use rnorm(n, mean, sd) however there is no place for me to input the sample size of four. I also thought of using rbinom since rbinom has parameters for (n, size, prob)… 

Any advice would be greatly appreciated. 
",en
1108320,2011-11-18 20:45:03,statistics,Will you help me design a stats101 achievement system?,mh6qp,alanpost,1188403539.0,https://www.reddit.com/r/statistics/comments/mh6qp/will_you_help_me_design_a_stats101_achievement/,3.0,9.0,"I have a software development background.  I work in data analysis and have reached a point where I'm no longer getting by with the statistics I've picked up along the way.  My software skills are very solid, my lack of a formal math background is no longer sufficient.  A coworker and I would like to collaborate on something like a statistics 101 course.

The direction that I think will work for us is to arrange a set of achievements/challenges, each of which we can accomplish with 3-6 hours of work.  The inspiration for that idea comes from [here](http://blogs.p2pu.org/blog/2011/09/30/loads-of-learning/) and [here](http://jasonrudolph.com/blog/2011/08/09/programming-achievements-how-to-level-up-as-a-developer/).

We're currently evaluating these resources: MIT OpenCourseWare, UC Berkeley videos, moodle, peer-to-peer university, and Khan Academy.

Will you tell me your story about learning statistics?  Comment on any resources you've found valuable?  Tell me what a reasonable set of achievements are--even just the first one to go for?  We're both very comfortable with software and the mathematical tools available--something that leveraged that would be best.

Thank you!",en
1108321,2011-11-18 21:31:35,statistics,"Simple probabilities, can someone check my reasoning?",mh8lz,sigma_noise,1255550515.0,https://www.reddit.com/r/statistics/comments/mh8lz/simple_probabilities_can_someone_check_my/,1.0,5.0,"This is NOT homework! This is for a nerdy personal project I’m working on.

Say you have two summed asynchronous signals that each have a duty cycle (DC) of, say, 25% (likely with different periods) what is the chance that that both signals will be ‘on’ simultaneously at any given instant? Intuition tells me it will be the product of the two duty cycles, or 12.5%, right?

With more than two signals, the probability of two (*not* three or more) overlapping signals would be the sum of the all the probabilities of any two signals overlapping.

= DC1\*DC2 + DC1\*DC3 + DC2\*DC3...

Next, what is the chance that one, or any two, will be on at any instant? Again, going from my intuition, I think it is the sum of all the duty cycles, minus the time that any two would be on simultaneously, or, 25% + 25% - 12.5%, or 37.5%. So, 37.5% of the time there will by some sort of signal, whether it is one signal alone, or any two super imposed. Right?

Finally, *given that there is a signal present*, what percentages of the time will it actually be two signals together (and not just one)? I think it would be the probability of two *overlapping* signals, divided by the probability of *any* signal. In this case that would be 12.5% divided by 37.5%, or 33%. So, given that *some* signal is there, 33% of the time it will actually be two overlapping signals, not just a single one.

Is this all correct? My situation actually involves more than just two signals, but I just want to have someone verify my thinking before I forge ahead.
",en
1108322,2011-11-18 23:13:50,statistics,Can anyone help me with binomial probability distributions and basic hypothesis testing?,mhcnq,blasphemicmonk,1304899965.0,https://www.reddit.com/r/statistics/comments/mhcnq/can_anyone_help_me_with_binomial_probability/,2.0,2.0,"I am currently trying to prove my teacher wrong by passing his excuse for a class and have a goal of 82 on my final. He told me I would be better off dropping the class as I got a 73 on my first test and a 55 on my second with stupid mistakes in the work like switching letters but still getting the correct answer but still lost the 10 points per question. 


I am just looking for someone to give me a few example problems and answers or a sample test for me to practice binomial probabilities. I'm pretty set on binomial pdf equations, its the cdf ones that I have a hard time identifying which are 1-binomcdf(a,b,c), 1-binomcdf(a,b,c-1) and those two without the 1-binomcdf.


An example of the type of questions he is asking is for 1-binomcdf is:
A nursery advertises that 90% of its trees will survive the winter. If 15 trees are randomly selected and planted, what is the probability that at least 13 trees will survive the winter?

the answer being 1-binomcdf(15, .90, 12)=.816


We just began learning hypothesis testing for a binomial test and I have 2 sample problems but I want to get this down. An example would be:
Suppose that normally the percentage of persons with high blood pressure is at most 10%, but for people over 40 it is suspected to be higher. a random sample of 50 individuals over 40 indicates that 10 have high bp. is the suspicion valid?

answer:
n=50                      np=50(.10)=50
T=10
H sub O: p&lt;or=.10
H sub 1:p&gt;.10
P(T&gt; or = 10)
1-binomcdf(50, .10, 9)=.025
alpha=.05
.025&lt;.05 therefore the hypothesis is rejected

So please, if anyone can help me, even by just changing the numbers and some words in these two problems I would really appreciate it",en
1108323,2011-11-19 04:55:25,MachineLearning,Data Scientist vs Statistician?,mhodz,SinisterSamurai,,https://www.reddit.com/r/MachineLearning/comments/mhodz/data_scientist_vs_statistician/,19.0,41.0,Hi I thought this would be the most appropriate sub reddit for this kind of thing. My question is what exactly is the difference between the two? I tried googling the answers but most people are dodging the question or give an inaccurate description of statisticians.,en
1108324,2011-11-19 18:47:56,statistics,Help with plots in SAS,mi4wm,[deleted],,https://www.reddit.com/r/statistics/comments/mi4wm/help_with_plots_in_sas/,0.0,5.0,,en
1108325,2011-11-19 19:56:47,statistics,Help With Response Surface ,mi6xt,[deleted],,https://www.reddit.com/r/statistics/comments/mi6xt/help_with_response_surface/,1.0,0.0,"My group is doing a central composite design with 2 factors for our senior engineering design course.
We want to run a response surface analysis to determine the relative influence these factors have but we're not sure what to do exactly. We want to use MATLAB rstool, but don't know what to input. Data has not been collected yet, we've only set up the experiment with MATLAB dCC = ccdesign(2) to give us the number of trials and the coded values for each variable. 
It is my understanding that we have to assign values for our factors to the coded values. For example if the coded values are -1.5, -1, 0, 1, and 1.5, and if say our factor is temperature then would we run a trial at -5C, 0C, 10C, 20C, and 25C?
What exactly will the response surface tell us with regards to each factor. Our project deals with cleaning so can we make a recommendation as to which factor or combination of factors has the greatest impact on cleaning, and if so what would allow us to reach that conclusion? I posted this in MATLAB, but forgot to mention that I need help with interpreting the data as I have nearly 0 background in statistics. Thank you!",en
1108326,2011-11-19 20:21:12,statistics,Problems with survival analysis - Cox proportional hazards model (PROC PHREG) with categorical variables ,mi7na,Wesley1331,1291676859.0,https://www.reddit.com/r/statistics/comments/mi7na/problems_with_survival_analysis_cox_proportional/,3.0,5.0,"I am using PROC PHREG to analyze survival times (time until death) of planted trees. I have run into several issues including extremely high hazard ratios and standard errors, and questionable parameter estimates.

I have two fixed categorical treatments (seven and four levels respectively) and I am interested in determining the differences in survival among the combinations of treatment levels. I am anticipating significant interaction between the two treatments. I believe my problems are the result of the following two issues.

  1. I have combinations of treatment levels where 100% of the planted trees survived (no event occurred for a subset of trees). For these combinations of treatments 100% of the data are right censored.

  2. I have a fractional factorial design. Not all levels of my second treatment were set up for all levels of my first treatment.

Is it possible to analyze my data in their current form using PHREG or any other PROC? I have successfully analyzed a subset of the data when I eliminated the above problems. Any help is much appreciated.
",en
1108327,2011-11-20 00:08:07,artificial,Could someone get me .java files for alpha-beta and minimax tic tac toe for my science fair project?,miepz,[deleted],,https://www.reddit.com/r/artificial/comments/miepz/could_someone_get_me_java_files_for_alphabeta_and/,0.0,0.0,"I'm no good at java, but i have all the reasearch, i just need the games to do the test, It'd be nice if it were commented as well. Thanks!",en
1108328,2011-11-20 03:06:41,statistics,Birthday paradox + Zipfian distribution quandary,mik7q,CockOfTHeNorth,1315145381.0,https://www.reddit.com/r/statistics/comments/mik7q/birthday_paradox_zipfian_distribution_quandary/,1.0,0.0,"I'm trying to find a closed-form solution for the following problem. There are two sets A (cardinality=i) and B (cardinality=j), each containing items separately drawn from a set S, with the items having a Zipf distributed chance of selection (size=n, alpah=a). A and B are independent of each other, but within them each item is drawn without replacement. 

What is the probability that A and B share at least one element?

This would be easy if S was uniformly distributed, as it is, I am stumped. Any suggestions/comments would be greatly appreciated.",en
1108329,2011-11-20 04:56:16,statistics,Relative risk: can you swap the exposures and events?,minfi,agoat,1292640735.0,https://www.reddit.com/r/statistics/comments/minfi/relative_risk_can_you_swap_the_exposures_and/,3.0,6.0,"This isn't a homework question, but I'm still hesitant to post this since it is related to school work. I can't find a straight answer anywhere else though, so I'll post it - feel free to downvote if you think it doesn't belong here.

My immunology textbook is talking about calculating relative risk as it relates to HLA (a membrane protein) alleles and the occurrence of diseases. It says you can calculate relative risk like this:

    RR = (frequency of allele in diseased population) / (frequency of allele in general population)

Correct me if I'm wrong, but isn't this a reversal of the normal definition of relative risk (the ratio of how much greater the risk is in one exposed population vs another control population)? Is this a valid definition of relative risk? 

The text goes on to give this example: ""...individuals with the HLA-B27 allele are 90 times more likely (relative risk of 90) to develop the autoimmune disease ankylosing spondylitis ... than individuals who lack this HLA-B allele."" Is this a valid statement based on the equation? based on a relative risk of 90 I think it should say, ""individuals with ankylosing spondylitis are 90 times more likely to have the HLA-B27 allele than individuals without it.""

Thanks in advance!

Edit: the textbook is Kuby Immunology, 6th ed.",en
1108330,2011-11-20 05:30:32,statistics,Validation of Software for Bayesian Models Using Posterior Quantiles,miohm,snipewiz,1256189534.0,https://www.reddit.com/r/statistics/comments/miohm/validation_of_software_for_bayesian_models_using/,2.0,0.0,,en
1108331,2011-11-20 05:31:15,statistics,Regression Analysis with Time Trend: How do I remove it?,mioik,ucla_derp,1312401659.0,https://www.reddit.com/r/statistics/comments/mioik/regression_analysis_with_time_trend_how_do_i/,1.0,3.0,"I am running a regression analysis on some data. When I plot the dependent variable against time, I see a very strong quadratic trend in the data. I ran a quadratic regression, i.e. 

Y = a + bt + ct^2 + dt^3 + et^4,

and got a very good fit to the data. My question is: now that I have the trend, what is the best way to remove it from the data?",en
1108332,2011-11-20 07:12:12,statistics,Linear Regression question,mirdr,amirightfolks,1280970578.0,https://www.reddit.com/r/statistics/comments/mirdr/linear_regression_question/,6.0,11.0,"I have a model with 4 predictor variables. All 4 variables are significant but some are much more significant than others. I was wondering if there was a way to somehow weight the variables based on magnitude of the coefficients and significance. For example, these are close to the magnitude and significance of my variables:

* B1=.005, p=.0000001
* B2= .45, p=.001
* B3= .57, p=.035
* B4= -.33, p=.04

I'm not sure how important this is, but B1 is continuous and B2-B4 are categorical (only 2 levels, so basically indicator variables)",en
1108333,2011-11-20 10:54:08,datasets,Attempt #2: Want to help reddit build a recommender? -- A public dump of voting data that our reddit users have donated for research : redditdev,miw5e,TedFromTheFuture,,https://www.reddit.com/r/datasets/comments/miw5e/attempt_2_want_to_help_reddit_build_a_recommender/,19.0,0.0,,en
1108334,2011-11-20 12:01:01,MachineLearning,Am I planning it right for a PhD in ML? ,mix4c,marshmallowsOnFire,1311921521.0,https://www.reddit.com/r/MachineLearning/comments/mix4c/am_i_planning_it_right_for_a_phd_in_ml/,1.0,31.0,"I am a Master's student in EE from a pretty decent school, specializing in Image Processing and Machine Learning. I want to do a PhD in the long run, but I want to be fully prepared, armed with a solid knowledge of the basics and good experience in the field before taking my 5-year plunge. Basically, when I start my PhD, I want to hit the ground running, doing research, and not waste the first year just studying the basics.

Toward this end, my plan after my Master's is to do a research internship in some lab for a year and hope to get a/some publications out.

I just want to ask you guys, will this really help my PhD applications? I want to get into a good PhD program.

 What about working in a ML startup? I know there are a LOT of companies out there with TONS of data, will it boost my resume if I worked in some such core ML company (even if only a small startup), gained good knowledge, but got no publications?

Thanks a lot!
",en
1108335,2011-11-20 20:37:27,MachineLearning,Ask r/ML: What to do when you the size of the feature set is much larger than the training set?,mj6up,tshauck,1304289778.0,https://www.reddit.com/r/MachineLearning/comments/mj6up/ask_rml_what_to_do_when_you_the_size_of_the/,10.0,14.0,"Hi,


As mentioned above what should I do?  If there are resources that I can read about this that'd be great too.  Thanks 


EDIT: Was going to use NN and SVM...",en
1108336,2011-11-21 03:36:06,computervision,How do I import openCV into Eclipse?,mjljw,[deleted],,https://www.reddit.com/r/computervision/comments/mjljw/how_do_i_import_opencv_into_eclipse/,2.0,0.0,"I am trying to do image processing on Android. I downloaded the latest openCV library available and am trying to import it. I amfollowing the guidelines given here

http://opencv.willowgarage.com/wiki/AndroidPrebuiltOpenCV231 

but unlike what's mentioned, I do NOT get an Eclipse project anywhere, because of which when I try to import, I am getting the error, ""No projects to import""

Please help. Thank you very much.",en
1108337,2011-11-21 03:50:18,MachineLearning,Importing openCV library for Android- error message : No projects found.,mjm41,[deleted],,https://www.reddit.com/r/MachineLearning/comments/mjm41/importing_opencv_library_for_android_error/,0.0,1.0,"I am trying to do image processing on Android. I downloaded the latest openCV library available and am trying to import it into my workspace. I am following the guidelines given here

http://opencv.willowgarage.com/wiki/AndroidPrebuiltOpenCV231

but unlike what's mentioned, I do NOT get an Eclipse project anywhere, because of which when I try to import, I am getting the error, ""No projects to import"" I read somewhere that I must have some .class files, Eclipse can only import those. In the folder I have, I only see .cpp files and .java files. 

What do I do? I thought the openCV library available online should be directly 'importable'.

 Thank you very much.
",en
1108338,2011-11-21 04:49:10,MachineLearning,Any tips about becoming an AI engineer?,mjodn,Rakosman,1261170658.0,https://www.reddit.com/r/MachineLearning/comments/mjodn/any_tips_about_becoming_an_ai_engineer/,16.0,25.0,"I'm extremely interested in studying AI for graduate school. Any tips on where I should go,and what I can expect as a career? 

I am currently working on my Software Engineering degree at Oregon Institute of Technology.",en
1108339,2011-11-21 09:25:49,statistics,Popular Baby Names Walk-Through – Web Scrapping and ggploting,mjxt4,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/mjxt4/popular_baby_names_walkthrough_web_scrapping_and/,3.0,2.0,,en
1108340,2011-11-21 10:01:14,statistics,"Reddit, can you help me with my statistics problem?",mjyle,Lily_Davies_91,1321862423.0,https://www.reddit.com/r/statistics/comments/mjyle/reddit_can_you_help_me_with_my_statistics_problem/,1.0,2.0,,en
1108341,2011-11-21 18:12:31,statistics,Integrating values from first differences to levels?,mk954,latortilla,1299276404.0,https://www.reddit.com/r/statistics/comments/mk954/integrating_values_from_first_differences_to/,2.0,2.0,"Hi, I have a bit of a strange problem - I need to ""integrate"" time series (t) data from differences to levels, the overall idea is that:

My theoretical model suggest that:

* y = integral(x1 + x2) dt

to work in levels of x1 and x2 I am running the following regression:

* dy = b1*x1 + b2*x2

where:

* dy = (y_{t}-y_{t-1})

is the first difference in the y variable. And i estimate b1 and b2.

Now, the x1 variable is relatively stable over time, just flucuates up and down around 0. x2 however trends upwards. Given that x1 is stable, and x2 trends upwards, I would like to show in a simple way that I need x2 to explain the upward trend in y, not dy but the initial variable y. Does anyone have a good idea of how to integrate out this equation to get back to y, and then to show that we need x2 to explain the trend in y, while x1 can only explain short term fluctuation?

Hope this is not too confusing....I guess the key points are:

* how do I move from dy back to y, given that I have an equation for dy

* how do I integrate out the effect to show that x2 drives the trend, while x1 does not. And is this affected by b1 and b2?

------------------------------------------------

EDIT: Ok, I had an idea, what if I did it this way, maybe someone can tell me whether that is right:
I know that:

* y_{t} - y_{t-1} = b1*x1 + b2*x2

therefore:

* y_{t} = y_{t-1} + b1*x1 + b2*x2

or re-written:

* y_{t} = b1*x1_{t} + b1*x1_{t-1} + b2*x2_{t} + b2_{t-1} + y_{t-2}

keep substituting for y_{t-2}, {t-3....} and I get:

* y_{t} = b1* sum(x1_t) + b2 * sum(x2_t) + y_0

Then I could argue that since sum of x1_t is around 0, this cannot possibly drive the trend in y_t.
How does that sound?

",en
1108342,2011-11-21 19:00:30,MachineLearning,Functional and Parallel time series cross-validation,mkayb,pandemik,1240356623.0,https://www.reddit.com/r/MachineLearning/comments/mkayb/functional_and_parallel_time_series/,5.0,1.0,,en
1108343,2011-11-21 19:58:16,statistics,Over reliance on p-values... unsurprisingly data dredging yields high levels of significant results that aren't.,mkd6x,blossom271828,1241312098.0,https://www.reddit.com/r/statistics/comments/mkd6x/over_reliance_on_pvalues_unsurprisingly_data/,25.0,12.0,,en
1108344,2011-11-21 21:43:17,statistics,Request for comments: article about racism and meritocracy in Silicon Valley.,mkhfn,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/mkhfn/request_for_comments_article_about_racism_and/,1.0,3.0,,en
1108345,2011-11-21 23:33:36,datasets,"Trying to find a data set of prescription drug prices in the 2000s, specifically forms of oxycodone (Oxycontin, etc)",mkm2q,theloniousnole,1315515927.0,https://www.reddit.com/r/datasets/comments/mkm2q/trying_to_find_a_data_set_of_prescription_drug/,4.0,2.0,I haven't had much luck so far though. Where can I find this data?,en
1108346,2011-11-22 00:03:03,artificial,What is the Current State of BioNLP with Regard to Information Extraction?,mknb7,[deleted],,https://www.reddit.com/r/artificial/comments/mknb7/what_is_the_current_state_of_bionlp_with_regard/,4.0,6.0,"I am working on a project that necessitates information extraction from massive numbers of PubMed abstracts. I know there exists NLP parsers such as the Stanford NLP that provides information regarding the syntax and structural information. I was wondering wether there exists any BioNLP, commercial or academics, that is capable of extracting relevant information (or even better, meaning) out of biological abstracts?

Any information would be great!  Thank you!",en
1108347,2011-11-22 03:06:18,computervision,Computer Vision really cool ideas for a thesis?,mkuo6,rsaborio,1154538563.0,https://www.reddit.com/r/computervision/comments/mkuo6/computer_vision_really_cool_ideas_for_a_thesis/,9.0,16.0,"Hi Reddit. I'm planning to do my Master thesis on Computer Vision (CV) and I'd been brainstorming ideas for my scientific thesis (my degree is on Computer Science). I'm totally newbie on CV but I'd been reading a LOT about everything on CV during the last 4 months and the experience has been really enjoyable. The more I read, the more I get interested. 

The problem is that every really cool project is already taken or developed (I think). I need to define my proposal as soon as possible (prototype on December, final proposal on March). I would like to hear from you guys. Can you share your ideas on really cool projects on CV? ANY ideas will be really appreciated. I mean it!.

Thanks in advance.
(Sorry for my english)",en
1108348,2011-11-22 03:09:04,datasets,"Trying to get access to DEA STRIDE database, or find other data sets with city level drug prices, quality, etc.",mkusl,[deleted],,https://www.reddit.com/r/datasets/comments/mkusl/trying_to_get_access_to_dea_stride_database_or/,5.0,0.0,"I am currently trying to find some data on city-level drug prices and purity over time. I've already contacted the local DEA branch in my city, but they keep giving me the run around. I was wondering if anyone else knows how to get access to the DEA STRIDE data sets or any other data sets that contain illegal drug prices and purity. I am looking for cocaine prices, but heroin or methamphetamine would both work as well for my purposes.

PS: I know the DEA posts some stride data on its website, but it is not specific enough for my purposes.",en
1108349,2011-11-22 03:12:21,statistics,How are statistical theories proven?,mkux7,[deleted],,https://www.reddit.com/r/statistics/comments/mkux7/how_are_statistical_theories_proven/,7.0,14.0,"Or ""supported"". Take bootstrapping for example. Does it have some sort of real world correlate like science? Does it use an innate logical system like mathematics? How do we know it is correct?",en
1108350,2011-11-22 07:35:04,statistics,"Interesting observations with PCA, R's prcomp(), and ""Standard deviations""...",ml4vg,brews,1263715301.0,https://www.reddit.com/r/statistics/comments/ml4vg/interesting_observations_with_pca_rs_prcomp_and/,3.0,2.0,"I've been playing with some math (as I'm prone to do with my evenings). I've noticed some odd behavior with R's `prcomp()`. I realize this might be a better question for stack overflow... but, humor me:

Lets say:

    X &lt;- matrix(c(1:8), ncol = 2)
    svd(X)
    eigen(t(X) %*% X)
    prcomp(X, center = FALSE)


According to prcomp's manpage (help page, whatever), the ""Standard deviation"" of the principal components should be the squar root of the eigenvalues... okay, this jives with what I remember about PCA: the 'explain variance' is the square root of the eigenvalues. But, as you can see if you play around with the code above, this doesn't appear to be the case. What gives? Have I forgotten something, or what am I missing?

I've poked about the google and seen [this](http://r.789695.n4.nabble.com/prcomp-function-td3036088.html). I'm not sure that this really helps me with what I'm after.",en
1108351,2011-11-22 07:36:01,datasets,Data Sets Used by the Treasury ,ml4wt,[deleted],,https://www.reddit.com/r/datasets/comments/ml4wt/data_sets_used_by_the_treasury/,1.0,0.0,"I thought /r/datasets might find the following tidbit interesting:

https://www.fbo.gov/index?s=opportunity&amp;mode=form&amp;id=4a19b60627a7eef4bf54145598b09517&amp;tab=core&amp;_cview=0

Solicitation number: CC12HQQ0003

[...]
The system includes a one-year subscription to the following databases: USECON; USINT; USNA; ASREPGDP; CAPSTOCK; LABOR, IP; EMPL; CEW; OES; CPIDATA; PPI; PPIR; FFUNDS; SURVEYS/SURVEYW; GOVFIN; WEEKLY; DAILY; DLINQ; CBDB; BCI; REALTOR; ICI; FDIC; HOUSING; RAILSHAR; EEI; SPM; SPW/SPD; SPAH; INDUSTRY; ASM/QFR; USDA; G10+; EUDATA; RANCE; GERMANY; ITALY; SPAIN; IK; IRELAND; BENELUX; NORDIC; ALPMED; JAPAN/JAPANW; ANZ; CANSIM; CANIMR; INTDAILY; INTWEEKLY; INTSRVYS;  EMERGE; EMERGELA; EMERGEPR; EMERGECW; EMERGEMA; OECDMEI; OECDNAQ; OECDNA; OUTLOOK; OECDFDI; IFSANN; IFS; IMFBOP/IMFBOPA; IMFDOT/IMFDOTM; IMFWEO; BIS; WBPRICES; WBDEBT; WDI; INTCOMP; UNPOP; LABORR; EMPLR; EMPLC; CEWR;  BEAEMP/BEAEMPM; GSP; GDPMSA; PIQR; PIR; PIRMSA; PICOUNTY; PIRC1 - PIRC9; USPOP; USPOPC; PERMITA; PERMITY; PERMITS; PERMITC; PERMITP; MBAMTG; BANKRUPT; GOVFINR; REGIONAL/REGIONW; EXPORTSR; EXPRQ; PORTS; FDICR; ASMR: DLINQR; EULABOR; PREALTOR; CREALTOR; M4CAST; MA4CSTL; OEFQMACR; OEFAMACR; and OEFQIND. 
[...]

I'm really interested in finding the source of of several of these (BANKRUPT in particular).",en
1108352,2011-11-22 14:34:46,MachineLearning,Can someone provide an intuitive explanation of Heckman correction?,mle3w,sanity,1149365629.0,https://www.reddit.com/r/MachineLearning/comments/mle3w/can_someone_provide_an_intuitive_explanation_of/,11.0,23.0,"I'm aware of the [Wikipedia article](http://en.wikipedia.org/wiki/Heckman_correction), but it is unnecessary dependent on economics jargon (eg. the explanation assumes that you know what the ""determinants of wage offers"" are).

Can anyone provide a more intuitive explanation, particularly as it might relate to correcting training data for a machine learning algorithm?

Specifically, can Heckman correction be applied when a supervised learning algorithm is responsible for choosing the samples which are used to train future versions of that supervised learning algorithm?

As a concrete example: We must choose between 100 ads to show people when they visit our website, and our goal is to maximize the number of people who click on the ads.

We can train a logistic model to predict the probability that a user will click on any given ad, and then run this model on all 100 ads for each visitor to our site, showing them the ad that has the highest probability of a click.

But now when we collect the results of these ad recommendations we have a biased sample, the bias being determined by a previous iteration of our supervised learning algorithm.

Can Heckman correction be used to correct this?  Are there other techniques that I should investigate?",en
1108353,2011-11-22 16:46:47,MachineLearning,ML interview questions,mlhas,amair,,https://www.reddit.com/r/MachineLearning/comments/mlhas/ml_interview_questions/,32.0,27.0,"What questions would you ask a candidate, for a job that required applying machine learning, to test their skills?",en
1108354,2011-11-22 18:59:49,computervision,Identifying what city a photo was taken in,mlm6l,DeadLikeJake,1313727429.0,https://www.reddit.com/r/computervision/comments/mlm6l/identifying_what_city_a_photo_was_taken_in/,2.0,2.0,Does a program exist for this anywhere?,en
1108355,2011-11-22 19:23:49,datasets,200K+ Jeopardy Questions and Answers,mln4j,cavedave,1128052800.0,https://www.reddit.com/r/datasets/comments/mln4j/200k_jeopardy_questions_and_answers/,24.0,2.0,,en
1108356,2011-11-23 01:10:12,computervision,Help Finding more Training and Validation Data,mm1do,herrtim,1289338761.0,https://www.reddit.com/r/computervision/comments/mm1do/help_finding_more_training_and_validation_data/,1.0,0.0,"Does anyone know of some good training and validation data sets like this one I found for images today: [Visual Object Classes Challenge 2011](http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2011/index.html#data)? This is perfect because a training and validation set is provided as well as other researcher's results for benchmarking. I'm also looking for biomedical, audio, text, or any other type of data sets that can be used by a classification system. Thanks in advance!",en
1108357,2011-11-23 01:13:06,statistics,A paradox with correlation results i am getting.,mm1h6,[deleted],,https://www.reddit.com/r/statistics/comments/mm1h6/a_paradox_with_correlation_results_i_am_getting/,2.0,19.0,"I do analysis for satisfaction surveys. Customers rate on a 1-5 scale. 4 or 5 is considered satisfied. 


Over the course of year there is a drastic drop in ""Overall Satisfaction"" (90% satisfied to 75% satisfied) and a drastic drop in satisfaction with the wait time (50% to 40% satisfied). Also a drastic increase in wait time as self reported by customer (different variable than satisfaction of wait time) 60% to 70%.


We do a simple Pearson correlation between overall satisfaction and satisfaction with other attributes, like wait time, courtesy, etc., etc. Satisfaction with length of wait time, comes out to have a r=.39, which is very low compared other unrelated attributes like courtesy or professionalism which are like 0.7ish. 


Could it be the 1-5 scale, causing some sort of ceiling effect? Could it be the way I run the correlation? Since satisfaction wait time is always generally low, it will not correlate with generally higher rated variable like “Overall Satisfaction” even if there is a direct relationship in the two.


Do you have any idea what might be going on? Is the drastic drop just a coincidence? 

THANKS!!!!!!

Edit: The paradox at least to me is the large decrease in satisfaction happens at the same time a large decrease in satisfaction in length of time but it is a super low r-value.

Edit2: Mystery kind of solved. Running a cross tab by time and courtesy/professionalism while only looking at the dissatisfied people, there was a equally huge drop in satisfaction in these two attributes. Which explains why they are highly correlated.

Looking at everyone was just masking the large decreases in courtesy/professionalism. Satisfaction with wait time actually increase slightly for these dissatisfied people. Suggesting the change in wait time is just a coincidence.",en
1108358,2011-11-23 01:43:28,MachineLearning,machine learning in genomics,mm2of,delinquentme,1298180126.0,https://www.reddit.com/r/MachineLearning/comments/mm2of/machine_learning_in_genomics/,3.0,7.0,"Anyone happen to work in this area?  machine learning and bioinformatics?

I'm REALLY interested in applying this stuff to some real problems ... something a little more hefty than housing prices :D",en
1108359,2011-11-23 12:41:46,MachineLearning,Why to deal with big data in R,mmlv3,[deleted],,https://www.reddit.com/r/MachineLearning/comments/mmlv3/why_to_deal_with_big_data_in_r/,0.0,0.0,,en
1108360,2011-11-23 15:26:51,statistics,SMBC: Normal person vs Statistician,mmoo8,ani625,1160717062.0,https://www.reddit.com/r/statistics/comments/mmoo8/smbc_normal_person_vs_statistician/,53.0,10.0,,en
1108361,2011-11-23 16:03:14,MachineLearning,Introduction to Matrix Factorization Collaborative Filtering,mmpk2,mralexlin,1322054372.0,https://www.reddit.com/r/MachineLearning/comments/mmpk2/introduction_to_matrix_factorization/,23.0,5.0,,en
1108362,2011-11-23 16:56:58,artificial,Any ideas for an AI bachelor's thesis?,mmr48,vinnl,1213866877.0,https://www.reddit.com/r/artificial/comments/mmr48/any_ideas_for_an_ai_bachelors_thesis/,10.0,18.0,"I'm writing my bachelor's thesis the coming months, and the subject I had in mind turned out not to be such a great fit. I'm studying A.I. and though I can think of other subjects, I'm wondering if anyone knows of some subjects that might actually be useful to someone _and_ suitable for a bachelor's thesis.",en
1108363,2011-11-23 19:48:22,statistics,My statistics professor gave us this version of the Monty Hall problem on our exam. He and I disagree on the answer. Can someone here help?,mmx8x,0utland3r,1291186272.0,https://www.reddit.com/r/statistics/comments/mmx8x/my_statistics_professor_gave_us_this_version_of/,3.0,7.0,"Question:

""In class we discussed the ""Monty Hall problem"": a game show with 3 unopened doors, one of which has a prize. Normally, the contestant chooses one door, and the host Monty Hall (who knows where the prize is) opens one of the two remaining doors (behind which he knows there would not be a prize) and then asks the contestant whether he would like to switch his choice to the other unopened door.

But suppose one day there was a malfunction at the TV studio and one of the two unselected doors accidentally blew open (but revealed no prize); it really was an accident. The host Monty Hall then continued with his normal offer and asked whether the contestant would like to switch to the unselected closed door. What is the probability that the prize is behind this door offered by the host?""

My view is that the probability is 2/3, which is what it would be in the normal version of the problem. My professor argues that the probability is 1/2. Does anyone have any thoughts on this?",en
1108364,2011-11-23 20:35:48,MachineLearning,"Genetic Algorithm for generating ""music""?",mmz3r,Fuck_the_police,1259270142.0,https://www.reddit.com/r/MachineLearning/comments/mmz3r/genetic_algorithm_for_generating_music/,3.0,7.0,"I was inspired by this video series: [Experimental One Line Music](http://www.youtube.com/watch?feature=player_detailpage&amp;v=qlrs2Vorw2Y#t=16s) which was posted on r/programming a little bit ago. With a cursory introduction to genetic algorithms, I thought that it would be interesting to use them to generate these strings that generate music.  Creating a generator of these strings randomly is pretty trivial, I think:

(where iter is the iteration variable, i)

* s -&gt; (val operator val) 
* val -&gt; s | iter | const
* const -&gt; 0-4096
* operator -&gt; / | * | ^ | &gt;&gt; etc.

From what I understand about GA's, and correct me if I'm wrong, but we need to encode this string into a string of bits, which is where I ran into my first problem. Since this is a kind of grammar, you can't just change around any of these bits willy nilly, because then it might not be a valid sentence. So, rather than encode the string as single bits, I thought I could encode the string as integers which are then treated as bits in a traditional GA. Depending on the value of the integer, I could determine if it was a value or an operator and swap or replace accordingly. I think this would work, but I'm not familiar enough with GA's to know if this is a method that has been done before.

The second problem I have, and don't have a solution for, is what fitness function to use. I'm completely at a loss. I was thinking of checking the cycles that are generated, maybe for length or complexity? But I'm not sure, and that's where I'd like your advice. I'm not sure if I wan't a fitness function for a 'traditional' music generating GA, because it's not traditional music, but I don't know what is out there, so I'm up for suggestions.

Thanks!",en
1108365,2011-11-23 22:43:38,statistics,Analyze your Google Scholar Citations page (R functions),mn47z,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/mn47z/analyze_your_google_scholar_citations_page_r/,2.0,0.0,,en
1108366,2011-11-23 22:52:55,MachineLearning,The perils of treating a model’s predictions as actual probabilities (with an improvement to the Pair-Adjacent Violators algorithm),mn4ko,[deleted],,https://www.reddit.com/r/MachineLearning/comments/mn4ko/the_perils_of_treating_a_models_predictions_as/,10.0,27.0,,en
1108367,2011-11-23 23:00:49,statistics,How should I report a Welch's F for a one-way anova ? How can the degrees of freedom have decimals ?,mn4vf,Chrysippos,1296239065.0,https://www.reddit.com/r/statistics/comments/mn4vf/how_should_i_report_a_welchs_f_for_a_oneway_anova/,1.0,2.0,"For example if this is the SPSS output : 

            Statistic      Df1          Df2
Welch    44.545        6              55.434    

Is this appropriate ?  F(6,55.43)= 44.55, p&lt;.05. Thank you in advance for your reply.",en
1108368,2011-11-23 23:41:48,statistics,Rpart Question,mn6fu,amirightfolks,1280970578.0,https://www.reddit.com/r/statistics/comments/mn6fu/rpart_question/,1.0,3.0,"I am trying to build a decision tree using the rpart function. The response is binary, whether the person has a certain disease or is healthy (210 have the disease, 1004 are healthy). My predictors are alcohol consumption (categorical, 4 levels),  tobacco consumption (categorical, 4 levels), and age (categorical, 6 levels). 

When I tried to run the model, it only gave me one node in which it separated it into the original numbers from the data (210/1004). Why might this be? I think it could be a problem of too few cases of disease compared to healthy but am not sure. 

Also, I am able to run the model with Disease as a numeric factor (0,1), but this is not what I want. Is there any solution to this?",en
1108369,2011-11-24 01:07:10,statistics,Random Forest Question (related to classifiers),mn9i2,statsfitter,1322089016.0,https://www.reddit.com/r/statistics/comments/mn9i2/random_forest_question_related_to_classifiers/,0.0,3.0,"I recently got introduced to Random Forests and had a question on classifiers. Let's say X and Y are independent classifiers and we have f(X,Y) result values. The question is I can derive more information from X and Y (say some H(X,Y),G(X,Y) etc) and get the corresponding values of results. With this process, my dataset is expanding and can train on that - but if I apply Random forest algorithm to it, would it end up doing more ""curve fitting"" ? I guess this may not be related to RF per se, but a generic training question. If you had the option to expand the classifiers, what thumb rules should I use ? May be a dumb question, but would love to hear from experts. Thanks.
",en
1108370,2011-11-24 03:58:50,statistics,help r/statistics: Trying to remove arbitrariness from a particular significance test.,mnezj,snoa,1322091440.0,https://www.reddit.com/r/statistics/comments/mnezj/help_rstatistics_trying_to_remove_arbitrariness/,0.0,2.0,"Hi everybody,

Sorry if this is something simple that I should have known.

I am using an instrument that is taking a measurement (expression level) for each of a number of items (gene products) and I want to test if a particular measurement is significantly different from the mean. In addition, measurements taken at higher overall intensity levels (in the instrument) show less variation than those taken at lower intensity.

What some do with the data is separate it into bins by intensity, and then perform a t-test with the data in the respective bin to assign a p value.  My concern is that because the separation into bins is arbitrary and the t-test can differ when the number of measurements in the bin are changed, then the result also has some ""arbitrariness.""

1) So is there a better way to test for significance that takes into account this third parameter, intensity?  I have tried googling around but many pages suggest using tests for multivariate distributions.  However, I am still only concerned with one dimension, the measurement itself, and should only expect less significant values at low intensity.

2) There may be other ""quality control"" metrics other than the intensity used in the example above.  Can the answer to 1 be expanded to higher dimensions?  The other problem with binning the data is that when I apply it to higher dimensions, suddenly there are only a few datapoints in the sample of what are supposed to be ""high quality"" measurements and easily visible changes are suddenly less significant.



Thanks in advance.",en
1108371,2011-11-24 19:53:44,MachineLearning,Where can I learn about these things on the web? ,mo22c,pharshal,1263887385.0,https://www.reddit.com/r/MachineLearning/comments/mo22c/where_can_i_learn_about_these_things_on_the_web/,6.0,13.0,"Came across this link, http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=ufldl

Many of the lectures are not yet available there. The topic names are so intriguing for me! (newbe in ML)  


V. APPLICATION TO CLASSIFICATION


IV. UNSUPERVISED FEATURE LEARNING AND SELF-TAUGHT LEARNING


V. APPLICATION TO CLASSIFICATION


VI. DEEP LEARNING WITH AUTOENCODERS


VII. SPARSE REPRESENTATIONS


VIII. WHITENING


IX. INDEPENDENT COMPONENTS ANALYSIS (ICA)


X. SLOW FEATURE ANALYSIS (SFA)


XI. RESTRICTED BOLTZMANN MACHINES (RBM)


XII. DEEP BELIEF NETWORKS (DBN)


Google can be helpful here, but asking here just in case anyone knows a specific resource that may not show up in top 10 in Google.
",en
1108372,2011-11-24 21:26:43,statistics,Turning a variable into an integer for linear regression... Am I doing it right?,mo4t4,sais,1201756650.0,https://www.reddit.com/r/statistics/comments/mo4t4/turning_a_variable_into_an_integer_for_linear/,6.0,18.0,"This may be a really stupid question (I'm teaching myself stats and am new to this whole field), but hopefully this makes sense. I have a variable that I just coded as either ""H"" or ""L"" (high or low). If I want to add this variable to my linear model, I need to encode it as an integer. What I did was make H equal to 1 and L equal to 0. I essentially treated it as a success/fail score. Does it matter what value I use? Could it be 2 and 5? Does it matter which variable gets which value?
Essentially my question is, what is the best way to take this variable and analyze it using my stats program?
",en
1108373,2011-11-24 21:39:39,MachineLearning,Finding similarities and equivalences...,mo580,dhk8,1261328948.0,https://www.reddit.com/r/MachineLearning/comments/mo580/finding_similarities_and_equivalences/,2.0,5.0,"Hey y'all... I'm engaged in a project where I'm trying to answer two questions

* Is this item the same as these other items?
* Is this item similar to these other items?

Where I already have knowledge of a population of about a million items that are assembled into typically, but not always, mutually-exclusive, sub-groups

Where each item is a phrase of 2-10 words, abbreviations and the odd character string. 

For the first approach, I'm using jaccard indexes and for the second I'm using Naive Bayesian Classification.

And, if an item is the same as another item, then it's going to be in the same sub-group(s).

Questions are, what are alternate mechanisms that people might use to supplement these matching processes? ",en
1108374,2011-11-25 04:19:44,MachineLearning,"Newbie to ML, not sure if these problems are suitable for solving with ML techniques",mogjk,malvim,1319474771.0,https://www.reddit.com/r/MachineLearning/comments/mogjk/newbie_to_ml_not_sure_if_these_problems_are/,10.0,23.0,"Hi, all.

I'm a total newbie to ML, I'm taking Stanford's ml-class right now. Since this is more of a general question, I thought it would be better to post it here rather than there.

So here's my problem: I'm trying to figure out some application for the techniques that I'm studying (trying to find some nails to use this new hammer on...), and so far I've had two ideas that I could use. They're both related to signal (sound) processing, something that I also have NO experience with, which makes them that much harder. I'm not saying I could implement them right away, but I would like to know if ML techniques are the way to think about this problems or not. Here they are:

1) A ""Wilhelm Scream Finder"". It seems to me that, given some sample extracts from movies with the ""[Wilhelm Scream](http://en.wikipedia.org/wiki/Wilhelm_scream)	"" on them (say, from [this movie](http://www.youtube.com/watch?v=cdbYsoEasio), I could get some training examples and train a Neural Network or an SVM to classify whether some other extract of some other movie has the scream in it or not. That seems doable, but what about the REAL problem, of having a whole movie sound track and actually FINDING Wilhelm Screams in it? That seems like a whole other problem in itself, which I have no idea how to even start solving.

2) Similar to the ""cocktail party problem"": I have several (well, three or four) separate tracks of a song. On each of them, one of the instruments (guitar, bass, drums) is higher than all the others, but all of them have have been recorded using the same ""background"", which is a low-volume base track with all the others playing together. I would like to ""remove"" this background noise, which is very similar in all of them. Again, no idea of how to solve this.

Are these even good problems for ML to solve, or should I just look for other solutions (or other problems!)?

Thanks!",en
1108375,2011-11-25 05:25:18,MachineLearning,Anomaly detection in related time-series,moidn,[deleted],,https://www.reddit.com/r/MachineLearning/comments/moidn/anomaly_detection_in_related_timeseries/,1.0,0.0,"Hey guys, I have a difficult problem I need help with!

As an example, I've got a stress sensor attached to a rail (think trains) and a temperature sensor nearby. These are related, because temperature variations have a huge effect on the stresses in the rail.

Now what I want to do is find anomalies in the stress time-series that are not related to changes in temperature. I want to find anomalies in the temperature-corrected stress data, *without* having to explicitly do the thermal expansion calculations. 

I did a machine learning course a long time ago and I am a good programmer. However, it wasn't a very practical course, and I can't remember much of it. Where should I begin? 

Heres an example set of data - green is temperature, blue is stress
http://i.imgur.com/XJoIk.png",en
1108376,2011-11-25 11:42:20,statistics,Discussion with teacher about a certain statistics exercise.,morig,Problem_Santa,1310971230.0,https://www.reddit.com/r/statistics/comments/morig/discussion_with_teacher_about_a_certain/,2.0,2.0,,en
1108377,2011-11-25 22:18:02,statistics,Using the integral of a minimum for a confidence interval to determine rank. ,mp5w8,[deleted],,https://www.reddit.com/r/statistics/comments/mp5w8/using_the_integral_of_a_minimum_for_a_confidence/,1.0,0.0,"Hey guys,
I've got a microarray experiment with 2 time series (n=3 and n=4). I've modeled gene expression using polynomials. After identifying all the constants given the experimental design, and the priors which are consistent between samples, i've generated an integral for the minimum value of the fdr corrected confidence interval. 

Multiple testing was performed for the number of tests at each point (# of genes) but not the number over the course of all points. 

Is there anything wrong with ranking genes using difference in expression between genes minus the positive value of the integral for the minimum of the confidence interval? (negative numbers were forced to zero)? ",en
1108378,2011-11-25 23:23:25,MachineLearning,Automating cross-pollination in research,mp80c,jdh30,1135659600.0,https://www.reddit.com/r/MachineLearning/comments/mp80c/automating_crosspollination_in_research/,2.0,10.0,I recently noticed that the union-find algorithm is rarely mentioned in literature on garbage collection despite seeming quite relevant. Then I wondered if subjects that might stand to gain from more cross pollination could be identified automatically. Has anyone tried to use machine learning techniques to identify combinations of subjects that are ripe for research?,en
1108379,2011-11-26 02:32:32,MachineLearning,Advice please: Making predictions by correlating to weather *forecasts*,mpdut,cultic_raider,1295221344.0,https://www.reddit.com/r/MachineLearning/comments/mpdut/advice_please_making_predictions_by_correlating/,6.0,12.0,"My business has a personal service component, and demand for that service is weather dependent. When a blizzard or hurricane hits, we do less business.

I am looking to build a demand forecasting model that factors in weather. That's a straightforward task. 

But the mystery for me is how to account for weather *forecasts*. I want to predict demand a week or so advance, so we can staff up or down as needed.

Since weather forecasts are nor perfectly correlated to weather,  I need to handle the difference somehow. My ideas are:

* Build a model that measures forecast accuracy, and use that to put error bars on my demand predictions.
* Train models on actual historical weather, but test/validate/combine models on a dataset on recent weather forecasts that I collect myself.
* Find a dataset of historical weather *forecasts*, and train my model on that data.
* Assume forecasts are perfect, and don't spend time or effort worrying about forecast error.
* Don't try to predict business on a daily grain; use a weekly grain where actual (average) weather and forecasts are more closely matched

**TL;DR: when building a predictive model of a weather-correlated effect, do I care that weather forecasts are imperfect?**",en
1108380,2011-11-26 07:44:00,MachineLearning,Missing Data Advice,mpmdq,dtwhitney,1322285087.0,https://www.reddit.com/r/MachineLearning/comments/mpmdq/missing_data_advice/,6.0,9.0,"I am taking the Stanford ML Course, and really enjoying it. My question is what are some reference materials I can read to decide what to do about missing data? Specifically, if I have 1500 input variables and 1500 training examples, and typically 2% of those variables are missing in a fairly predictable way, meaning they tend to be the same subset that are missing, what should I do? Are there any quick rules of thumb? Removing the variables and shrinking the set down seems reasonable, but some of them seem valuable to me.  Thanks for any input.",en
1108381,2011-11-26 21:45:24,MachineLearning,neural network question,mq3jp,giror,1269318363.0,https://www.reddit.com/r/MachineLearning/comments/mq3jp/neural_network_question/,8.0,12.0,"Hello All, 

I was wondering if there is a good reason for using or not using a neural network to predict a multidimensional output instead of asking for just one most correct output out of m possible outputs. I am *not* asking about multi-class classification, I am asking if I could use a neural network as a mapping function into a high dimensional space where I expect high action in multiple output nodes. 

In other words: 
If you are classifying digits, the output is a 10 dimensional vector where we expect a 1 in only one dimension. I am asking if neural networks could be reliably used in cases where we expect the output to contain ones in multiple dimensions. 

Thanks in advance!

*Edited for clarity*

",en
1108382,2011-11-27 15:26:17,statistics,forgot name of open innovation website,mqu03,bobbin_threadbare,1296148953.0,https://www.reddit.com/r/statistics/comments/mqu03/forgot_name_of_open_innovation_website/,7.0,3.0,I saw some open innovation website that is geared towards data analysis where people bring data and other people analyse for a prize or something of the like. Anybody remember the name of such a place?,en
1108383,2011-11-27 17:53:03,AskStatistics,"Economics, multiple regression",mqwn0,Starjar,1322409094.0,https://www.reddit.com/r/AskStatistics/comments/mqwn0/economics_multiple_regression/,4.0,8.0,"i'm doing a multiple regression on the amount of bowling centers in US states. I'm using the variables; population, real income per capita, density (pop/area) , percentage living in urban area, percentage above 65 years. Does anyone have a suggestion for another relevant variable i can include ?",en
1108384,2011-11-27 18:02:02,statistics,books on statistical consulting,mqwua,fuzonc,1296357930.0,https://www.reddit.com/r/statistics/comments/mqwua/books_on_statistical_consulting/,3.0,1.0,anyone have any suggestions?,en
1108385,2011-11-27 18:31:12,rstats,Need help with panel plot in lattice package,mqxkq,Dangaroo44,1289658279.0,https://www.reddit.com/r/rstats/comments/mqxkq/need_help_with_panel_plot_in_lattice_package/,3.0,2.0,"I'm trying to fit smooths to my panel xyplots, but only to a few of the plots.  Does anyone know how to omit the panel.loess function to only certain plots?  Here is an example of my code:

xyplot(DOC~ Discharge|factor(Station), data=neuse.dom, ylab= 'DOC (mgL-1)', xlab='Discharge (m3s-1)', panel=function(x,y){
panel.xyplot(x,y)
panel.loess(x,y)}) 

The plots are split by Station. I would like to omit 2 of the 11 stations.  

Thanks!",en
1108386,2011-11-28 04:04:31,statistics,Newbie question: but does this kind of graph have a name? Can it be done in R?,mrgpy,shanahdt,1292424859.0,https://www.reddit.com/r/statistics/comments/mrgpy/newbie_question_but_does_this_kind_of_graph_have/,26.0,11.0,,en
1108387,2011-11-28 04:13:06,MachineLearning,Apache Mahout could use your help,mrh1p,mycall,1183346313.0,https://www.reddit.com/r/MachineLearning/comments/mrh1p/apache_mahout_could_use_your_help/,22.0,10.0,,en
1108388,2011-11-28 04:25:31,statistics,Has anyone else used John Freund's Mathematical Statistics book?  I think the authors purposely list incorrect solutions in the back of the book.,mrhhd,mnky9800n,1242751739.0,https://www.reddit.com/r/statistics/comments/mrhhd/has_anyone_else_used_john_freunds_mathematical/,1.0,0.0,,en
1108389,2011-11-28 10:14:55,datasets,Looking for Preferential Data Sets,mrt4o,amathstudent,1322431594.0,https://www.reddit.com/r/datasets/comments/mrt4o/looking_for_preferential_data_sets/,3.0,0.0,"Hey /r/datasets I just tried posting this in /r/math but they sent me here. So I'm a 3rd year maths student in the UK and I'm writing a project on voting systems. I'm trying to find a free online data set which has preferential rankings, i.e. Voters choices on a set {A,B,C} with 1st, 2nd and 3rd choices marked, which I can perform some analysis on. Any one got any suggestions??",en
1108390,2011-11-28 11:00:50,MachineLearning,Basics of MCMC,mru1g,amatsukawa,1321921342.0,https://www.reddit.com/r/MachineLearning/comments/mru1g/basics_of_mcmc/,16.0,3.0,,en
1108391,2011-11-28 13:21:38,statistics,Statistics software,mrwfd,EvLoEv,1322478762.0,https://www.reddit.com/r/statistics/comments/mrwfd/statistics_software/,3.0,7.0,"Hi,
Our small department of statistics at my university have been granted some money for (almost) any purpouse. The ""problem"" is that we've got almost everything we need. My professor asked us if we wanted some new statistics software. Any really god software out there? We are mostly students in actuarial studies. We are also looking for good textbooks aswell.",en
1108392,2011-11-28 14:44:57,datasets,"Online copy of Domesday Book
",mrxw6,cavedave,1128052800.0,https://www.reddit.com/r/datasets/comments/mrxw6/online_copy_of_domesday_book/,9.0,0.0,,en
1108393,2011-11-28 15:16:46,statistics,"Followup on a post from a few weeks ago, asking about when a renal tumor formed.",mryjb,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/mryjb/followup_on_a_post_from_a_few_weeks_ago_asking/,3.0,3.0,,en
1108394,2011-11-28 17:16:31,statistics,Holiday Gift Cards by the Numbers (Infographic),ms1o7,[deleted],,https://www.reddit.com/r/statistics/comments/ms1o7/holiday_gift_cards_by_the_numbers_infographic/,1.0,0.0,,en
1108395,2011-11-28 19:47:56,statistics,Parameter estimation question,ms76t,[deleted],,https://www.reddit.com/r/statistics/comments/ms76t/parameter_estimation_question/,1.0,0.0,"Got asked this question today:

When cycling home at night, I notice that sometimes my rear light is switched off when I arrive home. Presumably the switch is loose and can flip from on to off or back again when I go over bumps. I suppose that the number n of flippings per trip has a Poisson distribution with mean L. 

If the probability that the light is still on when I arrive home is p, find L.

SO FAR:
The probability p, is the sum of the even terms of the Poisson distribution. But then I can't get L out of the summation to solve for it. Is there another way to do this without resorting to machines. ",en
1108396,2011-11-28 20:18:08,MachineLearning,"A general PSA to the new recruits to the subreddit from the ML-CLASS: Your toolbox contains a few new hammers, but not every problem is a nail",ms8gh,shaggorama,1233555004.0,https://www.reddit.com/r/MachineLearning/comments/ms8gh/a_general_psa_to_the_new_recruits_to_the/,31.0,7.0,"I've seen several posts recently that appear to be solutions searching for a problem. Posts along the lines of ""Is this problem suitable for me to apply this new technique I learned?""

I understand that you all are looking for ways to apply your new skills, and you absolutely should. If you don't practice your new knowledge, you're likely to forget it. Unfortunately though, walking around with a hammer in search of a nail is generally a bad way to go about it. You need to review available problems and consider possible solutions independent of the tools you'd prefer to exercise. 

I'm sorry I don't have any resources to offer. My general suggestions for getting some practice are to poke around on [Kaggle](http://www.kaggle.com/), or pick up a ML textbook and work through some exercises. Maybe find a problem that has already been solved by a technique you want to practice and try figuring out how to do it yourself.

A general warning: try not to be like that guy in your office who just figured out how to make pretty graphs in excel and is looking for excuses to plot anything he can as a 3D bubble chart. It just doesn't work that way.",en
1108397,2011-11-28 21:42:01,statistics,"""One direct, stark statistical error that is so widespread it appears in about half of all the published papers surveyed from the academic neuroscience research literature""",msbz5,aaaxxxlll,1262625719.0,https://www.reddit.com/r/statistics/comments/msbz5/one_direct_stark_statistical_error_that_is_so/,4.0,3.0,,en
1108398,2011-11-29 06:23:00,datasets,Any foreclosure databases?,msxsb,Mottebayo,1283551388.0,https://www.reddit.com/r/datasets/comments/msxsb/any_foreclosure_databases/,2.0,2.0,"Do you guys know of any foreclosure databases that are free? It would also be great if there was a foreclosure database with geolocation data, or a point file that could be usable in GIS.",en
1108399,2011-11-29 06:29:11,AskStatistics,Completely at a lost.  I want to test correlation between different variables across time &amp; country.,msy1s,casatap,1190187024.0,https://www.reddit.com/r/AskStatistics/comments/msy1s/completely_at_a_lost_i_want_to_test_correlation/,0.0,10.0,"I'll begin by apologizing for my ignorance.  I'm writing a paper on monetary theory and am investigating the link between dollarization (the amount of foreign currency held in a country) and inflation.  I have data that proxies those indicators across different countries and years, but I am not sure what tests to run given the fact that I have both time series and data across different countries.  Ideally, I'd also like to run granger causality tests.  My main conceptual hurdle is that I don't understand if I need to aggregate my data or alternatively if I need to slice it (e.g., run a separate correlation test for each country over all of the years in the data).

Any help would be greatly appreciated.

P.S. I've included the data source
[Media Fire Link](http://www.mediafire.com/?acwbxhbvqzyi7p0)",en
1108400,2011-11-29 06:53:18,statistics,Why do we need statistics?,msz20,trancelogix,1266451483.0,https://www.reddit.com/r/statistics/comments/msz20/why_do_we_need_statistics/,12.0,33.0,"Please forgive me, math is not my strong point.  And it bothers me. 

I do excellent in every other subject in college, but I failed my statistics class, because I cannot grasp the understanding of why it's useful, which makes it harder to apply and learn for me.

I understand normal distribution, mu and sigma, but I don't get the concept behind statistics.  Can someone explain, like you're speaking to a 10 year old, why statistics is important and a fundamental concept of it?  

I'd really like to learn without banging my head against the wall.",en
1108401,2011-11-29 12:26:49,MachineLearning,Conditional Expectation what is this Joint Probability?,mt7mv,nickponline,1285653350.0,https://www.reddit.com/r/MachineLearning/comments/mt7mv/conditional_expectation_what_is_this_joint/,5.0,5.0,"I'm trying to compute (or at least upper and lower bound) the conditional expectation of E[X | X &gt;= x0] where X is distributed standard normally with mean = 0 and std = 1 and x0 is some initial value.  

I wrote the final expression as:

Integral X . P(X, X &gt;= x0) / P(X &gt;= x0) dX

But what is the expression for P(X, X &gt;= x0)? Isn't it just the same as P(X &gt;= x0). How can you have a joint distribution if it's the same variable?",en
1108402,2011-11-29 13:20:40,statistics,Fixed effect multinomial logit model - how to?,mt8h5,ATrolle,1320075744.0,https://www.reddit.com/r/statistics/comments/mt8h5/fixed_effect_multinomial_logit_model_how_to/,3.0,1.0,"I do educational research and I have come across a model-related problem, that I hope you can help me with.
I have a polytomous categorical variable (choice of tertiary education) as my dependent variable and a hierarchical database (individuals clustered in families).

What I would like to do is estmiate a multinomial logit model with a cluster-specific fixed effect . Under normal circumstances (a binary or continuous outcome) I would just calculate the within-cluster differences by hand, but since this is a categorical variable, I'm at a loss.

Any advice? I use STATA12, and so far the best option seems to be the ""femlogit"" command descriped [Here](http://www.stata.com/meeting/germany11/desug11_pforr.pdf) (PDF), but it doesn't seem to be available in STATA yet...

Thank you!",en
1108403,2011-11-29 13:42:57,statistics,Conditional Expectation?,mt8ut,nickponline,1285653350.0,https://www.reddit.com/r/statistics/comments/mt8ut/conditional_expectation/,5.0,8.0,"I'm trying to compute (or at least upper and lower bound) the conditional expectation of E[X | X &gt;= x0] where X is distributed standard normally with mean = 0 and std = 1 and x0 is some initial value.
I wrote the final expression as:
Integral X . P(X, X &gt;= x0) / P(X &gt;= x0) dX

But what is the expression for P(X, X &gt;= x0)? Isn't it just the same as P(X &gt;= x0). 

How can you have a joint distribution if it's the same variable?",en
1108404,2011-11-29 17:11:09,artificial,Everyone likes Scrabble... right? Can you help researchers at NCSU with AI and cognitive modeling by playing a short game.,mtdpa,titusbarik,1312810412.0,https://www.reddit.com/r/artificial/comments/mtdpa/everyone_likes_scrabble_right_can_you_help/,16.0,8.0,,en
1108405,2011-11-29 18:20:50,statistics,Anyone out there have any experience using the market model when analysing the impact of news stories on stock prices?,mtg6l,jafoooli,1292199169.0,https://www.reddit.com/r/statistics/comments/mtg6l/anyone_out_there_have_any_experience_using_the/,7.0,12.0,,en
1108406,2011-11-29 18:32:46,data,"Nope, it's actually only 4 degrees of separation.",mtgmi,winniechimp,1309463088.0,https://www.reddit.com/r/data/comments/mtgmi/nope_its_actually_only_4_degrees_of_separation/,1.0,0.0,,en
1108407,2011-11-29 20:36:36,MachineLearning,"Layperson-level explanation of state abstraction, policy abstraction, and their interaction? And what are the core steps for state identification/abstraction/aggregation?",mtlr7,[deleted],,https://www.reddit.com/r/MachineLearning/comments/mtlr7/laypersonlevel_explanation_of_state_abstraction/,2.0,1.0,"Neuroscientist here. Your field has been a big inspiration to me (and to many cognitive neuroscientists), but I find your papers incredibly difficult to decipher.  

Policy abstraction seems to be implemented in a nice set of dissociable neuroanatomical structures, but there are some large portions of cortex that do not perfectly fit this framework, and there are some indications that these areas may be dissociable both functionally and structurally. One possibility is that these areas are performing something like state abstraction. 

The problem: I don't know exactly what that means (beyond ""aggregating action-relevant features of the environment and discarding action-irrelevant features to yield a more compact state space""), nor whether there are currently thought to be dissociable subproblems/subcomponents of this larger problem, nor how they are  thought to interact with policy abstraction (or the classes of available actions), nor situations under which we could make state abstraction more difficult, complex, or higher-order without affecting policy abstraction (a necessity if we're going to attempt to use functional neuroimaging). Is there even really a hierarchy to states, in the same way there can be a hierarchy to policy?

*Any* clarification would be much appreciated. In addition, references to decipherable scholarly work would also be appreciated.  Thanks all.

EDIT: Here's some background for those that don't have any idea what I'm talking about - sorry, I guess I did bumble my way into an obscure sector of ML.

http://www.cs.berkeley.edu/~russell/papers/aaai02-alisp.pdf

http://www.cs.rutgers.edu/~lihong/pub/Li06Towards.pdf",en
1108408,2011-11-29 21:49:57,MachineLearning,What is the use of manifold learning?,mtov5,DoorsofPerceptron,1276810914.0,https://www.reddit.com/r/MachineLearning/comments/mtov5/what_is_the_use_of_manifold_learning/,4.0,9.0,"Sorry for the provokative title, but we were discussing  manifold learning at work, and I can't quite work out when you'd use it, and why.

Given the difficulties standard manifold learning techniques have with unwrapping or flattening  T shapes where two surfaces intersect (like `_|_` ), and closed surfaces (like the surface of a ball), why do people use manifold learning instead of just finding connected subspaces in the original feature space?

I get that visualisation is the killer app for manifold learning, but are there any other benefits in unrolling your data into a plane, or into a 3-space? ",en
1108409,2011-11-29 22:10:38,statistics,Analysis of Time Series,mtptr,theunseen,1250395987.0,https://www.reddit.com/r/statistics/comments/mtptr/analysis_of_time_series/,8.0,13.0,"I was just wondering what the best test was to determine whether there is a statistical difference between the time series plots of two different treatment conditions. To be specific, given two treatment conditions, A and B, to determine whether B has a lower half-life than A. An intuitive explanation of the test would also be appreciated. Thanks.",en
1108410,2011-11-30 01:08:33,statistics,How the Fuck do I calculate a Z-Score for a set of data using Excel?,mtxs4,ThisGuy182,1285709394.0,https://www.reddit.com/r/statistics/comments/mtxs4/how_the_fuck_do_i_calculate_a_zscore_for_a_set_of/,0.0,3.0,,en
1108411,2011-11-30 02:42:31,statistics,[Statistics-related] Sights for Sore Eyes,mu1qq,mickymause,1322612674.0,https://www.reddit.com/r/statistics/comments/mu1qq/statisticsrelated_sights_for_sore_eyes/,1.0,2.0,"If a friend of yours was told that they could lose the last of their eyesight in as early as three months, and they wished to pursue a graduate degree in Statistics, which visualizations would you suggest that they look at to better prepare them for their studies?",en
1108412,2011-11-30 02:45:06,statistics,House keeping: If y'all would be so kind as to click the report button for homework-related posts it would help me spare y'all.  Thank you.,mu1vd,RA_Fisher,1299707119.0,https://www.reddit.com/r/statistics/comments/mu1vd/house_keeping_if_yall_would_be_so_kind_as_to/,17.0,10.0,,en
1108413,2011-11-30 03:02:12,MachineLearning,Is a PhD worth it in machine learning?,mu2ly,BanachSpaced,1283233964.0,https://www.reddit.com/r/MachineLearning/comments/mu2ly/is_a_phd_worth_it_in_machine_learning/,33.0,38.0,"I'm in a graduate program in statistics right now, and have dabbled in machine learning and data mining for a while. I'm currently getting my master's, and I'm trying to decide if it's worth it to get a Ph.D if this is the field I want to get into.

What is your experience with the MS/PhD requirements for ML/DM jobs? Is a master's or master's + experience equivalent to a phd for most jobs? Most research jobs? And are most ML/DM jobs research oriented or not?

Is having a degree in statistics, instead of CS, a hindrance?
Can a strong statistical background make up for a less strong CS background?

Lastly, what skills are most valuable for work in machine learning, or what skills are too often overlooked?",en
1108414,2011-11-30 10:26:03,MachineLearning,Graduate Level Machine Learning Course,muj9k,leonoel,1284436447.0,https://www.reddit.com/r/MachineLearning/comments/muj9k/graduate_level_machine_learning_course/,2.0,8.0,"Hello,

I'm working with my former university to create a Machine Learning Class for Grad Students in CS. I want to use the power of the collective thinking and expertise to do this. In your opinion, what would be a good set of topics to have in a one semester (17 weeks, 2 times a week) introductory course on ML. The students are supposed to have good backgrounds on Math and Computer Science, most of them have by this time taken several courses on probability, geometry and functional analysis.

Thanks for your suggestions.

If you have a link of a particular ML course you consider particularly good (I already know Andrew NG's material) it would be greatly appreciated as well.

Thanks",en
1108415,2011-11-30 10:58:47,MachineLearning,BBC News - Coding wizards offered X Factor style competition,mujy1,fishandchips,1161341942.0,https://www.reddit.com/r/MachineLearning/comments/mujy1/bbc_news_coding_wizards_offered_x_factor_style/,9.0,6.0,,en
1108416,2011-11-30 20:16:35,statistics,Calculating r(wg) in SPSS?,muz6b,Palmsiepoo,1315548686.0,https://www.reddit.com/r/statistics/comments/muz6b/calculating_rwg_in_spss/,2.0,4.0,"Hi all, I have a dataset where I need to calculate the interrater agreement within groups (aka rwg) in SPSS. The data were collected by factory workers and we asked them a bunch of questions about the nature of their team's leadership but we can't aggregate their responses to the team level without first knowing they agree as a team. The rwg statistic addresses this issue to give us a value of how much each team agrees on the nature of the team's leadership.

I need a method, software, or technique to calculate rwg in each team in SPSS. Any help would be awesome. Thanks! (Reddit gets a shout out if this gets published).",en
1108417,2011-11-30 20:59:34,statistics,stata commands or some better software to do this,mv137,MyNameCouldntBeAsLon,1311784142.0,https://www.reddit.com/r/statistics/comments/mv137/stata_commands_or_some_better_software_to_do_this/,2.0,2.0,"Hello:


I would like to test the hypothesis of coincidence (with an F test, more than likely) between two regression models.


The problem is that I posses a very big dataset (ordered by country and a number of years), and the pairwise comparison would have to be in the hundreds, if not thousands:



Here is a more throrough explanation. My dataset has different indicators for the US, Canada, France, Mexico, Germany, etc from 1959 up to 2010. So I have US 1959, US 1960... Us 2010.


I would like to compare two regression models, the particular form is not important, but here is an example:

Y1=B1X1+B2X+E vs Y2=B3X3+B4+E


Is it possible to run these pairwise comparisons, or is it better (and possible) to use it as a time series, and if so, which software would be best suited for this?



I am more 'fluent' in Eviews for time series analysis, and stata for cross sectional data.",en
1108418,2011-11-30 21:16:41,MachineLearning,A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition [PDF],mv1uh,roger_,1178076247.0,https://www.reddit.com/r/MachineLearning/comments/mv1uh/a_tutorial_on_hidden_markov_models_and_selected/,15.0,8.0,,en
1108419,2011-12-01 03:40:48,statistics,"If you had to impart one piece of wisdom about teaching (statistics), what would it be?",mvifm,blind_swordsman,1318716377.0,https://www.reddit.com/r/statistics/comments/mvifm/if_you_had_to_impart_one_piece_of_wisdom_about/,12.0,11.0,"This is not specific to statistics, but I would say mine is to attribute problems in the classroom to your failure as a teacher, rather than the students not being smart/hard working/prepared enough. 

I find that this approach makes you much more humble, and helps you consistently improve your teaching abilities. Doesn't do wonders to your ego, although for most academic types I don't think that's a problem ;]",en
1108420,2011-12-01 03:46:25,AskStatistics,research approach,mvinj,suncloud,1316308396.0,https://www.reddit.com/r/AskStatistics/comments/mvinj/research_approach/,0.0,0.0,"I use mainly quantitative data, a survey method and a deductive approach. However, I have incorporated open text boxes where my respondents can write their own ideas/thoughts/opinions to obtain ‘richer data’. This is of course qualitative data and I will try to build a theory from this. I have chosen this because it suits my aims and objectives.

Question: is my research philosophy positivism or pragmatism?
",en
1108421,2011-12-01 14:13:14,statistics,"[r-bloggers] The mean of an absolute Student’s t
",mw1ze,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/mw1ze/rbloggers_the_mean_of_an_absolute_students_t/,1.0,0.0,,en
1108422,2011-12-01 18:53:53,statistics,Teaching an online class?,mwa6p,[deleted],,https://www.reddit.com/r/statistics/comments/mwa6p/teaching_an_online_class/,1.0,0.0,"Hi all,

Because I'm a little bit crazy (read: bored and under-stimulated), I've had a bit of an inclination to try and ""teach"" an online class related to statistics. I've heard of University of Reddit and Wikiversity, but am not familiar with other options and which one is the best, both in terms of attracting participants and organization.

Does anyone have any input on this? Also is anyone interested in such a class? Are there preferences? I was thinking of a technically more lower level course on biostatistics, but I suppose anything could work.",en
1108423,2011-12-01 19:07:17,statistics,Following Digital Breadcrumbs To 'Big Data' Gold,mwaq0,bitethemuffin,,https://www.reddit.com/r/statistics/comments/mwaq0/following_digital_breadcrumbs_to_big_data_gold/,4.0,0.0,,en
1108424,2011-12-01 19:14:56,statistics,"Was this random sampling, or convenience sampling?",mwb1m,[deleted],,https://www.reddit.com/r/statistics/comments/mwb1m/was_this_random_sampling_or_convenience_sampling/,0.0,1.0,"I conducted a small research project involving students on a university campus. 

Essentially, it was about helping behaviour, such that I would drop a set of items around campus and record the amount of time it took for an individual to help.

The trials were run at various locations with some standardization of distance and number of people present, and I would then drop the items and start the time.

My group believes that it was random sampling, but I feel that is was more-or-less convenience sampling as not everyone had an equal opportunity around campus, and such that we ran the trials when we felt like it was reasonable.

tl;dr: Was it convenience sampling?",en
1108425,2011-12-01 19:46:47,statistics,"A plea for help: incredibly vague prof demands we get ""creative"" with our statistics assignment.",mwccv,[deleted],,https://www.reddit.com/r/statistics/comments/mwccv/a_plea_for_help_incredibly_vague_prof_demands_we/,1.0,0.0,,en
1108426,2011-12-01 22:21:06,statistics,"Here's a question for you, I need some help.",mwj1u,[deleted],,https://www.reddit.com/r/statistics/comments/mwj1u/heres_a_question_for_you_i_need_some_help/,0.0,0.0,,en
1108427,2011-12-01 22:23:49,data,Finding truth in the wrong answers (aka the solution to too much data is more data),mwj65,winniechimp,1309463088.0,https://www.reddit.com/r/data/comments/mwj65/finding_truth_in_the_wrong_answers_aka_the/,1.0,0.0,,en
1108428,2011-12-02 01:00:36,statistics,Doing a research project and I need help with a (presumably) easy excel issue and I figured you guys would be the only people really dealing with this.,mwq9p,fantasygod777,1298403778.0,https://www.reddit.com/r/statistics/comments/mwq9p/doing_a_research_project_and_i_need_help_with_a/,0.0,7.0,"I'm attempting to copy the values from an excel document into a text document for SAS to use and I keep getting my paste values in a grid. The problem is that I just need a space between each value and no grid. Any idea on how to just paste the values in the cells without creating a spreadsheet in the text document.

Using a school computer. Using terminal to do SAS and textedit to create the document. ",en
1108429,2011-12-02 01:08:28,statistics,Which statistical test should I use (minitab) and why?,mwqmr,justwannaboogie,1309387007.0,https://www.reddit.com/r/statistics/comments/mwqmr/which_statistical_test_should_i_use_minitab_and/,0.0,1.0,,en
1108430,2011-12-02 02:23:18,statistics,Can I compare two sets of data (with some overlapping items) using a t-test?,mwtyz,sais,1201756650.0,https://www.reddit.com/r/statistics/comments/mwtyz/can_i_compare_two_sets_of_data_with_some/,3.0,8.0,"I don't know if I'm doing this right. Basically I've taken six measurements and each measure is rated by 5 people. This gives me 30 scores of which 10 are correct and 20 are incorrect. I want to see if the 10 measurements in the correct group are significantly different from the 20 measurements in the incorrect group. The problem, however, is that since really I only have six different values for the measurements, there are overlapping values in each group. What I ended up doing was running a t-test with H0 being correct=incorrect. I keep thinking something there is wrong...",en
1108431,2011-12-02 03:35:45,statistics,What are the implications of changing my convergence criterion in a principal components analysis?,mwx4e,Frl_Bennet,,https://www.reddit.com/r/statistics/comments/mwx4e/what_are_the_implications_of_changing_my/,10.0,3.0,"I'm doing an analysis of pilot survey results and this is my first time trying a PCA. The analysis with no rotation was pretty bad (10 factors extracted, none loading on subscales as predicted). I was also going to try the varimax and oblimin rotations, but I'm not getting convergence. I changed my convergence criterion to 0.01, but I don't know the theoretical implications of this...I don't understand well enough to know if this is a responsible thing to do.

Any help would be super appreciated. =)
",en
1108432,2011-12-02 04:51:31,statistics,"Product reviews: comparing R, Matlab, SAS, STATA, SPSS",mx0et,djent_illini,1312414426.0,https://www.reddit.com/r/statistics/comments/mx0et/product_reviews_comparing_r_matlab_sas_stata_spss/,12.0,8.0,,en
1108433,2011-12-02 05:13:08,artificial,Best Graphic Design Features: What Is Graphic Design: | best-graphicdesign.com,mx1bt,sanabaig,1312551860.0,https://www.reddit.com/r/artificial/comments/mx1bt/best_graphic_design_features_what_is_graphic/,1.0,2.0,,en
1108434,2011-12-02 20:35:31,statistics,Advice on learning Hadoop.,mxsd8,RA_Fisher,1299707119.0,https://www.reddit.com/r/statistics/comments/mxsd8/advice_on_learning_hadoop/,21.0,19.0,"It seems these days that the five major technological tools that statistics / machine learning job advertisements include are: 

* 1) R  
* 2) SAS
* 3) Python 
* 4) C++
* 5) [Hadoop](https://www.google.com/search?gcx=c&amp;sourceid=chrome&amp;ie=UTF-8&amp;q=hadoop#sclient=psy-ab&amp;hl=en&amp;source=hp&amp;q=hadoop&amp;pbx=1&amp;oq=hadoop&amp;aq=f&amp;aqi=&amp;aql=&amp;gs_sm=e&amp;gs_upl=0l0l0l3846l0l0l0l0l0l0l0l0ll0l0&amp;bav=on.2,or.r_gc.r_pw.r_cp.,cf.osb&amp;fp=7f4a23307f26fc94&amp;biw=1680&amp;bih=949)  

Further it seems that [Hadoop](https://www.google.com/search?gcx=c&amp;sourceid=chrome&amp;ie=UTF-8&amp;q=hadoop#sclient=psy-ab&amp;hl=en&amp;source=hp&amp;q=hadoop&amp;pbx=1&amp;oq=hadoop&amp;aq=f&amp;aqi=&amp;aql=&amp;gs_sm=e&amp;gs_upl=0l0l0l3846l0l0l0l0l0l0l0l0ll0l0&amp;bav=on.2,or.r_gc.r_pw.r_cp.,cf.osb&amp;fp=7f4a23307f26fc94&amp;biw=1680&amp;bih=949), a tool for distributed computing, seems to differentiate the statistics / ML jobs into those that are business as usual (same 'ole theory) and those offered at start-ups (new advanced ML techniques). 

 I know R and SAS well, and over the next year plan to improve my C++ and learn to use it within R, and learn Python and Hadoop.  

Right now I'm focusing on Hadoop.  I just bought the [O'Reilly reference for Hadoop](http://www.amazon.com/Hadoop-Definitive-Guide-Tom-White/dp/1449389732/ref=sr_1_1?ie=UTF8&amp;qid=1322850703&amp;sr=8-1), do y'all have any other good references or suggestions on learning Hadoop?  I've seen warnings that it's best to know Java first, which I know none of -- do you have thoughts on this?

I do have experience running ML routines in R (for [Kaggle](http://kaggle.com)) that take a while for my laptop i7 to complete.  I have a laptop with an Intel Core 2 Duo that I was thinking about ""Hadooping"" along with my i7 for these ML problems.  Thoughts?  Would this be a good experiment to learn Hadoop?

Thanks for any tips, suggestion, or advice!!",en
1108435,2011-12-02 23:23:32,statistics,Stata help needed: Generating a wage variable,mxzf3,[deleted],,https://www.reddit.com/r/statistics/comments/mxzf3/stata_help_needed_generating_a_wage_variable/,1.0,0.0,,en
1108436,2011-12-02 23:56:19,datasets,data on indivuals,my0ry,JSmith666,1294201704.0,https://www.reddit.com/r/datasets/comments/my0ry/data_on_indivuals/,1.0,2.0,"I am looking for data that will give me basic iinformation on a per person basis such as age, education, race, gender, religion etc. it is for econometric analysis",en
1108437,2011-12-03 02:26:27,AskStatistics,How would you best explain the Evolution of Man?,my6g2,[deleted],,https://www.reddit.com/r/AskStatistics/comments/my6g2/how_would_you_best_explain_the_evolution_of_man/,0.0,1.0,"I hope I'm asking this in the right spot. I'm new here and this actually my first post. (I signed up for the r/atheism and science). But some kid i know walked into the class babbling on about God and about how evolution is stupid and false (without any scientific education of course) and i was trying to tell him that he has no proof of God or anything but they never listen.. But he had this question ,"" If we evolved from Monkeys,and then how come we still have monkeys?"" I thought he was trolling at first, but i soon realized he wasn't joking. However, i answered him, but i don't feel like i answered him with a good enough response.. I became an atheist in 7th grade and started becoming interested in Science in the Summer of 8th grade going into 9th. (I'm a Freshman now) So i didn't have a lot of time to study enough on evolution. I did enough studying to be able to answer basic questions and how we evolved from man, but in my own words.

 But isn't there a real scientific theory on how we as humans got here? From some kind of monkey? Which monkey? What was our ancestors? What did they look like? What happened through the many years for that species of monkey to turn into what we are today?

 I know if that kid in my school said what he said in class on here, he, and his beliefs would soon be demolished. So basically, what would you tell him? How do YOU best explain the evolution of monkey to man. (unless there is already a scientific theory on how it happened i didn't hear about.)",en
1108438,2011-12-03 02:33:44,statistics,Quick comparison question,my6pu,[deleted],,https://www.reddit.com/r/statistics/comments/my6pu/quick_comparison_question/,1.0,1.0,"I'll preface this with saying that I have absolutely no understanding of statistics. I'm not necessarily looking for homework help(just an understanding so I can do it myself) so I hope this doesn't get deleted. What I want to know is if I am seeking to look for an association between an ordinal variable and categorical variable, what is the proper method?",en
1108439,2011-12-03 03:44:30,statistics,stats packages,my953,JSmith666,1294201704.0,https://www.reddit.com/r/statistics/comments/my953/stats_packages/,1.0,0.0,"I have a data set which i got along with script files. THe files are for spss, stata or sas. Is there a way to convert the script files to work in gretl?",en
1108440,2011-12-03 05:41:35,statistics,Need help with exploratory factor analysis,mycxv,myranthor,,https://www.reddit.com/r/statistics/comments/mycxv/need_help_with_exploratory_factor_analysis/,1.0,5.0,,en
1108441,2011-12-03 06:32:48,MachineLearning,Controlling for Variables in Machine Learning,myeko,songanddanceman,1288384615.0,https://www.reddit.com/r/MachineLearning/comments/myeko/controlling_for_variables_in_machine_learning/,6.0,3.0,"This may be a a really simple question, but it has me pretty stumped.

I want to fit a Classification Tree or maybe SVM on some data to predict income, but my worry is that some of my new variables that using (e.g.Height) may be redundant with other previous variables that have already been established (Gender), and so the new variables wouldn't be meaningful/interesting because the only reason its predicting income is due to a variable we already know is important and probably more effective.

What would be the best way to see if the new variables are giving extra predictive power above and beyond the old variables?

Relatedly, is there any machine learning technique fit predictors controlling for another variable? For example, if I want to predict Income, but I want to do it in such a way where a person's gender (a known predictor) is controlled for so that the results are not driven by gender differences.",en
1108442,2011-12-03 06:46:03,computervision,Interpretation of Eigenfaces/Fisherfaces,myezf,[deleted],,https://www.reddit.com/r/computervision/comments/myezf/interpretation_of_eigenfacesfisherfaces/,1.0,0.0,"I've been learning about using eigenfaces and fisherfaces for prediction, but I'm a little lost concerning the interpretation of some things.

Really I have two big questions about the procedures:

For the ""ghost faces"" that we get representing the individual components, is there any way to visually inspect what the ghost images represent? Are the places where the face is dark the areas that have a lot of variation/discriminate classes? Aside from visual inspection how do people usually determine what the ghost face/component represents?

Second. if I project the eigenvectors onto the data to get the scores for each component, how do the scores relate to the ""ghost face"" from that component? Does a higher score mean that the variable is more similar to the ""ghost face""?",en
1108443,2011-12-03 15:57:13,statistics,A statistics enthusiast here would like to know what folks out here think of my long term approach to move make  a career in statistics,mypd2,kkiran7,1138247073.0,https://www.reddit.com/r/statistics/comments/mypd2/a_statistics_enthusiast_here_would_like_to_know/,1.0,6.0,"I am a person with very strong background in mathematics and computational methods in my undergrad and can still claim to be strong, inspite of being out of touch with academics for last 11 years. I have actually wanted to do a PhD in Applied Math at [Courant Institute](http://cims.nyu.edu/) immediately after my undergrad, applied and got an admission, tuition fee wavier and TA support. But because of some personal reasons I could not pursue that line of interest. For last 12 years I have been working in IT industry with great degree of success.


Now I would like to get back to serious math that has always interested me. My objective is to be able to earn my living doing something that I love to. In 12 years of my IT career I happened to have worked in an organization that sells statistical software and I have picked up some decent amount of statistics (Regression techniques, Log Linear models, Logistic regression, Robust regression GLM, ANOVA, Correspondence Analysis, Design of Experiments etc). I have also implemented some of them. With this background, and considering that I am still primary breadwinner, I think it would be best if I could apply for a home study program and give examinations in statistics or related field I can slowly chalk out a route towards my objective. To this end, I have today applied for [this program](http://www.actuariesindia.org) which will steer me towards becoming an Actuary.

The syllabus of the program is [here](http://www.actuariesindia.org/student/examination-actuaries/syllabus.html). I will be able to complete the CT series in 1 year with the background I already have; perhaps another half year if I stretch it for some unforeseen reasons. I don't yet know how other courses are to be able to make a clear assessment. So, here are my questions:

* In how many years would reddit think I will be able to call this profession as my home?
* How are the employment prospects
* May be at some point of time, I might want to teach at an university. Am I doing the right thing? Is there a better way I can address my interests

tl;dr - How does a program in actuary suit a person who has active interests in statistics but currently working in an IT industry.

PS:

I did quite a lot of serious number crunching in my undergrad and I still have good hold on those techniques (SVD, Sparse Matrices, Eigen Values, robust interpolation using radial basis functions etc), if it helps!

Thanks for your time.",en
1108444,2011-12-03 17:57:15,statistics,EM for missing data,myrph,[deleted],,https://www.reddit.com/r/statistics/comments/myrph/em_for_missing_data/,1.0,0.0,"I originally posted this problem in r/homework help, but nobody replied for a few days, and I need to figure this out rather urgently.

Suppose we have a sample Yi~iid N(mu, sigma2), where i=1...m are observed, and i=m+1...n are missing. 

Use the EM algorithm to find the MLE of theta = (mu, sigma2).

I have a decent grasp of how to apply EM for estimating parameters for mixture distributions, but I am not quite sure what to do here. In particular, I don't know how to write the likelihood function for the complete data.

Any ideas where to start? ",en
1108445,2011-12-03 21:33:14,statistics,How to create an unfair coin and prove it with math,myy8k,PokerPirate,1272309547.0,https://www.reddit.com/r/statistics/comments/myy8k/how_to_create_an_unfair_coin_and_prove_it_with/,0.0,1.0,,en
1108446,2011-12-04 03:54:57,statistics,Ask r/statistics: Improve Pure Math Background?,mzawc,tshauck,1304289778.0,https://www.reddit.com/r/statistics/comments/mzawc/ask_rstatistics_improve_pure_math_background/,9.0,6.0,"Hi,


I'd like to know what I can do to improve my pure math fundamentals.  I've taken/ am in a few applied stats courses at the PhD level (Econometrics, Stats for Operations Management) and am doing fairly well, but I think there is a ceiling to how good I can get without a more fundamental understanding of math. 


I've taken Linear Algebra (I) and Vector Calc and have a pretty good understanding of the two.  I've also TA'd Calc and undergrad stats so I'm good at that, but what I'd like to know is what're the next pure math steps I should take that will help my with statistics.  I was thinking Analysis or maybe studying some advanced Linear Algebra?


Thanks",en
1108447,2011-12-04 03:58:03,statistics,Found a surprising well made website of data archives,mzb09,javes1,1288135284.0,https://www.reddit.com/r/statistics/comments/mzb09/found_a_surprising_well_made_website_of_data/,3.0,1.0,,en
1108448,2011-12-04 04:45:14,statistics,Question about SPSS and graphing something,mzch8,impactblu,1221198645.0,https://www.reddit.com/r/statistics/comments/mzch8/question_about_spss_and_graphing_something/,5.0,10.0,"I'm basically trying to learn the procedures in SPSS to create [this type of graph](http://i.imgur.com/S41kK.png). 


I'm actually not sure if it's possible in SPSS, if it isn't can someone let me know? I have the data set and everything, I'm just not sure how to make both the X and Y axis on a 0-100 scale, and having a third mediating variable calculated in two different percentages controlling for  the other two.  


I've tried looking into all the graphing options as well as the plot options available under linear regression. I'm not exactly well-versed in SPSS, so any help will be appreciated, thanks!
",en
1108449,2011-12-04 07:34:12,statistics,Career with BSc in stats?,mzhkk,HadACigar,1310623703.0,https://www.reddit.com/r/statistics/comments/mzhkk/career_with_bsc_in_stats/,7.0,27.0,"I'm currently in my first year of engineering, however, I fear I may fail out despite my best efforts. I have an interest in statistics, and loved the classes I took in it during highschool. In case engineering doesn't work out, I'm considering transferring into a math degree. Would I be able to get a career with just a bachelors degree, or would I have to get a masters/doctorate as well? Any advice, or information regarding yearly income? I wouldn't want to do any biostats stuff, something like data analysis for a company would be really neat. I was pretty good at programming in highschool, I understand most statistics jobs are R heavy?",en
1108450,2011-12-04 08:43:33,statistics,"Reddit, I need advice with SPSS, please.",mzj7g,[deleted],,https://www.reddit.com/r/statistics/comments/mzj7g/reddit_i_need_advice_with_spss_please/,2.0,3.0,,en
1108451,2011-12-04 11:01:47,artificial,Modern Paintings: Best And Creative Artwork For Art Lovers,mzm0f,sanabaig,1312551860.0,https://www.reddit.com/r/artificial/comments/mzm0f/modern_paintings_best_and_creative_artwork_for/,1.0,0.0,,en
1108452,2011-12-04 17:44:07,artificial,Practical Computer Vision Question - Facial Recognition ,mzsot,learc83,1175899960.0,https://www.reddit.com/r/artificial/comments/mzsot/practical_computer_vision_question_facial/,12.0,11.0,"I'm doing the stanford AI, and ML classes and it's got me thinking bout a problem that I work on during my day job (I own a small business focused on retail automation)

Every time a person uses a kiosk I take a picture of their face for security review.

I'd like to set up facial recognition to go over, say the last 5 photos of a user, and if 2 or more of them don't match the rest flag them for human review. (I'm checking to see if a user is sharing his logon with someone else--maybe use something like k-means to check for more than 1 cluster?)

From tutorials I've seen I feel pretty confident I could Principal Component Analysis with OpenCV (not sure if the accuracy will be high enough though), but I wanted to know if there are any good libraries or open source projects, or even commercial projects to suggest, that already do facial recognition.",en
1108453,2011-12-04 19:17:10,statistics,Which stats test would you use for this data?,mzvbf,[deleted],,https://www.reddit.com/r/statistics/comments/mzvbf/which_stats_test_would_you_use_for_this_data/,5.0,10.0,"Suppose it's a three factor design (a 2 x 2 x 3) with only one data point for each condition.  Would you just use a three-factor ANOVA?

The DV is how many times something happened in a ten minute period.

Edit: Thanks everyone, problem solved!",en
1108454,2011-12-04 19:38:20,AskStatistics,"If correlation is not necessarily causation, how do you prove causation?",mzw1r,JimJamieJames,1306594694.0,https://www.reddit.com/r/AskStatistics/comments/mzw1r/if_correlation_is_not_necessarily_causation_how/,0.0,6.0,That is all.,en
1108455,2011-12-05 01:44:47,rstats,Are there any good online courses for learning R?,n0b5e,bloofa,1239408076.0,https://www.reddit.com/r/rstats/comments/n0b5e/are_there_any_good_online_courses_for_learning_r/,19.0,9.0,"I've been using SPSS for many years but would like to become proficient in R relatively quickly. I'd be using it for basic crosstabular reporting and analysis but also factor and cluster analysis of both small and large datasets. I can probably get my employer to pay for a course, but I am having trouble finding one that is not wrapped up in a ""Introduction to Statistics"" course. I know statistics; I just don't know R.",en
1108456,2011-12-05 02:47:03,statistics,EM algorithm for censored data,n0drr,blind_swordsman,1318716377.0,https://www.reddit.com/r/statistics/comments/n0drr/em_algorithm_for_censored_data/,6.0,9.0,"Does anybody know any good references explaining how to apply the EM algorithm for problems with censored data? For example, if you want to compute the MLE given an iid sample of size n, of which m observations are censored. I think I have a decent grasp of how to apply EM for finding parameters of mixture distributions, but in this case, I am not sure where to start here. How would one express the expected log-likelihood of the complete data?
",en
1108457,2011-12-05 02:55:45,statistics,Value of test statistic?,n0e5q,[deleted],,https://www.reddit.com/r/statistics/comments/n0e5q/value_of_test_statistic/,1.0,0.0,"There is substantial interest in the health benefits of the consumption of high amounts of fiber in diets. A market research team is interested in the public acceptance of a new high-fiber cereal (more than 8 gm of fiber per serving) that is to be marketed. To that end, the researchers selected a random sample of subjects from one region of the country. The selected subjects were provided with two bowls of cereal. One bowl contained the new cereal and the other bowl a well-known and popular cereal. The bowls were presented in random order and subjects asked which cereal they preferred. The study was repeated independently in a second region. In region 1, of the 400 subjects 220 preferred the new cereal; in region 2, 195 of the 300 subjects indicated a preference for the new cereal. 
Reference: Ref 8-20

What is the value of the test statistic?
Answer   A. z = –3.84 
  B. z = –2.70 
  C. z = –2.67 
  D. t = –2.67 
  E. t = –2.70 
",en
1108458,2011-12-05 06:33:39,statistics,Does this test make sense?,n0okc,chirpychirp,1277776610.0,https://www.reddit.com/r/statistics/comments/n0okc/does_this_test_make_sense/,3.0,1.0,"Let's say I have two continuous variables A and B, that I collected in two different types of locations, cc and dd. 

My primary question of interest is whether the linear relationship between A and B is significantly different between locations cc and dd. This suggests to me that the appropriate test is a GLM with an interaction term, and I am mainly concerned with whether the interaction term is significant. However, even if the interaction term is statistically significant, it doesn't tell me whether the relationships within cc and dd are significant or meaningful in any way.

So I guess my question is.....in a study design like this, what steps seem most appropriate? Would it be valid to have a 2-step approach, testing first for the interaction term (to determine whether the relationships are different), and then separating the data and testing for a linear relationships within the two groups? How does this change the error rate? I suspect that the relationships are different, but that in site cc the trends are strong, but in dd they are non-existent. 


Does anyone have advice for a more appropriate test? Thanks.",en
1108459,2011-12-05 06:57:24,statistics,Appropriateness of Constrained Segmented Univariate Polynomial Regression Model,n0pmg,midnite13,1278030223.0,https://www.reddit.com/r/statistics/comments/n0pmg/appropriateness_of_constrained_segmented/,6.0,5.0,"Hi r/stats, 

I've learned that in unconstrained polynomial regression, the optimal order can be determined using two F tests : one to test for the significance of the overall regression, the other to test for the significance of the higher coefficients (assuming the first test passed of course).

However in my application, I'm interested in testing for the appropriateness of a segmented polynomial fit subjected to be first order continuous, with boundary conditions placed at the end of the entire domain as well. The polynomials do **not** have to have the same order between segments.  I should also mention that the join points are known, so I don't have to estimate them.

The closest paper I could find that broaches this topic is Gallant and Fuller's work [1]. Here, they also have a segmented polynomial fit with C^1 continuity, but with no constraints. Frustratingly, they make up a test statistic for the appropriateness of their fit ""by analogy to linear models theory"", yet they provide no references.

I've tried to search for other papers on this topic but to no avail. This leads me to question - is this test statistic trivial to derive for the constrained case, and if so, could you please point me to resources that could help me understand how to do it?

Thanks in advance for your help!


**References**

[[1]](http://www.jstor.org/pss/2284158) Gallant, A.R. and Fuller, W.A. (1973). Fitting segmented polynomial regression models whose join points have to be estimated. *J. Amer. Statist. Assoc., 68, 144-147*


p.s. If it helps, I'm an engineer who regrettably didn't pay too much attention in his stats course, so any material no matter how trivial it may seem would be appreciated!",en
1108460,2011-12-05 08:58:31,MachineLearning,Do you think LDA can be used with PCA to get a better topic model?,n0ui0,visarga,1166994643.0,https://www.reddit.com/r/MachineLearning/comments/n0ui0/do_you_think_lda_can_be_used_with_pca_to_get_a/,5.0,4.0,"I am passing a corpus of news through LDA (using Vowpal Wabbit) and the results are nice, but some real world topics like football for example are spread over a number of LDA topics.

So I was thinking, if I run PCA on top of that, I could perhaps separate the topics even better - it could be a good unsupervised topic discovery method.

What do you think?",en
1108461,2011-12-05 13:31:41,statistics,Natural Search Engine Position: Top 10 Searches and Top News Stories,n10f6,pluspecial,1241577277.0,https://www.reddit.com/r/statistics/comments/n10f6/natural_search_engine_position_top_10_searches/,1.0,0.0,,en
1108462,2011-12-05 16:50:58,MachineLearning,Explain Conditional Random Fields like I'm stupid?,n14ov,crfplease,1323096504.0,https://www.reddit.com/r/MachineLearning/comments/n14ov/explain_conditional_random_fields_like_im_stupid/,38.0,11.0,"I have been reading about CRFs and nothing is seeming to click.  Can somebody give me a general idea of what CRFs are doing, persay?  I understand HMMs, and people often relate them in the literature but I don't understand the relationship.  

Why are CRFs undirected?  When would you use CRFs over HMMs?  What extra power do they provide?  Thanks!",en
1108463,2011-12-05 17:06:28,statistics,"Is anyone else doing a College Bowl Pick 'Em this year?  Building a model, looking for collabs.",n1582,RA_Fisher,1299707119.0,https://www.reddit.com/r/statistics/comments/n1582/is_anyone_else_doing_a_college_bowl_pick_em_this/,9.0,26.0,"Each year my buddies run a [College Football Bowl](http://www.cbssports.com/collegefootball/bowls/predictions) Game Pick 'em.

Last year I built a simple OLS model based on data from a few prior years.  I came in second place out of 15 people.

This year I plan to get more sophisticated and try to work with decision tree analysis and machine learning methods.

I was wondering if anyone else is planning to do this and would be interested in sharing ideas over the next week and a half?

Edit: **So these are my picks for 2011**
See column B.
https://docs.google.com/spreadsheet/ccc?key=0Ao0su6Krt1MedDNmRmZaeVpER29zbk5wU0c4V2wwOGc",en
1108464,2011-12-05 21:46:30,computervision,Best books/papers/articles for Image Segmentation Algorithms?,n1gf0,rsaborio,1154538563.0,https://www.reddit.com/r/computervision/comments/n1gf0/best_bookspapersarticles_for_image_segmentation/,3.0,1.0,"I'm a newbie on Computer Vision, right now I'm doing my proposal for a master degree on Computer Science and I'm planning to build a new algorithm for Image Segmentation (IS) as part of my thesis. I would like to know if there are good books focused specifically on IS. I've downloaded a lot of papers but they aren't very informative about some existent algorithms. Thanks in advance and sorry for my english.",en
1108465,2011-12-05 22:29:28,statistics,Comparing groups during a repeated measures,n1iay,HPDerpcraft,1307658689.0,https://www.reddit.com/r/statistics/comments/n1iay/comparing_groups_during_a_repeated_measures/,5.0,13.0,"Hi,

I'm analyzing a measure (distance moved), at three time points (days). 
I have three between-subject variables (genotype, diet, and sex).

I have a main effect of day, as well as an interaction with genotype*diet*day (this is what I was expecting to find, that diet affects performance in this measure, but only in certain groups).

Now, my options are to look at total distance moved within each group, but is there a way to compare (on each day) the performance between these groups? Or do I just have to decide a-prior and do multiple comparisons (and the accompanying correction)?

EDIT: I'm using spss 16.0 btw.",en
1108466,2011-12-05 22:45:17,computervision,"My PhD on cell tracking, and point cloud tracking",n1j15,oulipo,1263496989.0,https://www.reddit.com/r/computervision/comments/n1j15/my_phd_on_cell_tracking_and_point_cloud_tracking/,15.0,5.0,,en
1108467,2011-12-05 23:10:08,artificial,Take a choose-your-own-adventure study for science!  NCSU AI research,n1k4f,mwfendt,1323119178.0,https://www.reddit.com/r/artificial/comments/n1k4f/take_a_chooseyourownadventure_study_for_science/,0.0,0.0,,en
1108468,2011-12-05 23:18:33,data,"Looking for a Job? Learn Ruby, Python and be a Team Player (infographic)",n1kip,winniechimp,1309463088.0,https://www.reddit.com/r/data/comments/n1kip/looking_for_a_job_learn_ruby_python_and_be_a_team/,1.0,0.0,,en
1108469,2011-12-06 01:21:44,statistics,Biostatistics Ryan Gosling,n1q18,thechink,1309555006.0,https://www.reddit.com/r/statistics/comments/n1q18/biostatistics_ryan_gosling/,42.0,3.0,,en
1108470,2011-12-06 01:43:51,AskStatistics,"How do you present a ""weighted"" percentage?",n1qz7,NewRino,1299112695.0,https://www.reddit.com/r/AskStatistics/comments/n1qz7/how_do_you_present_a_weighted_percentage/,2.0,6.0,"How do you represent a percentage that is ""weighted"". So I have data from several counties some returned 1of 2 others 5 of 10. I want to show that the counties that had 5 of 10 to be ""weighted"" better. What is the correct method to show this?.. Thank you Reddit!",en
1108471,2011-12-06 07:18:46,statistics,"I'm looking for a TED talk I saw a while back about statistics and election fraud that I can't find now, can you all help?",n25gg,[deleted],,https://www.reddit.com/r/statistics/comments/n25gg/im_looking_for_a_ted_talk_i_saw_a_while_back/,1.0,1.0,"The TED talk (guy) talked about how he could look at the data from voter districts and that the final two digits of voter blocks should be equally distributed. They found that in one country where voter fraud was likely that the final digits were numbers that are next to each other, like 45 or 23 and less digits that were far from each other like 82 or 17. Does anyone have any idea which TED talk this is?

http://scienceblogs.com/cognitivedaily/2009/06/nice_analysis_of_why_the_irani.php this talks about the phenomenon",en
1108472,2011-12-06 07:23:10,MachineLearning,"International Open Data Hackathon
",n25mj,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/n25mj/international_open_data_hackathon/,3.0,0.0,,en
1108473,2011-12-06 08:23:20,rstats,"How do I call only the last X instances of a vector? (E.g. last 27 variables in a vector called ""degrees"")",n27xd,randombozo,1266016617.0,https://www.reddit.com/r/rstats/comments/n27xd/how_do_i_call_only_the_last_x_instances_of_a/,6.0,7.0,,en
1108474,2011-12-06 11:58:30,statistics,A Research Neuroscientist needs your help with something very simple,n2d3m,deepobedience,1298331408.0,https://www.reddit.com/r/statistics/comments/n2d3m/a_research_neuroscientist_needs_your_help_with/,6.0,10.0,"I'm going to not use the neuroscience example, because hopefully that will make it simpler.

Imagine I have a bunch of people in a room. I am going to ask them to clap their hands. Each persons clap is identically loud, and it has a loudness of Q. The number of people in the room is N. The probability of each person clapping when I ask them, is P.

We can say then that the mean loudness of the rooms clap is L and L=N.P.Q
The variance of each persons clap is P.(1-P).Q^2.
And the variance of the whole rooms loudness is N.P.(1-P).Q^2.
This is where I am at.

Now, what happens if instead of there being a fixed number of people in the room, but it is varying somewhat. So now we have N and var(N)
I'm sure the mean loudness of the room is still L=N.P.Q

But what happens to the variance of the loudness of the room?
",en
1108475,2011-12-06 13:32:19,statistics,Could really use some help with logistic regression!,n2eq0,[deleted],,https://www.reddit.com/r/statistics/comments/n2eq0/could_really_use_some_help_with_logistic/,1.0,2.0,,en
1108476,2011-12-06 15:25:33,statistics,Am I doing this correctly?,n2gxd,[deleted],,https://www.reddit.com/r/statistics/comments/n2gxd/am_i_doing_this_correctly/,1.0,0.0,"Are these null hypotheses correct?

The hypotheses are:

1)	Problems in parent-child relationships are more likely to be present in two-parent families in 1980 if parents divorced between 1980 and 1992 than if parents remained married
2)	Associations between problems in the parent-child relationship in 1980 and later parental divorce are reduced substantially and rendered nonsignificant when controls are introduced for the parents’ marital happiness in 1980
3)	Parents’ marital happiness in 1980 is negatively associated with problems in parent-child relationships in 1980
4)	Parents’ marital happiness in 1980 is negatively associated with divorce between 1980 and 1988
5)	Parents’ marital happiness in 1980 is positively associated with parental affection for children in 1988
6)	Problems in parent-child relationships in 1980 are negatively associated with parental affection for children in 1988
7)	Divorce is negatively associated with parental affection for children in 1988

Null hypotheses??:

1)	Problems in parent-child relationships are not more likely to be present in two-parent families in 1980 if parents divorced between 1980 and 1992 than if parents remained married
2)	
3)	Parents’ marital happiness in 1980 is positively associated with problems in parent-child relationships in 1980
4)	Parents’ marital happiness in 1980 is positively associated with divorce between 1980 and 1988
5)	Parents’ marital happiness in 1980 is negatively associated with parental affection for children in 1988
6)	Problems in parent-child relationships in 1980 are positively associated with parental affection for children in 1988
7)	Divorce is positively associated with parental affection for children in 1988

I wasn't sure about the second one, but am I writing the others correctly?  I've been looking at this for so long that I keep confusing myself.
Thanks in advance!
",en
1108477,2011-12-06 16:17:07,artificial,Creating Artificial Intelligence Based on the Real Thing - NYTimes.com,n2i6z,rhiever,1304006023.0,https://www.reddit.com/r/artificial/comments/n2i6z/creating_artificial_intelligence_based_on_the/,15.0,7.0,,en
1108478,2011-12-06 16:41:43,MachineLearning,Can R be used with SQL Server in a production environment?,n2iwx,optiontrader1138,1173821436.0,https://www.reddit.com/r/MachineLearning/comments/n2iwx/can_r_be_used_with_sql_server_in_a_production/,0.0,2.0,I'm looking into replacing some of our old data mining algorithms implemented in SQL Server with R. I'm wondering if it's fast and stable enough to run in a high-volume production environment (I'm using Revolution R on win64).,en
1108479,2011-12-06 17:44:29,MachineLearning,Dataspora is hiring a Chief Analytics Officer! Based in either Boston or San Francisco. Apply state-of-the-art machine learning techniques to interesting problems.,n2kz5,ohsnaaap,1270647929.0,https://www.reddit.com/r/MachineLearning/comments/n2kz5/dataspora_is_hiring_a_chief_analytics_officer/,15.0,0.0,"**Job Description**

We are looking for a smart, innovative and detail-oriented Chief Analytics Officer to join our predictive analytics team. You should thrive on working in a fast-paced and exciting team environment in one of the fastest growing areas today – predictive analytics.  You will be working with leading domestic and global companies and Via Science’s predictive analytics team, bringing to bear your scientific and communications acumen, along with both proprietary and open-source analytics tools, to address critical business questions for our clients’ C-suites.  Work may include incorporation of analytics models built by the Dataspora team into client workflows and operational systems. 

**Skills**

* Demonstrate thought leadership in big data analytics through compelling blogs, white papers, public presentations, and existing/prospective client meetings.
* Able to work in a fast-paced, multi-functional team environment and have a proactive mindset.
* Comfortable handling and manipulating large datasets.
* Excellent communication skills in order to relay technical findings to clients who may have no technical or mathematical background.
* Ability to produce high-quality, impactful visualizations of scientific results highly desirable.

**Requirements**

B.Sc with relevant work experience (internships or cooperative placements) is a minimum; M.Sc or Ph.D with relevant post-graduate research strongly preferred, in each case in a quantitative scientific discipline with a heavy statistical or other mathematical component. Statistical programming experience in R preferred. Ability to extract insights from large datasets using an array of methods from clustering to regression to network modeling. Experience implementing analytics solutions with Hadoop/Hive/Pig. Strong SQL knowledge to extract data from and to load data to databases.  Excellent written and oral presentation skills required. Experience in Bayesian inference, noSQL, machine-learning methods, and network modeling (social networks or general network analysis) is a bonus

Please PM me for more details.
",en
1108480,2011-12-06 19:47:06,statistics,40 years of boxplots,n2pt6,Professor_IR,1310512226.0,https://www.reddit.com/r/statistics/comments/n2pt6/40_years_of_boxplots/,14.0,4.0,,en
1108481,2011-12-06 20:02:35,statistics,When is it necessary to test the significance of a correlation coefficient?,n2qgo,any_name,1323194093.0,https://www.reddit.com/r/statistics/comments/n2qgo/when_is_it_necessary_to_test_the_significance_of/,12.0,7.0,"Say, for example, I have a correlation coefficient of 0.97 with a sample size of 55,000. Is there a certain cut-off value for correlation coefficient or sample size where the significance test become extraneous? Thanks in advance for your help!",en
1108482,2011-12-06 22:21:40,statistics,Sampling window for statistics of discrete events,n2wf2,NotSoMagicalTrevor,1320765177.0,https://www.reddit.com/r/statistics/comments/n2wf2/sampling_window_for_statistics_of_discrete_events/,3.0,0.0,"Given two treatments (A, B) that might result in a different number of discrete events (count) over time, what's the best way to measure and determine statistical significance between them? One way (current) is to bucket the data into the number of events per day and run the data over a month to get 30 samples. Alternatively, I could count the samples in each week period and then have 4 samples with a (likely) smaller variance. OTOH, the extremes (sampling at 1ms intervals, or just comparing the counts for the entire month) are obviously not right... but what is the right way?

Specifically, I'm looking at the number of clicks/day of a web-site, and I'd like to know if a particular change has any statistically significant impact. ",en
1108483,2011-12-06 22:37:11,statistics,Question about measurement uncertainty for you stats guys/gals....,n2x4p,darylb,1273089488.0,https://www.reddit.com/r/statistics/comments/n2x4p/question_about_measurement_uncertainty_for_you/,3.0,6.0,"Okay, here's the question. If i'm measuring variable A with 2 devices, B &amp; C and both devices have a known uncertainty, is there any way to use statistics to account for the benefit of having 2 measurements to determine a reduced uncertainty level?",en
1108484,2011-12-07 00:33:04,statistics,I need help with multiple linear regression,n32j1,[deleted],,https://www.reddit.com/r/statistics/comments/n32j1/i_need_help_with_multiple_linear_regression/,1.0,1.0,,en
1108485,2011-12-07 03:13:57,rstats,Mixed model anova : am i doing right?,n39hf,[deleted],,https://www.reddit.com/r/rstats/comments/n39hf/mixed_model_anova_am_i_doing_right/,2.0,4.0,"Hello!!

Newbie in R here. Well i have the experimental design as follow :
- two independent group of rats (variable G);
- every rat in these groups has two experimental condition : a dosage (variable D);
- we then measure their performance in a task (variable P).

The way i compute my anova for this between (groups) and within (dosage) design is as follow :

aov(P~(G*D) + Error(Subject/D))

An expert here could confirm if i'm making a mistake or if i'm right here?

Thank you!",en
1108486,2011-12-07 03:29:54,statistics,A simple question about how to analyze some data,n3a6r,[deleted],,https://www.reddit.com/r/statistics/comments/n3a6r/a_simple_question_about_how_to_analyze_some_data/,2.0,4.0,"Hey Reddit, I'm a bit stuck right now and could use some suggestions. In my ornithology class we had the option of doing a mini-study or writing an essay so I chose to do the study, but I'm realizing that I have little knowledge of statistics beyond the chi square test. I was hoping someone could suggest a better form of analysis for my data. Here are the variables: 
- How often a gull strikes a lid in a vertical orientation
- How often a gull strikes a lid in a horizontal orientation 

There are 2 test lids and one control, I'm figuring I could do a simple chi square using a 50% probability of a gull striking a lid in any orientation, but I was hoping there might be a more meaningful way to interpret the data. If anyone has any suggestions they'd be greatly appreciated. Thanks! ",en
1108487,2011-12-07 04:43:38,MachineLearning,Any ideas on how to use machine learning to analyze fMRI data to determine if someone has ADHD?,n3df8,p01ym47h,1312832397.0,https://www.reddit.com/r/MachineLearning/comments/n3df8/any_ideas_on_how_to_use_machine_learning_to/,6.0,9.0,"For my CS senior project next semester I'm going to be analyzing 4-dimensional (3 in space, 1 in time) fMRI data using machine learning. Ultimately, the goal is to be able to tell whether or not someone has ADHA given their fMRI data.  

I just scored this gig a week ago and I'm very new to ML (I'm just finishing Stanford's free online course).  I thought I'd ask if anyone out there has any good ideas of how to go about it or where I should think about starting. My professor told me to ""throw an SVM at it.""",en
1108488,2011-12-07 08:31:40,statistics,Choosing the right school for statistics:.do the US News rankings really matter in our field? ,n3mq1,[deleted],,https://www.reddit.com/r/statistics/comments/n3mq1/choosing_the_right_school_for_statisticsdo_the_us/,2.0,12.0,"So, I've been ""unofficially"" offered a spot as a doctoral candidate/master's student at school I like, with faculty I like and students I can definitely get along with. The program gives a good mix of theory and application and gives students write a few research opportunities (and starts them fairly quickly). The classes are fairly small, and they're taught by faculty with experience in the subject being lectured.

Everything sounds great about the opportunity, except the fact that the school isn't highly rated. It is a slightly newer program. 

How much will this hurt me in getting a position in academia or consulting/industry? I mean, I would love to take advantage of this, but I don't want to hurt myself in the long run. ",en
1108489,2011-12-07 08:41:08,statistics,The cranky guide to trying R packages,n3n15,RA_Fisher,1299707119.0,https://www.reddit.com/r/statistics/comments/n3n15/the_cranky_guide_to_trying_r_packages/,15.0,0.0,,en
1108490,2011-12-07 08:51:13,statistics,Statistical adjustment of map and geographic information.,n3nck,RA_Fisher,1299707119.0,https://www.reddit.com/r/statistics/comments/n3nck/statistical_adjustment_of_map_and_geographic/,11.0,0.0,,en
1108491,2011-12-07 15:38:12,statistics,Determining confidence of pricing model,n3vxa,sapro,1263476527.0,https://www.reddit.com/r/statistics/comments/n3vxa/determining_confidence_of_pricing_model/,2.0,5.0,"We recently performed a pricing test for our products and I'm in the process of building an ""ideal price"" calculation from this data. The data is flawed in two ways: 1) there are currently only two price points in the data, which are very dissimilar (eg 80% off in some instances) and 2) there are relatively small numbers of purchases within each price point.

While I still feel the tool can be useful, I want to caution my team against relying too heavily on the estimate generated, so I'd like to attach some sort of p-value to this tool. Basically, some way to represent the smaller the purchase data the less reliable the 'ideal price' estimate, and that even the best estimate isn't 100% reliable.

I initially used a simple formula that I use in A/B testing ... but after thinking about it, this is only telling me if the lower price has a statistically significant improvement in # of sales, which is a given. Any ideas?

I'm working with excel right now but I do have a copy of minitab laying around I could dust off if this would help...",en
1108492,2011-12-07 16:21:26,MachineLearning,Help. Would a markov chain suffice,n3x2a,ReFrainOcO,1316422459.0,https://www.reddit.com/r/MachineLearning/comments/n3x2a/help_would_a_markov_chain_suffice/,9.0,7.0,"I'm trying to quantify the likelihood of observing certain trajectories in a discrete (n by n by n) 3D space, i.e. moving from one bin to any adjacent bin. Someone proposed looking into HMMs, however, I'm not interested in the unobserved state transitions.

Would a markov chain be a sufficient formulation of the problem? 
Any good tutorials on how to condition/learn a markov chain given historical data?

(excuse the ignorance)",en
1108493,2011-12-08 03:34:21,statistics,"Need help generating ID #s in a data set by month and facility, starting over each month.",n45af,anothernameagain,1254666862.0,https://www.reddit.com/r/statistics/comments/n45af/need_help_generating_id_s_in_a_data_set_by_month/,1.0,0.0,"Hey, stattit, I hope this is the right place for this...

I have a data set that's something like this (only with &gt;10,000 records and like 50 variables):
    
    FACILITY  DATE
    1         10/2
    1         10/10
    1         10/17
    1         10/30
    2         10/1
    2         10/3
    3         9/1
    3         9/2
    3         9/4
    3         10/1
    3         10/5
    3         10/6

What I need to do is only keep the first 40 variables from each month for each facility. Facilities have different numbers of records each month and the dates may span only a few days or the whole month. I was figuring I could somehow generate an integer for each facility for each month like this:

    FACILITY  DATE     ID
    1         10/2     1
    1         10/10    2
    1         10/17    3
    1         10/30    4
    2         10/1     1
    2         10/3     2  
    3         9/1      1
    3         9/2      2
    3         9/4      3
    3         10/1     1
    3         10/5     2
    3         10/6     3

Then, if I'm keeping the first 2 in each month in this example, I could 'drop if ID&gt;2'. 

But I cannot get Stata's 'seq' command to cooperate while sorting on two variables.

Any suggestions? I have access to Excel and Stata primarily and also SAS, but would rather avoid SAS if I can.",en
1108494,2011-12-08 19:56:08,artificial,Abundance | Why the Future Will Be Much Better Than You Think,n52ln,marshallp,1239903633.0,https://www.reddit.com/r/artificial/comments/n52ln/abundance_why_the_future_will_be_much_better_than/,6.0,7.0,,en
1108495,2011-12-08 21:14:08,datasets,"Ask r/datasets: Looking for datasets about piracy 
(music, movies, etc)",n569v,grotos,1249400234.0,https://www.reddit.com/r/datasets/comments/n569v/ask_rdatasets_looking_for_datasets_about_piracy/,4.0,2.0,"I'm looking for any data about music, movie and software piracy. I know there exists datasets about pirated Oscar movies (http://waxy.org/2010/02/pirating_the_2010_oscars/) but I need more information (not only dates).

Basicly I am going to analyze the impact of piracy on the markets. I don't know yet how to estimate number of people downloading illegal stuff, but for a start any data would be useful.",en
1108496,2011-12-08 21:36:29,artificial,"AI Students create a ""bot"" that plays Minecraft",n57b5,BerickCook,1296862994.0,https://www.reddit.com/r/artificial/comments/n57b5/ai_students_create_a_bot_that_plays_minecraft/,13.0,4.0,,en
1108497,2011-12-08 22:50:28,statistics,On the road to compromise: calling R from SAS's IML Studio,n5anw,[deleted],,https://www.reddit.com/r/statistics/comments/n5anw/on_the_road_to_compromise_calling_r_from_sass_iml/,9.0,8.0,,en
1108498,2011-12-09 01:47:56,statistics,Distance or comparative measure for time series data sets.,n5isz,[deleted],,https://www.reddit.com/r/statistics/comments/n5isz/distance_or_comparative_measure_for_time_series/,3.0,1.0,"I have a few different forecast scenarios for the number of occurrences of an event in each 15 minute interval within a given time period.

As well as the forecasts, I have the *actual* data for that time period, which is again recorded as the number of occurrences in each 15 minute bucket.

My simple task is to determine which of the forecasts is ""the most accurate"". My immediate impulse is to just use the average of the error in each bucket, but I can't convince myself that this is a sane way to measure what I am looking for.

I would appreciate any advice: feel free to tell me what I need to read or study, rather than expressly feed me a solution.",en
1108499,2011-12-09 02:12:18,artificial,"Jeff Hawkins: Advances in Modeling Neocortex and Its Impact on Machine Intelligence (1:13:01, but worth it) (crosspost r/neuro)",n5jt5,imitationcheese,1170866696.0,https://www.reddit.com/r/artificial/comments/n5jt5/jeff_hawkins_advances_in_modeling_neocortex_and/,9.0,3.0,,en
1108500,2011-12-09 03:55:48,statistics,"Just got my MS in Stats.  Yay!  Want a relevant job and have no industry experience.  Boo!  I can't afford to do an unpaid internship... /r/statistics, any advice for getting that coveted 2-5 years industry experience?  Or am I pretty much fucked unless I apply to PhD programs?",n5o59,Crotchfirefly,1289629047.0,https://www.reddit.com/r/statistics/comments/n5o59/just_got_my_ms_in_stats_yay_want_a_relevant_job/,11.0,23.0,"I'm not starving, but being serially underemployed is pretty disheartening.  I could really use some advice :(",en
1108501,2011-12-09 05:21:39,statistics,Statistics Problem; conditional inclusion,n5rqb,sheppa28,1274041127.0,https://www.reddit.com/r/statistics/comments/n5rqb/statistics_problem_conditional_inclusion/,5.0,1.0,"My coworker and I are trying to solve this problem which is best explained by example. You have n discrete distributions X_n that you sample from {x1,x2,..,xn}, define Y to be a weighted sum of {x1,...,xn}. For the values {x1,...,xn,Y} you take the overall average of only those values that are below a threshold, the rest are ignored. We are trying to solve for the resulting distribution.

Example: If the samples were 10,20,5,13 and the weighted sum was 3, with a threshold was 12, you would take the average of only the {10,5,3} values; 6. If this was a simulation, repeat 1e6 times. 

Due to the coefficients of the weighted sum it could be that all the xi values are above the threshold, but Y is below the threshold; the result would then be Y. If all values, including Y, are above the threshold the entire sample is ignored.

Is there a way to find the resulting distribution? 

We figured it out without the Y term, but that extra term adds dependence into the average.

Any suggestions? Thanks!



",en
1108502,2011-12-09 11:18:47,MachineLearning,A Word Cloud with Spatial (also special) Meaning,n62uw,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/n62uw/a_word_cloud_with_spatial_also_special_meaning/,0.0,0.0,,en
1108503,2011-12-09 15:36:15,statistics,What is statistically significant in Benford Analysis?,n67gu,rdiss,1253275053.0,https://www.reddit.com/r/statistics/comments/n67gu/what_is_statistically_significant_in_benford/,10.0,7.0,"As an auditor, I use [Benford's Law](http://en.wikipedia.org/wiki/Benford%27s_law) quite a bit. We use Excel to analyze long lists of check amounts, but I've always struggled to find an easy way to tell what deviations are statistically significant. All the literature says to use the Z-statistic, but for the life of me I can't figure out an easy way to implement this in Excel.

Can someone show me the way? Our ""database"" is pretty simple: just a long listing (usually ~1,000) of check amounts in Excel. About 30.1% should start with a 1 and so forth, but at what point does the deviation from this become statistically significant?
",en
1108504,2011-12-09 17:55:22,artificial,Where Robot Cars (Robocars) Can Really Take Us,n6boa,marshallp,1239903633.0,https://www.reddit.com/r/artificial/comments/n6boa/where_robot_cars_robocars_can_really_take_us/,16.0,0.0,,en
1108505,2011-12-09 20:06:14,datasets,County level census data 1980-1996,n6gyv,wordsarentenough,1316648837.0,https://www.reddit.com/r/datasets/comments/n6gyv/county_level_census_data_19801996/,4.0,1.0,"Hi everyone,
I'm looking for some county level census data (i.e. race, poverty, income characteristics, etc.) from 1980-1996, and on the censtats website I can only find data from roughly 2000 onward, and nothing in so nice a form as a table.  Does anyone know where or how I could acquire something along these lines?  Thanks a lot!",en
1108506,2011-12-10 00:21:51,statistics,Why a Chi-square test is (almost) always better than a fisher's exact test.,n6roj,GrynetMolvin,1280148125.0,https://www.reddit.com/r/statistics/comments/n6roj/why_a_chisquare_test_is_almost_always_better_than/,13.0,2.0,,en
1108507,2011-12-10 00:28:21,MachineLearning,The stability of classification trees vs logistic regression,n6rxo,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/n6rxo/the_stability_of_classification_trees_vs_logistic/,3.0,0.0,,en
1108508,2011-12-10 04:27:00,datasets,College Football data.,n704t,RA_Fisher,1299707119.0,https://www.reddit.com/r/datasets/comments/n704t/college_football_data/,5.0,3.0,"Does anyone know of easily accessible historic college football data?  I'm looking for things like the number of turnovers by season, or points for by season, etc.  I'm specifically planning to use it to build a model to predict college bowl game outcomes.",en
1108509,2011-12-10 05:02:00,statistics,What statistical test can I use to analyze this data?,n71bk,[deleted],,https://www.reddit.com/r/statistics/comments/n71bk/what_statistical_test_can_i_use_to_analyze_this/,1.0,5.0,"I recorded the number of times each letter appeared in a text that was rewritten at various points in the history of the English language.


Here is the data that I currently have: http://i.imgur.com/STQnJ.jpg


I would like to know if there are any trends such as: is the letter ""n"" becoming less common in the English language? That is, is it statistically significant that ""n"" makes up a smaller percentage of the language throughout time?


What tests can I do to check this? Would I have more options if I obtained more samples from these 4 texts?

As a bonus, are there other questions that I answer about this data through the use of statistical tests?
",en
1108510,2011-12-10 05:04:59,MachineLearning,looking for advice on manually labeling portions of unlabled images ,n71ew,giror,1269318363.0,https://www.reddit.com/r/MachineLearning/comments/n71ew/looking_for_advice_on_manually_labeling_portions/,3.0,6.0,"I have a set of images that are somewhat complicated - they have many examples of what I am looking for on each image and they are currently unlabeled. 

I am looking for a tool that would let me select rectangles on the image using my mouse/pointer to build up a training data set. Do you all know of any tools or simple code examples that might already do this? 

code preference (best first): python, octave/matlab, R, java, C

**EDIT: thanks all, here is an implementation in python if anyone is interested**


	from pylab import *
	
	fish= imread('./stevesFish/1b2.tif')
	
	figsrc = figure()
	figzoom = figure()
	
	axsrc = figsrc.add_subplot(111)
	axzoom = figzoom.add_subplot(111, xlim=(0,20), ylim=(0, 20), autoscale_on=False)
	
	axsrc.set_title('Click to zoom')
	axzoom.set_title('zoom window')
	
	axsrc.imshow(fish)
	axzoom.imshow(fish)
	
	
	def onClick(event):
	    if event.button!=1: return
	    x,y = event.xdata, event.ydata
	    axzoom.set_xlim(x-25, x+25)
	    axzoom.set_ylim(y-25, y+25)
	    figzoom.canvas.draw()
	
	def onKeyPress(event):
	    if event.key!='d': return
	    print axzoom.get_xlim(), axzoom.get_ylim()
	
	
	figsrc.canvas.mpl_connect('button_press_event', onClick)
	figsrc.canvas.mpl_connect('key_press_event', onKeyPress)
	show()
",en
1108511,2011-12-10 06:57:12,statistics,I knew something wasn't right about the green ones.,n74s4,[deleted],,https://www.reddit.com/r/statistics/comments/n74s4/i_knew_something_wasnt_right_about_the_green_ones/,0.0,0.0,,en
1108512,2011-12-10 20:17:03,statistics,Are there people that actually enjoy using SAS?,n7l3l,cbrunos,,https://www.reddit.com/r/statistics/comments/n7l3l/are_there_people_that_actually_enjoy_using_sas/,28.0,94.0,"I don't want to start a flame war, but I am genuinely curious. Are there people that given the choice between SAS or any another program, would choose SAS? I had an introduction to SAS this semester, and while using it I physically felt pain in my brain. For example to compute the mean of a variable:

    proc means data=my.data;
    var balls;
    run;

Look at that. Just look at that. You need to type a ton of stuff to do the most basic things. I can understand that maybe SAS does some things very well, and that people use it for those things, but only because they're forced to, right? Right?!",en
1108513,2011-12-10 21:57:06,statistics,AMSTAT salary survey for Statistics faculty ,n7odd,blind_swordsman,1318716377.0,https://www.reddit.com/r/statistics/comments/n7odd/amstat_salary_survey_for_statistics_faculty/,3.0,0.0,,en
1108514,2011-12-10 22:25:08,statistics,Can anyone suggest an alternative to using a categorical variable analysis here?  ,n7pa9,ruinmaker,1232670106.0,https://www.reddit.com/r/statistics/comments/n7pa9/can_anyone_suggest_an_alternative_to_using_a/,1.0,2.0,"Background:  I'm running a biological assay on substance P in the bood of 2 different groups (one independant variale) of people given 2 different exposures (the other IV).  I hoped I could get a continuous variable for easy analysis.  Unfortunately, such an assay isn't available for SP and so we've used an assay with a rather high ""cannot detect"" threshold.  Now I have continuous response down to 40 and &lt;40 below that.  I know that the usual distribution of the variable is 73 with a standard deviation of 84 so it's skewed and a large number of samples are in that &lt;40 range.  Is there an alternative to just making this an ordinal/categorical variable?  SP doesn't have any kind of clinical thresholds that I could use to create the cutoff between levels so I'd be stuck with just using +2 standard deviations.

If there isn't another option (which is my suspician) then I'm looking at some kind of ordinal/categorical analysis.  I'm currently leaning towards discriminant functional analysis or Wilcoxon rank-sum test.  Does anyone have any other suggestions or a preference for one test over the other?

**tl;dr** I'm trying to find an alternative to treating my variable as categorical/ordinal.  Failing that, I'm looking for suggestions for an appropriate analysis technique.",en
1108515,2011-12-11 01:15:42,statistics,Help me refine my survey please,n7vcj,[deleted],,https://www.reddit.com/r/statistics/comments/n7vcj/help_me_refine_my_survey_please/,3.0,12.0,"I came up with a political theory a couple of weeks ago, made a survey, and got about 100 people to take it.  The results seem to support my theory.  I want to refine my questions so that I can get better results and/or user fewer questions.  Basically I want to get more out of each question.

Ok so I have two dimensions: one is more vs less government, and the other is more vs less utopianism.  I had four questions for each axis, each had four responses.  I assigned a point value to each of the four answers, 0 for the least and 3 for the most.  Then I took the score for each question and added them together to find the score for that axis.  So for each poll taker I get a pair (x,y) where x is from 0 to 12, and y is 0 to 12.

So I have heard about correlation coefficients and I get the very basics of them.  I was wondering if I could use them to improve my questions.

I want each question to correlate highly with the overall axis, but I don't want to ask the same question over and over again, so I want each question to have a low correlation with each of the individual questions that make up that axis, am I right?  So let's say that I had an infinite number of questions:  Each question would have a correlation of 1 with the axis, and a correlation of 0 with the other individual questions that make up that axis.",en
1108516,2011-12-11 03:03:50,statistics,Not a homework question...,n7yz4,erniebornheimer,1306262415.0,https://www.reddit.com/r/statistics/comments/n7yz4/not_a_homework_question/,6.0,40.0,"...but rather a general question prompted by a group project in my statistics class. If that's enough to delete the post, so be it.

Here's the story. For the group project, we were to gather data on people's reaction time to a certain stimulus. Details on request. So I compiled the data from the group members and the distribution is not bell-shaped, it's skewed heavily to one side. Thinking back to when I was doing my part of the data-gathering, I realized I threw out the results when the subject failed the test (as I thought of it at the time). Now I see that we should have realized earlier that the test protocol is flawed.

So, the project is due on Monday, and I've emailed the professor, but judging by past performance, he won't reply and we're on our own. I can think of a way to re-do all the data, but it would involve all the group members physically meeting (so we can all calibrate our testers)...not practical.

So, one of the first questions is ""Is the data normally distributed?"" I think it would be if one end of the distribution were not cut off. The thing is, many of the formulas we've learned (and which we need to use in this project) only work on normal distributions. 

So, just because I didn't know what else to do, I decided to proceed as if the data were normally distributed, and see what that produced. I calculated the mean and standard deviation. Now I'm stuck on a question that asks for a cutoff value to separate the bottom 95% of data points from the top 5%. I calculated the z score from the probability, then plugged z, mu, and sigma into what I believe is the correct formula (x=mu+(z*sigma)). Unfortunately, the answer is higher than the max value of the results. I checked the inputs and the arithmetic multiple times. The problem is I don't have an intuitive understanding of what a z value is, so it's not clear to me if the problem is 
- because I've made a mistake somewhere, or
- if the data really is not normal, and applying that formula to non-normal data can produce bizarre results.

Without answering specific questions about my particular project, can someone talk about z values and non-normal distributions?

Thank you!

EDIT: link to a histogram of the data: http://dl.dropbox.com/u/23994/Right%20hand%20histogram%20v01.jpg.",en
1108517,2011-12-11 03:56:04,statistics,IBM SPSS: create two groups of scores (low and high) with estimations from a known distribution? How do I do that?,n80o5,[deleted],,https://www.reddit.com/r/statistics/comments/n80o5/ibm_spss_create_two_groups_of_scores_low_and_high/,0.0,3.0,,en
1108518,2011-12-11 09:49:47,MachineLearning,The Machine Learning Personality Test,n8axk,tigerthink,,https://www.reddit.com/r/MachineLearning/comments/n8axk/the_machine_learning_personality_test/,8.0,2.0,,en
1108519,2011-12-11 12:12:36,MachineLearning,Rescuing Twapperkeeper Archives Before They Vanish,n8ddu,[deleted],,https://www.reddit.com/r/MachineLearning/comments/n8ddu/rescuing_twapperkeeper_archives_before_they_vanish/,0.0,0.0,,en
1108520,2011-12-11 20:32:45,statistics,Need help with a basic question in RStudio,n8ncf,[deleted],,https://www.reddit.com/r/statistics/comments/n8ncf/need_help_with_a_basic_question_in_rstudio/,4.0,9.0,"I'm trying to conduct an independent samples t test. I've imported my csv dataset and entered the command to check the variances of the two groups (called GroupA and GroupB). When I enter

var.test(GroupA,GroupB), I get an error saying ""Error in var.test(GroupA, GroupB) : object 'Group1' not found""

I then just tried to do the t-test without checking the variances (to see if it was a problem specific to the data or to the variance test, and I go the same error: ""Error in t.test(GroupA, GroupB) : object 'GroupA' not found""

Is there something I need to do to make it recognize my data (other than import the dataset"" to make it recognize my groups?

Thanks in advance for your help.

",en
1108521,2011-12-12 00:04:49,statistics,Scaling of the distribution of fluctuations of financial market indices. Does Reddit care to explain this paper to the layman?,n8v76,leopardprintlife,1319484417.0,https://www.reddit.com/r/statistics/comments/n8v76/scaling_of_the_distribution_of_fluctuations_of/,2.0,2.0,,en
1108522,2011-12-12 01:04:00,artificial,Google X Lab - Robotics and AI,n8xc0,marshallp,1239903633.0,https://www.reddit.com/r/artificial/comments/n8xc0/google_x_lab_robotics_and_ai/,4.0,0.0,,en
1108523,2011-12-12 01:28:53,statistics,In desperate need of homework help- multivariate analysis,n8ya3,BearD0WN,1321215610.0,https://www.reddit.com/r/statistics/comments/n8ya3/in_desperate_need_of_homework_help_multivariate/,1.0,1.0,,en
1108524,2011-12-12 03:17:29,MachineLearning,Anyone know where I can get the Netflix Prize dataset?,n92cg,descentintomael,1318268224.0,https://www.reddit.com/r/MachineLearning/comments/n92cg/anyone_know_where_i_can_get_the_netflix_prize/,11.0,3.0,"It seems to have been taken down from all of the normal channels, but I was wondering if one of you might know where I could grab a copy?  Or maybe even torrent it from someone?",en
1108525,2011-12-12 05:59:28,statistics,Why you can not to use statistics to dispute magic,n98qi,[deleted],,https://www.reddit.com/r/statistics/comments/n98qi/why_you_can_not_to_use_statistics_to_dispute_magic/,1.0,0.0,,en
1108526,2011-12-12 06:00:50,statistics,Why you can't use statistics to dispute magic,n98sl,jambarama,1130472000.0,https://www.reddit.com/r/statistics/comments/n98sl/why_you_cant_use_statistics_to_dispute_magic/,13.0,8.0,,en
1108527,2011-12-12 11:26:05,MachineLearning,Perceptual Learning Incepted by Decoded fMRI Neurofeedback Without Stimulus Presentation,n9ihf,[deleted],,https://www.reddit.com/r/MachineLearning/comments/n9ihf/perceptual_learning_incepted_by_decoded_fmri/,1.0,0.0,,en
1108528,2011-12-12 15:13:20,statistics,Statistical evidence of vote fraud in Russian elections,n9m5n,kal00ma,1297504585.0,https://www.reddit.com/r/statistics/comments/n9m5n/statistical_evidence_of_vote_fraud_in_russian/,34.0,0.0,,en
1108529,2011-12-12 17:13:20,MachineLearning,"A nice, readable article on semi supervised naive bayes",n9p12,rrenaud,1150739310.0,https://www.reddit.com/r/MachineLearning/comments/n9p12/a_nice_readable_article_on_semi_supervised_naive/,34.0,16.0,,en
1108530,2011-12-12 19:47:07,MachineLearning,Videos and slides from useR2011 - the official R conference,n9ujt,[deleted],,https://www.reddit.com/r/MachineLearning/comments/n9ujt/videos_and_slides_from_user2011_the_official_r/,0.0,0.0,,en
1108531,2011-12-12 20:24:31,MachineLearning,"Course website of course on mathematical problem solving techniques: Optimization, SVD, Kalman filters etc.",n9w1e,satsatsat,1301349517.0,https://www.reddit.com/r/MachineLearning/comments/n9w1e/course_website_of_course_on_mathematical_problem/,9.0,3.0,,en
1108532,2011-12-12 21:25:44,statistics,Standard errors versus standard deviations: the key to understanding hypothesis testing,n9ybe,[deleted],,https://www.reddit.com/r/statistics/comments/n9ybe/standard_errors_versus_standard_deviations_the/,2.0,8.0,,en
1108533,2011-12-13 09:11:30,statistics,Washed up stats major with a low GPA.,naqn7,not_a_creative_alias,1323759510.0,https://www.reddit.com/r/statistics/comments/naqn7/washed_up_stats_major_with_a_low_gpa/,10.0,18.0,"Well I came here looking for advice, and hopefully someone can help. 

I'm studying a statistics and economics double major in Canada (uoft). Just finishing the first semester of my 4th year and my GPA is ass. I'm at a 2.5 right now and it's just going to get worse after this semester for sure. I don't have much of an excuse. My GPA really plummeted because of my 3rd year, I only got 2 A's and the rest were C's and D's. However I will say that I have decent marks in the statistics courses I have taken (alongside the econometrics). My math courses are killing my GPA, alongside the computer science courses which I took (mandatory and hated every bit of it). 

Anyway, this year is looking even worse, and that 2.5 might turn into a 2.3 or worse...

I really want to break into finance, (risk management) particularly, and  after my shoddy undergrad performance, no half decent grad school will even take me for a masters in stats or financial engineering or whatever. I only held one job that is somewhat relevant, doing research for a small firm last summer. 

I am really lost and don't know what to do with my life after I graduate...my GPA is the thing I'm most worried about right now. Sure many people say that job hunting isn't solely GPA dependent, but progressing in life might be. I know that several senior level positions might require a masters of some kind (like an MBA or a masters in math/stats etc. if in risk management). 

It sucks, I only just realized how important a fucking number is in determining your entire life. I wish I could take it all back, but i can't.

TL;DR:

My GPA is shit, not much work experience, have a higher chance of learning how to fly than going to grad school. What in the fuck can I do (if anything)?
",en
1108534,2011-12-13 15:25:03,statistics,Correlation or Causation? Need to prove something you already believe? All you need are two graphs and a leading question.,nayfu,robotrebellion,,https://www.reddit.com/r/statistics/comments/nayfu/correlation_or_causation_need_to_prove_something/,14.0,6.0,,en
1108535,2011-12-13 16:52:51,MachineLearning,I summarised my experiences with learning a Partially Observable Markov Decision Process given input/output data - in case it helps anyone.,nb0vb,danielMe,1322516030.0,https://www.reddit.com/r/MachineLearning/comments/nb0vb/i_summarised_my_experiences_with_learning_a/,11.0,4.0,,en
1108536,2011-12-13 17:58:20,datasets,Raw data from the contested 2011 Russian elections,nb37t,Quintote,1269890504.0,https://www.reddit.com/r/datasets/comments/nb37t/raw_data_from_the_contested_2011_russian_elections/,17.0,1.0,,en
1108537,2011-12-13 19:14:46,analytics,GA: Cross sub-domain conversion tracking ,nb6ak,thesupermikey,1289685099.0,https://www.reddit.com/r/analytics/comments/nb6ak/ga_cross_subdomain_conversion_tracking/,1.0,0.0,"I am trying to set up conversion tracking on a new website. The traffic will be landing on example.com, but the final step of the transaction takes place on app.example.com. This is the first time I have ever done something like this and Google's documentation does not have a lot of details about how to do this in GA.

1) Does the tracking code need to be installed on example.com and app.example.com?

2) when setting up the goal page does it need to be /landing-page/thank-you or app.example.com/landing-page/thank-you

3) is this well documented in google's help finals and i am just calling it the wrong thing?",en
1108538,2011-12-13 19:47:26,statistics,Experimental Design Book?,nb7lw,wagthesam,1170550569.0,https://www.reddit.com/r/statistics/comments/nb7lw/experimental_design_book/,2.0,5.0,I'm a stats undergrad that will probably graduate without any experimental design/sampling coursework (I chose to emphasize data vis and ml). Is there a good book out there that is a good overview? In my next job I need to implement A/b testing for a large web application and I want to do it right.,en
1108539,2011-12-13 23:12:16,MachineLearning,Divvy - unsupervised learning data exploration tool (OS/X),nbgpk,b0b0b0b,1144868232.0,https://www.reddit.com/r/MachineLearning/comments/nbgpk/divvy_unsupervised_learning_data_exploration_tool/,15.0,10.0,,en
1108540,2011-12-14 01:39:22,statistics,Best Bayesian Book,nbn8h,fastparticles,1281547257.0,https://www.reddit.com/r/statistics/comments/nbn8h/best_bayesian_book/,16.0,16.0,I am not a statistician but I have taken some graduate level statistics classes (notably multivariate analysis and experimental design). I was hoping you guys could recommend me a good book for learning Bayesian statistics. Thank you!,en
1108541,2011-12-14 09:14:13,datasets,Looking for datasets on English speaking Internet users by country.,nc5ae,Supertrinko,1314925894.0,https://www.reddit.com/r/datasets/comments/nc5ae/looking_for_datasets_on_english_speaking_internet/,2.0,0.0,"Hi, As said in the title, I'm trying to make a list of countries by ""English speaking internet users"".

Two separate Wikipedia pages list the two statistics:

[List of countries by English-speaking population](http://en.wikipedia.org/wiki/List_of_countries_by_English-speaking_population).

[List of Internet users by country](http://en.wikipedia.org/wiki/List_of_Internet_users_by_country).

This is almost what I want:

[Internet users by language](http://en.wikipedia.org/wiki/Global_Internet_usage#Internet_users_by_language)

But this only shows the amount of users of each language, not where they're from.

I want:

""List of countries by English speaking internet users""

---

Reason for the data? Countries have borders. But the internet is an international phenomenon. The main borders are language borders, with crossings in multi-lingual people, and tourists in internet translators.

It's pretty clear based on the data I do have access to (above data separately), that English Speaking Americans are the majority of internet users. They have over twice as many English speakers as second place India, and over twice as many internet users regardless of language.

This shows that the United States has the largest influence on the English speaking internet (obvious to most).

And this is the goal, looking at the influence different countries have on the English Internet.

---

India, it has the second highest amount of English speakers, and fourth highest on the amount of Internet users (hold your jokes please). Though the majority of their English speakers have it as an additional language, so that's probably why we see more British influence on the internet than Indian.

---

Perhaps I ask too much, I expect I could only find such data in a census. But it can't hurt to ask, perhaps someone has an idea.",en
1108542,2011-12-14 10:46:26,MachineLearning,Fast and Accurate k-means For Large Datasets,nc7cy,[deleted],,https://www.reddit.com/r/MachineLearning/comments/nc7cy/fast_and_accurate_kmeans_for_large_datasets/,0.0,1.0,,en
1108543,2011-12-14 16:41:34,datasets,Request: Dataset of ISP's by zipcode?,nceoa,b00ks,1174191893.0,https://www.reddit.com/r/datasets/comments/nceoa/request_dataset_of_isps_by_zipcode/,7.0,2.0,"Don't know if this exists, but I want to get a visual reference to the places in the US that don't have access to ESPN360.",en
1108544,2011-12-14 17:07:08,computervision,Amsterdam Computer Vision Meetup - January 16th of 2012,ncffa,gijzelaerr,1227791829.0,https://www.reddit.com/r/computervision/comments/ncffa/amsterdam_computer_vision_meetup_january_16th_of/,2.0,0.0,,en
1108545,2011-12-14 17:34:19,MachineLearning,Is there a rule of thumb for how many layers should be in a neural net?  What about the weights initial range?,ncgbl,pclogos,1283351290.0,https://www.reddit.com/r/MachineLearning/comments/ncgbl/is_there_a_rule_of_thumb_for_how_many_layers/,5.0,31.0,"I have a dataset with one factor response and about 12 inputs, some factors.  I've been using R's nnet to try and create a nn and have the two questions listed in the title.   Thanks for any help.",en
1108546,2011-12-14 18:29:01,statistics,"If you have a series of points that are better approximated by a higher order polynomial (say r^2 = 0.9), but you estimate it with a linear regression (say r^2 = 0.6) and integrate over that function: what is the difference in accuracy of your integral? ",ncicj,[deleted],,https://www.reddit.com/r/statistics/comments/ncicj/if_you_have_a_series_of_points_that_are_better/,1.0,8.0, ,en
1108547,2011-12-14 19:47:47,statistics,"Statistics wizards of reddit, I desperately need help running through my practice exam... can anyone help?",nclkn,BacktoWoWJohn,1323734041.0,https://www.reddit.com/r/statistics/comments/nclkn/statistics_wizards_of_reddit_i_desperately_need/,1.0,0.0,,en
1108548,2011-12-14 20:41:12,statistics,Relearning statistics online,ncnwv,maverick566,1295662983.0,https://www.reddit.com/r/statistics/comments/ncnwv/relearning_statistics_online/,3.0,4.0,"So next quarter I have to take an intermediate statistics class for my major. The problem is I took a year off last year, and need to relearn intro stats and prepare myself for the upcoming quarter. With regards to online resources, I've found [this](http://educator.com/mathematics/statistics/son/). But, being a broke college student, I was wondering if anyone here knows of anything similar that's a little cheaper? Thanks!",en
1108549,2011-12-14 21:28:31,MachineLearning,Proceedings of the Fifth ACM Conference on Recommender Systems,ncq0w,[deleted],,https://www.reddit.com/r/MachineLearning/comments/ncq0w/proceedings_of_the_fifth_acm_conference_on/,0.0,1.0,,en
1108550,2011-12-14 22:30:19,artificial,The Thinking Machine (Artificial Intelligence in the 1960s),ncsv7,jabandie,1301513960.0,https://www.reddit.com/r/artificial/comments/ncsv7/the_thinking_machine_artificial_intelligence_in/,11.0,10.0,,en
1108551,2011-12-15 02:09:50,statistics,"Probability Primer: A series of videos giving an introduction to some of the basic definitions, notation, and concepts one would encounter in a 1st year graduate probability course.",nd2i1,mycatharsis,1294046118.0,https://www.reddit.com/r/statistics/comments/nd2i1/probability_primer_a_series_of_videos_giving_an/,28.0,1.0,,en
1108552,2011-12-15 02:29:15,MachineLearning,Is there a machine learning algorithm that does what I describe? (See text),nd38j,randombozo,1266016617.0,https://www.reddit.com/r/MachineLearning/comments/nd38j/is_there_a_machine_learning_algorithm_that_does/,7.0,4.0,"While reading a textbook on data mining, I had an idea for an algorithm: it'd work like KNN, except that instead of averaging the 'K' nearest neighbors to estimate the response variable, all neighbors are weighed according to 1) their distance and 2) importance of the X variables (e.g. if X2 is more correlated/valuable than X1, distances among neighbors' X2 variables are weighed more heavily than X1).

I'm sure this already had been thought of, so what family does something like this belong to?",en
1108553,2011-12-15 03:32:58,MachineLearning,Well taught and extensive machine learning video series in a format a bit like Khan Academy,nd5vo,mycatharsis,1294046118.0,https://www.reddit.com/r/MachineLearning/comments/nd5vo/well_taught_and_extensive_machine_learning_video/,67.0,4.0,,en
1108554,2011-12-15 10:30:00,AskStatistics,Someone please explain my professor's use of correlation between two exams to justify boosting the second exam grades,ndkoe,bdalebs,1317937454.0,https://www.reddit.com/r/AskStatistics/comments/ndkoe/someone_please_explain_my_professors_use_of/,2.0,8.0,"My professor - well, graduate assistant - gave each student 6 extra points on the second exam and claimed that she had calculated the correlation between our first exam grades and our second exam grades (post-extra-points). She said she felt that the correlation was low, but that she would rather not calculate the correlation before the extra points.
I've aced AP Statistics and Stat2 at a state university, but from everything I know about correlation, adding equally to all observations should not have affected the correlation. I asked her about it a week or so later and she claimed that I was doing a different type of correlation and she was looking at how well individual students' exam grades correlated with one another. 
Maybe I'm just being incredibly dense, but can someone please explain how the extra points would have had any affect on the correlation?",en
1108555,2011-12-15 10:42:13,statistics,Question on point processes and GPD,ndkz6,canteloupy,1287091489.0,https://www.reddit.com/r/statistics/comments/ndkz6/question_on_point_processes_and_gpd/,1.0,0.0,"Can you use point process modeling (for instance pp.fit from the ismev package) if you don't have all points, but only points above a certain threshold? I would like to model data with time as a covariate to see if the distribution of these extremes changes over time, but I only have points above a certain threshold.

I am wondering, because I am inclined to model this with a GPD, but I was told not to use GPD with covariates because of threshold stability.",en
1108556,2011-12-15 12:52:43,computervision,Large-Scale Live Active Learning: Training Object Detectors with Crawled Data and Crowds,ndnmy,[deleted],,https://www.reddit.com/r/computervision/comments/ndnmy/largescale_live_active_learning_training_object/,1.0,0.0,,en
1108557,2011-12-15 17:15:48,statistics,"Which software to use for class: R, JMP, SPlus, Matlab",ndu5v,[deleted],,https://www.reddit.com/r/statistics/comments/ndu5v/which_software_to_use_for_class_r_jmp_splus_matlab/,9.0,37.0,"I have free access to all of these products (I think R is free) and I have to choose between them for a class.  I was hoping you all could help me narrow it down.

The class is called intro to probability and statistics for engineers and I am a strong programmer, but have no experience with statistics or these products/languages.",en
1108558,2011-12-15 18:18:11,statistics,Need help getting RStudio to use/recognize my data,ndwig,[deleted],,https://www.reddit.com/r/statistics/comments/ndwig/need_help_getting_rstudio_to_userecognize_my_data/,1.0,0.0,"Hi all,

I'm trying to use RStudio to run an independent samples t-test, and I cannot get it to run the t-test because it's not recognizing/finding my data. So, here's exactly what I did:

1. Went to 'import dataset' in the upper right hand window

2. Chose ""From Text File...""

3. Chose my dataset, which is named 'data1.csv,' and [here's](http://i.imgur.com/mBD7T.png) what it looks like

4. When the window popped up [this window](http://i.imgur.com/Nf9IA.png), I selected ""import""

5. RStudio then looked like [this,](http://i.imgur.com/AXrf3.png) so I could see my dataset in the upper left window. I opened a new script (called 'Untitled1' in this photo) so that I could work with the data and start my t-test.

6. I have tried various commands to begin the t-test but keep getting error messages. I entered the command:

data &lt;- read.csv(""data1.csv"")

in the script window (upper left), and when I do that, in my console, I just get a blank command line:

&gt;

When I try to do the t-test, I enter:

t.test(data$group1,data$group2)

And I get the following error message:

Error in data$group1 : object of type 'closure' is not subsettable

If you can help me get to the point where I can conduct my damned t-test, I will mail you a cupcake. 

Most sincerely,
ItWillBeMine",en
1108559,2011-12-15 19:27:44,rstats,What R functions have made your programming life easier?,ndzdf,CauchyDistributedRV,1301382121.0,https://www.reddit.com/r/rstats/comments/ndzdf/what_r_functions_have_made_your_programming_life/,23.0,4.0,"Whether you're experienced with R or not, I'm sure all of us have stumbled on a few functions that have made our lives much easier. 

do.call( ""cbind"", x ): You probably know about tapply() and the other apply functions. However, if the function you use in tapply() returns a vector of results, the final output will be an ugly list. You can put that all together into a nice data frame with do.call( ""cbind"", x ). Eg:

    library(nlme)
    tmp &lt;- with( Loblolly, tapply( height, Seed, summary ) )
    do.call( ""cbind"", tmp )

droplevels(): If you've ever had a dataset that already has some columns coded by factors, and you start subsetting that dataframe, I can almost guarantee that the factor levels will cause you some frustration. Eg: you try to fit a linear model on a subsetted data set, of which only 2 of the 10 levels are actually presented in your subsetted data set; however, R brings forward all 10 levels and does some craziness when you try to fit a model to the data. As of R 2.12.0, we finally have the 'droplevels()' function, which will get rid of those extraneous levels that don't appear in your dataset.

    levels( sleep$group )
    [1] ""1"" ""2""
    x &lt;- subset( sleep, group == ""1"" )
    levels( x$group )
    [1] ""1"" ""2""
    x &lt;- droplevels( x )
    levels( x$group )
    [1] ""1""

What other R functions have made your life easier that others might not have stumbled upon?",en
1108560,2011-12-15 20:51:15,statistics,"Need help with making script run in RStudio (also, damn the histogram)",ne31o,[deleted],,https://www.reddit.com/r/statistics/comments/ne31o/need_help_with_making_script_run_in_rstudio_also/,1.0,8.0,"I'm running an independent samples t-test and need to be able to view a histogram when I run the script. The script ran fine before (conducted a variance test to see if they are equal between the two groups; they are), then conducted a t-test (significant), but when I added in the command to run a histogram, it will create the histogram but stops running the variance test/t-test.

Why, R?? WHY???

If you can help me, I will mail you a cupcake. Also, [here's](http://i.imgur.com/NIcSd.png) a screenshot. the histogram is sad 'n ugly.

Question number 2: Also, it appears as though my histogram is only plotting data from group 1 (based on the title), and I have two groups. Damn the R. Damn it to hell.",en
1108561,2011-12-15 21:41:49,analytics,Analytiks gives you the perfect snapshot of your Google Analytics on iOS,ne56o,shegeek,1234888235.0,https://www.reddit.com/r/analytics/comments/ne56o/analytiks_gives_you_the_perfect_snapshot_of_your/,1.0,0.0,"If you want to be able to checkup on your site or blog stats anytime, anywhere, this app will give you quick access to the data you need to know...and more. ",en
1108562,2011-12-15 22:07:11,AskStatistics,Studying for a Final that's tomorrow need help with a Hypothesis Test question,ne6a6,Kelaos,1290045905.0,https://www.reddit.com/r/AskStatistics/comments/ne6a6/studying_for_a_final_thats_tomorrow_need_help/,0.0,4.0,"In a test of null hypothesis: mu=8 vs alternative hypothesis mu does not equal 8, a sample of size 96 leads to a P-value of 0.034. Which  of the following must be true?

Their answer is ""A 95% confidence interval for mu calculated from these data will not include 8.""

But we think it should be ""A 95% confidence interval for mu calculated from these data will be centered at 8""

Can someone explain why their answer is correct?

Thanks very much!

**EDIT**: Thanks for the help, we've figured it out, seems as though the p-value given was already modified to fit the two-tailed test so 0.034&lt;.05",en
1108563,2011-12-15 23:34:05,rstats,MySQL with R using xampp,nea7o,talgalili,1271226645.0,https://www.reddit.com/r/rstats/comments/nea7o/mysql_with_r_using_xampp/,1.0,0.0,,en
1108564,2011-12-16 01:45:35,statistics,Binomial/Bernoulli Test?,neg41,latortilla,1299276404.0,https://www.reddit.com/r/statistics/comments/neg41/binomialbernoulli_test/,0.0,4.0,"Hi,

I have the following questions - starting point is:
In my sample I have a series of k binary variables (0,1), then I take the sum S of these k variables.

* First question: can I automatically assume that my k binary variables are Bernoulli distributed? And thus, that the sum S is binomially distributed?

* Second question: given the value S, can I find the distribution of this value, and thus test whether it is equal to something? E.g. 1/k * S = 1 vs 1/k * S =! 1. My basic first thought was that I could potentially say that each k is Bernoulli, thus S is binomial, and that should make it possible to test whether 1/k * S = some value. Is this thinking correct?

Thanks!

edit: an extension to this is then, what happens when all the k Bernoulli trials are not independent?",en
1108565,2011-12-16 05:56:51,statistics,"Does anyone know where I can get archives of all the podcasts, ""More or Less?"" Or...",neqjb,benjeye,1292457824.0,https://www.reddit.com/r/statistics/comments/neqjb/does_anyone_know_where_i_can_get_archives_of_all/,3.0,2.0,...does anyone know of a similar podcast that's still producing new episodes?,en
1108566,2011-12-16 08:27:44,statistics,Making sense of the lmer() output in R,new3g,sais,1201756650.0,https://www.reddit.com/r/statistics/comments/new3g/making_sense_of_the_lmer_output_in_r/,1.0,2.0,"My factor has three levels - A, B and C. 
When I run the lmer() command and get p-values, I get:

(intercept)

variable

factorB

factorC

My understanding is the intercept is factorA. Is this correct? So that p-value for the (intercept) is essentially the p-value for factorA? 

Am I missing something? How else do I see if factorA is significant or if it is a main effect like factorB (p&lt;.001) or factorC (p&lt;.02)?",en
1108567,2011-12-16 15:56:20,MachineLearning,Suggestions for Quickly Coming Up to Speed in Math for Machine Learning,nf5mv,epios,1286976021.0,https://www.reddit.com/r/MachineLearning/comments/nf5mv/suggestions_for_quickly_coming_up_to_speed_in/,12.0,16.0,"Hi r/MachineLearning,
I know there's lots of resources about what to learn, and they are really great.  If anything I have an embarrassment of riches and I am looking to prioritize.  The background is that I got accepted into a graduate certificate program in Data Mining that covers a lot of Machine Learning topics, and I am trying to make sure my math is up to speed.  I've got about a month before classes start and I'm trying to spend it on building math skills.  So I never had Linear Algebra so I am focusing on that and I am also planning to get in an overview of Probability Theory.  If I have extra time over the holidays (ha), I'm going to work on the rest of Statistics.  Is this the priority you guys would follow?  Is there another subject that I absolutely need to digest to hit the ground running?  I am obviously planning to continue to work to back fill my math as I get time, but looking to plan this initial flurry of activity 

EDIT: Thanks everyone for the great suggestions!",en
1108568,2011-12-16 17:12:25,statistics,Need help with doing percentile bootstrapping in RStudio,nf7rh,[deleted],,https://www.reddit.com/r/statistics/comments/nf7rh/need_help_with_doing_percentile_bootstrapping_in/,1.0,0.0,"Hi all,

I'm attempting to do some percentile bootstrapping in RStudio, and I specifically want to get the confidence intervals.

I have two independent groups with 100 observations each. I asked R to find the difference between the means of the group (and this value is called ""obsdif."" The observed difference between the means right now is -1.73, with the mean of group1 being smaller than the mean of group2. Then I asked for it to sample, with replacement, 1000 times from both groups and to print the observed differences along with the 5% and 95% range of values.

When I run it, it only gives me that original -1.73 value, which tells me that it's not really sampling 1000 times with replacement, or I'd have a variety of values. If anyone could locate the bug in this script, I would certainly appreciate it.

Here's the script (it's fairly short):

obsdif = mean(data$group1) - mean (data$group2) #We want to find the observed difference for the bootstrapping procedure, and the difference between the means is -1.7350

N = 1000 
stat = numeric(N)
for (i in 1:N){
  Group1 = sample (data$group1, replace =T)
  Group2 = sample (data$group2, replace =T)
  stat [i] = mean(data$group1) - mean (data$group2)
}
print(obsdif) #this is printed in the console, and is -1.73508
print(""range excluding bottom and top 5% of values"")
print(quantile (stat, c(0.05, 0.95)))

And [here's](http://i.imgur.com/F7PjU.png) a screenshot. Thanks, r/statistics! You guys have been very helpful and I've been learning a lot about RStudio through these conversations.


",en
1108569,2011-12-16 18:19:56,statistics,r/science is currently making a lot of noise about a newly released data-exploration tool called MINE. What do you guys think?,nfa4k,shaggorama,1233555004.0,https://www.reddit.com/r/statistics/comments/nfa4k/rscience_is_currently_making_a_lot_of_noise_about/,20.0,12.0,,en
1108570,2011-12-16 19:24:28,statistics,FWP happened to me today...,nfckw,hillset,1318127882.0,https://www.reddit.com/r/statistics/comments/nfckw/fwp_happened_to_me_today/,9.0,5.0,,en
1108571,2011-12-16 20:45:35,datascience,Ask /r/datascience: Any tips for picking up work?,nffn5,fourxy,1281551499.0,https://www.reddit.com/r/datascience/comments/nffn5/ask_rdatascience_any_tips_for_picking_up_work/,2.0,0.0,I am quitting my job and starting a data science consultancy firm at the first of next year.  Do any data science redditors have tips on how to acquire data science work?  How do you market yourselves?  How do you explain 'data science' to people who need it but aren't familiar with the phrase?  Thanks.,en
1108572,2011-12-17 01:24:16,statistics,Validated SAS can give you validated segfaults,nfqsp,[deleted],,https://www.reddit.com/r/statistics/comments/nfqsp/validated_sas_can_give_you_validated_segfaults/,7.0,4.0,,en
1108573,2011-12-17 03:17:33,computervision,"learning to ""borrow"" examples for object detection. 
(what do you do when you can't get any more 
labeled cats?) Lim et al, NIPS 2011",nfup2,jeanlucpikachu,1159634949.0,https://www.reddit.com/r/computervision/comments/nfup2/learning_to_borrow_examples_for_object_detection/,1.0,0.0,,en
1108574,2011-12-17 04:53:39,statistics,Trying to do a simulation in RStudio,nfxpf,[deleted],,https://www.reddit.com/r/statistics/comments/nfxpf/trying_to_do_a_simulation_in_rstudio/,6.0,5.0,"I'm trying to do an experimental simulation to confirm some power estimates and have no idea how to go about it in terms of creating the script. Here's what I want to do:

1. repeatedly sample (N=5000) two groups of 18 from two normal distributions with a small effect size (Cohen's d = .20). 

2. For each pair of samples, I want to conduct a t-test and record the p-value.

3. I want to perform this sampling and test procedure 5000 times and calculate the proportion of tests that obtained significance (and print out this value). 

If you can help me with any of these items, I will draw you a pretty picture. Thanks, r/statistics! You've been a great help with all my RStudio questions.",en
1108575,2011-12-17 09:29:49,MachineLearning,Is anybody going to the ICMLA conference this year?,ng5kz,anthonyt,1271611934.0,https://www.reddit.com/r/MachineLearning/comments/ng5kz/is_anybody_going_to_the_icmla_conference_this_year/,4.0,4.0,,en
1108576,2011-12-17 13:05:40,computervision,Making a stencil generator using OpenCv - help with the bridge construction part. (X post compsci),ng98i,mrpogiinspace,1311244341.0,https://www.reddit.com/r/computervision/comments/ng98i/making_a_stencil_generator_using_opencv_help_with/,4.0,1.0,"Hey guys! this is the paper I'm using to create the bridge automation function (http://www.cs.utah.edu/~bronson/papers/bronson_stencil.pdf). I've already made the binarization part and island identification - it's pretty straightforward, i just used the native functions in OpenCv and a simple fill algorithm. Now the difficult part is applying the bridge. I can do all the coding ones I've fully understood the process. I just want you to explain the bridging process for me LIfreshman. This will be a cool stencil generator for designing Tshirts when I finish this. Thanks.",en
1108577,2011-12-17 19:16:42,statistics,Why are there N degrees of freedom in a population but N-1 in a sample?,nggi1,[deleted],,https://www.reddit.com/r/statistics/comments/nggi1/why_are_there_n_degrees_of_freedom_in_a/,24.0,6.0,"I understand that the N-1 for the sample is because the sum of deviations from the mean must equal zero.  Why isn't that true for a population (and if it is, why don't we use N-1?)",en
1108578,2011-12-17 23:07:39,statistics,"Wanted: math whiz, who make may or may not be into sports.",ngnqk,queenw_hipstur,1301949232.0,https://www.reddit.com/r/statistics/comments/ngnqk/wanted_math_whiz_who_make_may_or_may_not_be_into/,0.0,1.0,"I've started sports gambling and am doing pretty well, but I have an idea for a mathematics equation that i need help with. Essentially I'm trying to figure out the whether i should bet on the over/under. Over/under is a betting line that is used to predict whether or not the two teams with **combine** to score a certain amount of goals/points. Say the NHL game has an over under of 5 goals. That means, if you bet the over, -nd the final score is 4-2, you win the bet. This site im using is also pretty cool, because their are odds on top of picking whether or not it will be over or under. 
Example:
Vancouver vs Toronto 
5.5+ (this means the total amount of goals must be over or under 6)
Over 1.80(if you bet $1, you get $1.80 back)
Under 2.05

Vancouver 
Goals for per game 3.23
Goals against per game 2.42

Toronto
Goals for per game 3.03
Goals against 3.19

I think it's pretty simple, I just don't know exactly what I'm looking for. Any help would be much appreciated.
Thanks math!",en
1108579,2011-12-17 23:29:40,MachineLearning,Is it possible/worthwhile to implement batch learning for Kohonen networks?,ngofk,eubarch,1301180238.0,https://www.reddit.com/r/MachineLearning/comments/ngofk/is_it_possibleworthwhile_to_implement_batch/,7.0,5.0,"The learning algorithm I'm familiar with adjust the network in response to a single training example at a time.  I can think of a couple ways off the top of my head to implement batch learning, but I was wondering if there is literature showing that one method performs better than others, or that batch learning is sort of pointless for SOFMs.",en
1108580,2011-12-17 23:34:12,statistics,Good books to read for a better understanding of statistics?,ngoks,jflann,1262489140.0,https://www.reddit.com/r/statistics/comments/ngoks/good_books_to_read_for_a_better_understanding_of/,7.0,6.0,"I've taken two statistics courses and I have a basic understanding of how to do a number of procedures like ANOVA, regression, chi-squared tests, etc.  However, i feel like I don't really understand the basis for these procedures, how distributions work, stuff like that.  I think I'm going to major in statistics, so I'd like to be able to understand what I'm doing in more depth.",en
1108581,2011-12-18 06:53:07,statistics,I just took a lean Six Sigma course and I'd like to learn about statistics,nh1o8,PrincipalBlackman,1282844618.0,https://www.reddit.com/r/statistics/comments/nh1o8/i_just_took_a_lean_six_sigma_course_and_id_like/,5.0,8.0,"I'm starting from zero, can anyone recommend a good, basic introduction to statistics that would be useful in the business world?",en
1108582,2011-12-18 11:14:21,MachineLearning,Would anyone who is well versed in machine learning (and related areas) be willing to let me bounce some ideas off of them?,nh7n8,FertileCroissant,1240789788.0,https://www.reddit.com/r/MachineLearning/comments/nh7n8/would_anyone_who_is_well_versed_in_machine/,5.0,12.0,"I don't have a CS or math background, so my understanding of these concepts is relatively limited.

I'm working on an idea for a recommendation website/service, and I'm trying to figure out first if it's feasible, and if it is, what types of people should I be looking for to help build it, and what the most practical approach might be. Without getting too specific (yet), I'm thinking of something along the lines of Pandora, Netflix, and Amazon. 

From what I understand, Pandora actually has real people listening to every song and choosing (from a predefined list) which attributes best characterize it. When you ""like"" or ""dislike"" a song or artist, it uses that data to make future recommendations. However, others such as MusicBrainz/MusicIP use acoustic fingerprinting and feature extraction to gather data. 

* What are some of the advantages and disadvantages of each method, and when might you use one over the other?

* How about when applied to other multimedia, such as images and video?

* Does the challenge lie more in figuring out how to choose meaningful attributes, the learning algorithm itself, or both equally? 

* What are the advantages and disadvantages of a top-down method (pre-defined attributes typically hidden from the enduser) versus a bottom-up approach (something along the lines of tagging or user-defined attributes)? The most sucessful recommendation algorithms seem to use the former method, is there a reason for that?

Any insight would be very much appreciated.",en
1108583,2011-12-18 15:36:55,statistics,r/statistics' thoughts on direct gradient analysis,nhb27,Eist,1299261197.0,https://www.reddit.com/r/statistics/comments/nhb27/rstatistics_thoughts_on_direct_gradient_analysis/,2.0,2.0,"Hi, r/statistics. I'm having a little disagreement with my supervisor about the robustness of direct gradient analysis techniques, more specifically DCA and CCA. [This](http://ordination.okstate.edu/robust.htm) is a pretty good overview, but my supervisor is hesitant of letting me use these techniques at all, and wants me to look at using a more ""manual"" multivariate regression work-around technique. Anyway, I'm doing community ecology, and my data has two sets of variables (environmental variables and species [percent cover with many 0's]). The study is designed so that there is high correlation between some of the variables in some of the samples. I have more than enough data points.

As you can probably tell, I'm not by any means a statistician, and I'm not so well versed at this. I'd be happy to further explain what the hell I'm trying to achieve, if desired. Nevertheless, I would appreciate any advice/comments that you may have! Thanks.",en
1108584,2011-12-18 22:34:35,MachineLearning,Sample Search Data Needed,nhlxh,descentintomael,1318268224.0,https://www.reddit.com/r/MachineLearning/comments/nhlxh/sample_search_data_needed/,6.0,5.0,"I'm working on a personal project to use GA to enhance the configuration settings for Apache Solr, but the problem is that I don't have any test data to work with.

Does anyone know where I could get some sample search data with associated queries and data on what the user clicked on?",en
1108585,2011-12-18 22:49:19,statistics,How can I make such a map with R?,nhmeo,cbrunos,,https://www.reddit.com/r/statistics/comments/nhmeo/how_can_i_make_such_a_map_with_r/,12.0,17.0,,en
1108586,2011-12-18 23:05:56,rstats,How can I make such a map? X-post from /r/statistics,nhmzk,cbrunos,,https://www.reddit.com/r/rstats/comments/nhmzk/how_can_i_make_such_a_map_xpost_from_rstatistics/,7.0,4.0,,en
1108587,2011-12-18 23:39:23,statistics,Negative mean when support is positive? Beckmann Distribution,nho7z,sheppa28,1274041127.0,https://www.reddit.com/r/statistics/comments/nho7z/negative_mean_when_support_is_positive_beckmann/,4.0,4.0,"I was exploring properties of the [Beckmann Distribution](http://reference.wolfram.com/mathematica/ref/BeckmannDistribution.html) on Wolfram|Alpha and then came across [this answer](http://www.wolframalpha.com/input/?i=Mean%5BBeckmannDistribution%5B0%2C0%2C1%2C6%5D%5D) which shows a negative mean when the support is positive, and the distribution is defined to be non-negative. The negative answer agrees with [their formula](http://www.wolframalpha.com/input/?i=Mean%5BBeckmannDistribution%5B0%2C0%2Cs1%2Cs2%5D%5D) for the mean. Is their formula incorrect or am I missing something subtle?
",en
1108588,2011-12-18 23:45:42,datasets,Request: Dataset of duration of unemployment spells,nhogf,cbrunos,,https://www.reddit.com/r/datasets/comments/nhogf/request_dataset_of_duration_of_unemployment_spells/,3.0,2.0,"Hello,

I'm looking for a dataset that has the duration of unemployment spells for each individual in the sample as well as other variables, like age, gender, level of education, etc…My searches have been infructuous for now. Thanks!",en
1108589,2011-12-19 01:43:38,datasets,Request: historical weather datasets,nhsk4,bitethemuffin,,https://www.reddit.com/r/datasets/comments/nhsk4/request_historical_weather_datasets/,8.0,5.0,"Hi r/datasets!

I'm wondering if there are publicly available historical weather datasets out there for some cities (particularly interested in the east coast US/CANADA, but any leads would be greatly appreciated).

Temperature would be the most important variable, but others (sunny/rainy/etc, atmospheric pressure, and so on) would be great to have as well.

Also any good api's out there through which i can get such a dataset?
",en
1108590,2011-12-19 09:15:43,datasets,Open Data for Africa platform has been launched by African Development Bank with a lot of data on Africa in a single place,ni9gc,vbougay,1324277996.0,https://www.reddit.com/r/datasets/comments/ni9gc/open_data_for_africa_platform_has_been_launched/,8.0,0.0,,en
1108591,2011-12-19 11:06:11,statistics,Has anyone here experimented with D3 or other javascript libraries for interactive graphs?,nibwd,Pinatubo,,https://www.reddit.com/r/statistics/comments/nibwd/has_anyone_here_experimented_with_d3_or_other/,3.0,8.0,"In the last couple of weeks I've been experimenting with the D3 javascript library to produce interactive versions of some graphs in a paper I recently wrote.  Has anyone else tried something like this?  If so, what have you been doing, and how did it go?  I'd be happy to elaborate on what I've been doing if people are interested.",en
1108592,2011-12-19 12:42:33,AskStatistics,Is there an intuitive explanation for why ‘accepting’ the null is not the same as evidence for no difference?,nidnf,dainthevta,1318648403.0,https://www.reddit.com/r/AskStatistics/comments/nidnf/is_there_an_intuitive_explanation_for_why/,1.0,8.0,"I’ve recently been asked to provide some stats consultation for people who have no idea about stats (although they should!). I’m looking for whether anyone has a nice intuitive explanation for why you cannot use the null hypothesis to ‘test’ for no difference. To give some context, I have had two (separate) people hypothesising that there would be no difference between groups on some dependant variable and then attempting to use a p&gt;.05 as evidence for this. Any suggested explanations I can use when teaching?

Edit: Thanks heaps everyone. I have been able to take a little from all your wisdom- Cheers!",en
1108593,2011-12-19 16:55:32,statistics,Choosing between many models / model exploration.,niitl,RA_Fisher,1299707119.0,https://www.reddit.com/r/statistics/comments/niitl/choosing_between_many_models_model_exploration/,13.0,3.0,"[This post](http://www.r-bloggers.com/ripley-on-model-selection-and-some-links-on-exploratory-model-analysis/) on r-bloggers pointed to some interesting materials from Hadley Wickham and Brian Ripley, two great statisticians:

[Exploratory Model Analysis](http://had.co.nz/model-vis/2007-jsm.pdf) -- Hadley Wickham

[Selecting Amongst Large Classes of Models](http://www.stats.ox.ac.uk/~ripley/Nelder80.pdf) -- Brian Ripley


Credit: [Statistical Modeling, Causal Inference, and Social Science Blog](http://andrewgelman.com/2011/12/ripley-on-model-selection-and-some-links-on-exploratory-model-analysis/)",en
1108594,2011-12-19 18:28:45,computervision,Segmentation/cropping,nilwd,strategosInfinitum,1279064470.0,https://www.reddit.com/r/computervision/comments/nilwd/segmentationcropping/,1.0,2.0,"In computer vision are image segmentation and cropping discrete from one another is image cropping a form of segmentation, im worried i may have misinterpreted what segmentation is.",en
1108595,2011-12-19 19:11:39,MachineLearning,Tool detects patterns hidden in vast data sets | Broad Institute of MIT and Harvard,ningw,jdw25,1220557205.0,https://www.reddit.com/r/MachineLearning/comments/ningw/tool_detects_patterns_hidden_in_vast_data_sets/,13.0,11.0,,en
1108596,2011-12-20 09:50:40,MachineLearning,Computerized pathology,njmg2,[deleted],,https://www.reddit.com/r/MachineLearning/comments/njmg2/computerized_pathology/,15.0,5.0,,en
1108597,2011-12-20 15:13:37,statistics,I'm no expert in statistics but this got me a little suspicious. Does the title of this article or any of the statements make sense?,njt2a,erikhun,1280500725.0,https://www.reddit.com/r/statistics/comments/njt2a/im_no_expert_in_statistics_but_this_got_me_a/,6.0,13.0,,en
1108598,2011-12-20 18:14:38,statistics,RStudio + BibTeX,njyjl,RA_Fisher,1299707119.0,https://www.reddit.com/r/statistics/comments/njyjl/rstudio_bibtex/,1.0,1.0,"Does anyone here have experience using BibTeX inside of RStudio?

For whatever reason, I whenever use the commend:

\citet{reference_x}

I get something that looks like (author?)[1]

I found an [RStudio discussion here](http://support.rstudio.org/help/discussions/questions/191-calling-an-external-bibtex-file-from-within-rstudio) that indicated that the environmental variables of R needed to be set.  I did indeed set the variables mentioned here to the path that contains my .bib and .bst file.  No luck.

Taking a shot in the dark that someone else out here has had the same problem and solved it.

",en
1108599,2011-12-20 22:48:20,statistics,Maximum Information Coefficient - correlation for the 21st century,nkbaf,bitethemuffin,,https://www.reddit.com/r/statistics/comments/nkbaf/maximum_information_coefficient_correlation_for/,6.0,17.0,,en
1108600,2011-12-21 00:12:13,statistics,"I have a Nx2 table of counts, how can I identify outlier rows?",nkf40,[deleted],,https://www.reddit.com/r/statistics/comments/nkf40/i_have_a_nx2_table_of_counts_how_can_i_identify/,0.0,4.0,"This is for my research, not homework.  I'm feeling pretty dumb for being stuck on this, because it's fairly straightforward, so hopefully you guys can give my brain a kick-start with some ideas.

The gist is:  I have around 2000 text files, and I have counts for commas and semicolons in each one (adjusted by the length of the text file, if you prefer).  They are almost all by the same author, and the goal is to pick out the ones that are *not* by that author.  

To that end, I want to identify the outliers in terms of comma/semicolon use.  Using too many or too few of either ought to be important, as well as using them in a strange proportion to each other.

I've considered a few options, but none of them seem to be getting at what I want.  Any thoughts?

Thanks!",en
1108601,2011-12-21 00:32:08,artificial,N-gram models favor shorter sentences?,nkfyp,learc83,1175899960.0,https://www.reddit.com/r/artificial/comments/nkfyp/ngram_models_favor_shorter_sentences/,9.0,5.0,"I'm playing around with writing an n-gram sentence comparison script. The model heavily favors shorter sentences, any quick suggestions on the best way to weight it more towards longer sentences?",en
1108602,2011-12-21 06:45:51,statistics,generalized linear regression on principal components,nkvai,FailedLifeForm,1299165736.0,https://www.reddit.com/r/statistics/comments/nkvai/generalized_linear_regression_on_principal/,7.0,3.0,"I am following the paper by [Marx et al.(pdf)](http://www.web-e.stat.vt.edu/vining/smith/Principal_Component_Estimation.pdf) to get parameter estimates for a probit model. I can easily get the parameter estimates (using eqn 9), but i am having difficulty getting standard errors on parameter estimates (using eqn that is above eqn 16 gives numbers that are too small).  here is what i am doing: y = binary data, X = predictors. g(y) = X*B where X = U*L*M' (by svd). Let Z = U*L, thus X=Z*M'  and g(.) is the link function. XM = Z, thus g(y) = X*M*M'*B = Z*a, where a = M'*B.    
I can solve for a using Newton-Raphson and I get B back by using B = M*a. how do i get se(B)? 
",en
1108603,2011-12-21 14:42:01,statistics,NEED HELP VERY FAST FINAL IN AN HOUR. CONDITIONAL/EVENTS Your my last hope,nl6yb,Kryvo,1312688585.0,https://www.reddit.com/r/statistics/comments/nl6yb/need_help_very_fast_final_in_an_hour/,1.0,1.0,,en
1108604,2011-12-21 22:11:58,MachineLearning,Anyone know of an online course with video lectures &amp; homework (and sol'ns) on Signal Processing?,nln1w,lpiloto,1310021871.0,https://www.reddit.com/r/MachineLearning/comments/nln1w/anyone_know_of_an_online_course_with_video/,5.0,6.0,"I work on time-series analysis (specifically fMRI) and it seems like the appropriate subject to teach myself.  If you guys have any other suggestions on relevant topics to study, please feel free to add them!",en
1108605,2011-12-22 03:56:24,MachineLearning,Show r/ML: Classifying New Posts into Subreddits,nm1d4,webspiderus,1233104480.0,https://www.reddit.com/r/MachineLearning/comments/nm1d4/show_rml_classifying_new_posts_into_subreddits/,46.0,2.0,"Hi r/ML - I just finished a class in Applied Machine Learning, for which I was required to implement an open-ended project that would use some of the things taught to us during the quarter. Inspired by [this article](http://blog.notdot.net/2010/06/Trying-out-the-new-Prediction-API), I decided to build a system which could learn to predict the intended subreddit for a given post. Although I initially wanted to use dimensionality reduction techniques like PCA or LDA, it turned out that they offered little marginal benefit to warrant the added computational complexity. In the end, I decided to train independent supervised classifiers for the post's title text and its domain, and combined the two probability estimates for each subreddit to arrive at a holistic score for the post. I experimented with several types of classifier models (logistic regression, stochastic gradient descent, and naive bayes), and used the one-vs-all approach  to generalize the first two model types to handle more than two subreddits. Unfortunately, I did not have as much time as I hoped to explore the various avenues of this project (partially because I spent too long playing around with PCA and LDA, partially because I ended up spending more time than I wanted to on other classes), but I still feel I managed to get some decent results (which were comparable with the Prediction API results). 

You can read my [project report](http://dl.dropbox.com/u/700763/229a_project.pdf) for more details about the exact implementation. Although it lacks any real figures because of the class-imposed 5 page limit, you can see some sample results in [this table](http://dl.dropbox.com/u/700763/table.pdf). You can also see the title text classifier features with the highest coefficients for each subreddit in [this table](http://dl.dropbox.com/u/700763/words.pdf), which can be thought of as the strongest indicators that a post should belong in that respective subreddit. Finally, you can see the code that I wrote for this project on [github](https://github.com/smoreinis/classificator), which also contains a data directory with a script to scrape new posts from Reddit (inspired by the same post from Nick Johnson) and some sample data for people to play around with.

Code Miscellany: This code was written in python, and uses the numpy, scipy, simplejson, Stemmer, sklearn packages (and in fact was based on an sklearn tutorial on classifying newsgroup articles). The extract.py file handles the conversion of a JSON file full of posts into a feature dictionary, while models.py handles the actual supervised classification models provided by sklearn (as well as using CV grid search to find optimal parameters, and the CombinedClassifier which provides overall score posts). The gold.py file runs the whole thing - give it json files as arguments, and it should spit out the results for each type of sklearn classifier, which include a confusion matrix, overall F1-score, and a classification report like seen above. Finally, you can use the --topFeatures flag to print the highest coefficients (as seen above), but only if you are classifying on title text only (which is the way the code is set up now). To change it to classify posts based on title text and domain, change default value of combine to True on line 153 in models.py, comment out lines 173 an 178 in gold.py, and uncomment lines 174 and 177 (this is one of those things I'd have refactored to be prettier if I had the time, but hopefully it's not too confusing).

I hope this can be of use to some of you - it seems there are several people trying to build recommendation systems for posts, so maybe this can provide at least a little insight into how much information can be gained from a post's title and domain. Although all posts are currently uniformly weighted when training the supervised classifiers, changing this weighting (to either reflect the post's number of upvotes, or some personalized ranking) could also yield some interesting results and potentially make the model's results even better. Let me know if there are any questions!",en
1108606,2011-12-22 06:36:33,statistics,"I just had a realization that really blew my mind, one that basically gave geometrical shape to probability densities.",nm7nd,bangsecks,1252478802.0,https://www.reddit.com/r/statistics/comments/nm7nd/i_just_had_a_realization_that_really_blew_my_mind/,0.0,5.0,"This is probably old hat to you all, but I'm a stats novice still and just realized out of nowhere that the normal distribution is not only a two dimensional curve but a three dimensional bowl.

The curve is just a cross section of this bowl and it can be inverted upon itself so the peak of the curve is actually the inside vertex of this concave, 3D parabola, and just as the center of a drain is the most probable place for water to flow down into, this deepest point in this bowl, this inner vertex represents the highest probability density region and it pulls, as if by gravity, the most probable event towards it.

I had another related image wherein the curve is actually one forth of a cross section of a toroidal shape.  If the curve were mirrored across the x-axis so you had a sort of flying saucer shape, then you took this and you shifted to the left where normally the center of the distribution is on the y-axis so half the values are positive and half are negative this mirrored curve is shifted to the left so that the upper tail is at the origin.  Then we reflect this whole shape across the y-axis so you have these two flying saucer shapes, four normal curves, that form one piece and this is actually the cross section of this toroidal shape whose inner ""doughnut hole"" area actually describes an inverted 3D distribution so it sort of transfers volume as an expression of probability to a surface? ",en
1108607,2011-12-22 08:32:12,MachineLearning,"This picture is on page v of Christopher Bishop's ""Pattern Recognition and Machine Learning"".",nmbwh,MasCapital,1290915566.0,https://www.reddit.com/r/MachineLearning/comments/nmbwh/this_picture_is_on_page_v_of_christopher_bishops/,0.0,0.0,,en
1108608,2011-12-22 16:36:53,statistics,Paul Meier - Statistician who saved millions of lives,nmmxt,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/nmmxt/paul_meier_statistician_who_saved_millions_of/,30.0,1.0,,en
1108609,2011-12-22 20:53:00,MachineLearning,"This website aims to find out if it is possible to predict hits in the UK singles charts. They've come up with a system for working out which songs are more likely to 'make it' in the charts, and also the features which make up a hit.",nmwq1,urish,1221689900.0,https://www.reddit.com/r/MachineLearning/comments/nmwq1/this_website_aims_to_find_out_if_it_is_possible/,11.0,4.0,,en
1108610,2011-12-22 21:24:15,rstats,"R 2.14.1 is released
",nmy5l,talgalili,1271226645.0,https://www.reddit.com/r/rstats/comments/nmy5l/r_2141_is_released/,2.0,0.0,,en
1108611,2011-12-23 02:24:00,statistics,"How many subreddits does ""Random"" choose from?",nnalu,Flopsey,1319467261.0,https://www.reddit.com/r/statistics/comments/nnalu/how_many_subreddits_does_random_choose_from/,10.0,6.0,"So I posted a question in Theory of Reddit and a very cool redditor came up with this 

http://www.reddit.com/r/TheoryOfReddit/comments/nn6hn/how_random_is_random/c3aevar

&gt;500 hits of r/random resulted in 390 unique subreddits

&gt;Number of Hits -	Subreddits
&gt;
&gt;1 ---------------------	301
&gt;
&gt;2 ----------------------	73
&gt;
&gt;3 ----------------------	12
&gt;
&gt;4 ----------------------	3
&gt;
&gt;5 ----------------------	1

So my question is how many subreddits would Random be choosing from to fit this curve?

If you could list the assumptions you make I'd be interested to learn.",en
1108612,2011-12-23 06:11:23,artificial,Question about graph-based searches,nnj1m,Ace_Of_Spayeds,1307251551.0,https://www.reddit.com/r/artificial/comments/nnj1m/question_about_graphbased_searches/,4.0,4.0,"I just finished the online Stanford AI class, and I've been trying to put the (annoyingly vague) algorithms they taught into practice.  Luckily, Stanford's CS221 site has some python programming assignments up on their [course schedule.](http://www.stanford.edu/class/cs221/schedule.html)

The assignment starts out pretty easy, using BFS,DFS,etc. to help guide pacman to a single piece of food in a maze.  I've been using a graph-based search, using a dictionary to remember which states have been visited.  Since there's only one piece of food, pacman should never need to visit a space twice, so the state space consists of positions.  I just used a dictionary called ""visited"" and did ""visited[state] = True"", and then made sure new expanded nodes weren't in the dictionary before adding them.

However, a later problem involves getting pacman to touch all of the corners of the board, so I wrote a new class ""CornersState"" that holds the position of pacman, and a dictionary that keeps track of which corners have been visited (T/F for each corner).  My problem is that ""visited[state] = True"" no longer works, since it uses the objects location in memory for the dictionary.  This made it so that two identical states, which were created as two different instances of CornersState, will be different keys.  I made a kind of work around by writing a hash function for CornersState that depends only on the position/values in the corners dictionary.  By doing ""visited[hash(state)] = True"" I got the dictionary to work, but I'm not sure if this is a good way to do it.  Any suggestions?

TL;DR is it okay to use a hashed value of an object for dictionary keys, or am I doing something wrong?  ",en
1108613,2011-12-23 07:31:34,statistics,How about a chart that places 8.6% unemployment higher than 8.8%?,nnlvf,efrique,1219734906.0,https://www.reddit.com/r/statistics/comments/nnlvf/how_about_a_chart_that_places_86_unemployment/,13.0,13.0,,en
1108614,2011-12-23 15:31:55,artificial,Idea's for an AI gaming project?,nnwi4,bleeeeghh,1309163424.0,https://www.reddit.com/r/artificial/comments/nnwi4/ideas_for_an_ai_gaming_project/,14.0,14.0,"I need to implement an AI into a game for an university course. First, I tried to make my own game but I noticed that game design is already quite a challenge by itself. So now I'm trying to find some simple games where I can implement an AI, preferably an AI that can be trained with reinforcement learning or genetic algorithms.

Do you guys have any suggestions that I can look into??",en
1108615,2011-12-23 17:03:18,MachineLearning,How important is it to reach a local minimum during training?  ,nnyju,PeoriaJohnson,1254792524.0,https://www.reddit.com/r/MachineLearning/comments/nnyju/how_important_is_it_to_reach_a_local_minimum/,6.0,11.0,"My minimization routine takes a very long time.  Specifically, if I allow it to run until it finds a local minimum to within machine precision, it can require thousands of iterations.  (Not surprisingly, the marginal improvement on each iteration falls steadily.)  Since I have limited CPU-hours to work with, how close to a local minimum should I get?  

This question becomes especially relevant when I repeatedly retrain my neural network while iterating various parameters (regularization; number of features; network topology; size of dataset).  I'd like to determine good choices for each of these parameters.  Will halting minimization early interfere with that?  

(Related: I'm using Octave and have implemented [fmincg.m](http://mlclass.googlecode.com/svn-history/r11/trunk/mlclass-ex5/fmincg.m) as created by the folks running the Stanford ML Class.  Suggestions for better minimization algorithms, particularly those with more documentation or with an adjustable learning rate, are welcome.)  ",en
1108616,2011-12-23 17:07:19,statistics,Nassim Talel's Black Swan Theory - comments for the beginner?,nnyny,jherazob,1145462835.0,https://www.reddit.com/r/statistics/comments/nnyny/nassim_talels_black_swan_theory_comments_for_the/,14.0,20.0,"I'm mostly a beginner when it comes to statistics (most of what i know i've hastily learned for the recent Stanford AI and ML Classes), but then i came across this guy and his outright hate for anything gaussian. So i wanted to get some input from people who, unlike me, actually know what they're doing, and have considered the subject. 

I've found references to [an American Staticician issue](http://pubs.amstat.org/toc/tas/61/3) that tackled this, but it's behind a paywall. Plus it's probably for a more learned audience anyway.

What are the general thoughts of the statistics community on this? ",en
1108617,2011-12-24 16:23:41,datasets,"Data files on 407 banks, between the dates of 2007 to 2009, on the daily borrowing with the US Federal Reserve bank.",np54i,talgalili,1271226645.0,https://www.reddit.com/r/datasets/comments/np54i/data_files_on_407_banks_between_the_dates_of_2007/,15.0,0.0,,en
1108618,2011-12-24 17:46:31,statistics,Why use (x-X)^2 instead of abs(x-X) everywhere in statistics?,np6vi,panicker,1299532820.0,https://www.reddit.com/r/statistics/comments/np6vi/why_use_xx2_instead_of_absxx_everywhere_in/,19.0,16.0,Also why sqrt(x^2 + y^2 ...)/N instead of (x+y+..)/N,en
1108619,2011-12-24 21:27:45,statistics,"Okay Reddit, if the Central Limit Theorem doesn't grant me cake day karma... then I'll know I've been lurking in the wrong place this last year.",npdjg,TaekJinChang,1293144201.0,https://www.reddit.com/r/statistics/comments/npdjg/okay_reddit_if_the_central_limit_theorem_doesnt/,0.0,5.0,,en
1108620,2011-12-25 02:08:39,datasets,RickRolled by a dataset,npmrs,pomber,1300156715.0,https://www.reddit.com/r/datasets/comments/npmrs/rickrolled_by_a_dataset/,14.0,0.0,,en
1108621,2011-12-25 16:28:18,statistics,Design effects (or variances) of sampling methods?,nq615,[deleted],,https://www.reddit.com/r/statistics/comments/nq615/design_effects_or_variances_of_sampling_methods/,2.0,1.0,Where I can find comparison of variances (or design effect) of different sampling methods?,en
1108622,2011-12-25 22:24:10,MachineLearning,"What is the prerequisite knowledge to *really* ""get"" conditional random fields, HMMs, etc?",nqfi0,nopesocks,1324844512.0,https://www.reddit.com/r/MachineLearning/comments/nqfi0/what_is_the_prerequisite_knowledge_to_really_get/,18.0,4.0,"I do work in computer vision, but I have avoided these topics for a while since the Bayesian probability theory doesn't always seem to make sense to me in the context of computer vision.  

I want to understand it right down to the theory.  I'm going to have some off time over the holidays that I will do some reading in.  If I want this stuff to be entirely concrete to me, what should I be reading?  It doesn't seem like starting right from the initial CRF paper is the right approach :)",en
1108623,2011-12-26 09:01:53,statistics,Grad school in biostats?,nqyk4,hardonstar,1317520748.0,https://www.reddit.com/r/statistics/comments/nqyk4/grad_school_in_biostats/,9.0,13.0,"I recently took a biostats course and thoroughly enjoyed the material (a gross understatement). I've been thinking that this may be worth something pursing for grad school.

Bad news: I'm only a stat minor and I'm graduating. Did I mention I already found a job?

So my dear redditors, if this is something I'm really interested in:

a) What do I need to do in order to apply for grad school programs in a few years? 

b) What are biostats programs looking for in terms of candidates?",en
1108624,2011-12-26 16:59:32,datasets,Fed Once-Secret Loan Crisis Data Compiled by Bloomberg Released to Public - Bloomberg,nr6ec,bitethemuffin,,https://www.reddit.com/r/datasets/comments/nr6ec/fed_oncesecret_loan_crisis_data_compiled_by/,14.0,0.0,,en
1108625,2011-12-26 21:40:55,statistics,Help with LMMs and GLMMs,nre23,[deleted],,https://www.reddit.com/r/statistics/comments/nre23/help_with_lmms_and_glmms/,1.0,0.0,"I have a surface-level knowledge of mixed models, I'm looking to develop a stronger understanding of both. Are there any resources out there that do a good job of explaining these techniques to non-statisticians? It would be extra helpful if these sources are related to spatial and temporal autocorrelation in biological data. I've read much of Ben Bolker's stuff, but looking to take a step back -- as in, I don't quite have the necessary background to understand a lot of his key points.",en
1108626,2011-12-26 22:57:14,statistics,Signal Processing - What terms am I looking for here?,nrglg,Bitruder,1145203871.0,https://www.reddit.com/r/statistics/comments/nrglg/signal_processing_what_terms_am_i_looking_for_here/,6.0,9.0,"I'm working on a project that requires taking vectors of values with length around 25 [2,1,2,1,1,....0] and I'm trying to determine the amount of ""information"" or ""signal"" there is in that vector.  Sorry for being vague but I think this is the simplest way to explain the problem without going into the gory details and I would like to keep it abstract (it's a neuroscience project).

For example, lets say our vectors are of length 5 for simplicity and we have the following two:
[1,0,0,0,0] and [2,1,1,1,1].  Instinctively, I'm thinking that there is more information in the first one than the second one and I feel like this is related to the Fischer information available.  But what else can I use to figure out the amount of information available?

The ""signal"" I'm trying to pull out is whether or not a particular vector element is being represented or not and we could know both the value of the whole vector and even the variance for each vector element.  So if you have a vector [1,0.2,0,0,0] and you receive the value 1 then you can say with some confidence that the first element is being represented and a little less confidence that maybe it's the second value.  If you receive the value 0.1 then you can say with high confidence that it's not the value 1 (we don't care if it's the 4th or the 5th here, just that it's not the highest vector element).

Is this a problem for Bayesian statistics? Some kind of fourier analysis?  Sorry, I'm half familiar with these things but I don't know the correct path to start pursuing. I would like to take a vector [1,0,0,0,0] and quantify it with a scalar value for ""information"" or ""signal to noise"" or something like that.

Currently my field is defining [1,0,0,0,0] and [3,1,1,1,1] as ""equivalent"" classification of the vectors but I feel like the second one is much ""stronger"" in terms of the information it can convey.

Thanks",en
1108627,2011-12-27 08:44:00,MachineLearning,Opinion on most complete ML resource?,ns0yh,ants_rock,1306692040.0,https://www.reddit.com/r/MachineLearning/comments/ns0yh/opinion_on_most_complete_ml_resource/,5.0,9.0,"Hello friends. I am getting into the field of bioinformatics, after having pursued a degree in biomolecular engineering and biochemistry. Since I am not a CS student and still work in the field of biomolecular engineering, I do not have a significant amount of time to dump into learning machine learning, even though I think it is an amazing field. If you could recommend one resource/book, what would it be? I understand that there's no one resource to learn about all of the field.  ",en
1108628,2011-12-27 17:29:05,AskStatistics,Survey sampling: What is the difference between weighting under representated populations after vs. over sampling them before.,nsc0g,[deleted],,https://www.reddit.com/r/AskStatistics/comments/nsc0g/survey_sampling_what_is_the_difference_between/,2.0,3.0,"Assuming large n's in both cases:

When pulling a sample to be survey, assuming we know the response rates, why is it more adventagious to draw a larger sample of a hard to reach population, rather than just weighting the responses to be representative of the population after you collect the data.",en
1108629,2011-12-27 21:35:30,statistics,"HELP WITH STATA, PLEASEEEEE",nskwn,[deleted],,https://www.reddit.com/r/statistics/comments/nskwn/help_with_stata_pleaseeeee/,0.0,2.0,"Hey all, 

I am a research assistant at my university running some do files that my professor gave me. I am not too experienced with stata/programming in general so bear with me. This do file I am running first has the cd established in another location(not right for my computer), which I then modified for my computer. However, all estimates related commands (esttab, estpost, eststo, etc.) still don't work. My professor told me to ""put the code and log files in the Dropbox data directory in a condition where it can run."" I don't understand how to do this. Can someone enlighten me please!!!!!!!!!!!!!!!!!!!!!!!! I asked him for more clarification and he has yet to write me back (holidays). I need to get this done by tomorrow, so any help is appreciated. ",en
1108630,2011-12-27 22:23:26,datasets,Request: Vegetables and their geographical location.,nsn1u,fit_napper,1313208691.0,https://www.reddit.com/r/datasets/comments/nsn1u/request_vegetables_and_their_geographical_location/,0.0,2.0,"Just a few items I've come across that are somewhat close.
http://fera.defra.gov.uk/plants/plantVarieties/nationalListing/
http://en.wikipedia.org/wiki/List_of_culinary_vegetables
http://en.wikipedia.org/wiki/List_of_culinary_fruits",en
1108631,2011-12-27 23:09:17,rstats,Is there a Minimum/Least Absolute Error Linear Regression package?,nsp1f,randombozo,1266016617.0,https://www.reddit.com/r/rstats/comments/nsp1f/is_there_a_minimumleast_absolute_error_linear/,2.0,4.0,,en
1108632,2011-12-28 19:04:15,statistics,"Help with t test. One-tailed, two-tailed, paired, etc.",ntqm8,woodyallin,1301106545.0,https://www.reddit.com/r/statistics/comments/ntqm8/help_with_t_test_onetailed_twotailed_paired_etc/,1.0,4.0,"First off this is not homework.

So I'm an undergraduate researcher and I need to implement statistics in my research (big surprise there). I'm going to take a stat course next semester, but I already downloaded my textbook and it supplies minimal information on the different types of t tests and when it is appropriate to use them. 

I've tried online resources but they just confuse me with examples, I basically need an example free explanation on the differences between
one-tailed, two-tailed, and the other types (paired, homogeneous, heterogeneous)  

It would really mean a lot to me if someone out there would help me straighten out these issues.

Thank you so much!",en
1108633,2011-12-28 23:21:38,MachineLearning,Has anyone used Microsoft's data mining tools?,nu1kt,yellowyn,1324870269.0,https://www.reddit.com/r/MachineLearning/comments/nu1kt/has_anyone_used_microsofts_data_mining_tools/,13.0,12.0,"SQL Server ships with Analysis services. Within AS there's algorithms for dtrees, clustering, NB, regressions and more. It also looks like there's plugins for SVMs and parallelization. Does anyone have experience with this? Are there any resources for learning to use AS that you'd recommend? I'm somewhat hopeful about it, as SQL Server is quite good. ",en
1108634,2011-12-29 00:10:30,statistics,Any experiance with banner software? ,nu3k4,[deleted],,https://www.reddit.com/r/statistics/comments/nu3k4/any_experiance_with_banner_software/,2.0,6.0,"I've been tasked to help create 1,000 reports. They are just the same simple descriptive statistics for each report, but there are 1,000 of them. I am looking for a way to create them with out breaking out the slave labor. All the reports will draw from the same very big database, with one variable used to differentiate them.

My first thought would be to do this is SPSS, which is my default software of choice. But I think trying to do Custom Tables or a Split files might overload my SPSS outputs window. Plus, it would also involve alot of formating outside of SPSS to make the outputs look like *real* reports.


Is there a possible R-package or another cheap alternative to pump out that scale outputs in a standard format?


Edit: Thank you all for the great suggestions, I will be checking them all out. ",en
1108635,2011-12-29 17:55:14,computervision,Ideas for detecting oriented cards in still images.,nuzxc,brandf,1191037129.0,https://www.reddit.com/r/computervision/comments/nuzxc/ideas_for_detecting_oriented_cards_in_still_images/,8.0,9.0,"I'm new to CV, but have plenty of CG experience.  Links to research papers would be helpful.  I know this is a common problem, but I haven't had luck finding algorithms since I'm not sure what the term is for what I'm trying to do.

I'm working on a project to learn a bit about CV, so I want to write it all myself (i.e. no OpenCV, I'm looking for the algorithms).  I'm making a mobile app to detect Magic the Gathering cards and take you to information like rulings/pricing/etc.

I've created a database of known cards and am able to match reasonably accurately if the user manually aligns the picture. The next step is to automatically detect/align the cards so it's less of a hassle for the user.

The cards have a strong black or white border, so I'm thinking something based on edge detection and shape recognition would work.  If I could detect the corners, I can easily transform the image to align with what I'm expecting.

Can you guys help point me in the right direction for research on this topic?  Thanks.

[edit] btw, this is what the cards looks like: http://gatherer.wizards.com/Pages/Card/Details.aspx?action=random",en
1108636,2011-12-29 19:11:56,AskStatistics,Question regarding the statistical implications in terms of my questionnaire.,nv2tm,[deleted],,https://www.reddit.com/r/AskStatistics/comments/nv2tm/question_regarding_the_statistical_implications/,1.0,0.0,"Hello all :)

Sorry for bothering, but I am struggling to find any source that can help me with this question so I will try here.

In short I am making a paper whether people will choose company A over company B in an industry of uncertainty depending on certain variables. Then I will check if consumers will choose company A over company B in an industry of certainty using the same variables. Then I will check for differences. 

However, I feel that the questionnaire is getting too long. And I cannot really make it any shorter. But I was thinking that I could split the sample into two. So one group answers questions about their decision under certainty. And then another group answers the questions about their decision under uncertainty. 

But I cannot find any readings on the implications doing this has for my analysis. Are there any statistical limitations/implications on this approach (i.e. splitting it up into two groups?). Will doing this mess up my analysis?

Anybody able to help or put me on the right track for some sources on this? 

Any help appreciated :)


EDIT: Sorry for my horrible heading!
",en
1108637,2011-12-29 21:07:56,MachineLearning,Benchmarking time series models,nv7p0,pandemik,1240356623.0,https://www.reddit.com/r/MachineLearning/comments/nv7p0/benchmarking_time_series_models/,12.0,0.0,,en
1108638,2011-12-30 02:21:34,statistics,Trouble with Dataset in Stata,nvk71,adizzan5220,1313616651.0,https://www.reddit.com/r/statistics/comments/nvk71/trouble_with_dataset_in_stata/,5.0,7.0,"Hey r/statistics,

I was wondering if you guys could help me with this dataset that I am working for for a professor at my University. The dataset is in 23 csv files. The format is pipe delimited (vertical bar). I successful put the files through stata and removed the pipe delimited characters from all of the files and was able to merge them into one big master dataset. However, there is still one problem. Some of the files have string variables and some of the inputs have more than 244 string characters. Stata has a 244 string limit and it chops off the characters after 244. Is there a way to get around this limit?",en
1108639,2011-12-30 16:40:59,MachineLearning,Is JINR still alive?,nw9u1,spazzm,,https://www.reddit.com/r/MachineLearning/comments/nw9u1/is_jinr_still_alive/,4.0,1.0,"Is the [Journal of Interesting Negative Results in Natural Language Processing and Machine Learning](http://jinr.org/) still alive? I think the idea is sound (that negative results should be published so that others don't waste their time), but regrettably I see that the latest and so far only issue is over three years old. 

Is there any point submitting articles? 

Where else should negative results be submitted?

Perhaps negative results should not be published since there are effectively an infinite number of ML algorithms?",en
1108640,2011-12-30 21:21:26,statistics,Error propagation - how should I do this?,nwk2n,forever_erratic,1256947469.0,https://www.reddit.com/r/statistics/comments/nwk2n/error_propagation_how_should_i_do_this/,4.0,11.0,"I've got a measured mean +- SD. I'd like to ~~do a linear transformation~~ multiply my measured value by a constant because the more interesting way to see this data is after this multiplication.

My question is how I can transform the SD. Do I just ~~make the linear adjustment to the~~ multiply my SD by the constant, the same way I do to the mean, or does anything else have to be done?

Thanks!",en
1108641,2011-12-31 01:13:38,datasets,Mammal dataset from Weecology,nwtph,talgalili,1271226645.0,https://www.reddit.com/r/datasets/comments/nwtph/mammal_dataset_from_weecology/,2.0,0.0,,en
1108642,2011-12-31 02:08:13,statistics,I'm trying to make the argument that San Diego County had a statistically higher level voting for marijuana legalization compared to similar counties.  Could use some help.  Can pony up 1 month of reddit gold to best answer.,nwvtl,Cantholditdown,1286736178.0,https://www.reddit.com/r/statistics/comments/nwvtl/im_trying_to_make_the_argument_that_san_diego/,4.0,20.0,"Hey Guys here is the data I am looking at.  Should drop into excel pretty well.  I would like to show how prop 19 received a statistically higher percent of the vote for marijuana legalization by using the amount of people that voted for jerry brown as a measure of how conservative the county is.  Here are 5 counties with similar conservative profiles and their prop 19 voting.  I would like to do something more than just a ratio.  Even though it does get the point across it's not exactly scientific.

	Jerry Brown % Vote	Yes on 19 Votes	Population	Ratio

Fresno	42.8	35.8	930,450	1.195530726

Riverside	42.7	41.9	2,189,641	1.019093079

San Bernardino	45.4	41.2	2,035,210	1.101941748

San Diego	44.1	46.9	3,095,313	0.940298507

Ventura	45.4	44.8	823,318	1.013392857
",en
1108643,2011-12-31 02:09:11,statistics,"Kind of a broad question, but what are some applications of abstract algebra and group theory that pertain to statistics?",nwvux,mathsuu,1197264006.0,https://www.reddit.com/r/statistics/comments/nwvux/kind_of_a_broad_question_but_what_are_some/,8.0,6.0,,en
1108644,2011-12-31 11:45:39,AskStatistics,Principal component analysis versus Canonical analysis,nxe72,slypsy,1249806998.0,https://www.reddit.com/r/AskStatistics/comments/nxe72/principal_component_analysis_versus_canonical/,2.0,8.0,"What are the relative strengths and weaknesses of each and are there any general rules as to when to use one over the other?

Thanks!",en
1108645,2011-12-31 18:45:20,MachineLearning,How can I find clusters of people with similar likes in this Facebook dataset?,nxlwy,[deleted],,https://www.reddit.com/r/MachineLearning/comments/nxlwy/how_can_i_find_clusters_of_people_with_similar/,0.0,1.0,"Hi r/ML,

I've downloaded all of my friends' likes through the Facebook graph api, and have a sparse matrix of friends and likes like so:

* friend1: like1, like2
* friend2: like2
* friend3: like1, like 3

etc.

I'd like to use this data set to learn about clustering, could you suggest some ways to find groups of similar friends and items, and any other interesting algorithms to run on this data?

Thanks!

",en
1108646,2011-12-31 20:16:16,statistics,Best error propagation method?,nxoyk,[deleted],,https://www.reddit.com/r/statistics/comments/nxoyk/best_error_propagation_method/,2.0,3.0,"I have a bivariate experimental dataset and some model y=f(x,k,c ..) (where y,x are measured, k,c etc are unknown constants). I wish to determine the error/uncertainty in the dependent variable, y.

Which method (if either) is ""best"" and why?

a) Use the repeated measurements of the dependent variable (y) to calculate the standard error of the mean for each y.

or

b) Do a regression of the data to determine the constants in the model (and their least squares error) and then find the error in y using \sigma_{y}^2 = (\frac{\partial y}{\partial x} \sigma_x)^2 + ... 


Hope that's clear. Thanks.",en
1108647,2011-12-31 22:40:57,AskStatistics,An eponymous bias?  ,nxui0,[deleted],,https://www.reddit.com/r/AskStatistics/comments/nxui0/an_eponymous_bias/,1.0,0.0,"Suppose optimal conditions are created for some biological process to occur, and various substances are tested for their effect on that process.  If the original conditions are truly ""optimal"", then all of the test substances will have either no effect, or a deleterious effect.

If there a general name for this type of bias?  I am writing a review article in which a name for this bias would be useful when describing a large body of published work.  Wikipedia has a great list of cognitive biases (http://en.wikipedia.org/wiki/List_of_biases), but I don't recognize what I am looking for in the list.",en
1108648,2012-01-01 01:03:43,statistics,An eponymous bias?,ny025,[deleted],,https://www.reddit.com/r/statistics/comments/ny025/an_eponymous_bias/,1.0,8.0,"Suppose optimal conditions are created for some biological process to occur, and various substances are tested for their effect on that process. If the original conditions are truly ""optimal"", then all of the test substances will have either no effect, or a deleterious effect.

If there a general name for this type of bias? I am writing a review article in which a name for this bias would be useful when describing a large body of published work. Wikipedia has a great list of cognitive biases (http://en.wikipedia.org/wiki/List_of_biases), but I don't recognize what I am looking for in the list.
",en
1108649,2012-01-01 19:28:05,statistics,Top 20 R posts of 2011 (from R-bloggers),nyozw,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/nyozw/top_20_r_posts_of_2011_from_rbloggers/,21.0,0.0,,en
1108650,2012-01-02 11:22:48,AskStatistics,A question on time series data.,nzmm5,slypsy,1249806998.0,https://www.reddit.com/r/AskStatistics/comments/nzmm5/a_question_on_time_series_data/,3.0,7.0,"So the data is various disease infection rates by country. For each country for example, there are data on 21 different disease incidences (per 100,000). Data is generally available for 2006, 2007, 2008 and 2009 as a single number showing incidence of a disease for that year in that country.

I am used to experimental data with replicates, but here clearly there are no reps. Lack of reps means it is not possible to analyse whether any change is significant. Nor is it possible to do a regression. Is it possible to do any stats of use on such data?

Differences exist in data gathering between countries so it is not possible to say that observed differences are due to different rates of disease or whether they are due to sampling differences. Is there anyway of investigating/analysing this?",en
1108651,2012-01-02 17:42:00,statistics,How else will I be able to use my bachelors degree in Mass Communication if I pursue an MA in Stat?,nzu65,[deleted],,https://www.reddit.com/r/statistics/comments/nzu65/how_else_will_i_be_able_to_use_my_bachelors/,2.0,5.0,,en
1108652,2012-01-02 17:56:40,artificial,How do I pick a good representation for a board game tactic for a genetic algorithm?,nzulm,vinnl,1213866877.0,https://www.reddit.com/r/artificial/comments/nzulm/how_do_i_pick_a_good_representation_for_a_board/,2.0,2.0,"Hi Reddit,

I've been looking all over (perhaps my Google-fu is running out), but I can't find a good guide on actually doing this. Hopefully you guys can help me out.

For my bachelor's thesis I want to write a genetic algorithm that learns to play the game of Stratego (if you don't know this game, it's probably safe to assume I said chess). I haven't ever before done *actual* AI projects, so it's an eye-opener to see how little I actually know of implementing things.

The thing I'm stuck with is coming up with a good representation for an actual strategy. I'm probably making some thinking error, but some problems I encounter:

 - I don't assume you would have a representation containing a lot of transitions between board positions, since that would just be bruteforcing it, right?
 - What could branches of a decision tree look like? Any representation I come up with don't have interchangeable branches...
 - If I were to use a bit string, which is apparently also common, what would the bits represent?

I think I ought to know these things after three+ years of study, so I feel pretty stupid. Still, any help or tips on what to Google would be appreciated!",en
1108653,2012-01-03 03:05:01,AskStatistics,Question about planned comparisons in ANOVA,o0gug,thestalebread,1324191082.0,https://www.reddit.com/r/AskStatistics/comments/o0gug/question_about_planned_comparisons_in_anova/,0.0,6.0,"When an analysis of variance leads me to reject the null hypothesis, what does this tell me about *which particular* population means differ?",en
1108654,2012-01-03 03:54:33,AskStatistics,Statistics: Repeated-measures ANOVA,o0itb,thestalebread,1324191082.0,https://www.reddit.com/r/AskStatistics/comments/o0itb/statistics_repeatedmeasures_anova/,0.0,2.0,"I am trying to learn repeated-measures ANOVA (analysis of variance) in statistics for psychology. And I have questions that I need to answer for the class, but I need a greater understanding before I can do the computations. So any help you can provide for me will be greatly appreciated.

Things that I am not too sure about (the wording in the textbook is to superfluous, and confusing me):

* 1) Will a repeated-measures ANOVA (relative to a one-way ANOVA) be more likely to increase power to reject the null hypothesis when systematic differences between subjects emerge across time? **OR** is it when systematic differences between subjects **do not** emerge across time?)

* 2) Is it wrong to say that the error term in a repeated-measures ANOVA can be considered an interaction? I am pretty sure this is right but not sure how to explain why.

* 3) What is partial about r^2 *partial*? And why does it even make sense to compute effect size in this way for repeated-measures ANOVA?

* 4) If I find that, in a repeated-measures ANOVA, there was evidence that the assumption of compound symmetry was not met. What should I do?

Things that I gathered from reading the textbook and lecture slides:
    once again the wording for this class is not very clear and way too confusing - I am sure to get some of these wrong, so please correct me if I am.

* 1. The magnitude of the variance due to individual differences must be measured and subtracted out of the variance between treatments in the numerator of the *F*-ratio for a repeated-measures ANOVA.
* 2. A basic assumption required of repeated-measures ANOVA is that all observations are independent of one another.
* 3. A basic assumption required of repeated-measures ANOVA is the assumption of homogeneity of variances across experimental conditions.
* 4. A basic assumption required of repeated-measures ANOVA is that the population distribution of individual scores within each treatment is distributed normally (or at least that the population distribution of sampling means within each treatment is distributed normally).
* 5. A basic assumption required of repeated-measures ANOVA is the assumption of compound symmetry, which states that not only the variances are homogenous across experimental conditions but that the covariances between scores are homogenous across experimental conditions
* 6. A repeated-measures study uses a sample of n = 10 participants to evaluate the mean differences among three treatment conditions. The analysis of variance for this study will have dftotal = 9.
* 7. After completing a repeated-measures ANOVA with at least three time points, you should always use the pooled error term when comparing two specific means.
* 8. For a repeated-measures research study comparing 2 treatment conditions with a sample of n = 8 subjects, the F-ratio would have df = 1, 7.
",en
1108655,2012-01-03 09:12:32,statistics,"Stepwise OLS regression yields significant results, but Pearson's R is not significant for some of the independent variables.",o0umz,Ultimateamp,1153970720.0,https://www.reddit.com/r/statistics/comments/o0umz/stepwise_ols_regression_yields_significant/,1.0,0.0,"I'm curious how I should interpret this.  Should correlation be calculated first, and then only if there is a significant correlation, include the variables in regression? Or is it acceptable to use the variables either way?  In the ANOVA table, the F statistics are all significant, and the regression coefficients are all significant.  I found this to be somewhat of a surprise!",en
1108656,2012-01-03 14:09:40,statistics,Guys we need to amp this page up...,o10kh,splishsplashsplish,1311143787.0,https://www.reddit.com/r/statistics/comments/o10kh/guys_we_need_to_amp_this_page_up/,0.0,8.0,,en
1108657,2012-01-03 15:16:27,statistics,What does it mean when you transform continuous variables with their correlation matrix and get negative observations?,o11z0,[deleted],,https://www.reddit.com/r/statistics/comments/o11z0/what_does_it_mean_when_you_transform_continuous/,10.0,2.0,"A problem in comparative biology is the fact that species are related to each other in terms of their evolutionary history. If there's a so-called phylogenetic signal, closely related species display similar ecological characteristics, and the data you're making a regression on suffers from dependence. 
 
To rectify this error, one derives a correlation matrix from a phylogeny (a branching diagram of evolutionary relationships inferred from molecular evidence) and transforms the variables by multiplying the square root of the correlation matrix with the variable in question.  
 
My question, then, is: if you have a continuous variable, like body weight etc., and your transformed data point is negative, what does that actually mean? Obviously there's no such thing as negative body mass, so I'm interested in what this means for the question you're trying to answer by fitting your linear model. 

Thanks a lot.",en
1108658,2012-01-03 18:39:28,statistics,"Could someone ELI5 the recent paper ""Detecting Novel Associations in Large Data Sets""?",o185y,fragileMystic,1260042886.0,https://www.reddit.com/r/statistics/comments/o185y/could_someone_eli5_the_recent_paper_detecting/,16.0,10.0,"I'm talking about [this paper](http://www.sciencemag.org/content/334/6062/1518.full).

(You can also download it from [here](http://www.mediafire.com/?4xc57v4jfmdc1k3).)

Basically, they introduce a new measure of association for two variables, the Maximal Information Coefficient (MIC), which captures all types of relationships -- linear, quadratic, sinusoidal, smiley-face-shaped, etc.  I don't understand how it works though, could someone help explain it to me?  (ELI5 = explain like I'm 5)",en
1108659,2012-01-03 19:06:45,computervision,How I built a neural network controlled self-driving (RC) car!,o198h,cavedave,1128052800.0,https://www.reddit.com/r/computervision/comments/o198h/how_i_built_a_neural_network_controlled/,24.0,3.0,,en
1108660,2012-01-03 20:34:07,statistics,Stata version differences?,o1cuj,Bigbrass,1310437241.0,https://www.reddit.com/r/statistics/comments/o1cuj/stata_version_differences/,5.0,3.0,"I've been hitting the books on Stata version 11, and just recently noticed version 12 is the latest.

Am I shooting myself in the foot?  Or are the changes primarily cosmetic and the underlying concepts consistent between versions?

Thanks!",en
1108661,2012-01-03 22:10:46,MachineLearning,Clever and unintentional ways to overfit a data set,o1h2a,rrenaud,1150739310.0,https://www.reddit.com/r/MachineLearning/comments/o1h2a/clever_and_unintentional_ways_to_overfit_a_data/,46.0,0.0,,en
1108662,2012-01-04 04:15:34,statistics,BIC with polynomial regression,o1wvc,ATG77,1287029546.0,https://www.reddit.com/r/statistics/comments/o1wvc/bic_with_polynomial_regression/,2.0,5.0,"I am using MATLAB. Data mining some decision task data, and as a method of grouping tasks together, I am looking at their polynomial regression degree of best fit (is the pattern linear? What about quadratic? etc.). I used BIC to establish the ""best"" polynomial for each task. As the likelihood function, I used error variance (which I found suggested on Wikipedia [here](http://upload.wikimedia.org/wikipedia/en/math/e/3/c/e3c0db074d1cfe213bedd23790f6c2cb.png)) with the number of free parameters equal to the degree of polynomial plus one (for the intercept); a linear  regression has 2 free parameters, and a quadratic has 3 free parameters, etc. 

Results: My BIC are all negative, and the most negative BIC is consistent with a prediction I might make about each dataset if I were to eyeball  the data to see which polynomial regression was 'best'. 

My question is, did I mess up somewhere along the line with the connection between polynomial regression and BIC? Does anyone else have experience data mining in this manner? I don't want to go continuing in my assessment of the data or god-forbid publishing it if I've made some serious error in violating an assumption or something.

Plus I just don't trust Wikipedia completely.  I thought I'd ask about the validity of their BIC equations somewhere I feel a bit more comfortable about the level of expertise [i.e. here in r/statistics]. Also, I'll share my MATLAB function (which calculates the BIC from linear to the 5th order) if anyone wants to see it/use it.

Thanks!",en
1108663,2012-01-04 15:42:18,statistics,Testing for overlap,o2h8u,[deleted],,https://www.reddit.com/r/statistics/comments/o2h8u/testing_for_overlap/,1.0,0.0,"So here is my problem:  
Lets say I have a large set of conditions where genes are expressed (example shown below). Now I want to test if the overlap of 2 specific genes in all these conditions is significant.  
numbers that I have are:  
Total number of conditions, number of occurrences for geneX, number of occurrences of geneY and the number of occurrences containing both gene X and Y. If someone could tell me what test or sort of tests I should look for it would be greatly appreciated.  
  
Example:  
Condition A: gene1, gene6, gene19, gene40, gene50  
Condition B: gene4, gene8, gene19, gene28, gene29, gene36  
Condition C: gene5, gene6, gene39, gene40  
(about 10k of these sets each containing between roughly 1 and 1k genes)",en
1108664,2012-01-04 15:48:47,datasets,"Raw world, business, and tech news.",o2heg,autoencoder,1313227511.0,https://www.reddit.com/r/datasets/comments/o2heg/raw_world_business_and_tech_news/,5.0,2.0,,en
1108665,2012-01-04 18:47:24,statistics,Statistcs advice,o2nfn,tiramisu7,1325695336.0,https://www.reddit.com/r/statistics/comments/o2nfn/statistcs_advice/,2.0,3.0,"Hello,

I have 6 independent groups and one dependent variable. I did a univariate ANOVA analysis and the overall group significance was not significant. Is it okay to compare individual groups with the control group and do a series of unpaired t-tests? Do I need to do a Bonferroni correction? If so, how do I go about doing that in Spss?

Thank you in advance for any advice you can provide!",en
1108666,2012-01-04 19:54:05,statistics,Animation: trend&amp;variation for dummies ,o2q7f,gogolsnose,1325095258.0,https://www.reddit.com/r/statistics/comments/o2q7f/animation_trendvariation_for_dummies/,17.0,0.0,,en
1108667,2012-01-04 21:05:55,MachineLearning,Decision Tree implementation,o2ti2,rylko,1316880581.0,https://www.reddit.com/r/MachineLearning/comments/o2ti2/decision_tree_implementation/,16.0,20.0,"Hi,
I'm looking for implementation of Decision Tree algorithm which is

* very scalable, 
* supports classification / regression, 
* customizable (for example selection of masure - entropy based / Chi-square Statistic / ...),
* in C++ / Java
* open source &amp; free

It will be used on supercomputer with very large data.

Now I'm observing

* [OpenDT](http://opendt.sourceforge.net/)
* [OpenCV](http://opencv.willowgarage.com/documentation/cpp/decision_trees.html)

Can You suggest any other implementation of DT algorithm? (No Mahout/Hadoop and idally with some references / real-word use cases.)


EDIT:

Supervisor about size of data: ""For massive datasets, we remark that our basic requirement is to efficiently handle datasets of at least hundreds of thousands patterns in a higher than 10-dimension feature space. In other words we must be able to ingest data files higher than hundreds of MB (TB is the final goal when survey projects will prompt observed data).""",en
1108668,2012-01-04 22:58:04,statistics,"I don't know where to start, please point me in the right direction",o2yq5,mungk,1187810968.0,https://www.reddit.com/r/statistics/comments/o2yq5/i_dont_know_where_to_start_please_point_me_in_the/,1.0,1.0,"Hi all. I have a question which I hope I'm sending to the correct place. I'm also hoping the answer is not above my head as my only experience with statistics is a single class in college.

If any of you are familiar with [Magic: the Gathering](http://en.wikipedia.org/wiki/Magic_the_gathering), it might help you understand what I'm doing here. If not, I'll try to explain anyway.

In MtG, players can combine any cards they want in to a deck. The minimum is 60 cards for a deck and almost everyone plays no more than 60. However, there are almost 12,000 different cards to choose from. Additionally, you may not have more than 4 copies of any individual card in the same deck (except certain types of cards known as ""basic lands"".)

When these decks get made, there are certain types of decks that emerge that are very common (mostly because they are generally better than the rest). These decks get named and they get copied and played by lots of people. They generally have mostly the same cards in them but there can be a decent amount of variation between them. 

**I have access to a huge list of decks, the cards in them, and the type or classification of what kind of deck it is. My problem: given a new, unknown deck and its list of cards, how can I take what I know about thousands of existing decks and apply that knowledge to determine what category or archetype the new, unknown deck would best be described as?**

I don't mind trying to learn the math that I need, but I'm having a hard time even identifying what I should be learning. I'm looked at Bayes theory, Dempster–Shafer theory, rought set theory, and some others. Can someone provide me some guidance on what types of theories or formulas I should be investigating? 

As some background, I have a computer science degree and am a computer programmer. This work will be implemented as a computer program. If you feel this is something that is simply beyond me and my statistical experience, please let me know. If not, I'd love to get some arrows points me in the right direction.

Thanks! ",en
1108669,2012-01-05 01:36:46,rstats,Help with bootstrapping,o3675,theindianguy,1319088621.0,https://www.reddit.com/r/rstats/comments/o3675/help_with_bootstrapping/,4.0,4.0,I am a noob at R. i want to conduct bootstrap re sampling for a calculation of 3 variables. need bootstrap CI at 95%. please tell me how it can be done and are there any script files available.,en
1108670,2012-01-05 03:27:05,statistics,I keep track of my car's mileage all year.,o3az2,GyroMight,1279050496.0,https://www.reddit.com/r/statistics/comments/o3az2/i_keep_track_of_my_cars_mileage_all_year/,2.0,2.0,,en
1108671,2012-01-05 04:45:49,computervision,Version 4.0 of the Insight Segmentation and Registration Toolkit (ITK) has been released.  It now supports video processing.,o3ef8,rCX12,1273937703.0,https://www.reddit.com/r/computervision/comments/o3ef8/version_40_of_the_insight_segmentation_and/,3.0,0.0,,en
1108672,2012-01-05 04:57:30,statistics,SQL and SAS? ,o3ewf,[deleted],,https://www.reddit.com/r/statistics/comments/o3ewf/sql_and_sas/,1.0,2.0,"In my job search (not an attempt to network I promise) I notice that many positions request SQL knowledge, how, if at all, does SAS and SQL interact and what kind of knowledge are these positions looking for? ",en
1108673,2012-01-05 08:17:40,artificial,The Object Recognition problem.,o3n11,moscheles,1316266003.0,https://www.reddit.com/r/artificial/comments/o3n11/the_object_recognition_problem/,13.0,25.0,"An unsolved problem in artificial intelligence is that none of our engineered artifacts robustly perceive the world in terms of objects. This problem is not merely something that plagues computer vision, but also causes proximal issues for engineers in autonomous navigation and those concerned with knowledge representation of categories. 

Does anyone have links or citations to articles or books discussing the Object Recognition Problem in a thorough and serious manner? Philosophical/linguistic issues about categories are also welcome. Please post your pointers here.  
",en
1108674,2012-01-05 11:21:27,rstats,Mapping the Iowa GOP 2012 Caucus Results,o3s79,[deleted],,https://www.reddit.com/r/rstats/comments/o3s79/mapping_the_iowa_gop_2012_caucus_results/,1.0,0.0,,en
1108675,2012-01-05 12:09:36,statistics,Literature on alternating renewal processes with time-varying pdfs?,o3t8k,lkdo,1252531077.0,https://www.reddit.com/r/statistics/comments/o3t8k/literature_on_alternating_renewal_processes_with/,1.0,1.0,"I'm looking for resources on either alternating renewal processes with time varying density functions for the inter-event arrival times, or the more specific alternating Poisson process with time-varying rates. It is easy to find wiki articles, slides, book chapters for, e.g. , time-varying Poisson process, but I can't seem to get a good source introducing and presenting results for the two-state(alternating) case. Any help is much appreciated.",en
1108676,2012-01-05 16:33:25,statistics,Could I become a professional statician without any higher education?,o3yvx,[deleted],,https://www.reddit.com/r/statistics/comments/o3yvx/could_i_become_a_professional_statician_without/,4.0,52.0,"I'm a sophomore in high school, in a freshman algebra class, everything in class is just busy work, at least to me, and, I think I could learn really fast studying on my own. Perhaps I could get an honorary degree?",en
1108677,2012-01-05 17:25:48,statistics,Where did the frequentist-Bayesian debate go?,o40oo,inoyau,1296684632.0,https://www.reddit.com/r/statistics/comments/o40oo/where_did_the_frequentistbayesian_debate_go/,13.0,8.0,,en
1108678,2012-01-05 22:15:29,MachineLearning,A case study of web pornography search [PDF][SFW],o4dev,psyyduck,1219819893.0,https://www.reddit.com/r/MachineLearning/comments/o4dev/a_case_study_of_web_pornography_search_pdfsfw/,7.0,8.0,,en
1108679,2012-01-05 23:38:20,statistics,What to learn next?,o4haw,jsantos17,,https://www.reddit.com/r/statistics/comments/o4haw/what_to_learn_next/,2.0,9.0,"I'm a CS major and last semester I took a Statistics for Engineers class. We covered mostly hypothesis tests (tests on variance, proportion, means, two sample tests), estimator properties (maximum likelihood and such), regression analysis and ANOVA. I found it pretty fun and would like to learn more. What topics would you recommend to someone with my background? In math I'm past vector calculus, ODEs, PDEs, linear algebra and a little abstract algebra. Being a CS major I've also took discrete math (graph theory, algorithmic complexity, some combinatorics and formal languages).",en
1108680,2012-01-06 06:58:41,artificial,What is the name of this research and which professionals or labs are working on it?,o50wc,moscheles,1316266003.0,https://www.reddit.com/r/artificial/comments/o50wc/what_is_the_name_of_this_research_and_which/,13.0,19.0,"We present full-color photographs to participants in our control group of human volunteers. Within these photographs is a black square. There is an object behind this black square, but the entirety of the object is occluded completely by it. The volunteers are pressed with giving 5 of their best guesses as to what the missing object probably is. 

+ http://i.imgur.com/27ecY.jpg
+ http://i.imgur.com/1zrmm.jpg
+ http://i.imgur.com/zW2fO.jpg

Humans are capable of accurately identifying the missing object, even having never seen the photograph in their lives. Our next question is this:  How can we make a computer do this with nearly the same accuracy as humans?

",en
1108681,2012-01-06 09:27:45,rstats,"Has anyone else submitted a link here, only for it to get stuck in the spam filter?",o567b,[deleted],,https://www.reddit.com/r/rstats/comments/o567b/has_anyone_else_submitted_a_link_here_only_for_it/,1.0,0.0,"I've tried to submit some links here in the past, but they inevitably go to spam.  I have messaged the moderators about this a few times, but nothing happens: posts are still in spam, message is unanswered.  I get that two of the three moderators here are largely inactive, but the third's account does show activity since the most recent time I sent the message.

Am I the only one this happens to?  It's a shame to see a community stagnate because posts don't go through.",en
1108682,2012-01-06 10:27:51,artificial,Is a PhD in Artificial Intelligence good for getting jobs?,o57ru,rajicon,,https://www.reddit.com/r/artificial/comments/o57ru/is_a_phd_in_artificial_intelligence_good_for/,19.0,22.0,"I am currently an undergraduate (double majoring in cognitive science and math) thinking about going to graduate school for artificial intelligence.  I would like to know if this is a good idea, and advice on getting into  a good program would also be welcome.",en
1108683,2012-01-06 13:02:32,MachineLearning,Diagnosing the way a ML algorithm behaves on a problem is non trivial. Has anyone tried to apply machine learning on itself to automatically solve this?,o5ayp,visarga,1166994643.0,https://www.reddit.com/r/MachineLearning/comments/o5ayp/diagnosing_the_way_a_ml_algorithm_behaves_on_a/,17.0,11.0,"I got the idea from seeing the title of this paper: [Knows what it knows: a framework for self-aware
learning](http://paul.rutgers.edu/~thomaswa/pub/Li11Knows.pdf).

So, it would be nice to have an extra layer on top of a ML algorithm that would look at its performance and fine tune it: suggest features, number of examples needed, complexity of the model and so on?

At a higher level, this ""ML expert"" could suggest algorithms (and kernels) that work best on the dataset at hand - why rely on human intuition alone?
",en
1108684,2012-01-06 16:46:05,rstats,How to compute a correlation matrix to transform one's variables for a linear model? ,o5g74,[deleted],,https://www.reddit.com/r/rstats/comments/o5g74/how_to_compute_a_correlation_matrix_to_transform/,3.0,0.0,"I have two continuous variables, X and Y, that are correlated - they are not independent. To correct for non-independence, I have a known correlation structure, a matrix S. 
 
If one calls gls(Y ~ X, correlation = S), what I think happens is that, internally, gls() transforms X and Y in some way so that the regression ends up being S^(-1)*Y = S^(-1) * X. 
 
How is this transformation actually performed? From the literature I've consulted, I've seen everything from: 
 
X.transformed &lt;- solve(chol(S)) %*% X #The inverse of the Choleski decomposition of S times the vertical vector X, which in my case does nothing to the data
 
to 

X.transformed &lt;- chol(solve(S)) %*% X # which has negative values and gives meaningless values of X 
 
On a related note, is there any point to manually transforming the data in order to plot it? Do the transformed values have any meaning, or are they simply there to estimate regression coefficients? (In other words, if X is a variable of body mass figures, X values are not necessarily errant if they're negative since they're still linear?) 
",en
1108685,2012-01-06 17:31:18,data,Find the Best Windows Driver Backup and Restore,o5hrg,iGennie_M,1308032764.0,https://www.reddit.com/r/data/comments/o5hrg/find_the_best_windows_driver_backup_and_restore/,1.0,0.0,,en
1108686,2012-01-06 18:09:53,statistics,The ubiquity of gaussians:  an example of how (not) to think about robust estimators,o5j6g,claird,1249749559.0,https://www.reddit.com/r/statistics/comments/o5j6g/the_ubiquity_of_gaussians_an_example_of_how_not/,2.0,5.0,,en
1108687,2012-01-06 19:29:06,statistics,What do you mean by average?,o5me9,infracanis,1207234915.0,https://www.reddit.com/r/statistics/comments/o5me9/what_do_you_mean_by_average/,17.0,0.0,,en
1108688,2012-01-06 19:32:26,MachineLearning,Introduction to Conditional Random Fields,o5mjy,kapichu,1325871107.0,https://www.reddit.com/r/MachineLearning/comments/o5mjy/introduction_to_conditional_random_fields/,28.0,0.0,,en
1108689,2012-01-06 22:24:40,statistics,risk analysis: what do the numbers mean?,o5u66,martinus,1137128400.0,https://www.reddit.com/r/statistics/comments/o5u66/risk_analysis_what_do_the_numbers_mean/,5.0,5.0,"I am trying to understand this sentence: 

&gt; The multivariate relative risk of gout among men in the highest quintile of meat intake, as compared with those in the lowest quintile, was 1.41 (95 percent confidence interval, 1.07 to 1.86; P for trend=0.02)

Unfortunately I am pretty clueless about statistics, can anyone explain what all the numbers mean?",en
1108690,2012-01-06 23:28:03,rstats,R import from clipboard quickly,o5wz4,SirDigbyChknSiezure,1325041789.0,https://www.reddit.com/r/rstats/comments/o5wz4/r_import_from_clipboard_quickly/,11.0,4.0,"Maybe everyone else knows about this already, but I just stumbled upon this today and found it pretty useful

To import whatever tabular data is in the clipboard just use:
read.table('clipboard') with whatever other arguments you need.",en
1108691,2012-01-07 00:33:49,statistics,"Okay, so I want to use Bayesian reasoning for research. Now what?",o5zyc,Dissonanz,1320038047.0,https://www.reddit.com/r/statistics/comments/o5zyc/okay_so_i_want_to_use_bayesian_reasoning_for/,18.0,8.0,"The title probably says it all.

If I, as a research psychologist, want to use Bayesian methods for research, what exactly do I do? What changes about my research questions, what changes in how I answer those questions? How do I, for example, find out whether or not an experimental treatment changes a dependant variable in the test subjects?

I think I might be confused by what exactly Bayes theorem means for research.",en
1108692,2012-01-07 01:41:00,MachineLearning,Caterpillar to expand Chinese R&amp;D centre - KHL Group | HYDRAULICS - News,o62uu,kamilrhu,1226023594.0,https://www.reddit.com/r/MachineLearning/comments/o62uu/caterpillar_to_expand_chinese_rd_centre_khl_group/,1.0,0.0,,en
1108693,2012-01-07 04:15:15,statistics,"Post hoc, ergo propter hoc",o68vb,neuroPSYK,1322705815.0,https://www.reddit.com/r/statistics/comments/o68vb/post_hoc_ergo_propter_hoc/,1.0,0.0,,en
1108694,2012-01-08 03:06:46,statistics,"Basic stats for log file analysis, what can I do with this information?",o7dcq,berlinbrown,1135573200.0,https://www.reddit.com/r/statistics/comments/o7dcq/basic_stats_for_log_file_analysis_what_can_i_do/,4.0,1.0,"I work in financial services and I have an infinite amount of data.  Actually, it isn't infinite but it is time series log data that spans for the past 30 years.

I really want to look at the log data for the past week (7 days) for our web application.

I want to move beyond just keeping a count of  user clicks.

I have this data, userid, sessionid, amount of time of session, category of user, errors received during session, errors received on server at a particular time, processing time of applications during a particular time,  sql queries during user session.

...

My ultimate goal is to look at a series of user sessions and to see if there are any anomalies in the user session.  Or possibly see if there is too much data load on the server at the time during a session.

Also, a lot of this information comes for hundreds of different log files.    I want to break out key information and associate it with a particular file.

I have a lot of data, I just don't know some simple ways to break down the data. 

Based on this information, what are some graphs I could use? Some formulas or equations that would help me to visualize or highlight anomalies or interesting pieces of information.
",en
1108695,2012-01-08 04:24:08,statistics,Nonlinear is not a hypothesis,o7g89,brews,1263715301.0,https://www.reddit.com/r/statistics/comments/o7g89/nonlinear_is_not_a_hypothesis/,17.0,1.0,,en
1108696,2012-01-08 04:47:51,statistics,In what situations would you use the K-S or CvM tests instead of the A-D test?,o7h4z,[deleted],,https://www.reddit.com/r/statistics/comments/o7h4z/in_what_situations_would_you_use_the_ks_or_cvm/,3.0,6.0,"K-S: Kolmogorov-Smirnov

CvM: Cramér–von Mises

A-D: Anderson Darling
",en
1108697,2012-01-08 05:37:10,artificial,Building a bridge from perception to understanding; Probabilistic Models for Scene Understanding,o7iyg,moscheles,1316266003.0,https://www.reddit.com/r/artificial/comments/o7iyg/building_a_bridge_from_perception_to/,0.0,1.0,,en
1108698,2012-01-08 16:31:05,datasets,John Snow’s famous cholera analysis data in modern GIS formats,o7ymn,talgalili,1271226645.0,https://www.reddit.com/r/datasets/comments/o7ymn/john_snows_famous_cholera_analysis_data_in_modern/,16.0,0.0,,en
1108699,2012-01-08 18:58:31,artificial,Looking for recommended journals for publishing results of AI tests,o82jn,curiousai,1318189231.0,https://www.reddit.com/r/artificial/comments/o82jn/looking_for_recommended_journals_for_publishing/,7.0,26.0,"I do not come from academia, but I have spent a good portion of my life working on a general-purpose AI model. I plan to put together some tests and to record the results with hopes that I can publish them. The problem is there are thousands of journals. If anyone has done anything similar, or knows of good relevant journals, please do share. Also, any advice is accepted too.",en
1108700,2012-01-08 20:20:11,statistics,Modified Bessel function and Inverse-Gamma distribution,o85hg,mtaboga,1279629743.0,https://www.reddit.com/r/statistics/comments/o85hg/modified_bessel_function_and_inversegamma/,7.0,3.0,"I have not been able to find a textbook containing a proof of how the characteristic function of an Inverse-Gamma distribution can be derived. In particular, I do not understand how the modified Bessel function of the second kind comes into play. Any suggestion?",en
1108701,2012-01-09 07:47:03,statistics,"Can you suggest a good terse introduction 
to statistics ?",o8yj6,desmondbrennan,1325741904.0,https://www.reddit.com/r/statistics/comments/o8yj6/can_you_suggest_a_good_terse_introduction_to/,2.0,7.0,"A book I loved and found very useful was Kernighan and Ritchie's succinct introduction to the C programming language. It was self contained(e.g. explained programming itself), very clear - and useful afterwards as a reference.

I'm looking for something similar in statistics - I guess it should be agnostic as to any programming language - though if it used one for illustrations fine.",en
1108702,2012-01-09 12:50:55,rstats,Adding multiple ellipses into 1 scatterplot!?,o95zq,arctium,1325756985.0,https://www.reddit.com/r/rstats/comments/o95zq/adding_multiple_ellipses_into_1_scatterplot/,1.0,3.0,"I have to make a scatterplot graph with multiple point groups plotted into one. The thing I want to do is to draw a range ellipse around each group (I have 6 groups). It's basically a DCA ordination diagram.
Something like [this](http://i.imgur.com/qWDB8.jpg)!

I have used the ""lattice"" package so far.",en
1108703,2012-01-09 13:05:49,statistics,Stacked vs Layered Area Charts?,o968e,alexryane2,1307459660.0,https://www.reddit.com/r/statistics/comments/o968e/stacked_vs_layered_area_charts/,2.0,1.0,"When using area charts (i.e., filled in line graphs) with 2 or more series, I am unsure when I should use a stacked chart and when it should be plain/layered. This also confuses me when looking at certain area charts where none of the series cross over - how can I tell if it's stacked or the values just follow each other naturally?",en
1108704,2012-01-09 20:51:14,data,Infochimps introduces new Big Data Platform for Enterprises,o9lo3,winniechimp,1309463088.0,https://www.reddit.com/r/data/comments/o9lo3/infochimps_introduces_new_big_data_platform_for/,1.0,0.0,,en
1108705,2012-01-09 22:12:35,datasets,"4,000+ social media apps with screenshots and reviews",o9pkr,dgryski,1242396284.0,https://www.reddit.com/r/datasets/comments/o9pkr/4000_social_media_apps_with_screenshots_and/,5.0,0.0,,en
1108706,2012-01-09 22:43:19,statistics,Counter-intuitive property of social networks (e.g. – Facebook)... please explain!,o9r2x,ExperienceArchitect,1275492241.0,https://www.reddit.com/r/statistics/comments/o9r2x/counterintuitive_property_of_social_networks_eg/,7.0,13.0,"This question was prompted by a study of the social graph of Facebook. The paper is here: http://arxiv.org/abs/1111.4503

In this paper they describe various qualities of the social graph, including the ""your friends have more friends than you"" observation. 

They say that 83.6% of Facebook users have less friends than the median of their friends, and 92.7% have less friends than the average friend count of their friends.

I have a hard time imagining a set of numbers that have these characteristics... can someone explain how this works, please?

Thanks!",en
1108707,2012-01-10 03:48:51,MachineLearning,Complete notes of Stanford machine learning course,oa633,sunng,1283163374.0,https://www.reddit.com/r/MachineLearning/comments/oa633/complete_notes_of_stanford_machine_learning_course/,56.0,3.0,,en
1108708,2012-01-10 07:44:39,MachineLearning,Aggregation and Restructuring data (in R),oahd6,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/oahd6/aggregation_and_restructuring_data_in_r/,1.0,0.0,,en
1108709,2012-01-10 07:44:51,statistics,Aggregation and Restructuring data (in R),oahdj,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/oahdj/aggregation_and_restructuring_data_in_r/,18.0,0.0,,en
1108710,2012-01-10 09:10:30,MachineLearning,"Package ""sentiment"" now available on CRAN: naïve Bayes classifier for polarity classification (e.g. positivity/negativity)",oakg5,tymekpavel,1190535699.0,https://www.reddit.com/r/MachineLearning/comments/oakg5/package_sentiment_now_available_on_cran_naïve/,6.0,0.0,,en
1108711,2012-01-10 17:24:09,MachineLearning,The world's coolest machine learning internships,oaw4d,dataranch,1326208929.0,https://www.reddit.com/r/MachineLearning/comments/oaw4d/the_worlds_coolest_machine_learning_internships/,21.0,3.0,,en
1108712,2012-01-10 18:14:53,MachineLearning,JRS'12 Competition: design best algorithm for multi-label classification of biomedical papers,oay0a,datt,1276003335.0,https://www.reddit.com/r/MachineLearning/comments/oay0a/jrs12_competition_design_best_algorithm_for/,0.0,0.0,,en
1108713,2012-01-10 18:27:00,artificial,JRS'12 Contest: algorithm for automatic classification of scientific articles - good testbed for new AI ideas?,oayj9,datt,1276003335.0,https://www.reddit.com/r/artificial/comments/oayj9/jrs12_contest_algorithm_for_automatic/,3.0,0.0,,en
1108714,2012-01-10 18:38:21,MachineLearning,kNN distance measure,oayzk,kumquatz,1311839840.0,https://www.reddit.com/r/MachineLearning/comments/oayzk/knn_distance_measure/,2.0,2.0,I am traing a kNN with binary feature vectors. Why does it practically make no difference if I am using Euclidian distance or Hamming/Manhatten distance? Are they theoretically the same for binary data?,en
1108715,2012-01-10 20:31:22,rstats,R library for time series discontinuities?,ob3ws,[deleted],,https://www.reddit.com/r/rstats/comments/ob3ws/r_library_for_time_series_discontinuities/,2.0,3.0,"I can't seem to find the one I used previously, mostly due to the fact I can't remember its name. Can anyone shed some light on this?

",en
1108716,2012-01-10 22:49:13,datasets,Access open data right from your python shell,obacs,kemvi,1324871566.0,https://www.reddit.com/r/datasets/comments/obacs/access_open_data_right_from_your_python_shell/,9.0,4.0,,en
1108717,2012-01-11 02:03:58,datasets,"Major Hollywood Movies (2007-2011) - Budgets, Review Scores, Grosses, etc.",objz7,tst__,1216983046.0,https://www.reddit.com/r/datasets/comments/objz7/major_hollywood_movies_20072011_budgets_review/,2.0,0.0,,en
1108718,2012-01-11 02:37:55,statistics,Graph of the Week!,oblk4,dezert1,1326242243.0,https://www.reddit.com/r/statistics/comments/oblk4/graph_of_the_week/,6.0,4.0,,en
1108719,2012-01-11 14:53:54,rstats,RStudio v0.95 Preview Available -- includes git integration,oca7e,swiftsam,1198790310.0,https://www.reddit.com/r/rstats/comments/oca7e/rstudio_v095_preview_available_includes_git/,24.0,3.0,,en
1108720,2012-01-11 16:27:35,MachineLearning,"Can someone explain to me what a 
Bayes Point Machine is?",occsk,Jigsus,1261395941.0,https://www.reddit.com/r/MachineLearning/comments/occsk/can_someone_explain_to_me_what_a_bayes_point/,16.0,2.0,I'm having quite a bit of trouble understanding the original Herbrich paper. Can someone explain it to me in simple terms?,en
1108721,2012-01-11 18:13:25,MachineLearning,"Help a friend of mine with his master's thesis, trying to uniquely identify online users without cookies using ML",ocgwx,Emore,1209496721.0,https://www.reddit.com/r/MachineLearning/comments/ocgwx/help_a_friend_of_mine_with_his_masters_thesis/,0.0,3.0,,en
1108722,2012-01-12 09:09:04,MachineLearning,Camlock Coupling,odnb4,stephanieme,1323148668.0,https://www.reddit.com/r/MachineLearning/comments/odnb4/camlock_coupling/,1.0,0.0,,en
1108723,2012-01-12 15:47:51,computervision,Registering Astronomical Images,odvub,MachineVision,1284977835.0,https://www.reddit.com/r/computervision/comments/odvub/registering_astronomical_images/,2.0,7.0,"I am developing an application that needs to register (or align) astronomical images taken from a CCD. The images are usually just differ in rotation and translation, so these two operations can register the images. The trick is to detect the stars, sort them by magnitude and then find the transformation.

I am looking for an algorithm (or a library) that allows me to do this. I am somewhat familiar with OpenCV but I don't think that has anything that would allow me to do this. Any suggestions?",en
1108724,2012-01-12 18:09:11,MachineLearning,Fisher's Information Matrix,oe0s5,tncardoso,1316800601.0,https://www.reddit.com/r/MachineLearning/comments/oe0s5/fishers_information_matrix/,1.0,2.0,"Hi guys,

I am trying to implement an Active Learning algorithm using the Fisher's Information Matrix as the selection strategy. I tried different papers but I couldn't understand how the matrix is obtained. Some of these papers are:

A Probability Analysis on the Value of Unlabeled Data for Classification Problems
Tong Zhang, Frank J. Oles

Active Learning for Logistic Regression: an evaluation
Andrew I. Schein, Lyle H. Ungar

I will be grateful if someone can explain this matrix or provide me with good references. A reference implementation would be really nice too.

Thanks!",en
1108725,2012-01-12 19:27:10,statistics,/r/stata -- I didn't create it but I just stumbled into the empty community and would love if it there was a place to exchange tricks of the trade,oe4ej,diffeomacx,,https://www.reddit.com/r/statistics/comments/oe4ej/rstata_i_didnt_create_it_but_i_just_stumbled_into/,14.0,8.0,,en
1108726,2012-01-12 22:01:22,statistics,Predicting the Weather,oebzs,qadm,1314029027.0,https://www.reddit.com/r/statistics/comments/oebzs/predicting_the_weather/,5.0,1.0,,en
1108727,2012-01-12 22:19:12,statistics,Statistics and League of Legends Ranked - opinions on my assessment,oecx3,Leobardis,1297814676.0,https://www.reddit.com/r/statistics/comments/oecx3/statistics_and_league_of_legends_ranked_opinions/,3.0,3.0,,en
1108728,2012-01-12 23:33:48,analytics,Google Analytics Question,oegom,tylerr82,1310761039.0,https://www.reddit.com/r/analytics/comments/oegom/google_analytics_question/,0.0,0.0,"What is the difference between first click conversion, last interaction conversion and goal completions? I have 3 different numbers in my analytics.",en
1108729,2012-01-12 23:57:09,statistics,Epic Stats Project - Will take suggestions to make it better!,oehx1,StatsDude,1326404583.0,https://www.reddit.com/r/statistics/comments/oehx1/epic_stats_project_will_take_suggestions_to_make/,2.0,4.0,"I've embarked on something epic, and I wanted to include people on my journey to help me improve my methods and enjoy in the statistical nerdgasm that will be the by-product! 

I'm attempting to track absolutely everything in my life. That includes eating, sleeping, entertainment habits, sexual habits, bathroom habits, and more as they come to me. I'm in my first full week, and I wanted to see if this group would be interested in helping me come up with or improve my methods so I can mine this information and basically know what the heck I do with my life. Just think of the charts! Also imagine being able to see how different events in life (like a break up) impacts things like sleep. 

**Current Methods:** I should note that I have an iPhone and 95% of my life is spent around a computer if you have any ideas of how to improve my method. 

*Diet:* Tap &amp; Track - I'm using the iphone app and the website to keep up with what I eat. It's exportable to .CSV. 

*Bathroom/Sexual/Sleep:* Google Docs- Excel - I'm using the google docs version of excel since I can update it on any computer and on my iPhone (though the interface is lacking). It currently contains 4 columns. Date|Time|Category|Notes. 

*Entertainment:* I'm attempting to see if Netflix and Hulu has an option to create an .xml or .csv output file of my history. Depending on that data I might switch to excel. I would like to see how much I watch including which device I use. 

Once I get data for a month, I plan on using a tool like Business Objects to create a dashboard with a final objective of creating a website for people to review the results. 


",en
1108730,2012-01-13 00:32:39,MachineLearning,Reinforcement learning resources?,oejsh,[deleted],,https://www.reddit.com/r/MachineLearning/comments/oejsh/reinforcement_learning_resources/,8.0,6.0,"Any pointers to a good tutorial, or practical introduction into reinforcement learning? Thanks!",en
1108731,2012-01-13 08:39:43,artificial,Josh Bongard - The Robot Revolution (brief vid on evolutionary robotics),of5pt,NadsatBrat,1201850367.0,https://www.reddit.com/r/artificial/comments/of5pt/josh_bongard_the_robot_revolution_brief_vid_on/,6.0,3.0,,en
1108732,2012-01-13 11:48:49,MachineLearning,Is using PCA a good way to reduce dimensionality of text features?,ofac9,joelthelion,1146260183.0,https://www.reddit.com/r/MachineLearning/comments/ofac9/is_using_pca_a_good_way_to_reduce_dimensionality/,15.0,21.0,"I'm working on a classifier for reddit posts, and I have the impression that non-text features such as subreddit, author, domain or votes are being drown by the sheer number of features from the text (link title, and optionnally comments and linked page).

So I'm thinking of using some sort of dimensionality reduction on the text features before handing them to the classifier. Am I on the right path?

EDIT: thanks everyone for the answers!",en
1108733,2012-01-13 16:24:43,artificial,FHI Winter Intelligence Conference 2011 - collected lecture videos,ofg3h,NadsatBrat,1201850367.0,https://www.reddit.com/r/artificial/comments/ofg3h/fhi_winter_intelligence_conference_2011_collected/,4.0,2.0,,en
1108734,2012-01-13 17:09:47,statistics,Determining cliffs from satellite images (change points in a spatial domain),ofhni,[deleted],,https://www.reddit.com/r/statistics/comments/ofhni/determining_cliffs_from_satellite_images_change/,6.0,6.0,"I have heard that some statistical theory has been applied to satellite images in an effort to determine the location of cliffs, possibly intended for use in mapping other planetary bodies. 

Anyone know anything about this?",en
1108735,2012-01-13 18:00:29,artificial,Google's Peter Norvig answers questions on AI.,ofjnf,dr_dom,1324420067.0,https://www.reddit.com/r/artificial/comments/ofjnf/googles_peter_norvig_answers_questions_on_ai/,14.0,10.0,,en
1108736,2012-01-13 20:09:09,MachineLearning,Tell me about Lucene/SOLR,ofpc8,shaggorama,1233555004.0,https://www.reddit.com/r/MachineLearning/comments/ofpc8/tell_me_about_lucenesolr/,7.0,7.0,"I tend to find myself in forums talking about predictive statistics more than NLP, so I basically never see anyone talking about Lucene. All of a sudden at work everyone's talking about it like applying it to our problem will be a magic pill (which it may well be). Anyone here have any experience? What do you guys think of this tool",en
1108737,2012-01-14 01:29:33,MachineLearning,What is the best Java neural network library for research?,og42n,coopster,,https://www.reddit.com/r/MachineLearning/comments/og42n/what_is_the_best_java_neural_network_library_for/,8.0,20.0,"I am a PhD student in machine learning, and am now starting my 'serious' dissertation effort.  I will be doing research involving neural networks and I am likely going to use Java for development.  What would you say is the best Java library to use?  Knowing that I am going to be implementing my own training functions, architectures, etc., is there one that is more flexible than the others?

Currently, I'm leaning towards [Encog](http://www.heatonresearch.com/encog) or [Neuroph](http://neuroph.sourceforge.net/).  Any thoughts?",en
1108738,2012-01-14 04:05:42,statistics,Linear regression using a predetermined weight variable?,ogaax,[deleted],,https://www.reddit.com/r/statistics/comments/ogaax/linear_regression_using_a_predetermined_weight/,3.0,8.0,"I'm running a regression using basketball lineup combination data.  Basically, each observation is a ""stint"" on the floor, with a certain combination of 5 home players and 5 away players.  Depending on each coach's substitutions, those exact 10 players may only be on the court together for 1 possession, or 2 possessions, or 7 or 10 or 50 during the game.  The response variable is a measure of the home team's offensive efficiency, and the 10 players are binary predictor variables.

My intuition is that # of possessions should be used as the regression's weight variable, as I would assume a stint with 10 possessions should be weighted 10 times as much as a stint with 1.  But I asked my professor and he said that's not how it works... that weight variables should arise organically after looking at the variance of the residuals, and should not be predetermined.  I guarantee that the variance of the offensive efficiency is related to to # of possessions for that stint.

Is he right?  If so, how do I go about regressing data where there is a clear weight variable, making some observations more important than others.  Seems like this would be a pretty common form for regression data, and there should be an easy way to do this.",en
1108739,2012-01-14 04:08:47,statistics,Does Bayesianism really require the likelihood principle?,ogaec,Lithosphere,1313095396.0,https://www.reddit.com/r/statistics/comments/ogaec/does_bayesianism_really_require_the_likelihood/,16.0,4.0,"Suppose that one accepts that probability represents degree of belief, and one feels that statistical inference should consist of updating one's prior beliefs in light of the data. Does it follow that one can't make inferences about a parameter using anything other than its prior distribution and the likelihood function? [Dennis 1996](http://www.webpages.uidaho.edu/~brian/reprints/Dennis_Ecological_Applications_1996.pdf) puts it like this:

&gt; In the Bayesian approach, all conclusions about the value of the parameter are embodied in the posterior pdf. The data enter the conclusions only through the likelihood function… In particular, no sample-space probabilities, other than the actual realized value of the likelihood function, are admitted into the conclusions (Lindley 1982, 1990). Sample-space probabilities, such as the probability that a test statistic might exceed a critical value, involve ""data that didn't happen"", are are excluded from the analysis. The principle of including only the actual data in the analysis excluding consideration of all other sample-space possibilities is known as the ""likelihood principle"".

The fallout includes, for instance, [Example 11.10 in Wasserman](http://img542.imageshack.us/img542/2834/wassermanexample.png), where you're trying to estimate *c* given that *f*(*x*) = *c**g*(*x*) for some unknown PDF *f* and a known function *g* (imagining that *g* is too ugly or *x* is too high-dimensional to just compute *c* as the reciprocal of the integral of *g*) but from a Bayesian perspective, a sample from *f*(*x*) provides no information about *c*.

From a philosophical perspective, this makes no sense to me. Shouldn't my conclusions about a thing depend not only on my prior beliefs (including preexisting *knowledge*) about the thing itself but also about other things that may have implications for said thing? Is the problem with the particular method used to compute posterior distributions rather than the more general idea of Bayesian inference?

(Background: I'm a graduate student in experimental psychology, but I was also a math major as a undergraduate (my favorite topic being analysis), and I'm now trying to learn mathematical statistics on my own.)

(Edit: Fixed a broken link.)",en
1108740,2012-01-14 06:42:37,MachineLearning,A good place to start?,ogg3e,cheraphy,1313974903.0,https://www.reddit.com/r/MachineLearning/comments/ogg3e/a_good_place_to_start/,9.0,15.0,"What would be an ideal place to start self study in regards to Machine Learning? I have sparse bits of knowledge of the topic but no concrete base to begin teaching my self anything. So I'm wondering what subjects I should look into first and/or what books would be good to begin reading.

I am currently an undergrad Computer Science student in my second year, but I've been studying Computer Science in general for almost six years. This particular field of research has always been the most intriguing to me and what initially got me into programming as child.

I am a proficient imperative programmer and am trying to learn functional programming. 

Thank you in advanced.

**EDIT**: Someone pointed out to me how open ended this question could be and suggested I give specific areas in this field I'm interested. As I still have a very shallow understanding of the field I would have to guess that Machine Perception and Pattern Recognition are what interest me most.
",en
1108741,2012-01-14 23:55:16,statistics,"I'm doing a multiple regression on a dataset that reflects the population, not sample data. ",oh7sh,[deleted],,https://www.reddit.com/r/statistics/comments/oh7sh/im_doing_a_multiple_regression_on_a_dataset_that/,2.0,4.0,"Do I need to worry about biased parameters due to multicollinearity? 

Just curious. 

Also, I am a bit fuzzy about how exactly t-tests are used to construct confidence intervals. Anyone know of any good video lectures that explain this concept? 

Thank you!",en
1108742,2012-01-15 14:19:22,statistics,Merging two data.frame objects while preserving the rows’ order (a useful R function...),ohx3o,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/ohx3o/merging_two_dataframe_objects_while_preserving/,8.0,0.0,,en
1108743,2012-01-16 00:01:19,statistics,How do I calculate the t stat for the following t-Test: Paired Two Sample for Means of scale 1 and scale 2?,oieye,[deleted],,https://www.reddit.com/r/statistics/comments/oieye/how_do_i_calculate_the_t_stat_for_the_following/,2.0,1.0,"I am trying to figure out if there calibration difference between two scales.  We are seeing if it is 0.

scale 1
0.997
0.993
0.994
0.998
0.992
0.995
0.991
0.994
0.994
0.996

scale 2
0.989
1.000
0.997
0.990
1.000
0.996
0.987
0.994
0.988
1.001


Scale 1
Mean	0.994
Variance	4.14783E-06
Observations	10

Scale 2
Mean 0.994
Variance 2.7701E-05
Observations 10

Pearson Correlation	-0.084238712
Hypothesized Mean Difference	0
df	9

t stat?  I know the answer is 0.153249535 but I don't know how to make the equation.  This is for my job so I am just trying to figure out how to calculate future comparisons.  I have the answers from an old report, which is this, and am learning off of that.  Any help would be much appreciated!

I might also help to have the math for these as well.  I got stuck at the t stat:

P(T&lt;=t) one-tail	0.440791249
t Critical one-tail	1.833112923
P(T&lt;=t) two-tail	0.881582497
t Critical two-tail	2.262157158
",en
1108744,2012-01-16 01:19:21,computervision,I dont think I ever seen stuff like this even in USA heh,oii7z,[deleted],,https://www.reddit.com/r/computervision/comments/oii7z/i_dont_think_i_ever_seen_stuff_like_this_even_in/,1.0,0.0,,en
1108745,2012-01-16 05:15:17,statistics,How to make comparison of two means from populations of differing size significant?,oirya,stathelpplease,1326683291.0,https://www.reddit.com/r/statistics/comments/oirya/how_to_make_comparison_of_two_means_from/,4.0,20.0,"Hello -

First, I am by no means a user of stats. This will be one of the first times I have used stats, so I could really use a hand.

I have two means from two different populations of differing size. How can I make a comparison of these two means... meaningful?


I have to determine which source is best to buy from.
Say I have one population from source A with 100 purchased widgets with a mean price of $25. I also have another population of purchased widgets from source B, but the mean price is $20, however the population is 187. Obviously I want to show which source is better, and on the face of it I would think that the source A would be higher price, yet the populations are different, so maybe there were just more factors to take into consideration. 

Could anyone help me with this real world problem??",en
1108746,2012-01-16 11:42:08,MachineLearning,First I need to learn then a machine can learn! ,oj554,ijhyez,1326706714.0,https://www.reddit.com/r/MachineLearning/comments/oj554/first_i_need_to_learn_then_a_machine_can_learn/,0.0,2.0,,en
1108747,2012-01-16 13:45:41,statistics,P-value distribution under the null hypothesis,oj7df,ulchuchu,1326181262.0,https://www.reddit.com/r/statistics/comments/oj7df/pvalue_distribution_under_the_null_hypothesis/,2.0,15.0,"Typically in modelling of p-value distributions one assumes the so-called BUM (beta-uniform mixture) model (Pounds and Morris 2003, Bioinformatics). The BUM model assumes the p-value to be uniformly distributed under the null hypothesis (the ""noise"" part of the model) and models the signal as a beta distribution (the ""signal"" part). 

The signal part leads to a spike in the density on the left side of the distribution, i.e. where the p-values are small.

In reality however I often see a ""spike"" in the density on the right side, i.e. where the p-values are large. At the moment for example I'm looking at a set of hypergeometric tests. Most of the p-values are close to one where I would've expected them to be uniformly spread across the range ]0;1[. This is without multiple testing correction. That means that there are more p-values with a value of around 1 than expected from the uniform ""noise"" distribution.

Has anyone encountered this phenomenon before and can explain to me by what it is caused? I can try to supply some images should my explanation not be sufficient.

Cheers!",en
1108748,2012-01-16 15:45:48,MachineLearning,"New open-access journal on data science, now accepting submissions",oj9x5,fbahr,1275413362.0,https://www.reddit.com/r/MachineLearning/comments/oj9x5/new_openaccess_journal_on_data_science_now/,19.0,0.0,,en
1108749,2012-01-16 17:57:55,MachineLearning,What data stores do you use for data analysis?,ojdry,descentintomael,1318268224.0,https://www.reddit.com/r/MachineLearning/comments/ojdry/what_data_stores_do_you_use_for_data_analysis/,1.0,14.0,"I'm trying to do a side project where I need to store 30M+ rows of data.  Tried MySQL at first just to see what would happen (tl;dr; it didn't).  I'm looking at Cassandra next but it doesn't have a very robust query system.  I don't need anything complex, just something to throw data at and then say I need all rows (one at a time) which have or don't have column X as null.

Regardless of that, I'm curious what you all use on various projects?",en
1108750,2012-01-16 19:24:05,statistics,How many sides would a single die need to simulate rolling two six-sided die?,ojhb7,downfell,,https://www.reddit.com/r/statistics/comments/ojhb7/how_many_sides_would_a_single_die_need_to/,8.0,22.0,"My hypothetical die with n sides would be statistically indistinguishable from rolling two six-sided die.  So, there would be no side of the die with the number 1, only 1 with the number 2, only 1 with the number 12, etc.  What's the minimum number that n could be?  ",en
1108751,2012-01-16 20:06:57,MachineLearning,Anyone got 10 minutes to help me with a rapidminer user issue please?  [Beginner human error],ojjay,johnyma22,1278949669.0,https://www.reddit.com/r/MachineLearning/comments/ojjay/anyone_got_10_minutes_to_help_me_with_a/,2.0,2.0,"I just need a little human help via whatever comms method you prefer..  I have watched loads of tutorial videos and tried to get started but I got stuck on a ""label"" issue.

Thanks in advance",en
1108752,2012-01-16 21:57:17,datascience,Visualising Facebook Friends’ Likes With Data Grabbed Using Google Refine and Gephi,ojora,mhermans,1169219262.0,https://www.reddit.com/r/datascience/comments/ojora/visualising_facebook_friends_likes_with_data/,2.0,0.0,,en
1108753,2012-01-16 23:46:38,artificial,Folly (The Other Self): A Surreal Sculptural Installation ...,ojua1,oploreter,1326750215.0,https://www.reddit.com/r/artificial/comments/ojua1/folly_the_other_self_a_surreal_sculptural/,1.0,0.0,,en
1108754,2012-01-17 00:19:36,rstats,R- First Code,ojvy7,digitalselector,1300547430.0,https://www.reddit.com/r/rstats/comments/ojvy7/r_first_code/,0.0,2.0,"Newbie to R here. Can someone graciously provide some feedback on any redundancies I may have here?:

&gt; patient &lt;- c(1:24)
&gt; treatment &lt;- c(1,2,1,1,3,2,1,3,1,2,1,3,2,1,3,2,3,2,1,3,2,3,3,2)
&gt; thereff &lt;- c(1,4,1,2,3,4,3,2,2,3,2,1,2,2,1,2,3,2,2,2,3,3,4,2)
&gt; changed &lt;- c(2,4,2,3,3,3,2,2,2,3,2,2,2,4,3,4,3,2,2,4,2,3,3,3)
&gt; adveff &lt;- c(2,3,2,1,1,2,2,1,2,1,2,2,1,1,3,2,1,1,1,2,3,1,2,1)
&gt; factor (treatment)
 [1] 1 2 1 1 3 2 1 3 1 2 1 3 2 1 3 2 3 2 1 3 2 3 3 2
Levels: 1 2 3
&gt; problemset &lt;- data.frame(patient,treatment,thereff,changed,adveff)
&gt; problemset
   patient treatment thereff changed adveff
1        1         1       1       2      2
2        2         2       4       4      3
3        3         1       1       2      2
4        4         1       2       3      1
5        5         3       3       3      1
6        6         2       4       3      2
7        7         1       3       2      2
8        8         3       2       2      1
9        9         1       2       2      2
10      10         2       3       3      1
11      11         1       2       2      2
12      12         3       1       2      2
13      13         2       2       2      1
14      14         1       2       4      1
15      15         3       1       3      3
16      16         2       2       4      2
17      17         3       3       3      1
18      18         2       2       2      1
19      19         1       2       2      1
20      20         3       2       4      2
21      21         2       3       2      3
22      22         3       3       3      1
23      23         3       4       3      2
24      24         2       2       3      1
&gt; summary (problemset)
    patient        treatment    thereff         changed     
 Min.   : 1.00   Min.   :1   Min.   :1.000   Min.   :2.000  
 1st Qu.: 6.75   1st Qu.:1   1st Qu.:2.000   1st Qu.:2.000  
 Median :12.50   Median :2   Median :2.000   Median :3.000  
 Mean   :12.50   Mean   :2   Mean   :2.333   Mean   :2.708  
 3rd Qu.:18.25   3rd Qu.:3   3rd Qu.:3.000   3rd Qu.:3.000  
 Max.   :24.00   Max.   :3   Max.   :4.000   Max.   :4.000  
     adveff     
 Min.   :1.000  
 1st Qu.:1.000  
 Median :2.000  
 Mean   :1.667  
 3rd Qu.:2.000  
 Max.   :3.000  
&gt; sd (problemset)
  patient treatment   thereff   changed    adveff 
7.0710678 0.8340577 0.9168313 0.7506036 0.7019641 
Warning message:
sd(&lt;data.frame&gt;) is deprecated.
 Use sapply(*, sd) instead. 
&gt; newdata &lt;- problemset [which(problemset$treatment==""1""),]
&gt; summary (newdata)
    patient        treatment    thereff         changed     
 Min.   : 1.00   Min.   :1   Min.   :1.000   Min.   :2.000  
 1st Qu.: 3.75   1st Qu.:1   1st Qu.:1.750   1st Qu.:2.000  
 Median : 8.00   Median :1   Median :2.000   Median :2.000  
 Mean   : 8.50   Mean   :1   Mean   :1.875   Mean   :2.375  
 3rd Qu.:11.75   3rd Qu.:1   3rd Qu.:2.000   3rd Qu.:2.250  
 Max.   :19.00   Max.   :1   Max.   :3.000   Max.   :4.000  
     adveff     
 Min.   :1.000  
 1st Qu.:1.000  
 Median :2.000  
 Mean   :1.625  
 3rd Qu.:2.000  
 Max.   :2.000  
&gt; sapply (newdata,sd)
  patient treatment   thereff   changed    adveff 
6.0474316 0.0000000 0.6408699 0.7440238 0.5175492 
&gt; newdata1 &lt;- problemset [which(problemset$treatment==""2""),]
&gt; summary (newdata1)
    patient        treatment    thereff        changed     
 Min.   : 2.00   Min.   :2   Min.   :2.00   Min.   :2.000  
 1st Qu.: 9.00   1st Qu.:2   1st Qu.:2.00   1st Qu.:2.000  
 Median :14.50   Median :2   Median :2.50   Median :3.000  
 Mean   :13.75   Mean   :2   Mean   :2.75   Mean   :2.875  
 3rd Qu.:18.75   3rd Qu.:2   3rd Qu.:3.25   3rd Qu.:3.250  
 Max.   :24.00   Max.   :2   Max.   :4.00   Max.   :4.000  
     adveff    
 Min.   :1.00  
 1st Qu.:1.00  
 Median :1.50  
 Mean   :1.75  
 3rd Qu.:2.25  
 Max.   :3.00  
&gt; sapply (newdata1,sd)
  patient treatment   thereff   changed    adveff 
7.4976187 0.0000000 0.8864053 0.8345230 0.8864053 
&gt; newdata2 &lt;- problemset [which(problemset$treatment==""3""),]
&gt; summary (newdata2)
    patient        treatment    thereff         changed     
 Min.   : 5.00   Min.   :3   Min.   :1.000   Min.   :2.000  
 1st Qu.:11.00   1st Qu.:3   1st Qu.:1.750   1st Qu.:2.750  
 Median :16.00   Median :3   Median :2.500   Median :3.000  
 Mean   :15.25   Mean   :3   Mean   :2.375   Mean   :2.875  
 3rd Qu.:20.50   3rd Qu.:3   3rd Qu.:3.000   3rd Qu.:3.000  
 Max.   :23.00   Max.   :3   Max.   :4.000   Max.   :4.000  
     adveff     
 Min.   :1.000  
 1st Qu.:1.000  
 Median :1.500  
 Mean   :1.625  
 3rd Qu.:2.000  
 Max.   :3.000  
&gt; sapply (newdata2, sd)
  patient treatment   thereff   changed    adveff 
6.5410790 0.0000000 1.0606602 0.6408699 0.7440238 
&gt; hist (newdata$thereff, breaks=3, col= ""blue"", main= ""Histogram of Treatment Group 1"")
&gt; hist (newdata$thereff, breaks=3, col= ""green"", main= ""Histogram of Treatment Group 2"")
&gt; hist (newdata1$thereff, breaks=3, col=""brown"", main= ""Really this time: Histogram of Treatment Group 2"")
&gt; hist (newdata2$thereff, breaks=3, col=""purple"", main= ""Histogram of Treatment Group 3"")
&gt; ",en
1108755,2012-01-17 01:24:40,statistics,"When calculating confidence intervals, why is a sample size of 30 often taken as the cutoff between large and small samples?",ojz50,kbtrost,1320025392.0,https://www.reddit.com/r/statistics/comments/ojz50/when_calculating_confidence_intervals_why_is_a/,4.0,16.0,My econometrics book often makes the distinction that a sample of less than 30 is considered small and greater than 30 is considered large. The reason for the cutoff being 30 is not obvious to me. Can someone explain? Or is this just an assumption being made by my book?,en
1108756,2012-01-17 02:35:18,analytics,7 Alternatives to Google Analytics [X-Post from /PPC],ok2fm,insite,1238258385.0,https://www.reddit.com/r/analytics/comments/ok2fm/7_alternatives_to_google_analytics_xpost_from_ppc/,1.0,0.0,,en
1108757,2012-01-17 10:03:58,MachineLearning,Can anyone suggest a semester project related to machine learning/AI?,okm5c,GotGoose,1289159546.0,https://www.reddit.com/r/MachineLearning/comments/okm5c/can_anyone_suggest_a_semester_project_related_to/,8.0,21.0,"I'm in a undergraduate research/project type class this semester and need to write a project proposal soon. I've already taken an intro to AI class, so I know the basics of the field. My teacher recommended a Twitter crawler/analyzer which sounds interesting to me, but I was wondering what else is out there.",en
1108758,2012-01-17 16:56:05,statistics,Help! I need to know a statistic that measures how spread out choices are,okvjb,[deleted],,https://www.reddit.com/r/statistics/comments/okvjb/help_i_need_to_know_a_statistic_that_measures_how/,5.0,12.0,"Lets say that there are 3 choices, red, blue and yellow, and 9 people have to choose which one is there favourite color. If red is chosen 7 times, blue once and yellow once, the choices are much less spread out than if 3 people choose red, 3 choose blue and 3 choose yellow. What is the statistic that measures this kind of spread? Thanks in advance!

ed: Also if there are some terms and language that describe what I am saying I would be grateful if you point that out to me. Thanks.",en
1108759,2012-01-17 19:29:05,datasets,1200+ resting state fMRI (brain scans) dataset,ol209,whitemailbox,1313267149.0,https://www.reddit.com/r/datasets/comments/ol209/1200_resting_state_fmri_brain_scans_dataset/,1.0,0.0,"This seemed useful to a few people in various comments already, so I thought I would post it directly in the /r/datasets


http://fcon_1000.projects.nitrc.org/fcpClassic/FcpTable.html",en
1108760,2012-01-17 22:23:15,statistics,"How could you use a computer to model a 3D solid 
capable of acting like a single die and returning the 
7 different probabilities found when rolling 2 6-sided 
dice?",olajg,[deleted],,https://www.reddit.com/r/statistics/comments/olajg/how_could_you_use_a_computer_to_model_a_3d_solid/,0.0,0.0,,en
1108761,2012-01-18 01:36:39,statistics,Does anyone have a good guide to installing WinBUGS on a Mac for R to WinBUGS programming?,olkgr,TimTebowIsAnEliteQB,1326673154.0,https://www.reddit.com/r/statistics/comments/olkgr/does_anyone_have_a_good_guide_to_installing/,2.0,8.0,"I have Mac OS 10.6.8, and I've been struggling to get WinBUGS working succesfully on my computer, I've tried multiple online guides but none of them are working out. Does anyone have a guide they've seen with a high success rate and any accompanying tips? I've currently uninstalled all the software i accumulated from previous attempts, and ready to start fresh! PLEASE HELP ME ",en
1108762,2012-01-18 01:39:13,statistics,Need help deciphering statistics,olklk,[deleted],,https://www.reddit.com/r/statistics/comments/olklk/need_help_deciphering_statistics/,1.0,0.0,"Hi, I'm not a statistician or economist, and I'm trying to understand a chart. I was hoping someone could help me.

In this paper:

http://www.ciss.org.mx/pdf/en/studies/CISS-WP-09022.pdf

In Table 2, called ""Unemployment Rate Cyclical Component Behavior"", there's a column called 'Volatility' and it's divided into two sections: 'Absolute' and 'Relative'.

How can I find out what the average volatility of the unemployment rate, in terms of a percentage, is from this data? I don't know what the 'absolute' and 'relative' refers to.

",en
1108763,2012-01-18 01:43:38,statistics,"I was asked the following question on my job interview today, thought about emailing employer following approach. Critique?. If you think stupid, feel free to say so.",olkt2,[deleted],,https://www.reddit.com/r/statistics/comments/olkt2/i_was_asked_the_following_question_on_my_job/,7.0,18.0,"During my interview today, I was given 6 categories of HR data and was asked on how I would sort those into 3 groupings.  I answered I would use historical trends of long term successful employees and where they ranked in those categories.  Then use those trends to group the employees based on how close they fell in line with those trends.  

Afterwards, you asked how I could do that mathematically and I believe now that I have had time to think more on the subject that I have a better answer on how to do so.

Employee groupings based on historical trends
1)	 Compile a list of all employees from the past.  Then identify which employees have been successful.  A successful employee can be defined however you like. For example, based on manager reviews, High Performer for X amount of years, etc. 



2)	I would then go and look up where they ranked in those 6 categories historically.   For this example, I will just use the following categories and sub categories to keep it short and to the point.
Employee	Performance	Relocate	Successful  



3)	I would then form the following equation

Success of  Employee= High Performance(X) + Medium Performance(X2) + Low Performance(X3) + Relocate Yes(X4) + Relocate No (X5)

I could then run a statistical regression using Excel or STATA.  This regression would spit back values for X1, X2, etc showing how an employee being defined as Medium Performance was indicative of his long term success in the company.

4)	To explain how the results of the regression would be useful, let me put it in practice with the following example


Example

1)	The regression would calculate numerical values on how strongly indicative each category was for a successful employee.  Since I don’t have the data, I just assigned random numbers here for each


High	Performance =.05........medium performance-------.01......low performance....-.04...... relocate yes .01......relocate no.....0


The high performance would be a category  that is strongly indicative of a long term successful employee with a large positive value of .05. Alternatively, low performance strongly negatively impacts the employees long term success with the company with a large negative value of -.04.



2)	Plugging the values the regression calculated back into the previous equation yields

Successful Employee= High Performance(.05)+ Medium Performance (.01) + Low Performance(-.04) + Relocate Yes(.02)  + Relocate No(0)

3) This can then be used be used with current employees in the following way

TABLE SHOWING WHAT IT LOOKS LIKE WHEN I PLUG IN HYPOTHETICAL EMPLOYEES FRED, HARRY, SARAH

FOR EXAMPLE,  FREDS NUMBERS

=.05(HIGHPERFOMANCE) + .02(YES RELOCATE=.07



From this, you can see each employees potential to be a long term success with the company.    Harry has the highest number, so his long term potential ranking would be the highest.

You can then divide the employees into three groups based on how high their number is(PHRASED DIFF, IN WHAT IVE WRITTEN DUE TO TABLE INSTERTED HERE)

How can this be applied?
The HR team or store manager can then be told Ted is a  “Group A” type of employee which  means he is a strong candidate to be a long term success with the company.

I like the approach of using historical trends to show a future outlook of an employee, because some of the categories you listed such as potential are wild cards that may or may not come into place.  With a regression analysis, this is taken into account, as it will show how being ranked as a “3” potential employee effects the likely hood they are a long term successful employee.
",en
1108764,2012-01-19 13:25:26,MachineLearning,"Classifying [mean,stdev] data",on4wu,RagingDoug,1298138477.0,https://www.reddit.com/r/MachineLearning/comments/on4wu/classifying_meanstdev_data/,5.0,4.0,"I have a large dataset and each item is either of type A or type B. My data is a [mean,stdev] pair. What would be a good approach to classifying this data?

I know that one approach if I had a bunch of points I wanted to classify would be to take the mean and stdev and use it to model the pdf with a guassian classifier.

However, since each ""point"" in my dataset is a [mean,stdev] pair (which itself implicitly defines some distribution) I'm curious as to a way to build a classifier for this type of data.

Hopefully that made sense.

Cheers",en
1108765,2012-01-19 14:59:05,statistics,How do I account for both intra- and inter-individual variance when comparing the control with a conditioned test stimulus?,on760,simmmons,1285000309.0,https://www.reddit.com/r/statistics/comments/on760/how_do_i_account_for_both_intra_and/,1.0,0.0,"I have data that consists of 10 repetitions of a measurement per person, for all repetitions there is one control value and one test value (after conditioning). This data will be reproduced in multiple people. 

I am now thinking how to introduce the intra-individual variance into the statistical test. Without repetitions I would simply use a paired t-test if I can expect a normal distribution of the data. 

I would be very thankful if someone could point me to some literature or explain to me how to analyze this sort of experiment.",en
1108766,2012-01-19 15:57:02,rstats,SealOfHunter,on8wy,djonatan123,1326981260.0,https://www.reddit.com/r/rstats/comments/on8wy/sealofhunter/,0.0,0.0,,en
1108767,2012-01-19 19:09:39,MachineLearning,Air Force's Top Brain Wants a 'Social Radar' to 'See Into Hearts and Minds',onhax,jdw25,1220557205.0,https://www.reddit.com/r/MachineLearning/comments/onhax/air_forces_top_brain_wants_a_social_radar_to_see/,19.0,2.0,,en
1108768,2012-01-19 19:14:38,datasets,"A collection of datasets from the Dutch goverment, site is also in Dutch",onhj8,srcr,1326992190.0,https://www.reddit.com/r/datasets/comments/onhj8/a_collection_of_datasets_from_the_dutch_goverment/,1.0,0.0,,en
1108769,2012-01-19 19:56:05,statistics,Frank is a scoundrel: a quick Bayes Theorem problem.,onjlh,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/onjlh/frank_is_a_scoundrel_a_quick_bayes_theorem_problem/,1.0,1.0,,en
1108770,2012-01-19 21:17:24,statistics,Election Polling,onnmi,dtotherog,1297922013.0,https://www.reddit.com/r/statistics/comments/onnmi/election_polling/,1.0,0.0,"I have a simple election polling question. I already posted in homeworkhelp but to no avail. 

[original](http://www.reddit.com/r/HomeworkHelp/comments/omxiv/simple_stats_question/)

I know someone out there can point me in the right direction!",en
1108771,2012-01-19 21:40:40,statistics,Julia: A Fresh Approach to Technical Computing,onoqj,BirthDeath,1295140816.0,https://www.reddit.com/r/statistics/comments/onoqj/julia_a_fresh_approach_to_technical_computing/,22.0,2.0,,en
1108772,2012-01-19 22:57:29,statistics,How to propagate standard error when using geometric means?,onsoi,lolseal,1287125314.0,https://www.reddit.com/r/statistics/comments/onsoi/how_to_propagate_standard_error_when_using/,2.0,7.0,"I have two sets of normalized expression data from a qPCR experiment.  In order to perform my statistical analysis I've had to log transform the data from each set.  I'd like to get a standard error associated with the mean of the log transformed set.  Further, I'd like to back-transform (linearize) the data in order to get a meaningful fold-change (basically a ratio of the two means), along with an accompanying standard error.

This boils down to two questions:

 - How can I calculate a standard error
   for a back-transformed log mean?  In
   other words, how can back-transform
   the standard error of a set of
   log-transformed values?

 - How can I incorporate the standard
   errors from two different
   back-transformed log means into a
   single standard error for the
   accompanying ratio of
   back-transformed log means?

I realize this might be a bit confusing, and I'm not sure where else to ask about it.  Any help or pointers in the right direction would be greatly appreciated.",en
1108773,2012-01-20 01:40:06,statistics,Care to help with my statistics project?,oo13i,[deleted],,https://www.reddit.com/r/statistics/comments/oo13i/care_to_help_with_my_statistics_project/,0.0,2.0,"Hi there currently right now I'm taking a statistics class in school. My topic for my project is going to be The average redditor. If you can help me out with my data it would be much appreciated. All I need to know is your gender, age, and on average how long a day you are on Reddit. An estimate for the time is ok. If you can just shoot me back with the info that would be great. If not no worries either. No names or reddit names will be used. Thankyou for your time",en
1108774,2012-01-20 02:26:22,statistics,Converting to Real Values for comparison,oo3hp,stathelpplease,1326683291.0,https://www.reddit.com/r/statistics/comments/oo3hp/converting_to_real_values_for_comparison/,1.0,2.0,"This is probably an easy one for most of you. I may just need another coffee, but I would truly appreciate some help with this.

I am trying to compare the performance of salesmen. Salesman A sells higher priced cars. Salesman B sells lower priced cars. They both sell different numbers of cars as well. How do I convert their difference between the list price of the car (constant) and the sales price of the car (variable) into a real number that can be compared between the two. Can I index one against another so that I can see their performance on a relative basis?

Example:

A
Constant=30
Constant=50
Constant=100
Variable=20
Variable=35
Variable=87
Difference=10 x 100 cars =1000
Difference=15 x 200 cars = 3000
Difference=13 x 60 = 780

Total: 4780

B
Constant=12
Constant=15
Constant=20
Variable=11
Variable=9
Variable=17
Difference=1 x 300 = 300
Difference=6 x 12 = 72
Difference=3 x 55 = 165

Total = 537

Can anyone help? Sorry if this is a dumb one... not a math guy.",en
1108775,2012-01-20 11:03:07,datasets,"Is there a dataset of websites available anywhere? According to Netcraft, there are 175 million active sites.",oonte,visarga,1166994643.0,https://www.reddit.com/r/datasets/comments/oonte/is_there_a_dataset_of_websites_available_anywhere/,8.0,4.0,,en
1108776,2012-01-20 14:13:17,MachineLearning,Neural network gets an idea of number without counting,oor1a,bubbles212,1294796168.0,https://www.reddit.com/r/MachineLearning/comments/oor1a/neural_network_gets_an_idea_of_number_without/,30.0,16.0,,en
1108777,2012-01-20 16:15:10,statistics,"The Variability Hypothesis, and an example of Bayesian estimation in Python.",oou61,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/oou61/the_variability_hypothesis_and_an_example_of/,2.0,1.0,,en
1108778,2012-01-20 17:35:36,data,Why do you need an outside firm to collect your customer/employee data?,oowx3,LP99,1298392034.0,https://www.reddit.com/r/data/comments/oowx3/why_do_you_need_an_outside_firm_to_collect_your/,1.0,0.0,,en
1108779,2012-01-20 19:59:59,statistics,Stata 12 IC vs. SE (x-post from r/econometrics),op3ay,flannelcladjesus,1298824973.0,https://www.reddit.com/r/statistics/comments/op3ay/stata_12_ic_vs_se_xpost_from_reconometrics/,2.0,3.0,"Hi all, just started up in an econometrics class, I'm an international relations and economics double major. We have to buy a package of Stata for the course, and I know I want to get a perpetual license to take advantage of the student pricing, but I was wondering if it would be better for me to grab SE at the cheaper price or just enjoy the lower cost of IC? With the discount, the perpetual licenses are going for $179 for IC and $395 for SE. I am very interested in economics and have prior experience with JMP and have toyed around a bit in R. Thanks!",en
1108780,2012-01-20 20:13:47,statistics,"I'm not statistical, but I have been wondering on a problem for a while.  Maybe you guys can help?",op3ym,RocketTurtle,1292695203.0,https://www.reddit.com/r/statistics/comments/op3ym/im_not_statistical_but_i_have_been_wondering_on_a/,10.0,15.0,"Let's set up a thought experiment.  Say I have a bag of ping-pong balls, 999 are white and 1 is black.  Let's say I draw a ball from the bag at random, note it's color, and put it back.  I understand that the probability of pulling a black ball is 1 in 1000, and is that same probability for each pull because the ball goes back in.

Let's say a person were to pull balls from the bag (always returning the ball after each pull) until they pulled a black ball, and they noted how many pulls it took. Let's say they did this 100,000 times and noted how many times each number of pulls was needed to pull the black ball. (For instance, the black ball appeared on the 43rd pull 100 times out of the 100,000 tests.)

If one were to graph these results, with the number of pulls required to  find the black ball on the X axis and the number of tests out of 100,000 where it took that many pulls on the Y axis, what would the shape of the graph be?

To my mind, it would be a parabola, with a maximum somewhere between 1 and infinity.  I think this because I would imagine that the probability of pulling the black ball after only a few pulls would be fairly low, but increasing with the number of balls pulled.  I would imagine that the probability of pulling 3000 times without finding the black ball would also be fairly low, so I don't imagine that the function would be linearly increasing, either.

I once wrote a program (I'm working on a masters in computer science) to brute-force this problem (not using statistical methods, but counting random-number generation), and the graph that I got had it's highest point at x=1 and y=probability of pulling the black ball on first pull.  Then the graph dropped toward zero exponentially.  I don't really understand this result, but I'm fairly sure that my program accurately modeled the ball-pulling problem.

Can anyone illuminate me on the actual mathematics of this situation?  I really do apologize for assaulting you with this wall of text.",en
1108781,2012-01-21 00:16:42,datasets,In Search of Women College Graduate statistics by state,opfmm,RobotCowboy,1295412707.0,https://www.reddit.com/r/datasets/comments/opfmm/in_search_of_women_college_graduate_statistics_by/,1.0,3.0,I am doing some economic research for a professor and can not find a data set anywhere. Anybody know of an agency that would have this data?,en
1108782,2012-01-21 05:52:26,statistics,Searching for collaborators for a Bayesian application topic,opt07,AndrewKemendo,1190929741.0,https://www.reddit.com/r/statistics/comments/opt07/searching_for_collaborators_for_a_bayesian/,2.0,7.0,"I figured this or r/math would be the best place to post this. Unfortunately there are no mathematicians anywhere near where I am working and the online collaboration sites have proven unsuccessful as most help venues (StackExchange, MathOverflow) either do not deal with new applications or are working way over my experience level and take too much effort for them to translate the problem. I am not a professional statistician so it takes a little more effort for me to ask just the right question to the pros.

I am attempting to develop a forecasting process using Bayesian inference as the primary tool. This has never been applied to the problem set I am working on in such a fashion and would be a novel implementation. I have already worked most of the problem set, but am running into calculation problems for iterating and updating Bayes rule over many observations. I am looking for someone who is comfortable with graduate level statistics and is a whiz with iterating/updating Bayes theorem. 

This is simply a collaboration and would not be used for profit or educational credit. That said, if successful the process will be submitted to select defense journals.",en
1108783,2012-01-21 12:58:48,artificial,How Google Does AI,oq44j,marshallp,1239903633.0,https://www.reddit.com/r/artificial/comments/oq44j/how_google_does_ai/,27.0,8.0,,en
1108784,2012-01-21 17:32:24,datasets,"Request: Game dataset of ""Versus type games""",oq9i8,Ulvund,1177090152.0,https://www.reddit.com/r/datasets/comments/oq9i8/request_game_dataset_of_versus_type_games/,1.0,11.0,"e.g. FIFA 12 online data, League of Legends 5 vs 5 online data or any other game where 2 or more opponents battle to score some kind of points or victory

I want to run some ranking methods on these types of data",en
1108785,2012-01-21 22:17:29,statistics,SAS input formatting problem,oqk7d,theloniousnole,1315515927.0,https://www.reddit.com/r/statistics/comments/oqk7d/sas_input_formatting_problem/,3.0,6.0,"So I'm trying to write a data step so I can run Proc Ttest, but the raw data I have is in this format.

Name1 Name2

23       34

43       43

87       99

22       87


How do I write the input statement so it reads the first row as 'Names' and the rest as 'Times'. I can't run it through a column input format because then I'll have two variables, and I can only have one once I write the Proc Ttest step (CLASS *var*)",en
1108786,2012-01-21 22:26:32,statistics,"As a statistician, how much work do you do with SQL or other databases? ",oqkl6,Wonnk13,1287022683.0,https://www.reddit.com/r/statistics/comments/oqkl6/as_a_statistician_how_much_work_do_you_do_with/,9.0,13.0,"I know this is a broad question and will vary within specific fields. I'm going to complete my Masters this spring and I was wondering how important it is to have experience with databases on the job market? 
I've built plenty of datasets from scratch and worked with fairly large ones from the government or acedemia, but I store all my data in CSVs and have never had to connect to SQL or some sort of relational database. ",en
1108787,2012-01-21 23:00:41,MachineLearning,AutoCorpus - natural language corpora from large public datasets,oqlyh,autoencoder,1313227511.0,https://www.reddit.com/r/MachineLearning/comments/oqlyh/autocorpus_natural_language_corpora_from_large/,27.0,2.0,,en
1108788,2012-01-22 09:05:51,statistics,I'm a young engineer and I don't know much about statistics... can anyone point out what statistical method/technique could be used to solve my problem? ,or86q,asiandude,1254455204.0,https://www.reddit.com/r/statistics/comments/or86q/im_a_young_engineer_and_i_dont_know_much_about/,0.0,1.0,"In the past, I've developed software to plot these type of diagrams:
[1](http://www.mathworks.com/help/toolbox/comm/ref/2dcolor.png)
[2](http://www.mathworks.com/help/toolbox/comm/ref/3dcolor.png)

Using contour plotting libraries, I could generate an oscilloscope-view of a signal with a 3-dimensional matrix of data. (amplitude vs time vs intensity)

However, I've never been able to figure out how to plot a 2-D version of this using the same 3-dimensional data.
[3](http://www.mathworks.com/help/toolbox/comm/ref/2dline.png)

What statistical method should I look into to obtain these 2D lines/traces?",en
1108789,2012-01-22 16:22:27,statistics,68 95 99.7 rule..,orgek,FapCaptain,,https://www.reddit.com/r/statistics/comments/orgek/68_95_997_rule/,4.0,10.0,"Hey guys,

just a quick question, is there a similar rule that applies to lognormal distributions? The ""rule"" I see from applying mean*stdev^Z is 84-98-99 or something there-abouts.

I have tried finding more information on dealing with geometric means/stdev's and a possible ""Z-score table"", but most resources seem to focus on the arithmetic kind only.

Thanks for any help.",en
1108790,2012-01-22 18:28:45,statistics,SAS help-- Adding p-values to proc tabulate,orjp8,mbaums,1327249027.0,https://www.reddit.com/r/statistics/comments/orjp8/sas_help_adding_pvalues_to_proc_tabulate/,1.0,9.0,"Hey, long time reader, first time poster--things I'm horrible at: ODS outputs, proc tabulate. 

Here is the program I made:
http://tinyurl.com/75yyge8

The aim of it is to create a 2x2 table that has frequency and percent next to each other like # (#). I have that done, but I want more.
 
I want p-values in a column next to each category, after ""overall"". I'm thinking if I create a table of just the P-values with proc freq then merge that part back? I'm not sure exactly and thus I seek help.
",en
1108791,2012-01-22 23:41:52,statistics,Anyone here read the black swan?,orwm5,not_a_creative_alias,1323759510.0,https://www.reddit.com/r/statistics/comments/orwm5/anyone_here_read_the_black_swan/,8.0,4.0,"For those that read it, what did you guys think about it. I'm in the middle of reading it right now. What did you think about his arguments against prediction?",en
1108792,2012-01-23 02:00:32,MachineLearning,Do Random Forests Exhibit Prediction Bias?,os2vz,locster,1222291024.0,https://www.reddit.com/r/MachineLearning/comments/os2vz/do_random_forests_exhibit_prediction_bias/,6.0,20.0,,en
1108793,2012-01-23 05:36:06,datasets,Any music playlists dataset?,oscpp,pomber,1300156715.0,https://www.reddit.com/r/datasets/comments/oscpp/any_music_playlists_dataset/,11.0,4.0,"I'm looking for a dataset with playlists generated by users, something like playlists from grooveshark or 8tracks. 

Thanks",en
1108794,2012-01-23 07:58:40,statistics,Can someone help me confirm some calculations?,osja5,FST,,https://www.reddit.com/r/statistics/comments/osja5/can_someone_help_me_confirm_some_calculations/,6.0,8.0,"Hi, all. I hope it's alright to ask this question here. I recently saw in an article ([here](http://www.sciencemag.org/content/334/6061/1427.full), might require a subscription] the following sentence  
  
&gt; A greater proportion of female rats (6/6) than male rats (17/24) in the trapped condition became door-openers (P &lt; 0.05, χ-square)...  
  
Naturally, the sample size seemed to small for me (although I'm not a statistician, so who am I to judge!). Thus, I plugged these numbers into various online contingency table calculators for the Chi-square and Fisher exact probability tests. In both cases, the resultant p-value exceeded .05 by quite a bit.  
  
Can someone explain to me how they arrived at this result? Am I breaking some cardinal sin by plugging things in without having any idea what I'm doing, did I just goof up somewhere, or is the article itself wrong?  
  
Thanks in advance!!",en
1108795,2012-01-23 10:14:42,statistics,Nearly all Redditors are White??,osnk1,wiggersoe,1327306229.0,https://www.reddit.com/r/statistics/comments/osnk1/nearly_all_redditors_are_white/,1.0,0.0,,en
1108796,2012-01-23 17:50:30,statistics,"Fast vectorized formula for computing a running variance. Suitable for R, MATLAB, &amp; SAS ",osynx,RickWicklin,1327333524.0,https://www.reddit.com/r/statistics/comments/osynx/fast_vectorized_formula_for_computing_a_running/,15.0,1.0,,en
1108797,2012-01-23 20:08:19,statistics,Polychart - A new take on data visualization,ot4qe,jeeyoungk,1261068102.0,https://www.reddit.com/r/statistics/comments/ot4qe/polychart_a_new_take_on_data_visualization/,12.0,3.0,,en
1108798,2012-01-23 20:13:09,statistics,Need some spss help regarding varying levels of treatment,ot4y5,Freebush,1303177243.0,https://www.reddit.com/r/statistics/comments/ot4y5/need_some_spss_help_regarding_varying_levels_of/,2.0,6.0,"Hi guys, just really need some help. I'm currently confused about how I can test this scenario:

I am working on a project where I am trying to find clinical progress. They fill out a quick survey the very first time they come and each other time. The problem is 70% only come once, so I only have one set of data. The other 30% ranges from coming twice to 7 times. I have it in my mind that I will use a dependent samples t test, and go like 1v 2, 1v3, 1v4, 1v5, 1v6, 1v7, since spss will sort out that. The only concern there is the type 1 error increase.
Is there an alternative I can do? I plan on sorting this between male/female and then by the 5 clinics, if that makes a difference, plan on just using the split-file task on spss. 

Thanks for help!",en
1108799,2012-01-23 21:36:53,MachineLearning,How to find the split in regression trees,ot92r,erUserName,1290733949.0,https://www.reddit.com/r/MachineLearning/comments/ot92r/how_to_find_the_split_in_regression_trees/,6.0,14.0,"See: http://math.uprm.edu/~wrolke/esma6665/regtree.htm (How to Grow a tree). I'm having a bit of confusion. 

So we have some predicting variable X, and target Y. We want to find a value that splits X into 2 sets, that minimizes the sum of squared error. So let us say there are n data points. Then there are O(n^2) combinations we would need to test, but the page seems to indicate that this split can be found quickly O(n) maybe? I tried looking in ""The Elements of Statistical Learning"" but it says the same thing, without explaining how and only saying that it can be done very quickly. 

Can anyone provide a bit of enlightenment on this matter? Is O(n^2) somehow considered ""quick"" in this case, or am I just an idiot and missing something? ",en
1108800,2012-01-23 23:31:08,rstats,R Studio? ,otew3,2990299029902990,1327354150.0,https://www.reddit.com/r/rstats/comments/otew3/r_studio/,1.0,0.0,"Does anybody use R-Studio? Is there a strong benefit over the original R-GUI? Are there other free software packages to use R that are better and why? 
I'm thinking of switching to RStudio, but I've heard it has some issues with graphics and certain packages (namely lattice and ggplot?) and am looking for input",en
1108801,2012-01-24 00:51:37,statistics,"can you use statistics to test if odd correlations are 
really systemic errors?",otj6b,jmdugan,1221770863.0,https://www.reddit.com/r/statistics/comments/otj6b/can_you_use_statistics_to_test_if_odd/,1.0,4.0,"I was playing with the Google ngam viewer and got this

http://i.imgur.com/3y89j.png

from comparing these ngams
blue,red,green,yellow,sky,purple,grass,bright,elephant

I would expect that words that co-appear to have shapes that are correlated (similar rises and falls over time), but would not expect words like sky and elephant to have the same shape.

My guess is that there may be a systematic error in the way they've done their measurment, resulting the the characteristic shape, or some bias in the measured sample.  How could one use a statistical test to answer that question, ""Is this strange correlation I see really an error, or is it a real?""

",en
1108802,2012-01-24 02:57:12,MachineLearning,Beautiful Decisions: Inside BigML’s Decision Trees,otpj3,jjdonald,1192132770.0,https://www.reddit.com/r/MachineLearning/comments/otpj3/beautiful_decisions_inside_bigmls_decision_trees/,5.0,2.0,,en
1108803,2012-01-24 06:38:38,MachineLearning,Help a Library address digital content...,ou0p2,yacob_uk,1257357678.0,https://www.reddit.com/r/MachineLearning/comments/ou0p2/help_a_library_address_digital_content/,5.0,14.0,"I work for a large National library, in the digital arena. Almost every day I come across digital text objects that belong to sets, that we are describing (in a library kind of way). 

It strikes me that we could be doing things with ML, especially where we have a large set of related files, and the single description we use in library land is 'collection of 'things' from producer 'A' from Jan 1993.'

This collection set might be a hundred text files. I would love to demonstrate to my library colleagues the benefit we could get by automatically extracting key terms (dates, names, places etc) and producing some basic keyword based summary of the collection to augment the chronological record we produce at the moment. An added advantage of running an ML tool over the set would be a I guess to create a full text index that could also be searched - allowing users to search for their own keys amongst the collection. 

Does this sound (1) plausible and (2) achievable - especially if I do it myself, as one with only rudimentary python and even less MYSQL at the end of their finger tips. 

Suggestions, corrections, mild abuse (where appropriate) and offers of assistance all gratefully received.  ",en
1108804,2012-01-24 14:33:15,MachineLearning,Interactive Graphics with the iplots Package,oudyc,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/oudyc/interactive_graphics_with_the_iplots_package/,1.0,0.0,,en
1108805,2012-01-24 18:58:30,MachineLearning,"$50,000 The Challenge is looking for a portable device capable of a no-contact (“from a distance”) weight measuring of live pigs in the farm setting. (x-post from r/engineering)",oun0j,[deleted],,https://www.reddit.com/r/MachineLearning/comments/oun0j/50000_the_challenge_is_looking_for_a_portable/,8.0,19.0,,en
1108806,2012-01-24 19:39:58,statistics,SNK (Student - Newman - Keuls) grouping analysis question. ,ouovu,f4m1n3,1290015129.0,https://www.reddit.com/r/statistics/comments/ouovu/snk_student_newman_keuls_grouping_analysis/,2.0,0.0,"I understand how the groupings work for the most part but I haven't faced this situation before so i'm not sure how to interpret it.

This is the SNK output from a Latin Squares design ANOVA.  Basically I want to determine which treatment type is the best based on the lowest mean.  

I see that 'c' and 'b' are grouped together but 'e' and 'c' are also grouped together.  'e' and 'b' are not in the same grouping but since they aren't significantly different from 'c', my intuition is to say that there is no statistically significant difference between 'e', 'c', and 'b' thus those three would be considered the best choices since they have the lowest mean.


    Means with the same letter are not significantly different.
    SNK Grouping	Mean    N	 treatment
	    A	    3.4200	    5	    a
	    A			
	    A	    3.1400	    5	    d
				
	    B	    2.7600	    5	    e
	    B			
    C	B	    2.5400      5    	c
    C		    		
    C		    2.2600	    5    	b    ",en
1108807,2012-01-24 19:51:27,statistics,How can I recreate this regression? ,oupfx,helpwcurve,1327426978.0,https://www.reddit.com/r/statistics/comments/oupfx/how_can_i_recreate_this_regression/,0.0,0.0,"Hi guys, I'm not too savvy when it comes to statistics/graphing/excel/etc. so I wanted to see if you guys could give me a hand.

I want to recreate the curve seen in this scatter-plot (preferable in excel or stata, but open to other suggestions as well): http://imgur.com/YzrHS

As you'll see some points on the scatter-plot have names and some don't. But what I'm really interested is in recreating the curve. I'm not even sure how to explain the curve, but it seems like it passes through the highest points of all of the plotted points (or at each given x-value).

Thanks for the help guys!
",en
1108808,2012-01-24 22:35:13,statistics,Using statistics to determine the % of finishers for the 2012 Tour de France.,ouxob,dezert1,1326242243.0,https://www.reddit.com/r/statistics/comments/ouxob/using_statistics_to_determine_the_of_finishers/,7.0,0.0,,en
1108809,2012-01-24 23:55:34,statistics,I am serializing my new book on complexity science; comments welcome.,ov1u8,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/ov1u8/i_am_serializing_my_new_book_on_complexity/,18.0,8.0,,en
1108810,2012-01-25 02:09:45,computervision,"OpenCV, Visual Studio, and Python",ov8na,TikaGod,1264029053.0,https://www.reddit.com/r/computervision/comments/ov8na/opencv_visual_studio_and_python/,7.0,8.0,"I am writing a C. elegans tracker and I think that OpenCV would be a great tool to use. However, I'm running into a few problems, primarily, which language to use. I am familiar with Python and C++. Since my non-programming boss will be helping me, i think that python would be the best choice. However, he'd like to use an IDE like Visual Studio to help him along so he doesn't have to learn some of the basics. Unfortunately, I can't figure out how to get OpenCV and python to work with Visual Studio (or any other IDE for that matter).

Is there a way to get OpenCV, Python, and VS to work together nicely? Or should I give up and just use C++?",en
1108811,2012-01-25 03:27:02,statistics,Modelling successes for a random number of trials,ovcg4,hypermonkey2,1301460132.0,https://www.reddit.com/r/statistics/comments/ovcg4/modelling_successes_for_a_random_number_of_trials/,5.0,10.0,"Hi all! 

I'm trying to model the successes for a random number of trials. I mean that I have observations of the form ""8 successes out of 10"" and ""9 successes out of 13"" for each experiment, where the number of trials (8 and 10 in this case) were not set by the investigator and are random in this way.

A paper on the subject recommended a bivariate Poisson (meant in the multivariate sense) regression. I think this might be overly complicated... would a simple  kind (such as negative binomial) regression work?

thanks in advance!",en
1108812,2012-01-25 07:29:19,statistics,"If two sports teams are playing each other. How many more times 
would one team need to win more than the other before it is 
declared the truly better team rather than random chance, in a 
way that is statistically significant?",ovoht,[deleted],,https://www.reddit.com/r/statistics/comments/ovoht/if_two_sports_teams_are_playing_each_other_how/,3.0,10.0,,en
1108813,2012-01-25 11:36:29,statistics,Help for some normal distribution,ovvrm,[deleted],,https://www.reddit.com/r/statistics/comments/ovvrm/help_for_some_normal_distribution/,1.0,1.0,,en
1108814,2012-01-25 17:05:04,artificial,Anti-reCAPTCHA enhancement and porting project,ow3g5,nadie854,1295106489.0,https://www.reddit.com/r/artificial/comments/ow3g5/antirecaptcha_enhancement_and_porting_project/,9.0,0.0,,en
1108815,2012-01-25 17:16:38,MachineLearning,Excerpts from Redditor AllenDowney's new book 'Think Complexity' ,ow3vk,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/ow3vk/excerpts_from_redditor_allendowneys_new_book/,15.0,0.0,,en
1108816,2012-01-25 23:14:55,analytics,Omniture training? ,owkp0,Kick79,1326251090.0,https://www.reddit.com/r/analytics/comments/owkp0/omniture_training/,1.0,0.0,"I'm a student but will soon be graduating and looking for a job in web analytics. Does anyone know how to get training in SiteCatalyst? Can I, as an individual, get online training away from an office? The Omniture site isn't very clear on this issue. Thanks! ",en
1108817,2012-01-25 23:19:56,MachineLearning,RDF based image and topic dump,owkxz,Curry_Boy,1288033733.0,https://www.reddit.com/r/MachineLearning/comments/owkxz/rdf_based_image_and_topic_dump/,5.0,1.0,,en
1108818,2012-01-25 23:54:45,artificial,could the fact that computation is discrete but the universe is continuous present a problem for conscious AI? (x-post from neurophilosophy),owmrm,[deleted],,https://www.reddit.com/r/artificial/comments/owmrm/could_the_fact_that_computation_is_discrete_but/,0.0,7.0,please join the discussion [here](http://www.reddit.com/r/neurophilosophy/comments/owm6h/could_the_fact_that_computation_is_discrete_but/),en
1108819,2012-01-26 00:37:26,statistics,Chi-square related question in ecology,owoxe,Picea_mariana,1326836078.0,https://www.reddit.com/r/statistics/comments/owoxe/chisquare_related_question_in_ecology/,3.0,5.0,"I want to compare the density of trees in sites that are composed of two types of flora (cover). I have the total area of each site and the area of both floras as well. I want to test if there are significant differences between the number of trees found in each type. I've been told to use a Chi-square test, but that only works if I pool all my sites together to form a global frequency in each type. I would like to add a random effect of the sites in the test using SAS but i don't know how. 
Example of my code :
*/proc freq data = forested;
   tables cover/ chisq testp=(77.9754, 22.0246) ;
weight freq;
  run;/*

THANKS!

TL;DR : can you add a random effect (for sites) in a chi-square test in SAS v.9?",en
1108820,2012-01-26 00:37:51,computervision,Simple trig formula(s) I just can't remember (or find) for figuring position based on the visual distortion of a fix target.,owoxw,Jigokuro,1319435840.0,https://www.reddit.com/r/computervision/comments/owoxw/simple_trig_formulas_i_just_cant_remember_or_find/,4.0,11.0,"I asked in [/r/robotics](/r/robotics) but they totally missed the question (though recommended coming here), so to clarify before anything else; I have a great camera and viewing software, it is well calibrated and can find the target easily. This question has nothing to do with hardware or software. It is math, just math. &amp;#3232;\_&amp;#3232;

So my situation is this: I have a robot looking at a fixed rectangle from some position; knowing the size of the rectangle's sides, what formula would I use to find  
1. The distance to the target, (simple, I just forgot.)  
2. The angle from the target, (slightly more difficult.)

These probably aren't complicated, but I don't remember, and I don;t know what doing this is *called* (it isn't reverse keystoning, though that seems accurate) so it is hard to google/wiki it. :/",en
1108821,2012-01-26 02:45:49,artificial,"If you need realistic and natural looking artificial Areca Silk Tree, Just Beautiful. Check it out.",owv79,cchic,1312853008.0,https://www.reddit.com/r/artificial/comments/owv79/if_you_need_realistic_and_natural_looking/,1.0,0.0,,en
1108822,2012-01-26 03:46:22,statistics,Help with implementation of Scan Statistic (not easy),owxzz,Build-a-Reggit,1322454982.0,https://www.reddit.com/r/statistics/comments/owxzz/help_with_implementation_of_scan_statistic_not/,2.0,10.0,"Hello Reddit!

  I've implemented a method to examine a time series by successive applications of a KS test, sampling random segments of data and comparing it against similarly sized portions of my data.  The idea is to look for unusual events in the data.  It works well, but is not at all very efficient (it's killing my poor computer on Matlab).

  I've found a couple interesting papers on the ""Scan Statistic,"" but to be be honest I can't crack the notation and follow the logic.  Is there anyone here who can explain it to me or (better) point me to a  simple implementation in MATLAB?  I'm a physicist and comfortable with the math and programming, but don't speak stat like you guys.

Anyway, thanks for your help.  In the meantime, I'm having to press ahead with my KS test implementation as I am under some deadlines... but I really want to learn more!

Example:  http://www-stat.stanford.edu/~nzhang/2007_JASA_WeightedScan.pdf

Thanks for your help reddit!
  ",en
1108823,2012-01-26 03:47:55,statistics,How to do an orthogonal contrast in SPSS?,owy2n,[deleted],,https://www.reddit.com/r/statistics/comments/owy2n/how_to_do_an_orthogonal_contrast_in_spss/,1.0,1.0,"I am a biology undergrad trying to do statistical analysis for a project I did. My data consists of three different treatments (two of which being controls for the third) and two different time points obsevred (one a control for the other), so only one time point x treatment combination had the data I actually cared to see. The rest was to measure background or other confounding effects. 

I talked to my statistics professor today (I havent learned much of anything yet) and he told me that I had used a 3x2 factorial design and would want to do an orthogonal contrast. Using SPSS, how would I go about doing this? Thanks.",en
1108824,2012-01-26 11:11:37,statistics,Do you know of any online tutorials that teach basic programming with R?,oxeks,[deleted],,https://www.reddit.com/r/statistics/comments/oxeks/do_you_know_of_any_online_tutorials_that_teach/,0.0,0.0,"I'm teaching a intro statistics class with R.  I've been giving homework examples that require some simulation, such as tossing two dice and recording the sum.  Most of my students are CS majors or engineers who know matlab, and they do fine.  

However, I have a few students who are really struggling with the class. They've never heard of a for loop, have trouble accessing items in an array, don't understand why they can't see the variables in a function.  I'm looking for some resources that could give them a crash course so they won't fall too far behind.  

Most beginner tutorials I've found seem to focus on analyzing data with less info on making functions or programming concepts. Any suggestions?",en
1108825,2012-01-26 16:26:48,statistics,"Explaining coincidences. Some seemingly ""surprising"" events are actually likely to happen to someone, somewhere, sometime.",oxl3p,RickWicklin,1327333524.0,https://www.reddit.com/r/statistics/comments/oxl3p/explaining_coincidences_some_seemingly_surprising/,10.0,5.0,,en
1108826,2012-01-26 17:45:36,statistics,Open Access to Statistics Papers: arXiv.org e-Print archive,oxo2b,RA_Fisher,1299707119.0,https://www.reddit.com/r/statistics/comments/oxo2b/open_access_to_statistics_papers_arxivorg_eprint/,4.0,0.0,,en
1108827,2012-01-26 20:37:36,statistics,Using historical usage data to estimate how long a resource may last,oxw9l,[deleted],,https://www.reddit.com/r/statistics/comments/oxw9l/using_historical_usage_data_to_estimate_how_long/,1.0,2.0,"Can anyone get me pointed in the right direction to figure out how to use historical usage data to determine how long a resource could last.  I dont know statistics and would like to learn to use it to predict real world problems.

Example:
You have a bucket of candy.  You know the total number of pieces at time zero.  You know the total number of pieces remaining at the end of each day and this data goes back months.  Each day a different number of pieces are missing.  When will you run out of candy?",en
1108828,2012-01-26 23:02:52,datasets,Accessing OpenStreetMap API using R,oy3s4,talgalili,1271226645.0,https://www.reddit.com/r/datasets/comments/oy3s4/accessing_openstreetmap_api_using_r/,1.0,0.0,,en
1108829,2012-01-26 23:15:45,statistics,help on correlation tests for non normal distributions,oy4ew,needsomerest,1327278838.0,https://www.reddit.com/r/statistics/comments/oy4ew/help_on_correlation_tests_for_non_normal/,0.0,21.0,"Dear redditors,
I'd like to as you some hints on a problem I'm facing.
I'll start, I'm not a statitistician and the most sophisticated things I did on the subject were Pearson correlation tests and Chi squared tests.
I need to find if there's a correlation between two sets of independent data. These data belong to a population which is not normal (it's a power-law distribution). For this reason I was thinking to use Kendall's tau or Spearman correlation, but I'm unsure about that.
Do you have any suggestions or references I can read about this issue?

EDIT: wrong ""independent data"" phrasing: I mean independently measured data.",en
1108830,2012-01-27 01:58:48,statistics,Where can I find more info on the probabilities of mutually (jointly) independent events?,oycfu,bms42,1214582889.0,https://www.reddit.com/r/statistics/comments/oycfu/where_can_i_find_more_info_on_the_probabilities/,5.0,4.0,"First, I'm not a statistician, I am simply trying to solve a problem that's come up in the course of a game I'm designing.  In order to balance the rules, I need to figure out a set of probabilities.  My research suggests this is some form of [mutual independence] (http://www.cut-the-knot.org/Probability/MutuallyIndependentEvents.shtml).

The problem is this:  given probabilities for 5 independent events, what is the overall probability for *3 or more of the events occurring*?

In my spreadsheet I'm using these as an example

    P(A) = .9
    P(B) = .7
    P(C) = .5 
    P(D) = .3
    P(E) = .1

According to the matrix I've created, there are 16 unique ways for 3 or more events to occur, however it's obvious that I can't simply calculate the probabilities of each occurring and add them up (I get a value well above 1).

Any help you can offer?  I tried this first in /r/homework as it does have a ""homework"" type of tone to it, but I think it's too complicated to resolve there.  Tips on further reading, especially if it contains explanations in lay terms, would be much appreciated.

Thanks for your time!",en
1108831,2012-01-27 03:03:27,MachineLearning,Case Updates N-Series Loaders and Backhoes - Site Prep | HYDRAULICS - News,oyfgb,kamilrhu,1226023594.0,https://www.reddit.com/r/MachineLearning/comments/oyfgb/case_updates_nseries_loaders_and_backhoes_site/,1.0,0.0,,en
1108832,2012-01-27 04:30:23,statistics,College Bowl Pick 'Em 2011,oyjlr,RA_Fisher,1299707119.0,https://www.reddit.com/r/statistics/comments/oyjlr/college_bowl_pick_em_2011/,3.0,4.0,"This year I did a college bowl pick 'em challenge using statistical methods.  Please check out my blog post and feel free to give me critical feedback.  This post and code is all a work in progress!

http://rafisher.wordpress.com/2012/01/27/college-bowl-pick-em-2011/",en
1108833,2012-01-27 06:12:10,rstats,Exporting scoring models as C or Java,oyodz,pwsegal,1230629560.0,https://www.reddit.com/r/rstats/comments/oyodz/exporting_scoring_models_as_c_or_java/,1.0,0.0,"Does anyone know of any modules that will allow the scoring routines of models built in R to be exported out as C or Java. I had a look around but couldn't find anything, I don't expect this would be available for all the weird and wonderful modelling types around, but hopefully someone has implemented it for the common modelling types, like logistic regression, centroid based clustering, decision tree's, etc.
thanks.
",en
1108834,2012-01-27 14:12:38,MachineLearning,Probabilistic classification algorithm?,oz1go,solen-skiner,1298565628.0,https://www.reddit.com/r/MachineLearning/comments/oz1go/probabilistic_classification_algorithm/,2.0,12.0,"Say I have a dataset and their respective labels, which are discreet. However, examples with the same features often have different labels, while examples with different features can share label. What are some good algorithms to find the probability for every label given the input features?",en
1108835,2012-01-27 14:25:58,MachineLearning,"How to use spotty labeled training data, knowing there is correlation between features and whether an example is labeled or not?",oz1r4,solen-skiner,1298565628.0,https://www.reddit.com/r/MachineLearning/comments/oz1r4/how_to_use_spotty_labeled_training_data_knowing/,9.0,11.0,"I have a data-set with both labeled and unlabeled examples. Due to my knowledge of the domain I know that some of the examples features greatly affect whether an example is labeled or unlabeled, causing very biased label data and grave errors in prediction. How can I use this knowledge of the correlation between the features and the probability of an example having a label to reduce bias and prediction errors?",en
1108836,2012-01-27 15:54:34,statistics,Statistical analysis of emotions and opinions at Digg website,oz3sc,ComicFoil,1258761563.0,https://www.reddit.com/r/statistics/comments/oz3sc/statistical_analysis_of_emotions_and_opinions_at/,6.0,0.0,,en
1108837,2012-01-27 18:38:58,statistics,Estimating popularity based on Google searches: Why it's a bad idea,oz9y6,RickWicklin,1327333524.0,https://www.reddit.com/r/statistics/comments/oz9y6/estimating_popularity_based_on_google_searches/,2.0,3.0,,en
1108838,2012-01-27 18:52:07,statistics,Help fitting two gaussian mixture,ozaje,forever_erratic,1256947469.0,https://www.reddit.com/r/statistics/comments/ozaje/help_fitting_two_gaussian_mixture/,2.0,3.0,"Hello,

I have three treatments. Each repetition of the experiment within each treatment gave me data on the amount of flourescence across twenty positions (""bins"" since the actual positions differed slightly by rep). Plotting the means of fluorescence by bin for each treatment clearly shows  the fluorescence is made of two gaussians - this makes sense and was expected based upon the experiment.

Here is the plotted data: (warning pdf)

http://dl.dropbox.com/u/6341625/normalized%20fluorescence2.pdf

What I'm interested in is learning the means and variance for each gaussian in each treatment. I'm using R. I'm assuming I should use a tool like those in the library mixtools to do this but I'm having some trouble. I'm pretty sure mixtools expects the data to be histogram-like, where mine is more like a probability density function.

I'd appreciate any help you can offer, thanks!",en
1108839,2012-01-27 20:34:18,artificial,Small project in AI using the Python Pyke module (X-post with r/python),ozfbr,huhuh11,1305807358.0,https://www.reddit.com/r/artificial/comments/ozfbr/small_project_in_ai_using_the_python_pyke_module/,7.0,2.0,"Hi reddit,

As an assignment, we had to do an AI project involving multi-agents system. We decided to do it with Python and the knowledge engine Pyke (http://pyke.sourceforge.net/). We decided to release it because there isn't much code available online that uses Pyke. We hope this can help other people using Pyke. More details can be found on the project page.

Project page :
https://github.com/bok/AI-with-Pyke",en
1108840,2012-01-27 23:00:31,AskStatistics,"Say you are the first person(of three) to spin the wheel on The Price Is Right. If you land on 70 cents, should you actually spin the wheel again?",ozmb0,Mojo17,1272424357.0,https://www.reddit.com/r/AskStatistics/comments/ozmb0/say_you_are_the_first_personof_three_to_spin_the/,5.0,7.0,What system would give you the best chance to win?,en
1108841,2012-01-28 00:21:25,statistics,Is there a name for the interval containing 95% of the data in a sample?,ozq68,tadrinth,1148066286.0,https://www.reddit.com/r/statistics/comments/ozq68/is_there_a_name_for_the_interval_containing_95_of/,4.0,19.0,"Is there an official statistics term for an interval containing 95% of the data in a sample which is normally distributed?  I am trying to improve the lab manual for the lab I am TAing; the statistics appendix currently refers to that interval as a confidence level.  The confidence level usually just refers to the 95% part of a 95% confidence interval.  A fellow TA suggested maybe coverage interval, which seems to be used by meteorologists to mean what I want but means something different for statisticians.",en
1108842,2012-01-28 12:31:59,statistics,Help needed: How do I know whether a subject has executed the primary or the secondary task?,p0dop,b4rk,1325440599.0,https://www.reddit.com/r/statistics/comments/p0dop/help_needed_how_do_i_know_whether_a_subject_has/,0.0,6.0,"A friend of mine is devising an experiment involving a certain time estimation theory. The basic idea is that with rising cognitive load our time estimation ability suffers. To induce cognitive load he plans to use the [n-back task](http://en.wikipedia.org/wiki/N-back). After what they believe to be e.g. 5 seconds the subjects are then supposed to press a button.

The obvious problem with that there is no easy way to know whether the subjects have shifted their available ressources towards the time estimation task, the secondary task or somewhere in between. Furthermore, we do not know anything about the speed-accuracy-tradoff for the secondary task. Unfortunately, the subjects' criteria for these task executions are decisive for any interpretation of the outcome. 

Is there any elegant statistical solution for that? I was first thinking of Signal Detection Theory, but I convinced myself that for a n-back task the true positives / false positives distinction does not make much sense. second, there are two criteria involved: the first for whether to attend to the secondary task of not, the second whether to place emphasis on reaction time or accurate response (the speed-accuracy-tradeoff). I was then thinking towards structural equation modeling, but I only know a few basics and am not even sure that this is applicable here at all.

Any help on this would be greatly appreciated.",en
1108843,2012-01-28 14:22:06,computervision,top 10 H1-B Visa Users Offshore Companies,p0ff3,michalsemen,1303980907.0,https://www.reddit.com/r/computervision/comments/p0ff3/top_10_h1b_visa_users_offshore_companies/,1.0,0.0,,en
1108844,2012-01-28 14:42:52,MachineLearning,"The philosophy of complexity: part two of my book, Think Complexity [xpost from statistics]",p0fr0,AllenDowney,1300587223.0,https://www.reddit.com/r/MachineLearning/comments/p0fr0/the_philosophy_of_complexity_part_two_of_my_book/,10.0,0.0,,en
1108845,2012-01-28 20:37:03,statistics,Anyone interviewed with the Census Bureau?,p0pe5,[deleted],,https://www.reddit.com/r/statistics/comments/p0pe5/anyone_interviewed_with_the_census_bureau/,5.0,7.0,"Is there a subreddit for this type of question?

If anyone has interviewed with the Census Bureau, what was the interview process like?",en
1108846,2012-01-29 00:31:49,datasets,last.fm release data for 'best of 2011' series (look on right for link),p0z1b,[deleted],,https://www.reddit.com/r/datasets/comments/p0z1b/lastfm_release_data_for_best_of_2011_series_look/,7.0,0.0,,en
1108847,2012-01-29 00:36:59,statistics,Where can I find some good information on probability theory in computer networks.,p0z8t,PureLife,1283956555.0,https://www.reddit.com/r/statistics/comments/p0z8t/where_can_i_find_some_good_information_on/,2.0,1.0,"Mainly examples like the probability of how big a packet could be, the time it arrives etc.",en
1108848,2012-01-29 03:03:56,MachineLearning,"What methods are there for selecting an ""appropriate"" cutoff height in a dendrogram when performing hierarchical clustering on sample data points to partition a hyperspace into subspaces?",p14yx,hyppo,1288855163.0,https://www.reddit.com/r/MachineLearning/comments/p14yx/what_methods_are_there_for_selecting_an/,5.0,15.0,"Little bit of background: 

I'm looking at using the [CHAMELEON](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.5847) hierarchical clustering algorithm to create a [dendrogram](http://en.wikipedia.org/wiki/Dendrogram) of sample data points generated by a procedure that imitates an arbitrary problem space.  I'm looking for a way to choose an ""appropriate"" height in the dendrogram in order to partition the problem space in an ""intuitive way.""  Does anyone know of an algorithm for doing this?  I've searched, but either such an algorithm doesn't exist (which I highly doubt) or I'm choosing the wrong words in my searches.  If it makes any difference, the clusters will probably end up being hyper-ellipsoidal. 

**EDIT:** Also, I forgot to mention that these are unlabeled/uncategorized data points, so I cannot compare the clustering with any ground truth",en
1108849,2012-01-29 03:07:11,statistics,Graduate Programs in Environmental/Applied Science Statistics?,p152t,Copse_Of_Trees,1301173243.0,https://www.reddit.com/r/statistics/comments/p152t/graduate_programs_in_environmentalapplied_science/,0.0,2.0,"Hello all! I'm interested in gathering program recommendations for applied statistics graduate programs, especially those with a focus on the natural sciences. I graduated several years ago with a B.S. in Earth Science. I've since been working as a GIS contractor in environmental science and it's time to go get that M.S degree.

I've done a lot of work crunching spatially explicit data sets and want to learn more advanced techniques. Am very interested in data mining/visualization and hope to become an environmental statistician. Any program that utilizes GIS is some way is a bonus.

Reddit gold goes to any person who suggests an institution that I apply and then am accepted into.

Some schools I'm already looking at:

Oregon State

Colorado State

Ohio State

Rutger's

Penn State",en
1108850,2012-01-29 05:59:25,MachineLearning,what is the State of the art in finding 'features' in images,p1bl6,qwsazxerfdcv,1287517243.0,https://www.reddit.com/r/MachineLearning/comments/p1bl6/what_is_the_state_of_the_art_in_finding_features/,14.0,17.0,"i was going through the Computer vision and pattern recognition, conference  papers  over the last few years , and was wondering what is the state of the art in finding 'dense' features in an image which may then be used to do image based content retrieval. ",en
1108851,2012-01-29 14:45:53,MachineLearning,Ask not what accuracy your algorithm achieves but what value it can add,p1n14,piikac,1309800867.0,https://www.reddit.com/r/MachineLearning/comments/p1n14/ask_not_what_accuracy_your_algorithm_achieves_but/,9.0,1.0,,en
1108852,2012-01-29 16:58:40,artificial,"waffles - easy to use but powerful, modern and fast, command line machine learning tools",p1pi3,marshallp,1239903633.0,https://www.reddit.com/r/artificial/comments/p1pi3/waffles_easy_to_use_but_powerful_modern_and_fast/,19.0,0.0,,en
1108853,2012-01-29 21:40:25,MachineLearning,"National Science Foundation said it was coming in the '90s, here it is: machine reads echo cardiogram project 30 minutes start to finish...",p1z6t,SeanHallahan,1313775475.0,https://www.reddit.com/r/MachineLearning/comments/p1z6t/national_science_foundation_said_it_was_coming_in/,12.0,1.0,,en
1108854,2012-01-30 00:08:42,computervision,What kind of sorcery is this?!,p23z9,[deleted],,https://www.reddit.com/r/computervision/comments/p23z9/what_kind_of_sorcery_is_this/,0.0,2.0,,en
1108855,2012-01-30 10:35:01,computervision,How long does OpenCV traincascade take to complete?,p2t18,spazzm,,https://www.reddit.com/r/computervision/comments/p2t18/how_long_does_opencv_traincascade_take_to_complete/,2.0,4.0,"I've been running traincascade for a week now, and the last output was    

     ===== TRAINING 1-stage =====  
     &lt;BEGIN   
     POS count : consumed   991 : 991  

As I understand it, there are 20 stages (I use default parameters).   
Does training take over 20 weeks? When should I give up an try with different parameters?

I'm running the training on a Core 2 duo @ 3 GHz, 4 GB DDR2 RAM.

According to htop I'm only using one core (this is the default Ubuntu build, version 2.3.1).

At what point should I just give up and get a more powerful computer? Have I made some mistake that means the training will never complete?",en
1108856,2012-01-30 17:48:29,statistics,Omitted Variable Bias?,p336c,latortilla,1299276404.0,https://www.reddit.com/r/statistics/comments/p336c/omitted_variable_bias/,4.0,3.0,"Hi,
does anyone have a good source on omitted variable bias in regressions in more complex systems than 2 variables? 

I am very familiar with the basic y = b1*x1 + b2*x2 --&gt; if we run y = b1*x1, then estimated b1 = b1 + b2*bx1x2. I am looking for maybe articles/textbook that deals with specifications with more variables and consequences of correlations in larger systems when multiple variables are omitted and multiple included etc.
If anyone has any references/ideas that would be great!
cheers!",en
1108857,2012-01-30 19:12:59,MachineLearning,useR! 2012: Earlybird Registration,p36pf,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/p36pf/user_2012_earlybird_registration/,1.0,0.0,,en
1108858,2012-01-30 19:13:24,rstats,useR! 2012: Earlybird Registration,p36q3,talgalili,1271226645.0,https://www.reddit.com/r/rstats/comments/p36q3/user_2012_earlybird_registration/,6.0,2.0,,en
1108859,2012-01-30 19:14:00,statistics,Printing nested tables in R using {reshape} and {tables},p36r6,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/p36r6/printing_nested_tables_in_r_using_reshape_and/,3.0,4.0,,en
1108860,2012-01-30 20:05:55,statistics,Is this in any way possible? Adding confidence/propability.,p394c,Congruence,1276174673.0,https://www.reddit.com/r/statistics/comments/p394c/is_this_in_any_way_possible_adding/,2.0,15.0,"I'm trying to make a hypothetical example concerning bias in groups and among individuals. Truth be told, before I started trying to actually compute it, my brain intuitively told me that it should be possible, but as I've tried I'm beginning to doubt that it is - one thing is certain, my rudimentary knowledge of propability doesn't cut it, so I'm hoping you guys can take a quick look.

Assume two completely rational actors, who can accurately communicate their confidence. They choose the 1st answer to a binary question and both say that they are 60% certain it is correct.

Now, assuming that they are both completely rational, and both communicated their confidence perfectly, what would their joint confidence be?

Is it possible to compute this? Can you point me in the right direction? Any help would be fantastic!

Thanks!",en
1108861,2012-01-30 20:23:41,rstats,"Help Needed, populating fields with NA",p3a0u,[deleted],,https://www.reddit.com/r/rstats/comments/p3a0u/help_needed_populating_fields_with_na/,1.0,0.0,"The data set I'm using has some blanks in it. R has converted all of these blanks into NA's with the exception of 1 column which it has left as blanks.
ex: 1  2  1  3  4
    NA 3  4      4
     3  1 NA 3  5
The column left blank errors every time I try and use it in t.tests. When I've tried to populate the blanks with NAs using ""test[(test=="""")] &lt;- NA "" it populates it with a &lt;NA&gt; which also errors. 

Anyone know how to fix this?",en
1108862,2012-01-30 20:48:10,rstats,"Help Needed, Populating Blanks With NAs",p3b7o,[deleted],,https://www.reddit.com/r/rstats/comments/p3b7o/help_needed_populating_blanks_with_nas/,1.0,0.0,"The data set I'm using has some blanks in it. R has converted all of these blanks into NA's with the exception of 1 column which it has left as blanks. 
ex:
    1   2  1  3  4
    NA 3  4  _  4 
    1  NA 3  2  5 
The column left blank errors every time I try and use it in t.tests. When I've tried to populate the blanks with NAs using ""test[(test=="""")] &lt;- NA "" it populates it with a &lt;NA&gt; which also errors.

Anyone know how to fix this?
",en
1108863,2012-01-30 21:42:14,statistics,Need basic stats pointers: testing if a distribution is too perfect!,p3dw1,Stishovite,1286784491.0,https://www.reddit.com/r/statistics/comments/p3dw1/need_basic_stats_pointers_testing_if_a/,2.0,6.0,"Greetings statistical redditors!

I'm a geologist who is rather rusty on my stats; I thought it would be good to test myself against a real-world problem. I know this should be really easy but it's proving vexing:

My friend reviews movies and has accrued a healthy 330 reviews. These are distributed between 0 and 4 stars. When I looked at his statistics, I was struck by how closely they seemed to conform to a normal distribution (i.e. 2-star rated was the highest, similar counts of 0 and 4 star ratings).

I was immediately interested in how probable such a distribution (i.e. so close to perfectly normal) is given the small sample size. I used a normal test and received a P-value of .008,  which I take to mean that the distribution is normal with above 99% confidence. But how can I say something about how this relates to the average expected p-value of a ""typical"" normal distribution with n=330?

Ultimately I want to test whether the distribution is natural or has been doctored to conform too closely to the normal curve. Any pointers?",en
1108864,2012-01-30 22:35:12,rstats,"Help Needed, Populating Blanks with NAs",p3gi8,hanger,1289704485.0,https://www.reddit.com/r/rstats/comments/p3gi8/help_needed_populating_blanks_with_nas/,5.0,5.0,"The data set I have has some blanks in it. R has filled them in with NA for me with the exception of one column in which the blanks persist ([example](http://i.imgur.com/XeC7W.jpg)). When I try to fill in the blanks with ""mydataframe[is.na(mydataframe)]&lt;-NA "" the blanks are filled with &lt;NA&gt; ([example](http://i.imgur.com/Bk87P)) which still errors when I try to use the column in t.tests.

Is there a way to fill the empty cells with NA rather than &lt;NA&gt;?",en
1108865,2012-01-30 23:43:32,analytics,Tracking problem,p3jwf,oddprocedure,1327893335.0,https://www.reddit.com/r/analytics/comments/p3jwf/tracking_problem/,0.0,8.0,"I have analytics on my pages in wordpress. the issue is that the form that I use to collect leads is php, so the url doesnt change, the form and the success page are the same. 

How can I track these leads using adwords and analytics? ",en
1108866,2012-01-31 01:52:02,datasets,Request: SMS (Text) to English Parallel Corpus,p3qdx,[deleted],,https://www.reddit.com/r/datasets/comments/p3qdx/request_sms_text_to_english_parallel_corpus/,0.0,0.0,Is anyone aware of a publicly available parallel corpus of sms to proper english words? ,en
1108867,2012-01-31 02:12:41,statistics,How do statisticians calculate their margin of error?,p3red,[deleted],,https://www.reddit.com/r/statistics/comments/p3red/how_do_statisticians_calculate_their_margin_of/,3.0,4.0,"I recently saw that Gallup that stated 50% of Americans support marijuana legalization, with a +/- 4% margin of error, and a 95% confidence level.

Their sample size was 1,008 people of varying backgrounds. 

How do they pick a confidence level and a margin of error? Is there a margin of error when calculating a margin of error (meta-statistics?) ?

[Gallup Survey](http://www.gallup.com/poll/150149/record-high-americans-favor-legalizing-marijuana.aspx)",en
1108868,2012-01-31 06:11:29,statistics,Can anyone help me with some stats?,p439v,goatsftw,1322332270.0,https://www.reddit.com/r/statistics/comments/p439v/can_anyone_help_me_with_some_stats/,0.0,10.0,"Hello all,

I'm currently writing a scientific paper due in for marking in a couple of days. Due to being ill and then being lazy, I have a serious lack of stats tests knowledge.

What would the appropriate test be to check which of 3 species has the greatest distributional abundance at several different heights. Or, which of the 3 objects is the most common over several different stations. Also how would I enter this data using SPSS

It would be a great help if someone could at least put me on the right track!",en
1108869,2012-01-31 16:22:57,MachineLearning,Shree Isaradevi Machinery,p4jtt,bhaveshgajjar1414,1328019695.0,https://www.reddit.com/r/MachineLearning/comments/p4jtt/shree_isaradevi_machinery/,1.0,1.0,,en
1108870,2012-01-31 17:08:42,statistics,"Four essential functions for statistical programmers to understand. Distributions, PDF, CDF, quantiles, and random values...which is which?",p4liz,RickWicklin,1327333524.0,https://www.reddit.com/r/statistics/comments/p4liz/four_essential_functions_for_statistical/,3.0,2.0,,en
1108871,2012-01-31 17:58:05,statistics,"Journal of Statistical Software, Volume 46, Issue 1 (Jan 2012)",p4nhq,Professor_IR,1310512226.0,https://www.reddit.com/r/statistics/comments/p4nhq/journal_of_statistical_software_volume_46_issue_1/,2.0,1.0,,en
1108872,2012-01-31 20:19:02,statistics,Poker question (for fun),p4u1o,parrhesia,1295328337.0,https://www.reddit.com/r/statistics/comments/p4u1o/poker_question_for_fun/,5.0,10.0,"I'm trying to keep my mind intact while unemployed, so I'm working on a project that involves some stats. I've come up with a couple answers to this problem, but they don't match, so I thought I might get some input.

Suppose you're playing Texas Hold Em and you have two unmatching cards in your hand. What is the probability that by the end of the round you have a pair (and not anything better)? You can just assume that no cards are burned or anything like that -- just assume that you're holding two cards, then five are flipped over from the remaining 50.

Thanks for your help!",en
1108873,2012-01-31 20:37:13,computervision,Is there any free implementation of a bilinear filter that run in O(1) ?,p4uvy,[deleted],,https://www.reddit.com/r/computervision/comments/p4uvy/is_there_any_free_implementation_of_a_bilinear/,1.0,0.0,"I'm a complete newbie in image processing and just started playing with opencv in java (using the javacv wrapper). 

I started playing with my webcam applying some basic filters live and was trying to achieve a particular effect using the bilinear smooth in opencv. The problem is that for the effect that I want to accomplish I need rather big values of r and the implemented algorithm in opencv runs in O( r^2 ). This is way to slow for live video. 

My knowledge of C++ is rather poor but I managed to get a good understanding of the algorithm by taking a look at the opencv sources (that was a lot easier than reading the actual paper linked in the documentation). My first thought was that it should be rather easy to parallelize but after making some calculations I realized that without a good graphic card (and I don't have one) the speed up wouldn't be worth the work (I was thinking about taking a look at opencl). 

After some research I found some papers that claim to achieve bilinear in O(1) (or at least a very good approximation of it) but none of them seems to provide any code. 

So here  am I for the fist time in /r/computervision in the hope that someone can help me. If nobody can find code for this I guess I'll give it a shot and try to understand the papers in question but some code would be a lot better. 

[The link in opncv documentation about bilinear](http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html)

[This seems to be the most mentioned paper about bilinear in O(1)](http://www.merl.com/papers/docs/TR2008-030.pdf)

[Another paper claiming bilinear in O(1)](http://www.cs.cityu.edu.hk/~qiyang/publications/cvpr-10-qingxiong-yang-svmbf.pdf)",en
1108874,2012-02-01 00:38:13,computervision,Is there any free implementation of a bilateral filter that runs in O(1) ?,p56ur,karmaputa,1234209545.0,https://www.reddit.com/r/computervision/comments/p56ur/is_there_any_free_implementation_of_a_bilateral/,2.0,3.0,"I'm a complete newbie in image processing and just started playing with opencv in java (using the javacv wrapper). 

I started playing with my webcam applying some basic filters live and was trying to achieve a particular effect using the bilateral smooth in opencv. The problem is that for the effect that I want to accomplish I need rather big values of r and the implemented algorithm in opencv runs in O( r^2 ). This is way to slow for live video. 

My knowledge of C++ is rather poor but I managed to get a good understanding of the algorithm by taking a look at the opencv sources (that was a lot easier than reading the actual paper linked in the documentation). My first thought was that it should be rather easy to parallelize but after making some calculations I realized that without a good graphic card (and I don't have one) the speed up wouldn't be worth the work (I was thinking about taking a look at opencl). 

After some research I found some papers that claim to achieve bilateral in O(1) (or at least a very good approximation of it) but none of them seems to provide any code. 

So here  am I for the fist time in /r/computervision in the hope that someone can help me. If nobody can find code for this I guess I'll give it a shot and try to understand the papers in question but some code would be a lot better. 

[The link in opncv documentation about bilateral](http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html)

[This seems to be the most mentioned paper about bilateral in O(1)](http://www.merl.com/papers/docs/TR2008-030.pdf)

[Another paper claiming bilateral in O(1)](http://www.cs.cityu.edu.hk/~qiyang/publications/cvpr-10-qingxiong-yang-svmbf.pdf)",en
1108875,2012-02-01 11:23:30,statistics,Need help choosing a field of study for my Masters in Stats,p5xyh,generaal_kosie,1316260719.0,https://www.reddit.com/r/statistics/comments/p5xyh/need_help_choosing_a_field_of_study_for_my/,0.0,4.0,"Quick background: I am starting with my Masters degree in Mathematical Statistics this year. However, I am not exactly sure in which specific direction or field I would like to do my thesis in, although I was thinking along the lines of Machine Learning. This being said, I've always been very interested in the medical and biological sciences, and I am quite aware of all the applications of statistics there.

I am now in the position to choose between two bursaries that have been awarded to me. One of them is a very general one, and does not have any requirements that comes with it. I will thus be able to spend a lot of time on coursework and be open as to my thesis topic. The other scholarship is from an Epidemiological Modelling and Analysis Institute. This will keep me much, much busier as I will be required to do coursework for them as well as for the university, and I will have to do the Masters project that they want me to do.

I am also now in the unfortunate position where I have to decide which of these bursaries to accept. Both are about the same in value. Will it look good on my resume if I already did some applied work (i.e. in Epidemiology) even if that is maybe not what I will be doing in a few years time? Any input at all will be very much appreciated... Thanks!

tldr; got offered two bursaries, a general one and one for an epidemiological institute, and I am unsure which one to choose",en
1108876,2012-02-01 11:44:47,statistics,A good and understandable guide to statistical inference?!,p5ydr,fantasygod777,1298403778.0,https://www.reddit.com/r/statistics/comments/p5ydr/a_good_and_understandable_guide_to_statistical/,2.0,9.0,"I'm really sorry because I know this isn't the place. I'm not searching for homework help, I'm looking for a book/website/magic 8 ball that can actually tell me what the hell is going on this class?! I can't understand even the first assignment. 

I'm using the Bain/Engelhardt ""Introduction to Probability and Mathematical Statistics."" We start on Ch. 8. Seriously I need help understanding any of these concepts. So please people smarter than me, help me reach your level. Any suggestions are appreciated.",en
1108877,2012-02-01 14:49:09,statistics,[SPSS] Dealing with Missing data that has been given a '0' value,p623d,[deleted],,https://www.reddit.com/r/statistics/comments/p623d/spss_dealing_with_missing_data_that_has_been/,1.0,2.0,"I have a SPSS dataset where the missing data for it has been labelled as '0'. So when I look at the descriptive analysis, the minimum value is always 0 but it should be only between 1-5 since its a 5pt likert scale. I would like to get the missing values to just be shown as missing values instead of '0' or if there is a way I can change it back to missing values by going column by column, it would be alright as well. There are many cases so its not feasible for me to just do it one by one. I am wondering if anyone have any suggestions please? I tried looking for a possible answer but I couldn't find anything that could help.",en
1108878,2012-02-01 19:13:03,statistics,Good/best FOSS stat package for linux?,p6bp0,Cirri,1286460493.0,https://www.reddit.com/r/statistics/comments/p6bp0/goodbest_foss_stat_package_for_linux/,5.0,22.0,"I'm currently a stat/biology undergrad and I've gone through hell trying to find a free state package for use on my personal computer where I run xubuntu. I've asked my stat professors but they are all only familiar with windows. Anybody have any suggestions?

Edit: Looks to be an overwhelming consensus. ",en
1108879,2012-02-01 19:19:40,statistics,Should I drop the Baltic States?,p6c0g,[deleted],,https://www.reddit.com/r/statistics/comments/p6c0g/should_i_drop_the_baltic_states/,0.0,3.0,"I am running a Panel analysis on trade data on a bunch of European countries from 1970 up to today. One of the things I am interested in is seeing the size of the selectivity bias due to certain exogenous variables.

My model is:

Y= ß*X +σ * selectivity term + e

where is Y is trade value, X are economic variables for pair of countries and the selectivity term is mostly based on geographic variables.
My problem is that if I include the Baltic states, out of 20000 observations around 5000 are missing only because those countries didn't exist before 1990. In rest only about 2000 observations are missing. I cannot include an existance dummy in the selectivity term due to quasi-linear separability. I can include a dummy for communism which proxies very well without running into quasi-linear problems. Can the fact that so many variables are missing due to a single cause mess up my results?  If so what things would be affected ? Should I drop the Baltic states?",en
1108880,2012-02-01 19:20:09,MachineLearning,"New to machine learning, need some advice on the math involved.",p6c0o,imissyourmusk,1317942820.0,https://www.reddit.com/r/MachineLearning/comments/p6c0o/new_to_machine_learning_need_some_advice_on_the/,17.0,22.0,What math classes should I have under my belt to be able to fully grasp machine learning?  Any suggestions would be a big help.,en
1108881,2012-02-01 21:05:35,statistics,Greece's chief statistician may be imprisoned for life.  Talk about shooting the messenger!,p6hal,dezert1,1326242243.0,https://www.reddit.com/r/statistics/comments/p6hal/greeces_chief_statistician_may_be_imprisoned_for/,55.0,3.0,,en
1108882,2012-02-01 23:05:37,statistics,Better EM estimation of 2 gaussians using R?,p6nbt,forever_erratic,1256947469.0,https://www.reddit.com/r/statistics/comments/p6nbt/better_em_estimation_of_2_gaussians_using_r/,7.0,10.0,"Hello,

I am fitting a bunch of data that I know a priori is made of 2 gaussian using normalmixEM() from the library mixtools. Each piece of data comes from quantification of green intensity in an image along a line of pixels. I know there are two gaussians because the green brightness originates from two green ""lightbulbs"" of equal strength. The only thing that should differ between the lightbulbs is their location (mean of gaussians) and the width of the bulbs (SD of gaussians). 

A piece of data might look like this:

    data = [1 1 2 2 2 3 3 3 3 3 4 4 4 5 5 5 5 5 6 6 7]

The number corresponds to the position and the frequency of the number corresponds to how bright the green was. When I run the algorithm, I start it with reasonable values and with equal mixture.

My problem is this: when the gaussians are clearly distinct, the algorithm has no problem getting a great fit. However, when the separation between peaks is small (or non-visible), the algorithm has big problems.

It would be okay if it just didn't converge in cases like this, because then I could use that as data (e.g. ""x images had gaussians too overlapping to disentangle using EM""). However, often the algorithm will find two gaussians that are clearly wrong. One will be a fit covering the whole spread and the other will be a much smaller fit off to one side.

Is there any way in R to get around this? Ideally, I'd want an EM algorithm that does one (or more) of the following: 

1. Not converge when the the starting state of equal mixture proportions needs to be relaxed to some fixed amount (e.g. when the mixture proportion is &lt;0.2).

2. When the frequency distribution appears as one gaussian but I tell it two are present in equal proportions, use this information to develop two gaussians that when combined would look like the one given.

Finally, I am fitting this algorithm to over 300 pieces of data, so would prefer suggestions that can be applied to all cases without much individual tweaking. I'm pretty comfortable programming in R so don't let that hold you back from a complicated answer.

Any guidance will be much appreciated. You guys already got me partway on this from a post a week ago, so thanks for that and in advance for any more help!
",en
1108883,2012-02-01 23:17:43,statistics,Prestigious college desperately plays down shameful manipulation of statistics,p6nxh,Professor_IR,1310512226.0,https://www.reddit.com/r/statistics/comments/p6nxh/prestigious_college_desperately_plays_down/,4.0,1.0,,en
1108884,2012-02-02 09:27:48,statistics,any advice for a freshman stats major?,p7fsb,SJH823,1327382622.0,https://www.reddit.com/r/statistics/comments/p7fsb/any_advice_for_a_freshman_stats_major/,6.0,31.0,"So i started taking courses as a stats major this semester after transferring out of general engineering, and besides the fact that i've pretty much already taken the first 2 intro classes in high school (i took AP Stats) I love it! 

I was just wondering if some of the older and wiser people of r/statistics have any advice for what to take or not take, or anything else to do to help succeed at the undergrad level?

I have an idea of what basic algebra-based stats is like from AP Stats, but I don't really know what upper-level courses will be like. (although i somewhat enjoy calculus, so maybe that will help?)",en
1108885,2012-02-02 21:57:10,computervision,Obtain Canonical Representation of 2D Object,p84ht,zionsrogue,1210268911.0,https://www.reddit.com/r/computervision/comments/p84ht/obtain_canonical_representation_of_2d_object/,1.0,5.0,"Okay, let me frame the problem first. I am doing image retrieval on a set of 2D *symmetrical* objects in an image. Essentially, I have a birds-eye (looking down upon) view of the object, I am describing each image using a set of features (the features of which are irrelevant to the problem), and then performing searches using my index of features.

The objects in the images are easily detectable and a mask can easily be extracted without any noise, so that isn't a problem.

My features are invariant to rotation, translation, and scaling, but the problem is that my assumption is that I have this birds-eye view of the 2D object.

Under certain affine transformations (shearing for example), I no longer have the birds-eye view. I know that I could apply keypoint detection, describe the areas using SIFT/SURF/etc., and do some naive nearest-neighbor searching, that really isn't the point of my question. Furthermore, the objects in the images do not have a lot of texture, which makes SIFT/SURF techniques less reliable.

My question is simply, given a 2D *symmetrical* object, is it possible to:

1. Detect that the object is sheared. (I think this should be fairly straightforward. Since the objects are more-or-less symmetrical, I can just look at the distances from the contour points to the centroid of the object and ensure they are all within a certain threshold).

2. If the object is indeed sheared, apply some sort of transformation so I can re-obtain my birds eye view. ",en
1108886,2012-02-02 22:08:16,MachineLearning,Recommend and algorithm to identify clusters (cluster definition in comment),p8535,jagibers,1296689866.0,https://www.reddit.com/r/MachineLearning/comments/p8535/recommend_and_algorithm_to_identify_clusters/,6.0,12.0,"Hey all, I'm new to Machine Learning concepts and I'm pretty much just jumping in to the river here, swim or die. I think what I'm trying to go for in this situation is cluster analysis. If so, I'm looking for an applicable algorithm. If a better analysis exists, please point it out and what algorithm I should use.

Lets say I have a bunch of a events that occur. Each event identifies an object, and a deviation. The deviation may be an increase or a decrease of some value.

For example, lets say over a period of 1 minute, the following events occur:

    Object | Deviation
    A | +3
    B | -2.3
    A | +1
    C | +25.4
    C | +26
    D | -5.7
    E | -2
    F | +9
    D | -2

What I'd like to do, is determine which objects are clustered together--that is when an object of a cluster deviates in a direction, another object in the cluster will similar deviate. 
It would be nice if inverse deviations could also be determined such that if a cluster of objects move in one way, another cluster would move in the opposite.

Any one have any ideas on how to approach this problem?

Thanks.
",en
1108887,2012-02-03 03:51:22,statistics,ESPN Expert Picks (for NFL) Become Worse Over the Season,p8mbb,prhodes,1328233759.0,https://www.reddit.com/r/statistics/comments/p8mbb/espn_expert_picks_for_nfl_become_worse_over_the/,8.0,5.0,,en
1108888,2012-02-03 15:48:00,statistics,Advice on how to analyse some data,p983t,[deleted],,https://www.reddit.com/r/statistics/comments/p983t/advice_on_how_to_analyse_some_data/,4.0,17.0,"Hello guys, I'd like some advice about how to analyse the following data; I've got a dataset which contains a control (n=9) and experimental (n=10) condition. They're actually recordings of spontaneous activity in muscle cells, so each cell will have a variable number of events (some with as little as 8 events and others as many as 100). I want to look at things like the average peak amplitude of the events between the control and experimental conditions, however because of the variable number of events in each cell, this will bias the average toward the cells with the larger number of events and I don't want that.

Any suggestions? I was thinking of doing something like taking the average of each cell and then taking the average of that, but this seems like badness (I am not a statistician) and this will also 'hide' variance within each cell, which may be important when we're looking at only 8 events in a cell.",en
1108889,2012-02-03 17:44:00,artificial,"Brain imaging and evolutionary insights into human language ability, tom sereno, ucsd",p9c75,marshallp,1239903633.0,https://www.reddit.com/r/artificial/comments/p9c75/brain_imaging_and_evolutionary_insights_into/,8.0,0.0,,en
1108890,2012-02-03 20:06:29,statistics,Funny little SAS / SQL disparity,p9inq,[deleted],,https://www.reddit.com/r/statistics/comments/p9inq/funny_little_sas_sql_disparity/,6.0,13.0,"I'm trying to extract latest records from a dataset which has patientID and date as key fields. All the data is sorted accordingly. I had erroneously believed the two statements would do the same thing:

    data latestRec;
      set allRecs;
      by patientID date;
      if last.patientID or last.date then output;
    run;

versus

    create table latestRec as
    select * from allRecs
    group by patientID
    having date eq max(date);

but the number of records is different, so I can't figure out wtf is going on.",en
1108891,2012-02-03 20:09:44,MachineLearning,create a decision tree using entropy as the split criterion in matlab?,p9it2,waspinator,1202264096.0,https://www.reddit.com/r/MachineLearning/comments/p9it2/create_a_decision_tree_using_entropy_as_the_split/,0.0,0.0,"classregtree uses either Gini's diversity index, the twoing rule, or maximum deviance reduction as its split criterion. is there a way to create a generic tree with your own split criterion in matlab? or is there an ID3 (entropy/gain) implementation available in matlab already?",en
1108892,2012-02-03 20:41:24,MachineLearning,Startup Turns Data Crunching into a High-Stakes Sport,p9kav,willis77,1188267725.0,https://www.reddit.com/r/MachineLearning/comments/p9kav/startup_turns_data_crunching_into_a_highstakes/,9.0,2.0,,en
1108893,2012-02-04 00:18:59,statistics,"New favorite site.  Visualize two different data sets by state (the data set linked, representatives per population, is easy to understand, some of the others much more difficult).  You can even create your own",p9uv1,plato1123,1224031775.0,https://www.reddit.com/r/statistics/comments/p9uv1/new_favorite_site_visualize_two_different_data/,1.0,0.0,,en
1108894,2012-02-04 00:50:08,statistics,Can anyone figure this one out ? ,p9w8r,donrane,1320579996.0,https://www.reddit.com/r/statistics/comments/p9w8r/can_anyone_figure_this_one_out/,2.0,6.0,,en
1108895,2012-02-04 01:53:47,rstats,Anyone have experience using the capture.output command?,p9yzq,SurfaceThought,1320256168.0,https://www.reddit.com/r/rstats/comments/p9yzq/anyone_have_experience_using_the_captureoutput/,1.0,4.0,"Hello all,

I am trying to use the capture.output command to be able to save an ANOVA table and then open it with excel. I can use the command to save the output table as a .csv, but it doesn't actually insert any comma's when it does so. So when I try to open it in excel every value in each row is jammed into the A column. Does anybody know of any way to get the command to save the output table in a way were it will actually be opened in Excel correctly?

Thanks in advance.",en
1108896,2012-02-04 06:02:20,MachineLearning,Library/Toolbox for visualizing neural networks?,pa8lc,lpiloto,1310021871.0,https://www.reddit.com/r/MachineLearning/comments/pa8lc/librarytoolbox_for_visualizing_neural_networks/,4.0,12.0,"I've written a program that generates and trains a neural network, but I would like to be able to graphically display the resulting network.  Does anyone know of a free tool to visualize an already made network?  Preferably in C,C++, or Java, though I'll take anything I can get. Thanks!",en
1108897,2012-02-04 06:09:45,statistics,"Topic ideas for a study/survey.
",pa8uu,ChevalierNoiRJH,1304479307.0,https://www.reddit.com/r/statistics/comments/pa8uu/topic_ideas_for_a_studysurvey/,3.0,2.0,"Howdy everyone. I am a senior in high school taking the only other math class besides Calculus available, that being Probability and Statistics. It's a pretty small class and it's a great time to learn about a great subject. But, I digress. 

My class needs some topic ideas to survey the students at our school. To appease our teacher, these surveys must be serious and in some way tell us about our school, but not impossible to study. Two previous studies we have done were student participation in drugs during the school week, and the other was teen mothers and the effectiveness of a school sponsored day care. 

I was wondering if you guys have any other ideas, as we would like one more study for the year. ",en
1108898,2012-02-04 06:29:01,statistics,What can I do with just a Bachelor's in Statistics?,pa9kr,[deleted],,https://www.reddit.com/r/statistics/comments/pa9kr/what_can_i_do_with_just_a_bachelors_in_statistics/,3.0,5.0,"I'm currently in my second year of university and majoring in statistics. I love it, but I'd prefer to be finished with school after I finish my undergraduate. Is a Bachelor's degree in Statistics useful at all, or is a Master's/PhD basically required to get a decent job?",en
1108899,2012-02-04 16:28:55,statistics,Simple question,pamvf,geewizzery,1328365378.0,https://www.reddit.com/r/statistics/comments/pamvf/simple_question/,2.0,5.0,"Hi there,

Excuse me dropping by for something very simple. I'm reading something I think might have a mistake, and I just wanted to be 100% sure that I'm not going mad.

This is anthropometric data from a research paper.

""Eleven subjects (mean age, 27.2 ± 4.4 years; mean height, 173 ± 5.7 cm; mean weight, 85 ± 6.8 kg;  mean body mass index, 28.1 ± 2.4 kg/m2)."" This data is mean +/- SEM.

This seems really super-impossible.

Let's take the weight offered as an example: with a sample size of 11, and assuming the population lies within 2 SD, thus:

SD / sqrt(11) = SEM (given as 6.8)

Thus SD = 22.5

Thus +/- 2 SD from 85kgs is 40kgs to 130kgs.

Now, this is a homogeneous population that consists of healthy young men of normal height - no anorexics, no obesity.

The other variables show similar, massive variability.

Moreover, here is a sample of very similar people from another paper which shows what I would think is much more normal variation - the SEs are hugely reduced from the sample above:

Ten males participated in this study (age = 23.6 ± 0.37 years; body mass = 80.36 ± 0.90 kg; height = 1.77 ± 0.005 meters; mean ± standard error).

My conclusion: the first paper messed up and confused SEM with SD.

Or have I missed something?",en
1108900,2012-02-04 18:30:05,rstats,Monte Carlo'ing Monte Hall using R,paq45,likelihoodtprior,1318629271.0,https://www.reddit.com/r/rstats/comments/paq45/monte_carloing_monte_hall_using_r/,6.0,0.0,,en
1108901,2012-02-04 19:07:40,statistics,Monty Hall by simulation in R [r-bloggers],parde,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/parde/monty_hall_by_simulation_in_r_rbloggers/,6.0,1.0,,en
1108902,2012-02-04 19:47:35,statistics,In your opinion what are the 10 non beginner things someone should know in the fields if data analysis and statistics?,pasw3,dassouki,1215439790.0,https://www.reddit.com/r/statistics/comments/pasw3/in_your_opinion_what_are_the_10_non_beginner/,50.0,56.0,This question is primarily targeted beyond the basic first 2 stat courses  you take in college / university,en
1108903,2012-02-04 20:45:48,statistics,Calling awk from R?,pav1v,[deleted],,https://www.reddit.com/r/statistics/comments/pav1v/calling_awk_from_r/,1.0,0.0,,en
1108904,2012-02-04 20:57:35,statistics,He said I was average - but he was just being mean.,pavig,H4L9000,1297796030.0,https://www.reddit.com/r/statistics/comments/pavig/he_said_i_was_average_but_he_was_just_being_mean/,1.0,4.0,,en
1108905,2012-02-05 01:02:14,statistics,Confused on what experimental design to use for response surface analysis.,pb5hv,[deleted],,https://www.reddit.com/r/statistics/comments/pb5hv/confused_on_what_experimental_design_to_use_for/,2.0,0.0,"I plan on conducting a response surface analysis examining 3 factors (temperature, time, chem concentration) and their affect on the response (total mass of dirt removed from a soiled piece of steel). 
I don't know exactly which design setup I should go with: Box-Behnken, Central Composite Inscribed, or Central Composite Circumscribed. Fewer trials would be great, but I don't know if I would be getting the best results. I should also add that I believe the optimal condition of the 3 factors lies within the extremes of the ranges I will be examining. ",en
1108906,2012-02-05 02:36:24,statistics,Quick question about Gaussian processes,pb91t,C2Q,1323878093.0,https://www.reddit.com/r/statistics/comments/pb91t/quick_question_about_gaussian_processes/,2.0,2.0,"I was reading my notes and there was this statement my prof wrote. I was doing a question about proving Gaussian processes and was wondering if this is accurate.

A stochastic process is a gaussian process if:
E(Xt^2 ) &lt; infinity

that is if the second moment is finite.

Everywhere else gaussian processes are defined as having a joint normal distribution and I couldn't find anything that relates to the second moment. ",en
1108907,2012-02-05 09:17:18,MachineLearning,Is MelApp as accurate as a non-dermatologist physician?,pbmov,jonsca,1307621848.0,https://www.reddit.com/r/MachineLearning/comments/pbmov/is_melapp_as_accurate_as_a_nondermatologist/,3.0,1.0,,en
1108908,2012-02-06 00:57:04,statistics,confused amature seeking help,pcdwx,mangaferret,1312097862.0,https://www.reddit.com/r/statistics/comments/pcdwx/confused_amature_seeking_help/,3.0,5.0,"a psychology graduate friend of mine asked me if there was any way to compare means if she didn't have a variance/standard deviation, but i can't think of a way?

(i have a degree in pure maths, and a bit of knowlege in stats)",en
1108909,2012-02-06 02:54:21,computervision,Image matrices,pci5k,[deleted],,https://www.reddit.com/r/computervision/comments/pci5k/image_matrices/,1.0,0.0,"Does opencv give users different types of matrices based on files being read? particularly with ""imread"" e.g will a jpg have a matrix of floats while a gif file will be a matrix of ints?",en
1108910,2012-02-06 02:58:02,MachineLearning,How to improve RTextTools?,pcia7,tymekpavel,1190535699.0,https://www.reddit.com/r/MachineLearning/comments/pcia7/how_to_improve_rtexttools/,20.0,10.0,"I'm one of the authors of RTextTools: a free, open source machine learning package for semi-automated text classification. I've been working with social scientists and software engineers during the past year to create a simple yet functional R package to categorize text documents into discrete categories.

I'm sure there are statistical methods that haven't been implemented in RTextTools that could improve functionality and accuracy. I'd really appreciate if you could test the package ( in R 2.14+, *install.packages(""RTextTools"")* ) and provide some feedback. Thank you in advance for your help!

**EDIT:** If you'd like more details, RTextTools has a [website](http://www.rtexttools.com/).",en
1108911,2012-02-06 11:38:07,MachineLearning,Sentiment Classification using Machine Learning Techniques,pd0d2,j4n0,1233347332.0,https://www.reddit.com/r/MachineLearning/comments/pd0d2/sentiment_classification_using_machine_learning/,8.0,1.0,,en
1108912,2012-02-06 13:20:41,artificial,Analogical Reasoning,pd24v,marshallp,1239903633.0,https://www.reddit.com/r/artificial/comments/pd24v/analogical_reasoning/,7.0,17.0,,en
1108913,2012-02-06 16:36:16,rstats,General Bayesian estimation (MCMC) in R using MHadaptive,pd6q7,[deleted],,https://www.reddit.com/r/rstats/comments/pd6q7/general_bayesian_estimation_mcmc_in_r_using/,3.0,1.0,,en
1108914,2012-02-06 18:34:55,statistics,"Part Three of Think Complexity, a new book on complex systems.",pdb4s,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/pdb4s/part_three_of_think_complexity_a_new_book_on/,7.0,0.0,,en
1108915,2012-02-06 20:25:16,MachineLearning,"Part Three of Think Complexity, a new book about complex systems [x-post from statistics]",pdg0g,AllenDowney,1300587223.0,https://www.reddit.com/r/MachineLearning/comments/pdg0g/part_three_of_think_complexity_a_new_book_about/,17.0,9.0,,en
1108916,2012-02-06 21:56:17,statistics,Heteroscedasticity.,pdkbg,econometrician,1323865912.0,https://www.reddit.com/r/statistics/comments/pdkbg/heteroscedasticity/,27.0,8.0,,en
1108917,2012-02-07 00:20:44,datascience,"Is ""open source data science"" a thing? Are there any interesting public data sets lying around and are there examples of people finding ""gems"" in such sets?",pdqku,worldsayshi,1304333664.0,https://www.reddit.com/r/datascience/comments/pdqku/is_open_source_data_science_a_thing_are_there_any/,5.0,5.0,,en
1108918,2012-02-07 01:42:36,MachineLearning,Is automated essay grading an appropriate machine learning task?,pdu3v,shaggorama,1233555004.0,https://www.reddit.com/r/MachineLearning/comments/pdu3v/is_automated_essay_grading_an_appropriate_machine/,17.0,13.0,,en
1108919,2012-02-07 04:06:56,computervision,top 10 super computer,pe0r0,miros111,1328316212.0,https://www.reddit.com/r/computervision/comments/pe0r0/top_10_super_computer/,0.0,0.0,,en
1108920,2012-02-07 06:35:29,statistics,"How could you prove that there was not not vote fraud, p&lt;.01? I'm sorry that I'm an idiot, I just would really like to have this question answered ok?",pe85q,f45tEddie,,https://www.reddit.com/r/statistics/comments/pe85q/how_could_you_prove_that_there_was_not_not_vote/,1.0,5.0,,en
1108921,2012-02-07 16:16:54,statistics,"A x B = C where A,B &amp; C have prior probability distributions.",peo5r,[deleted],,https://www.reddit.com/r/statistics/comments/peo5r/a_x_b_c_where_ab_c_have_prior_probability/,2.0,8.0,"If I have three parameters A, B &amp; C and I have a prior probability distribution for each, how do I calculate what the posterior distribution is given that they must satisfy the equation A x B = C?",en
1108922,2012-02-07 18:43:24,artificial,Just a random observation while optimizing via genetic algorithms,petws,curiousai,1318189231.0,https://www.reddit.com/r/artificial/comments/petws/just_a_random_observation_while_optimizing_via/,11.0,23.0,"When I'm running optimization algorithms I will usually notice a peak efficiency, then the offspring quickly become slightly less efficient and bounce against the ceiling of efficiency. Some might spike up above the peak, but most do not. What I notice is at some point it almost becomes less probable for an offspring to be as efficient as a peaking ancestor. In other words, the flat-line range of efficiency peak ends with something less efficient than the most optimal historically. Sometimes I bend the rules and allow the best variant the ability to live forever (or clone themselves with the next generation), or at least until they are surpassed. Is this phenomenon common, and do you let your peak efficient breeds exist until outperformed?",en
1108923,2012-02-07 20:31:25,rstats,Gauging Interest in a Montreal R User Group,pez2l,likelihoodtprior,1318629271.0,https://www.reddit.com/r/rstats/comments/pez2l/gauging_interest_in_a_montreal_r_user_group/,5.0,3.0,,en
1108924,2012-02-07 22:28:09,artificial,The moral dilemma of an A.I. developer,pf4w5,BerickCook,1296862994.0,https://www.reddit.com/r/artificial/comments/pf4w5/the_moral_dilemma_of_an_ai_developer/,0.0,3.0,"As a hobbyist ""Strong"" A.I. developer, I'm struggling with what could be considered a moral dilemma.

During the development process I have coded, tested, and discarded numerous versions of my design. If I was just writing some regular business app or video game this would be no problem, but I'm attempting to create a digital lifeform. 

At what point, if any, does discarding a flawed design constitute murder?",en
1108925,2012-02-07 22:28:29,statistics,Any help for an amateur? (Need help understanding and analyzing this trial),pf4wt,bkvskaa,1324784231.0,https://www.reddit.com/r/statistics/comments/pf4wt/any_help_for_an_amateur_need_help_understanding/,4.0,15.0,"Any help understanding this trial would be much appreciated!

Sample size was calculated to be 160 patients per treatment group, which provided an ""85% power to achieve a 97.5% two-tailed confidence bound -15% or more for the treatment difference (treatment 1 minus treatment 2) in success rate.""

Patients were then stratified and and randomly assigned to treatment groups.  ""In the analysis of the primary efficacy endpoint, noninferiority was demonstrated if the one-tailed 97.5% confidence bound for the treatment difference in success rate (treatment 1 minus treatment 2) was 15% or more. If the bound exceeded zero, superiority of treatment 1 to treatment 2 was demonstrated. The confidence bound was based on the normal approximation to the binomial distribution without stratification.""

The results state ""In the treatment 1 group compared to the treatment 2 group, there was no between-group difference in the proportion of patients who achieved the primary endpoint (96.4% compared with 94.1%, 95% CI -2.19 to 6.88, P=0.443). The CI for the differences in success rates demonstrated the noninferiority of treatment 1 to treatment 2.

I need to evaluate this trial, and be able to explain it to my classmates. and seeing as my background in statistics is incredibly limited, I'm having trouble making sense of this myself. Also, the study did not mention a specific statistical test used to analyze this data...am I missing something here? I understand the implications of noninferiority and the overall results, but not the statistics portion of it. Any help?",en
1108926,2012-02-08 01:14:53,statistics,Spss help!,pfdbr,spsshelp,1328656373.0,https://www.reddit.com/r/statistics/comments/pfdbr/spss_help/,2.0,2.0,,en
1108927,2012-02-08 03:12:41,statistics,How would you determine what percentile a given person is on this curve?,pfj1t,sheepy1,1306822149.0,https://www.reddit.com/r/statistics/comments/pfj1t/how_would_you_determine_what_percentile_a_given/,6.0,6.0,,en
1108928,2012-02-08 03:35:01,MachineLearning,Strongest SITEMA Mould Closing Device PowerStroke FSK to date delivered  200 tons closing force | Hydraulics - News,pfk45,kamilrhu,1226023594.0,https://www.reddit.com/r/MachineLearning/comments/pfk45/strongest_sitema_mould_closing_device_powerstroke/,1.0,0.0,,en
1108929,2012-02-08 08:09:11,AskStatistics,ordinal regression w/ correlated random effects,pfwwk,luisdarcy,1307530128.0,https://www.reddit.com/r/AskStatistics/comments/pfwwk/ordinal_regression_w_correlated_random_effects/,3.0,4.0,"Hey,
turns out I've got to learn enough to be able to run an ordinal regression model with correlated random effects. Is this something I could reasonably teach myself? Apparently there is no canned package for this and I've been given JAGS code to do this but I have no idea. 
I ordered a basic book on doing bayesian analysis with R and BUGS, but so far I feel completely lost. Can't seem to find the right order of topics to learn.
Any help?",en
1108930,2012-02-08 11:42:26,statistics, Berlin Numeracy Test: Can we understand risk literacy in 4 questions?,pg2kt,Dumdidaa,1326350292.0,https://www.reddit.com/r/statistics/comments/pg2kt/berlin_numeracy_test_can_we_understand_risk/,3.0,0.0,"The website informs this adaptive test can inform us of a the grasp on basic probability/statistical principles of respondent. It can do it in minimum of 2 to maximum 4 questions
Here is the link:  http://www.riskliteracy.org/dnn/TRYIT.aspx

I wonder how useful would it be to gauge your client. Even if one does understand the client to be illiterate- we still need techniques to make them understand the risk (say a financial investment or going for a surgery).
The practitioners would also need to avoid biases such as confirmation bias or framing of facts .

What do you guys think?",en
1108931,2012-02-08 11:43:06,datasets,2010 Taoiseach (Irish Prime Minister) expenditure data,pg2ld,cavedave,1128052800.0,https://www.reddit.com/r/datasets/comments/pg2ld/2010_taoiseach_irish_prime_minister_expenditure/,3.0,0.0,,en
1108932,2012-02-08 18:10:08,statistics,"askmath: Stephen Baxter's ""Manifold : Time"" probability problem",pgd8m,borgs_of_canada,1328714656.0,https://www.reddit.com/r/statistics/comments/pgd8m/askmath_stephen_baxters_manifold_time_probability/,3.0,2.0,"In the novel ""Manifold : Time"", Cornelius Taine uses a probability problem to demonstrate the ""doomsday argument"". I have tried multiple time to solve this problem with the answer given in the book without succes. So my question is : Is the answer (two third) right ? If so, how was it calculated ? Using bayesian formula, I get at  P(N=10 | n=3) = 100/101, but I far from an expert in probability (VERY far). Can anyone explain how to solve this ? Here is the problem as proposed in Baxter's novel :

Cornelius reached under the table and produced a wooden box, sealed up. It had a single grooved outlet, with a wooden lever alongside. ‘In this box there are a number of balls. One of them has your name on it, Malenfant; the rest are blank. If you press the lever you will retrieve the balls one at a time, and you may inspect them. The retrieval will be truly random.

‘I won't give you the opportunity to inspect the box, save to draw out the balls with the lever. But I promise you there are either ten balls in here—or a thousand. Now. Would you hazard which is the true number, ten or a thousand?’

‘Nope. Not without evidence.’

‘Very wise. Please, pull the lever.’

Malenfant drummed his fingers on the tabletop. Then he pressed the lever. A small black marble popped into the slot. Malenfant inspected it; it was blank. Emma could see there was easily room for a thousand such balls in the box, if need be.

Malenfant scowled and pressed the lever again.

His name was on the third ball he produced.

‘There are ten balls in the box,’ said Malenfant immediately. 

‘Why do you say that ?’ 

‘Because if there were a thousand in there it's not likely that I'd reach myself so quickly.’ 

Cornelius nodded. ‘Your intuition is sound. This is an example of Baye's rule, wich is a technique for assigning probabilities to competing hypotheses with only limited information. In fact--’ He hesitated, calculating. ‘--The probability that you're right is now two thirds, on the basis of your ball being third out.’ 

So ? where that two thirds comes from ?

Thanks !",en
1108933,2012-02-08 18:41:28,statistics,Quick question from an idiot...please.,pgeo6,maybe-tomorrow,1308562245.0,https://www.reddit.com/r/statistics/comments/pgeo6/quick_question_from_an_idiotplease/,3.0,14.0,"need this for work within an hour!

I am looking to determine what percentage of a population has two independent characteristics. I know the number of every person in the population that has characteristic ""a"" and can calculate how many of those have characteristic ""b"". 

It turns out that 14% of all people with ""a"" also have ""b"".

Does that mean necessarily that 14% of people with ""b"" also have ""a""!

I think so but something does not feel right about the correlation going in both directions.",en
1108934,2012-02-08 21:05:32,MachineLearning,"""Machine Learning for Hackers"" table of contents",pglpe,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/pglpe/machine_learning_for_hackers_table_of_contents/,19.0,18.0,,en
1108935,2012-02-08 22:55:06,MachineLearning,Has anyone done topic modeling with LDA and Wikipedia?,pgrdb,Daishiman,1186069707.0,https://www.reddit.com/r/MachineLearning/comments/pgrdb/has_anyone_done_topic_modeling_with_lda_and/,6.0,7.0,"Hi all. I'm interested in topic modeling  applied to large corpora of documents. Has anyone worked with LDA and Wikipedia? I'm trying to get a topic model of it with a large number of topics, but given that I have no equipment it takes a prohibitively large amount of time to get those topics.",en
1108936,2012-02-08 23:56:33,statistics,What are the strengths of some of the top 20 graduate programs in statistics?,pguli,[deleted],,https://www.reddit.com/r/statistics/comments/pguli/what_are_the_strengths_of_some_of_the_top_20/,12.0,9.0,"Even after looking at research it's hard to get a good idea of what programs are strongest in what areas. 

If anyone is familiar with even just one or two programs I'd be more than happy to hear about them",en
1108937,2012-02-09 00:46:07,statistics,"Precision time: A matter of atoms, clocks, and statistics",pgx7w,prhodes,1328233759.0,https://www.reddit.com/r/statistics/comments/pgx7w/precision_time_a_matter_of_atoms_clocks_and/,1.0,0.0,,en
1108938,2012-02-09 10:18:55,analytics,"Tracking search results on AJAX autosuggest search 
with Google Analytics",phl6p,g2petter,1159347610.0,https://www.reddit.com/r/analytics/comments/phl6p/tracking_search_results_on_ajax_autosuggest/,1.0,0.0,"For an ecommerce site I'm working on, I've made a new search inspired by Facebook's autosuggest search. When you start typing, the server will run a query, returning the top 8 matches in a list of links like the ones shown in Facebook. The user can then either click on one of the results, keep typing, or press enter/search to go to the full results page.  

Here's what I want to track:  
1) The percentage of users who go to one of the autosuggest search results, and what each user was searching for.  
2) The percentage of users who go to the results page and then click  a product link, and what each user was searching for.  
3) The percentage of users who go to the results page and then don't click anything, and what each user was searching for.  
4) The percentage of users who go to the results page and get zero hits, and what each user was searching for.  

Is this possible to do, and if so, how? Have I forgotten anything that's useful to track? I was thinking maybe the percentage of people who go to a product and end up putting it in the shopping cart and/or buying it, comparing buy rates and which products they buy between customers who come via the autosuggest page and those who come via the full search page. Would that be possible?
",en
1108939,2012-02-09 11:48:35,statistics,Advice for subject on bachelors degree essay,phn4h,statsarefun,1328780296.0,https://www.reddit.com/r/statistics/comments/phn4h/advice_for_subject_on_bachelors_degree_essay/,5.0,5.0,"Hi!
I have trouble coming up with a subject for my bachelors degree essay in statistics. I need to come up with a subject that is relatively advanced, but still doable in 2 months time.
The essay is supposed to be written between april and june, that is, we have to months time to research the subject, find data and analyze it.
I would have no problem starting data collection and research right now as long as the topic is interesting. I also have no problems with learning new software just to be able to perform the analysis.

Im not looking for a complete topic/problem but rather for advice about topics in interesting fields.",en
1108940,2012-02-09 15:29:28,statistics,"Convert any multivariate normal data to standard MV normal where variables are uncorrelated and have unit variance. In short, convert correlated variables to uncorrelated, and vice versa. ",phqay,RickWicklin,1327333524.0,https://www.reddit.com/r/statistics/comments/phqay/convert_any_multivariate_normal_data_to_standard/,2.0,2.0,,en
1108941,2012-02-09 17:24:26,artificial,Short article on using parameterized dialogue generation in videogames,phu5x,_delirium,1246944723.0,https://www.reddit.com/r/artificial/comments/phu5x/short_article_on_using_parameterized_dialogue/,5.0,0.0,,en
1108942,2012-02-09 18:27:59,rstats,PCA given a correlation matrix,phx0e,Neurokeen,1307713928.0,https://www.reddit.com/r/rstats/comments/phx0e/pca_given_a_correlation_matrix/,1.0,0.0,"I'm going through some exercises in R, for which the original person used JMP, and I ran into a problem that presents a lower diagonal correlation matrix but no original data. I'm used to using princomp(), but this has thrown me for a loop; I can't seem to find anything that makes this case clear. How would I perform a principle components analysis in R in this case?",en
1108943,2012-02-09 21:45:44,MachineLearning,how the human brain works?,pi72d,qwsazxerfdcv,1287517243.0,https://www.reddit.com/r/MachineLearning/comments/pi72d/how_the_human_brain_works/,8.0,34.0,"this was on my Reddit wall a  while ago. http://i.imgur.com/uDYBK.jpg
look at the time(preferably the Seconds too)  before you open this and after you realize what the picture has in it .How long did it take ?

The picture has little (almost no) semblance of edges , color , texture scale space with the original , and yet our brains are so awesome in such tasks , I wonder if the ML/Comp vis community is close , or in how many decades we will be close to replicating such algorithms that the brain use ?
EDIT : the"" no resemblance "" was more of a qualitative statement to stress the idea ,than a quantitative statement.",en
1108944,2012-02-10 01:09:26,statistics,How to analyze Rankings data?,pihk6,y0ni,1266430219.0,https://www.reddit.com/r/statistics/comments/pihk6/how_to_analyze_rankings_data/,7.0,11.0,"I need some help analyzing a dataset that consists of rankings. I am not familiar with working with rankings data, and I would like to know what kind of analysis is possible for my data.


Note that by ""rankings"", I mean that distinct values are given to objects with no ties (e.g. 10 objects classified as 1st, 2nd, 3rd,... , 10th with no ties). This is, I found out, different from ""ranks"", which can have repeated values (e.g. 10 objects to be classified as Good, Neutral, Bad). After a couple days of research, I learned this the hard way.


Full disclosure: This is a project I am working on for an undergraduate class where we do statistical consulting. Clients from the university (usually masters students or PhD students from other departments) that require help with analyzing their data for their projects submit requests to our class. The class then splits up into teams and write a report for the client (for free!).

I would appreciate any advice related to dealing with rankings data (not rank data, but data with full rankings!) and possible statistical methodologies to investigate for my project.


**Description of the project:**

80 students were randomly and equally split into two groups, Group A and Group B. Each student was given information about 8 different teams competing for a prize, and were then told to rank the 8 teams on who deserved the prize the most based on the information given to them (no ties allowed).

Afterwards, the students were given additional information about the teams. The supplementary information provided to the groups differed in one regard: Group A was given some financial information about the teams (i.e. how much money/funding each team had available). After given this information, everyone was asked to re-rank the teams (or not).


**The data:**

I have 80 rows (people that were surveyed) and 16 columns (the initial rankings and the re-rankings). They actual values in each cell range from 1 to 8, and must contain the full rankings (i.e. each student must rank the 8 teams with the integers from 1 to 8, with no ties).


**My Questions:**

*Q1. How can I compare the initial rankings of Group A to the initial rankings of Group B?*

My initial thought was to treat Group B as the control group (i.e. expected rankings) and do a goodness of fit test between Group A's rankings and the Group B's rankings. However, I am not sure this is correct, and I am not sure how to compare one SET of rankings to another SET of rankings. Initial research comes up with something called Fleiss' Kappa Statistic, which I will be looking into.


*Q2. How do I compare the initial rankings with the re-rankings?*

I am thinking that the comparison has to be done for each participant. I am still searching for methods that would be applicable for making this comparison.


*Q3. Is there a way to compare the change in rankings in Group A and Group B?*

I have no idea if this is even possible.


*Q4. Given explanatory variables about the participants (such as age, sex, etc.), can I model how a given partipant will change his/her rankings?*

I am thinking of doing a regression, if possible, but I am unfamiliar with any methods to use with rankings data, or change in rankings data.


-----------


At this point, I would like some help with how to deal with rankings data. If anyone could point me to some readings, or suggest statistical methodologies, it would be greatly appreciated. Also, any thoughts or recommendations on regarding my questions of interest are welcome!

Thanks. ",en
1108945,2012-02-10 11:18:08,MachineLearning,Making Logistic Regression work for Handwritten Gesture Recognition with limited Training Data,pj6ay,rohitkumar0008,1328865176.0,https://www.reddit.com/r/MachineLearning/comments/pj6ay/making_logistic_regression_work_for_handwritten/,4.0,1.0,,en
1108946,2012-02-10 15:45:20,statistics,Can anyone give me a recommendation for a good data analysis application that costs less than a $800 (or preferably free)?,pjbzj,CenterOfTheUniverse,1263268546.0,https://www.reddit.com/r/statistics/comments/pjbzj/can_anyone_give_me_a_recommendation_for_a_good/,13.0,55.0,I'm looking for something I can install on a desktop (as opposed to a a server based application).  I've heard about Stata and SPSS but I'm not sure what the benefits of one are over the other.  I appreciate any thoughts and recommendations.  Thanks,en
1108947,2012-02-10 19:14:24,statistics,Analyse pretest and posttest measure with control group's pretest and posttest measure?,pjkbk,janerikz,1328892851.0,https://www.reddit.com/r/statistics/comments/pjkbk/analyse_pretest_and_posttest_measure_with_control/,1.0,3.0,"Hi,

I have made a survey measure before and after and intervention. Also the same went out to a control group, this at the same dates. I did a paired t-test on the intervention data and control data respectively and got some significant changes in intervention group. I also noticed some negative changes down to p=0.07 on the control group (suggesting seasonal changes) i.e. if intervention group is static this also indicate an improvement. What would be the scientific way to compare the intervention posttest measure to both pretest and control group measures?

Thanks :-) 

PS Not as prioritized is that I also picked the control to match the intervention group in terms of age and sex - can this also be baked into the equation?
",en
1108948,2012-02-10 19:19:44,statistics,Struggling with statistics,pjkll,Bigbrass,1310437241.0,https://www.reddit.com/r/statistics/comments/pjkll/struggling_with_statistics/,4.0,4.0,"Reddit, I need help.

I'm currently taking Theory of Statistics 5102, and it is giving me the business.  I took 5101 last semester, and in no uncertain terms it was the hardest course I've ever taken.  Math is no problem for me, but statistics just seems like fake math in that I can occasionally get formulas to work, but they are not intuitive and I cannot understand the logic behind them.

I have access to Stata and R, and so far Stata seems geared towards working with statistics as opposed to learning how.  As far as theory and computation with different distributions are concerned, would R be better suited?

Currently, we're working with Prior and Posterior distributions, MLEs, Bayes estimators, sufficient statistics, sampling distributions and Chi-square distributions.  I've never felt so lost in a course as I do now, and I was hoping someone here would be able to point me towards a resource that could help ease the burden of my own ineptitude.  

As a side note, I find the textbook we're using (Probability and Statistics 4e by DeGroot and Schervish) to be so far above my head that I can't finish a paragraph without missing a handful of beats.  If there are resources or other textbooks suited to someone of my level of understanding, I'd be eager to look into them.

I could really use any help at all, thanks for reading!",en
1108949,2012-02-10 19:20:06,statistics,Question about different definitions of Weibull distribution,pjkmb,desrosiers,1176856602.0,https://www.reddit.com/r/statistics/comments/pjkmb/question_about_different_definitions_of_weibull/,1.0,2.0,"Hey, I have a quick question about the different ways different sources define the distribution, and how to transform between them.

Most sources I've seen ( [wiki?](http://en.wikipedia.org/wiki/Weibull_distribution) ) define it in terms of the scale parameter and the shape parameter. But the class I'm in, the professor defined it in terms of a different one as well -- a location parameter. Such as [this](http://www.weibull.com/hotwire/issue14/relbasics14.htm). 

I'm using Matlab to do a Monte Carlo simulation, and I need to generate a number of numbers using the second method, but all of the methods I see use the first. So my strategy is this -- subtract the location parameter from the scale parameter, and use this new value with the Weibull slope (aka shape parameter) to generate a lot of numbers. These guys will have a minimum value of zero. Then, take all of those, and add back in the location parameter. Does this seem valid?",en
1108950,2012-02-10 20:15:07,statistics,What algorithm is appropriate for forecasting purchasing by geography (self.Statistics)?,pjn7s,betabob,1245073883.0,https://www.reddit.com/r/statistics/comments/pjn7s/what_algorithm_is_appropriate_for_forecasting/,2.0,6.0,"X-post from /r/machinelearning

Am trying to narrow down the general class of algorithm I should be using in the following situation.
What I'd like to do is forecast, by geographic area, where people are most likely to purchase certain products.
For example, what zipcodes are likely to see a high level of purchases of hybrid/electric vehicles.

This may may a multivariate regression. Not likely a clustering algorithm. Might be able to be represented by a Bayes model.
Any thoughts?",en
1108951,2012-02-10 20:46:34,MachineLearning,How is something like this done? (handwriting -&gt; latex formula),pjoo6,fas2,,https://www.reddit.com/r/MachineLearning/comments/pjoo6/how_is_something_like_this_done_handwriting_latex/,11.0,7.0,"I saw [this nice application](http://webdemo.visionobjects.com/equation.html?locale=default) recently and wondered how one would go about creating something like this. What are the features, what exactly is being classified? Something like sliding windows?.",en
1108952,2012-02-10 20:57:06,statistics,Estimating the Tuning Parameters for LASSO and Elastic Net,pjp76,[deleted],,https://www.reddit.com/r/statistics/comments/pjp76/estimating_the_tuning_parameters_for_lasso_and/,1.0,0.0,"I understand this may be a rather simple question I'm going to ask. I understand how these procedures are supposed to work on paper but when it comes to implement these procedures in R, how am I supposed to estimate the tuning parameter for LASSO? 

Looking at the code from the LARS package in R. 

&lt;pre&gt;&lt;code&gt;

library(lars); data(diabetes);attach(diabetes)

object &lt;- lars(x,y,type=""lasso"",trace=TRUE)

plot(object)

 # by looking at this plot I would assume between steps 7-9 would be a #good estimate

summary.lars(object)

 # step 7 has the lowest Cp Statistic and step 12 has the lowest RSS

  # to try to calculate the |beta|/max|beta| and match it up with the
 #plot (I apologize for using the for loop, I was going to fix that once I 
 # figure this out.)


xis &lt;-as.vector(rep(0,12))

for(i in 1:12){

Lbeta &lt;- object$beta[i,]

xis[i] &lt;-sum(apply(abs(t(Lbeta)),2,sum)/max(apply(abs(t(Lbeta)),2,sum)))

(xis)

}

xis 

detach(diabetes)

&lt;/code&gt;&lt;/pre&gt;

Back to my question, the more I am trying to analyze this I feel like the less I know about this and feel like I'm looking in the wrong places. 

If someone would be willing to point me in the correct direction, I will forever be in their debt. I've been playing with this code for quite awhile

Edit: I'm going to try to fix the formatting of the code in the post.",en
1108953,2012-02-11 01:30:21,rstats,Visualising the Metropolis-Hastings algorithm,pk2ft,likelihoodtprior,1318629271.0,https://www.reddit.com/r/rstats/comments/pk2ft/visualising_the_metropolishastings_algorithm/,2.0,1.0,,en
1108954,2012-02-11 04:57:51,statistics,Help with random variables??,pkava,[deleted],,https://www.reddit.com/r/statistics/comments/pkava/help_with_random_variables/,1.0,1.0,"It's a first year stats course and yes, I struggle. I hate stats.

This is my equation.

Var(X) = sum (x-mean)sq x P(x)

= (20-5.83)sq x 1/4 + (10-5.83)sq x 1/12 + (0-5.83)sq x 2/3

= 74.306


Whenever I tried calculating it I was getting 80 something, then my friend told me once I square each one, to divide it by 4, 12, and 3... when I did that I was getting like 65.

Help?",en
1108955,2012-02-11 05:48:30,MachineLearning,compare graph algorithms and implementations,pkcs2,endpnt,1328928494.0,https://www.reddit.com/r/MachineLearning/comments/pkcs2/compare_graph_algorithms_and_implementations/,2.0,4.0,"reading the source code of [five different implementations of the dijkstra's single source shortest path](http://pastebin.com/vrnGN3Ts) i was wondering if **anyone else** cares to compare implementations of graph algorithms. 

my questions are:

* what algorithm is implemented in what lib / database?
* how does the implementation perform with small, big, dense or sparse graphs?

More ideas? Comments? Anyone intrested to boostrap a pad or wiki to compile this?",en
1108956,2012-02-11 09:56:25,statistics,Visualising the Metropolis-Hastings algorithm [r-bloggers],pkkof,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/pkkof/visualising_the_metropolishastings_algorithm/,10.0,1.0,,en
1108957,2012-02-11 22:26:11,statistics,Generating directed Watts-Strogatz network,pl4fh,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/pl4fh/generating_directed_wattsstrogatz_network/,3.0,0.0,,en
1108958,2012-02-12 03:20:36,statistics,Please help!,plghw,silly_walks_,1325553165.0,https://www.reddit.com/r/statistics/comments/plghw/please_help/,1.0,5.0,"I am a PhD student in the humanities and I'm trying to use some statistics to support my research. I've come across an article that uses... what I'm confident is a standard deviation graph. Here's a brief quote explaining some of the methodology: ""An index of dissimilarity measures the proportion of one group who would have to change majors for the proportions of low-SES and high-SES students in each category to be equal. Values of the index rage from 0 to 1.""

Anyway, the article says that students from low-SES families are ""more likely"" to enter ""vocational"" majors (STEM, business, etc) than students from high-SES families, but because it's expressed in standard deviation the article never says *how much more* likely. The data is just posted on a graph. I want to be as precise (and succinct) as I can when using this stuff, so my question to you math/statistics gods is this: ""Is there's an easy way for me to convert this data into something that looks like raw percentage? I.e. 'students from low-SES backgrounds are {X%} more likely to major in vocational tracts than high-SES students'"".

I wouldn't ask you guys if I knew someone personally who could help me at the moment. Thanks for sharing your expertise! And I apologize in advance if it is a stupid question and can't be done. ",en
1108959,2012-02-12 06:25:31,datasets,List or database of pharmaceuticals?,ploe8,[deleted],,https://www.reddit.com/r/datasets/comments/ploe8/list_or_database_of_pharmaceuticals/,1.0,1.0,"This is probably a bit of a stretch, but can anyone find a free source of a pharmaceutical list?

If not, what would be my best bet to obtaining one or making my own?

I *could* write a program to scrape a website like drugs.com but I can't imagine how long that would take and I'm pretty sure it's against their TOS to say the least.

**From their site:**

&gt; Data sources include Micromedex™ (updated Jan 27th, 2011), Cerner Multum™ (updated Jan 3rd, 2012), Wolters Kluwer™ (updated Jan 31st, 2012) and others.

I haven't looked into those sources yet, but I'm willing to be they're not free!",en
1108960,2012-02-12 07:05:03,datasets,Looking for NYC crime and social indicator data by zip code,plptj,shrine,1259001940.0,https://www.reddit.com/r/datasets/comments/plptj/looking_for_nyc_crime_and_social_indicator_data/,1.0,0.0,"Columbia U has some resources but many of them don't seem to offer zipcodes or datasets - just summaries. Does anyone know of a public dataset for historical or current crime or social indicator data? Anything at all would be an amazing help - thanks.

http://library.columbia.edu/indiv/lehman/guides/stats/ny.html

",en
1108961,2012-02-12 08:36:07,MachineLearning,Predicting my favourite drink - need for help.,plsop,danielkorzekwa,1300550268.0,https://www.reddit.com/r/MachineLearning/comments/plsop/predicting_my_favourite_drink_need_for_help/,4.0,6.0,"Hello,

I'm trying to solve a problem of recommending/predicting 'my favorite drink' and I'm hoping to get some support from this community.


Problem definition:
There are 20 different drinks, e.g. pepsi, coke, fanta, etc.
There are millions of customers of a supermarket who were buying those drinks over a period of lets say last three months.

Data definition:

Drinks (id,name): 100:Pepsi,101:Coke,....

Transactions
customer_id, list of bought drink ids
1 100,100,100,101,101,101
2 100, 102,106,106,106...
....

Definition of 'my favorite drink' is a bit foggy. We don't have any training data we can learn from, e.g. list of fans for a given drink, the only thing we have are transactions, and customer data (id, age, postcode).
Customer may not have a favorite drink and this should be predicted as well.

Those are 4 approaches I came up for predicting 'my favorite drink'.

1) 50% ratio - The drink, I buy the most. If the percentage of a my drink transactions is &gt;50% then this is my favorite drink. Otherwise I don't have a favorite drink.

2) Gini index, more clever version of 50% ratio, If I bought pepsi 4 times, and other 6 drinks once only each, then Pepsi is my favorite drink. Gini index = 1 minus sum of squares of drink probabilities. In this case Gini = 1 - (4/10)^2 + 6*(1/10)^2. I have a favorite drink if gini is &lt;0.7.

3) Rationale - My favorite drink not necessarily has to be the one I drink the most. For example if I bought 49 CopaCopa drinks and 51 Pepsi drinks, then CopaCopa drink is more likely my favorite one. This is based on observations that customers who buy CopaCopa are more likely to buy Pepsi (because this is generally popular drink), than the other way round. If I buy the same number of unpopular CopaCopa and popular Pepsi drinks then it probably means I'm more likely a fan of CopaCopa.

Method 3a: Naive Bayes Text classifier. For this approach I calculate priors - probability of buying a given drink using Maximum Likelihood based on all customers transactions data, e.g. P(Pepsi)=0.2, P(CopaCopa)=0.02. And then I calculate conditional probabilities of buying a drink given I also bought something else, e.g. P(CopaCopa | Pepsi) = 0.03 and P(Pepsi|CopaCopa) = 0.07.

Customer has a favorite drink if a posterior, e.g P(CopaCopa | Pepsi, Pepsi, CopaCopa, CopaCopa) (probability of being a fan of CopaCopa given I bought both Pepsi and CopaCopa twice) is &gt;50%.

Data for bayes classification (one record for a single drink transaction). Those five records represent a customer who bought three drinks 101,101,102 and a customer who bought two drinks 105:
drink_id(prior) all_drink_ids_bought_by_customer_of_this_drink_transaction(prediction record)
101 101,101,102
101 101,101,102
102 101,101,102
105 105,105
105 105,105

Method 3b: Logistic regression, I represent transactions as

Target = transaction drink id, prediction variables = percentages of drinks for a given customer, who placed this transaction, e.g. for a single customer, who bought pepsi, pepsi, and copacopa, we have three classification records (one per transaction):

target, %pepsi, %copacopa, %coke,.....
pepsi, 2/3,1/3,0,0,0,0...
pepsi, 2/3,1/3,0,0,0,0...
copacopa, 2/3,1/3,0,0,0,0...

Customer has a favorite drink if a logistic regression predicts drink with &gt;50% confidence, e.g. I take a customer who is represented by classification record: 0.1(CopaCopa), 0.7(Pepsi),0(Coke)..... I'm fan of Pepsi with a confidence level of 0.64.

I would appreciate any feedback on presented approaches. Maybe there is a better way to address this problem? I would be also glad to hear on some papers describing similar prediction problems in various domains.

Regards.",en
1108962,2012-02-12 15:00:45,statistics,Evolution of an Econometrician,pm00h,yisthisnotanon,1298123417.0,https://www.reddit.com/r/statistics/comments/pm00h/evolution_of_an_econometrician/,0.0,7.0,,en
1108963,2012-02-12 17:24:41,statistics,I was wondering if you guys can help me out with this problem. I'm trying to find if there is a significance between my observed and calculated values,pm34m,dassouki,1215439790.0,https://www.reddit.com/r/statistics/comments/pm34m/i_was_wondering_if_you_guys_can_help_me_out_with/,1.0,6.0,,en
1108964,2012-02-12 18:32:35,statistics,Making sure I'm doing this right-z scores and percentiles,pm57p,[deleted],,https://www.reddit.com/r/statistics/comments/pm57p/making_sure_im_doing_this_rightz_scores_and/,0.0,2.0,"I was hoping someone could verify if my answer is coming up correct on this problem.  I need to know what percent of scores fall between 12 and 18 on a normally distributed curve with a mean of 21 and a standard deviation of 8.  

The z scores I calculated were -1.125 and -0.375, respectively and the percentage between those scores equaling 22%",en
1108965,2012-02-12 18:55:19,MachineLearning,Anyone know where I can find a data set of grocery store purchases?,pm608,shaggorama,1233555004.0,https://www.reddit.com/r/MachineLearning/comments/pm608/anyone_know_where_i_can_find_a_data_set_of/,7.0,6.0,"I'm specifically looking for transaction histories with user and item resolution, where users are likely to purchase the same item (or item type) across several transactions. Something along the lines of a years worth of receipts for a handful of shoppers would be good. 

Given a users purchase history, I'd like to build a model to predict future purchases. I feel like this is a pretty classic task and I know the data set must be out there somewhere, but I just can't find anything and it's getting frustrating.",en
1108966,2012-02-12 20:00:54,statistics,How do bookmakers make sure the house always wins?,pm8g5,Barbas,1261160751.0,https://www.reddit.com/r/statistics/comments/pm8g5/how_do_bookmakers_make_sure_the_house_always_wins/,5.0,13.0,"We've all heard about guys who sometimes manage to ""beat the system"", whether that is on blackjack tables, or guys with their own databases making money through sports bets.

What I am in interested in is the other side. How do the bookies decide the odds for each game, over-under and the various other bets that can be made? Are there any good books/sites/blogs that have information on sports bets and explanations on the statistics behind traditional gambling games? Anyone here care to explain?
",en
1108967,2012-02-12 22:35:33,statistics,The Age of Big Data: NY Sunday Review on the impact of big data sets on nearly all fields,pmf4r,thegabeman,1239690590.0,https://www.reddit.com/r/statistics/comments/pmf4r/the_age_of_big_data_ny_sunday_review_on_the/,3.0,0.0,,en
1108968,2012-02-12 23:53:46,MachineLearning,"If, as according to Marcus Hutter, compression is AI, then is the recent optimal data transmission method AI (or can be used in machine learning)?",pmin8,marshallp,1239903633.0,https://www.reddit.com/r/MachineLearning/comments/pmin8/if_as_according_to_marcus_hutter_compression_is/,1.0,3.0,,en
1108969,2012-02-12 23:56:43,statistics,"Multiple Regression question
",pmis2,skinnyvegan,1304024945.0,https://www.reddit.com/r/statistics/comments/pmis2/multiple_regression_question/,2.0,3.0,"I am working on my thesis project which involves analyzing whether a dichotomous independent variable has an effect on a continuous dependent variable, however there are a number of potentially confounding factors that may affect this relationship.   How do I go about doing a multiple regression in SPSS to determine which variables have a significant confounding effect? And how do I  control for those variables? Any help would be greatly appreciated! ",en
1108970,2012-02-13 00:15:20,artificial,"If, according to Marcus Hutter, compression is AI, then is this latest optimal data transmission algorithm the optimal AI algorithm?",pmjmy,marshallp,1239903633.0,https://www.reddit.com/r/artificial/comments/pmjmy/if_according_to_marcus_hutter_compression_is_ai/,12.0,14.0,,en
1108971,2012-02-13 05:49:09,rstats,R Meetup in Minnesota,pmyyc,Here4TheCatPics,1292778117.0,https://www.reddit.com/r/rstats/comments/pmyyc/r_meetup_in_minnesota/,6.0,1.0,,en
1108972,2012-02-13 09:40:31,statistics,Use of **?,pn82u,[deleted],,https://www.reddit.com/r/statistics/comments/pn82u/use_of/,0.0,7.0,"I am going through some information on distributions and the symbol ** is used often. I recognize * as being used to multiply, but does ** mean something distinctly different?",en
1108973,2012-02-13 17:53:47,statistics,"Dilbert, on causality",pnkar,cruiseplease,1309488847.0,https://www.reddit.com/r/statistics/comments/pnkar/dilbert_on_causality/,1.0,0.0,,en
1108974,2012-02-13 21:02:53,statistics,Learning ESS and Emacs for statistical computing.,pnt1y,RA_Fisher,1299707119.0,https://www.reddit.com/r/statistics/comments/pnt1y/learning_ess_and_emacs_for_statistical_computing/,8.0,16.0,"My statistical work lately has demanded that I use R, SAS, and Stata.  Further, I've been interested to begin seriously using Git or version control.  I have a basic [github account](https://github.com/cpeter9), but I don't use it very efficiently.  As a solution to coding in three/four separate editors, plus a TeX editor, I've begun entertaining learning Emacs and [ESS](http://ess.r-project.org/).  The problem is that Emacs has a steep enough learning curve as it is, so I was wondering if others out there know or have attempted learning Emacs + ESS -- and/or if there are any successful users that can give tips / suggestions?

**tl;dr: seeking tips on learning ESS and Emacs for programming R, SAS, Stata, LaTeX, Sweave, etc.**",en
1108975,2012-02-13 21:22:57,statistics,Advice for picking dissertation topics in graduate school?,pnu0t,hawthoreus,1329080406.0,https://www.reddit.com/r/statistics/comments/pnu0t/advice_for_picking_dissertation_topics_in/,5.0,3.0,"How much does choice of a particular dissertation topic affect job prospects upon graduation from statistics/biostatistics programs? Obviously, the quality of your work is very important, but I'm interested in knowing if there are particular topics that might enable or disqualify you from certain jobs (either in academia or in industry). I get the sense that in industry many employers simply want a bright person who has a solid and well-rounded background in stats methods. In academia, your choice of topic seems to matter a bit more based on the specific interests of other faculty in the department that hires you. Does this seem accurate?

I'm also interested in how common it is to change research topics as a post-graduate. If I write my dissertation on topic X, is it generally expected I will spend my career studying topic X? Or is it easy/typical to switch my focus to studying topics Y and Z?

I'm not looking for any personal recommendations, I'm just interested in hearing general thoughts on this issue, as well as any illustrative examples. I recently began a biostatistics phd program; I love everything about mathematics, research, and statistics, so I have no doubt that I will love any topic I eventually choose. I figure if there are additional pragmatic reasons for choosing one topic over another, I should take them into account.",en
1108976,2012-02-13 22:03:48,statistics,Question about p-values,pnw5w,songanddanceman,1288384615.0,https://www.reddit.com/r/statistics/comments/pnw5w/question_about_pvalues/,5.0,7.0,"I saw researcher who presented data at the individual level. In his data, he wanted to show that *all* participants have no difference in their individual means, and so his objective was to show a non-significant p-value for every person in this task. 

For 20 participants, 19 were non-significant, and 1 was not. He said that because the alpha level used for determining significance was .05, that one significant person is to be expected by chance and that these data are consistent with his hypothesis (that there is no evidence people show a difference).

My question is two-fold:

**First**, is his explanation where 1/20 significant values can be discounted due to the alpha level being set at .05  a correct interpretation of the p-value? I was led to believe that it merely represents the probability of obtaining differences at least as extreme as the one obtained if we assume there was truly no difference. The way he interprets it is more foreign to me because it sounds like he treats it as the type 1 error rate, which this paper makes out to be a common misunderstanding [link](www.uv.es/sestio/TechRep/tr14-03.pdf). Would 1 significant result out 20 trials be something that can be overlooked?

**Second**, if 1/20 results *are* to expected to be significant by chance and you can ignore one significant finding out of 20, does the p-value obtained for that one significant result matter in how skeptically it is treated. That is, if I run 20 exacts experiments and 19 are non-significant but 1 is significant (p=.04), does that one significant result get ignored as equally as a significant results that is p=.0001? My naive intuition is no, but I'd like to hear what people's thoughts and explanations are on the matter.",en
1108977,2012-02-13 23:40:13,datasets,Request: Software for graphing/visualizing start/stop times?,po19l,colindean,1202073270.0,https://www.reddit.com/r/datasets/comments/po19l/request_software_for_graphingvisualizing/,4.0,0.0,"Hey /r/datasets, I'm looking for some kind of software or howto with some existing piece of well-known graphing software (i.e. gnuplot) that I can use to visualize a set of start and stop times. Think of the data kinda like a computer [boot chart](http://www.bootchart.org/images/bootchart.png) showing at what time a process started and ended on a line.

My data has a label, a start date, and an end date. It's possible for a label to have multiple start and end dates, but if a software can't support that, then I can simply change the label (there's only one label like that right now).

**Any suggestions?**",en
1108978,2012-02-13 23:46:22,statistics,What do you call two coin flips on their honeymoon?,po1kv,Xkf,,https://www.reddit.com/r/statistics/comments/po1kv/what_do_you_call_two_coin_flips_on_their_honeymoon/,55.0,5.0,,en
1108979,2012-02-14 00:14:42,statistics,What distribution has CDF of Erfc[a/x]? ,po2zl,sheppa28,1274041127.0,https://www.reddit.com/r/statistics/comments/po2zl/what_distribution_has_cdf_of_erfcax/,2.0,4.0,"Can't seem to find the name of it, if it does have a name. ",en
1108980,2012-02-14 01:55:04,artificial,A Smartphone That Detects Whether Its User Is Depressed ,po8ck,AIBRO,1329177153.0,https://www.reddit.com/r/artificial/comments/po8ck/a_smartphone_that_detects_whether_its_user_is/,7.0,4.0,,en
1108981,2012-02-14 03:02:33,artificial,How long would it take to process the entire game tree for chess?,pobsq,jason-samfield,1248855567.0,https://www.reddit.com/r/artificial/comments/pobsq/how_long_would_it_take_to_process_the_entire_game/,7.0,21.0,,en
1108982,2012-02-14 03:05:46,statistics,Poisson regression with offset,pobyf,hypermonkey2,1301460132.0,https://www.reddit.com/r/statistics/comments/pobyf/poisson_regression_with_offset/,2.0,7.0,"Hi all!

I am dealing with a situation where count data from experiments were recorded. Essentially, for each experiment, the number of successes out of a ""random"" number of trials (i.e. the number of trials is not controlled by the investigator, but is ~10-15) was recorded.

We decided to model these data using Poisson regression using the number of trials as an offset.
The resulting model was overdispersed.

Therefore we tried scaling by deviance, as well as scaling by Pearson chi-square. Additionally, we tried fitting a negative binomial model.

Problem: Many of these solution models to overdispersion disagree with each other. What to do? The offset method seemed like a good idea, but don't look so good now.


Thanks very much!",en
1108983,2012-02-14 04:51:33,MachineLearning,S3VM implementations?,pohgw,jonsca,1307621848.0,https://www.reddit.com/r/MachineLearning/comments/pohgw/s3vm_implementations/,2.0,4.0,Do any (FOSS or not) machine learning packages have an implementation of S3VMs (Semi-supervised support vector machines)?  Are there any general packages available for semi-supervised work?,en
1108984,2012-02-14 05:59:41,artificial,The most realistic artificial plants on sale - with free shipping!,pol1m,cchic,1312853008.0,https://www.reddit.com/r/artificial/comments/pol1m/the_most_realistic_artificial_plants_on_sale_with/,1.0,0.0,,en
1108985,2012-02-14 07:57:08,computervision,Is there a tutorial for making OpenCV 2.3+ in Visual Studio?,poqlh,teh5thhorseman,1298262852.0,https://www.reddit.com/r/computervision/comments/poqlh/is_there_a_tutorial_for_making_opencv_23_in/,0.0,2.0,"Hi I have been working on a algorithm conversion from matlab to opencv and c++ for the ComSci grad department at my school, and I was wondering if there is any help out there to get OpenCV to like Visual Studio. It works great in Ubuntu and QTcreator, but not VS halp?",en
1108986,2012-02-14 12:42:16,artificial,"Has anyone ever pre-processed (at the very least) a very large, but still most likely only partial game tree for chess so that an AI chess playing agent could effectively dominate another AI agent that is performing only realtime game tree processing?",poy4g,jason-samfield,1248855567.0,https://www.reddit.com/r/artificial/comments/poy4g/has_anyone_ever_preprocessed_at_the_very_least_a/,0.0,3.0,,en
1108987,2012-02-14 12:54:38,artificial,"Has anyone ever taken a pre-processed, but deep game tree for chess and then compressed it into probabilities of winning based upon deeper branches of the tree for utilization as a heuristic reference for a realtime chess playing AI agent?",poyd1,jason-samfield,1248855567.0,https://www.reddit.com/r/artificial/comments/poyd1/has_anyone_ever_taken_a_preprocessed_but_deep/,0.0,3.0,,en
1108988,2012-02-14 15:39:53,MachineLearning,"RStudio in the cloud, for dummies
\",pp262,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/pp262/rstudio_in_the_cloud_for_dummies/,1.0,0.0,,en
1108989,2012-02-14 17:55:07,artificial,Natural looking and the most realistic artificial Areca trees.,pp7e4,cchic,1312853008.0,https://www.reddit.com/r/artificial/comments/pp7e4/natural_looking_and_the_most_realistic_artificial/,1.0,0.0,,en
1108990,2012-02-14 19:11:33,MachineLearning,Screen Printing Machine,ppazr,[deleted],,https://www.reddit.com/r/MachineLearning/comments/ppazr/screen_printing_machine/,1.0,0.0,,en
1108991,2012-02-14 19:17:43,computervision,"I have no formal background in computer science or programming, but I've become fairly interested in computer vision as related to creating bots. Would anyone be up for giving the (primitive) methods I've come up with a few critiques and discussion?",ppbah,[deleted],,https://www.reddit.com/r/computervision/comments/ppbah/i_have_no_formal_background_in_computer_science/,5.0,5.0,"I've been experimenting with ""monitoring"" events on screen. My methods are pretty simple, almost all involve using PIL to grab a snapshot of the screen, and then process it. I thought I'd lay out the things I've come up with in hopes you guys could tell me what's heading the right direction, completely off base, or perhaps a better way to go about this in general.

I've worked out three main methods for semi-successfully detecting on screen events. 

The first is simply testing expected exact pixel locations for expected values. Example:

ExpectedVal = (255,57,127)
Im = ImageGrab.grab() 

If im.getpixel((45,154)) == ExpectedVal:
Do something

It's obviously very fragile, but works quite well on static images. To detect things that are moving, for instance if I'm trying to 'see' a new character on screen, i'll ""shotgun"" my approach by spamming the getpixel() check across an area. Success rates were fairly low when taking the spam approach however.

Second method is loading the screenshots rub values into an array, summing it, and maybe averaging it (depending on the situation). This allows me to detect (under specific circumstances) if a new character is on screen because the average color of the play area goes up. This is very broad, as it doesn't give me the location of the event, but it does at least let me know that *something* has happened on screen. 

Img = ImageS.grayscale(ImageGrab.grab())
A = array(img.getdata())
Return sum(a)

The variation of this is using the sum of the array to figure out on screen menus and whatnot. By grabbing boxes from the screen and summing their average colors I can create a dictionary of expected values for certain items. I find this to be a good replacement for getpixel() for menus that share a lot of the same colors. 

The last method, again requires very strict circumstances (in this case, a static non-changing background), but having a base screenshot which I compare all subsequent snapshots to. 

By greyscaling and then subtracting newscreenshot from basescreenshot the resulting array is all zeros. So if anything new is present, it's a non-zero value. I can then use the array as a coordinate plane an map mouse movements to follow the non-zero values.  I initially thought this was a really neat idea, but in practice, by the time it runs through the arrays and finds a nonzero the object has moved to a new location on screen, and so mouse movements lag behind..

And that's my current toolbox. Are any of these approaching good ideas? What are better ways to go about this? 

Are there any good ""basic"" intro books to the field of computer vision? ...is compvision even the correct term for what I'm attempting to do? Most of the things I see relate to hooking into a camera feed.

Is it doable with python? I'm finding the biggest hurdle in my quest to control things on-screen is performance. Each snap with PIL takes a decent amount of time.

Anyway, that's all! 

**edit** yikes! Formatting is terrible! I wrote this all from my phone, i'll try to clean it up once I get to a computer!",en
1108992,2012-02-14 22:27:01,MachineLearning,Wolfram Blog: Launching a Democratization of Data Science,ppkxg,willis77,1188267725.0,https://www.reddit.com/r/MachineLearning/comments/ppkxg/wolfram_blog_launching_a_democratization_of_data/,27.0,4.0,,en
1108993,2012-02-14 23:09:00,rstats,I'm new to R and could use help please.,ppn4g,[deleted],,https://www.reddit.com/r/rstats/comments/ppn4g/im_new_to_r_and_could_use_help_please/,4.0,7.0,"Hello, I'm very new to R and I am trying to perform a linear regression. See [this image](http://i.imgur.com/DCZex.png). I don't know how to let R know that I want to define x=the data in the first column and y=the data in the second column. I know that I can use x=scan() and type in all the values, but is there a way to pull this information from the table with a command? Thanks a lot.",en
1108994,2012-02-14 23:24:09,statistics,Help with R and Linear Regression? [x-post from r/stats],ppnw3,[deleted],,https://www.reddit.com/r/statistics/comments/ppnw3/help_with_r_and_linear_regression_xpost_from/,1.0,0.0,"Hello, I'm very new to R and I am trying to perform a linear regression. See [this image](http://i.imgur.com/DCZex.png) . I don't know how to let R know that I want to define x=the data in the first column and y=the data in the second column. I know that I can use x=scan() and type in all the values, but is there a way to pull this information from the table with a command? Thanks a lot.",en
1108995,2012-02-15 02:04:15,statistics,"Would you rather get 3% cashback on a credit card, or an annual 3% chance of getting all purchases for the entire year for free?",ppw8l,[deleted],,https://www.reddit.com/r/statistics/comments/ppw8l/would_you_rather_get_3_cashback_on_a_credit_card/,13.0,27.0,,en
1108996,2012-02-15 02:24:44,statistics,Does anyone know where I can get the SPSS example data that comes with the program?,ppx9m,trotsky41,1288461836.0,https://www.reddit.com/r/statistics/comments/ppx9m/does_anyone_know_where_i_can_get_the_spss_example/,1.0,0.0,"I have the program, but somehow not the example data that comes along with it, and I need for my homework. If you could ",en
1108997,2012-02-15 02:25:46,statistics,Think Complexity: Part Four.  Small world graphs and scale free networks.,ppxba,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/ppxba/think_complexity_part_four_small_world_graphs_and/,8.0,3.0,,en
1108998,2012-02-15 06:32:32,statistics,Statistics is everything in mobile marketing,pq9gv,prhodes,1328233759.0,https://www.reddit.com/r/statistics/comments/pq9gv/statistics_is_everything_in_mobile_marketing/,1.0,0.0,,en
1108999,2012-02-15 06:42:33,MachineLearning,What is the current state of hand writing recognition technology?,pq9yq,asjohnson,1288157377.0,https://www.reddit.com/r/MachineLearning/comments/pq9yq/what_is_the_current_state_of_hand_writing/,10.0,21.0,I was at work today and noticed that a large number of hours go into converting scanned in documents into text and stored in a database. This got me wondering about how viable a hand writing recognition program would be. Are we close to reliable enough methods?,en
1109000,2012-02-15 10:21:34,statistics,How to tell if a difference between mean and variance defies Poisson?,pqhfg,[deleted],,https://www.reddit.com/r/statistics/comments/pqhfg/how_to_tell_if_a_difference_between_mean_and/,5.0,15.0,"For example, if you had a mean of 4.1 and a variance of 4.4, would this inherently not fit a poisson distribution? They should be the same, but is there any way to tell how off this is without a sample size?",en
1109001,2012-02-15 17:48:02,data,"/r/dataisbeautiful:  a new, active community for visual representations of data",pqsid,zanycaswell,1299619546.0,https://www.reddit.com/r/data/comments/pqsid/rdataisbeautiful_a_new_active_community_for/,2.0,0.0,,en
1109002,2012-02-15 17:49:24,statistics,Report exact p-values?,pqskl,hawthoreus,1329080406.0,https://www.reddit.com/r/statistics/comments/pqskl/report_exact_pvalues/,1.0,15.0,"Which is better, reporting p-values as inequalities (e.g. p &lt; 0.05), or reporting the exact probability (e.g. p = 0.0023)?

I have heard arguments both ways. On the one hand, exact probabilities convey more detailed information about the test result, and more information is better than less.

The counterargument asserts that the additional information conferred by exact p-values is useless and potentially misleading. If you've set an a priori alpha of x%, then the only thing that matters is whether the p-value is less than or greater than x%. You won't (shouldn't!) reject your null hypothesis ""more"" if the p-value is very small.

My inclination is to report the p-value as an inequality, along with the test statistic and any additional parameters necessary to calculate the exact p-value (e.g. df) if needed.

References to published discussions are appreciated, e.g. http://www.ncbi.nlm.nih.gov/pubmed/8239052

I'd like to avoid tangential discussions regarding the merits of p-values vs confidence intervals, or of frequentist vs bayesian vs likelihood paradigms, etc.",en
1109003,2012-02-15 18:08:53,statistics,Question about multiple comparisons,pqte5,iamhandsome,1322753918.0,https://www.reddit.com/r/statistics/comments/pqte5/question_about_multiple_comparisons/,1.0,16.0,"I'm hoping someone can shed light on my current understanding.  My background is not in statistics but I'm require to perform various analyses day-to-day.

So everyone always talks about multiple comparisons.  If you, for example, were to compare 20 interrelated factors and found 5 significant (P&lt;0.05) results (via univariate tests, such as Fischer's Exact) - people want to know what the ""adjusted/corrected"" p values.  Usually the suggestion is to perform Bonferonni's, Sidak's, or REGW.  This is fine and all, unless those 5 significant factors are only marginally significant (i.e 0.03-0.049 range) - the post-calculated values would likely wash out and you'd be likely left with 0 significant factors post-correction.

So I've always preferred to use multivariate logistics regression with stepwise model selection (Backward Wald, Backward Likelihood-ratio).  Using a program like SAS or SPSS, I'll determine which (or all) of the 5 factors are mutually exclusive, and more likely to be a real (not a false-positive) result in the final model.

**So I'm wondering if my approach is sound.  Rather than simply performing a Bonferonni's after univariate testing, I go straight into multivariate logistics regression with stepwise Wald or Likelihood Ratio - would such a method be more suitable for performing multiple comparisons?**",en
1109004,2012-02-15 19:20:20,statistics,What Do You Know About the University of Florida Stats PhD Program?,pqwt3,Heyooooh,1223998158.0,https://www.reddit.com/r/statistics/comments/pqwt3/what_do_you_know_about_the_university_of_florida/,5.0,9.0,"I've been offered a spot and they seem pretty good about funding, but I'd like to hear from the broader stats community about the program's reputation. I have visited Gainesville and met with professors in the program so I have my own (favorable) opinions. I figured I would ask around and see if anyone has had experience with the program or knows anyone who does.

Little context: I'm a decent programmer and looking to get better and perhaps a little light on the hard math. I'm going to school for statistics because I enjoy it, and I think data is a huge part of the future of nearly every field. I'm not targeting any specific field at the moment, but I have a social science bent and I think I would like to work in a more applied / technical capacity after school. ",en
1109005,2012-02-15 20:38:03,datascience,Career Advice,pr0oc,throwaway_account_2U,1329330065.0,https://www.reddit.com/r/datascience/comments/pr0oc/career_advice/,4.0,7.0,"I'm trying to get a job in the data science field and failing.

I have an MS in Computer Science and a MA in Statistics plus 2 years of experience as a software developer.  In academia I focused primarily on data analytics, machine learning, regression, ANOVA, and math theory courses such as linear programming, natural language processing, linear algebra, and probability theory.  My passion is data analysis, data mining, machine learning and applied statistics.

My background is definitely focused on theory rather than applications, and I am alleviating this difficulty by working on Kaggle problems, refreshing my R programming and studying Hadoop.

Would learning SQL be useful?

Any other advice would be appreciated.",en
1109006,2012-02-15 20:41:23,statistics,Books on statistics,pr0um,lol_fps_newbie,1294289929.0,https://www.reddit.com/r/statistics/comments/pr0um/books_on_statistics/,0.0,5.0,"Hi /r/statistics, I'm a graduate student in machine learning/data mining, and I feel as if my statistics background is quite weak. As a result, I'm looking for a stats book that will give me a nice introduction to statistics.

The things I'm most interested in are: hypothesis testing (i.e., is data set X distributed as Y), error estimation (i.e., given some data points X, and an estimate of the mean and standard deviation, what is the error in the estimation?). (As a side note, if anyone has any other areas that they think are particularly important, I'd love to hear about them. I'm not married to these areas, they just seem like the ones that I come across most often and therefore would definitely like more information about.

In addition to a book about the aforementioned basic statistics topics, I'm also looking for a time series analysis book. One book that has been recommended to me so far is: [Analysis of Financial Time Series \(Wiley Series in Probability and Statistics\)](http://www.amazon.com/Analysis-Financial-Wiley-Probability-Statistics/dp/0470414359/ref=sr_1_1?ie=UTF8&amp;qid=1329257412&amp;sr=8-1). Is this a good book for the topic? Is there something better?

Finally I'm also looking to learn some measure theory. If anyone has any information about that topic as well, that would be greatly appreciated. Thanks!",en
1109007,2012-02-15 21:25:14,statistics,"Mahalanobis distance: What it is, how it works, and why it's useful",pr32r,RickWicklin,1327333524.0,https://www.reddit.com/r/statistics/comments/pr32r/mahalanobis_distance_what_it_is_how_it_works_and/,14.0,4.0,,en
1109008,2012-02-15 22:41:50,statistics,"Personal assistant needs some help: Has anyone heard of a paper by a Stanford professor with a title like ""The Problem With Averages"" and has an analogy to someone walking on the bottom of a river, falling in a hole, and drowning?",pr73y,[deleted],,https://www.reddit.com/r/statistics/comments/pr73y/personal_assistant_needs_some_help_has_anyone/,1.0,2.0,"The analogy about the river shows that if you plan for the average water level you'll drown when the bottom of the river drops away.

He thinks it's a reasonably important/notable paper.

Any help would be much appreciated. He needs it for a presentation tomorrow and these are all the details of it that he recalls.

",en
1109009,2012-02-16 00:36:03,statistics,Statisticians,prd3o,TerraByte,1222350785.0,https://www.reddit.com/r/statistics/comments/prd3o/statisticians/,1.0,0.0,,en
1109010,2012-02-16 02:02:09,MachineLearning,Best ML technique for my problem...,prhbi,[deleted],,https://www.reddit.com/r/MachineLearning/comments/prhbi/best_ml_technique_for_my_problem/,4.0,4.0,"Hi everyone,

I was thinking about the best technique to solve a problem I have and would appreciate any input.

Here's the problem:
I have a set of features X which get put into a unknown algorithm with parameters A. The algorithm uses parameters A and features X to classify (binary classification) X. I have a bunch of truth data, each row of which has my features X and parameters A and output Y (y=1 for correct classification, y=0 for incorrect classification). The goal is to choose a set of parameters A for new incoming features X which will maximize my chances of correct classification.

It's interesting because while some set of parameters A might work optimally for some set of features X, that same set of parameters A could completely fail given radically different inputs X.

The method I am currently considering using is some type of collaborative filtering (similar to netflix problem), where each ""movie"" is one of parameters or features from the combined set of features and parameters X,A. And each entry (""user"") is the set of values for the features and parameters that correctly classified the features.
I would then have a matrix of correct classifications and a new entry would consist of only the features X, and i estimate the values of A.

Thoughts? I would appreciate any opinions on this.

Thanks! ",en
1109011,2012-02-16 03:02:29,artificial,Does anyone know the source of this? Does it make sense?,prke8,tralfamadorFTW,1311078161.0,https://www.reddit.com/r/artificial/comments/prke8/does_anyone_know_the_source_of_this_does_it_make/,3.0,15.0,,en
1109012,2012-02-16 04:57:55,statistics,What is the best book for Bayesian Statistics?,prq0u,sortizo,1288834170.0,https://www.reddit.com/r/statistics/comments/prq0u/what_is_the_best_book_for_bayesian_statistics/,14.0,9.0,Introductory book...,en
1109013,2012-02-16 06:48:52,statistics,Help with a comparison of proportions assignment,prvj3,showerpower12,1293945202.0,https://www.reddit.com/r/statistics/comments/prvj3/help_with_a_comparison_of_proportions_assignment/,1.0,1.0,"Hey all, I was hoping you could help me out with a stats assignment I recently got. The scenario is:

A large group of people was divided up into 5 sample populations. Each group was asked the same series of questions about how they feel about particular things (their job, their car, their doctor, etc.). However, each group was assigned a different rating scale. One received plus/minus, another got a 4-point scale, another got a 10-point scale, etc. My professor has me roleplaying as a market research company. A client comes in and wants us to find out which scale will give the most positive results. (i.e. they're asking the same questions, but in what manner should they have the test-takers score their responses)

So I've got all the data loaded in SPSS. I was given a textbook section that describes the formula for comparing two proportions. I figured I would find the proportion of responses that were in the positive half of the scale and compare them to each other. Going from the book, I did some formulas, figured out p^ and q^ for each proportion pair--as well as the difference in std dev--and finally a Z-score. For each pair of scales pitted against each other.

My question is: I don't really know what to do from where I am. How do I interpret these results once I've figured out the Z-score? How do I determine which one is the best scale across the board?

Edit: Ah shit, I just saw the""no homework help."" Sorry mods... If anyone happens to be feeling particularly generous, however...",en
1109014,2012-02-16 11:03:15,statistics,"Are there ways to evaluate statistically if someone ""cheated"" in a study and just clicked his/her way to the end?",ps40o,NillePille,1322438479.0,https://www.reddit.com/r/statistics/comments/ps40o/are_there_ways_to_evaluate_statistically_if/,5.0,22.0,"hi reddit,
I´m using SPSS to evaluate the data for my diploma thesis and was just wondering if there are ways to find out if some people just clicked randomly througout the study.

additional question:
If someone knows reliable sources which cases (outliers) i can cut out in a regression analysis, would be awesome. 

thanks for your help.
",en
1109015,2012-02-16 11:13:21,statistics,"podcast about R (first episode, I hope to see more like this) [r-bloggers]",ps47x,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/ps47x/podcast_about_r_first_episode_i_hope_to_see_more/,26.0,0.0,,en
1109016,2012-02-16 17:06:01,MachineLearning,Implementing Logistic Regression and Naive Bayes classifiers in Matlab,pscu0,dorian87,1329404158.0,https://www.reddit.com/r/MachineLearning/comments/pscu0/implementing_logistic_regression_and_naive_bayes/,2.0,7.0,"I am attempting to implement these two types of classifiers in Matlab. I understand the concepts, I just do not know Matlab very well. I know Matlab has a toolbox to do these, but I'm attempting to implement it myself. Does anyone know where I can find code or guides to push me in the right direction?
Thanks in advance for any help.",en
1109017,2012-02-16 18:00:13,MachineLearning,Can machine learning realistically apply to our everyday choices?,psf00,manliness,1312123596.0,https://www.reddit.com/r/MachineLearning/comments/psf00/can_machine_learning_realistically_apply_to_our/,2.0,6.0,"For example, could a program help us learn how our habits and choices (food consumption, sleep habits, daily actives, etc) determine our subsequent mood and productivity?

It seems that there are so many variables in our daily lives that accurate data recording would be overwhelming. Also, the problem of recording our overall experience (on some happiness or productivity scale) is subjective. On the other hand, I believe that if only we knew what to record, machine learning could help us improve our lives. No doubt, such a project would be an ambitious task.",en
1109018,2012-02-16 18:22:57,computervision,Help with video fingerprinting ,psg0r,tormentor308,1289454677.0,https://www.reddit.com/r/computervision/comments/psg0r/help_with_video_fingerprinting/,5.0,4.0,"Hello reddit

I am currently a computer science student and I recently became interested in the idea of video identification. So I though of creating my own project; a video player capable of identifying the video being played and based on that information it requests more information (from the internet) about the video (like Metadata for example) and presenting it to the user. Pretty simple huh?

I've done some research and found out that the best technique to identify a video is by video fingerprinting, a unique identifier extracted from video content. I can then compare the ""fingerprints"" from my database with that of the video till I find a match. video identified! 

However I can't find any references for algorithms or open source libraries to start with my project. I don't know how fingerprinting actually works and how can I create an efficient database of fingerprints which ensures fast comparisons. Any help would be appreciated. Thanks",en
1109019,2012-02-16 20:02:03,MachineLearning,"How Companies Learn Your Secrets -- “If we wanted to figure out if a customer is pregnant, even if she didn’t want us to know, can you do that? ”",psksj,jdw25,1220557205.0,https://www.reddit.com/r/MachineLearning/comments/psksj/how_companies_learn_your_secrets_if_we_wanted_to/,61.0,9.0,,en
1109020,2012-02-16 21:27:18,data,Simplified Relational Hierarchy Visualization,psp34,renaebair,1214746876.0,https://www.reddit.com/r/data/comments/psp34/simplified_relational_hierarchy_visualization/,1.0,0.0,,en
1109021,2012-02-17 00:41:14,statistics,“Just wait. We’ll be sending you coupons for things you want before you even know you want them.” - How Companies Learn Your Secrets - NYTimes.com,psyyi,atomofconsumption,1205649727.0,https://www.reddit.com/r/statistics/comments/psyyi/just_wait_well_be_sending_you_coupons_for_things/,2.0,1.0,,en
1109022,2012-02-17 06:16:28,AskStatistics,"Help with stata? (Helping me helps Asia’s poorest country) (X/Post ""Assistance"" and “techsupport”)",ptevm,toofarover,1329376123.0,https://www.reddit.com/r/AskStatistics/comments/ptevm/help_with_stata_helping_me_helps_asias_poorest/,1.0,0.0,"Hello, I am working with a (cash-strapped) NGO in Timor-Leste doing development work through promotion of the rule of law. I was just given a file that is apparently “from stata” (the person who gave me the data had English that was nearly as bad as my Tetum which in turn is comparable to my lack of computer knowledge so this might not make sense) that I need to use to write an important policy proposal using the data in the file. I don’t have stata, can’t afford stata, but really need the information. Is there some way to get this information to open in another program? It allegedly should contain an important data set.

Any help, including referral to another subreddit, would be greatly appreciated by me, and the citizens of Timor-Leste (who will benefit a great deal from the policy changes advocated in the future report facilitated by access to this data).

Thank you,

Toofarover

PS. In case you need proof that I am legit and that your help will have knock on effects I will refer you to my awesome and hilarious non-fiction blog at toofarover.blogspot.com. Its XKCD if XKCD was much suckier and based around non-fiction snapshots of the life of a development worker in Timor-Leste!
",en
1109023,2012-02-17 06:25:37,statistics,Target can tell if a woman is pregnant and estimate her due date by analyzing her purchase history.,ptf9z,[deleted],,https://www.reddit.com/r/statistics/comments/ptf9z/target_can_tell_if_a_woman_is_pregnant_and/,76.0,19.0,,en
1109024,2012-02-17 06:49:56,statistics,The ignorant Monte Hall Problem,ptgc5,[deleted],,https://www.reddit.com/r/statistics/comments/ptgc5/the_ignorant_monte_hall_problem/,0.0,1.0,"I hope this is the right subreddit.

Anyway, I was reading a post in [/r/engineering](/r/engineering) about the ignorant Monte Hall problem. I think most of you are aware of the Monte Hall paradox, and with a little careful thought (for me it was writing code to simulate the situation that made me understand before I finished) you can come to the right answer. So here are the two scenarios.



Behind one of three doors is a prize. You pick a door. The host (Monte), then shows you one of the doors you did not pick that is not a winner. You are then asked if you would like to switch. The advantage is switching, as there is a 2/3 chance the door you switch to will be the right one.


Now lets say you are in the same situation, you pick one of the three doors. Now Monte leaves the room. You check behind one of the doors you did not choose, and it happens to not be a winner. Do you switch? 


I have been told that switching or not switching provides the same probability of winning. Why is this?",en
1109025,2012-02-17 09:49:05,MachineLearning,What kind of statistics knowledge would I need for natural language processing? [x-posted to r/statistics],ptmpn,sodapressing,1329096711.0,https://www.reddit.com/r/MachineLearning/comments/ptmpn/what_kind_of_statistics_knowledge_would_i_need/,1.0,3.0,"I'm about to enroll for next quarter's classes, and I'm not sure which statistics classes would be the most relevant for a career in natural language processing (unfortunately, the stats course on pattern recognition and machine learning has been cancelled).
Any advice? Thank you in advance.

EDIT: Can someone weigh in on which would be more helpful for NLP?

* Intro to Mathematical Statistics (survey sampling, estimation, testing, data summary, one- and two-sample problems) or
* Statistics for Engineers (sampling distributions, statistical estimation (including maximum likelihood estimation), statistical intervals, and hypothesis testing)",en
1109026,2012-02-17 11:21:48,MachineLearning,Request for info about feature scaling,ptow3,tonnosense,1324634475.0,https://www.reddit.com/r/MachineLearning/comments/ptow3/request_for_info_about_feature_scaling/,6.0,10.0,"Having preprocessed some mean-normalized data by additionally performing feature scaling on all input features to fit an interval, say [-1,1] (possibly by dividing each data entry by the range), and having trained certain parameters using this data-set to output certain ""predictions"". If I were to use these parameters on new cases, I would need to use the same method to scale (preprocess) the new data (divide by the same number - possibly range of the training set).

Coming to the point, although this new data would come from a similar distribution as the training set, there is a possibility that it would not scale to [-1,1]. How would we handle such situations? Is it ""okay"" for the new data not to scale to [-1,1]?

I'm fairly new to Machine Learning and Statistical Regression and my question may reflect my experience (or the lack thereof) with the subject, I sincerely apologize for the possibility of this being a naive query.",en
1109027,2012-02-17 16:31:38,MachineLearning,what data should I try to collect,ptvy8,two_Thirds,1315670935.0,https://www.reddit.com/r/MachineLearning/comments/ptvy8/what_data_should_i_try_to_collect/,1.0,6.0,"I am making a bayesian predictor to recommend web comics based on  previously liked genres and comics.
Since I am going to make this user based, how should I go about figuring out what questions can help me predict what they might like? (gender, age, preferably something less personal)
 general comments welcome!",en
1109028,2012-02-17 23:19:58,statistics,WE'RE ALL GOING TO DIE!!! (slightly sooner than was previously believed),puefk,shaggorama,1233555004.0,https://www.reddit.com/r/statistics/comments/puefk/were_all_going_to_die_slightly_sooner_than_was/,7.0,1.0,,en
1109029,2012-02-18 04:58:50,artificial,What's the best way to go about gaining experience in the artificial intelligence field?,pus6f,[deleted],,https://www.reddit.com/r/artificial/comments/pus6f/whats_the_best_way_to_go_about_gaining_experience/,7.0,24.0,"I'm currently a second year computer sci/neuroscience double-major at a good school with a good GPA. You guys know of any cool opportunities for computational neuroscience, artificial intelligence, or neurotechnology research? I'd love any help I can get. ",en
1109030,2012-02-18 05:12:06,MachineLearning,What's a cool way to get experience as a student in a field like machine learning?,puson,[deleted],,https://www.reddit.com/r/MachineLearning/comments/puson/whats_a_cool_way_to_get_experience_as_a_student/,1.0,8.0,"I'm currently a second year computer sci/neuroscience double-major at a good school with a good GPA. You guys know of any cool opportunities for computational neuroscience, artificial intelligence, or neurotechnology research? 

Also, the weirder the topic, the better. T-hanks:)",en
1109031,2012-02-18 20:07:29,rstats,Plotting using R,pvfci,redditopus,1319318330.0,https://www.reddit.com/r/rstats/comments/pvfci/plotting_using_r/,11.0,14.0,Is there a comprehensive resource detailing every kind of plot you can make using R?  I am an idiot at R and need to use it for basic statistical and plotting crud.,en
1109032,2012-02-18 22:25:22,statistics,Need Help w/ PPC (AdWords) Significance Testing,pvkxa,[deleted],,https://www.reddit.com/r/statistics/comments/pvkxa/need_help_w_ppc_adwords_significance_testing/,1.0,0.0,"Hi Reddit,

I am currently working on a project where our team is trying to determine if the results from an A/B test (Ad A vs. Ad B) on AdWords is statistically significant. I have been doing much research in the past few days and have come across multiple different tools that compute this automatically.

However, this is not sufficient. I need to be able to interpret the results and I am not 100% of what is going on behind the scenes. Furthermore, I have found that many of the tools seem to use different methodologies and I was wondering which one makes the most sense? Some use chi-square testing and others are using significant testing w/ p values etc.

Tool 1: http://goo.gl/MuWVp
Tool 2: http://goo.gl/jiImn

Any guidance would be greatly appreciated! ",en
1109033,2012-02-18 23:12:37,artificial,What AI tests exist besides the Turing test?,pvmvy,[deleted],,https://www.reddit.com/r/artificial/comments/pvmvy/what_ai_tests_exist_besides_the_turing_test/,18.0,43.0,I'm working on a school project in which I'm comparing natural intelligence with artificial intelligence. For that I'm looking for an AI test other than the Turing test. I couldn't really find any other AI test on the internet.,en
1109034,2012-02-18 23:57:34,statistics,"In SPSS, when running post hoc tests following ANOVA, does SPSS automatically run Tukey-Kramer method (when Tukey-HSD is selected) when sample sizes are unequal?",pvopu,PinusPondo,1299045746.0,https://www.reddit.com/r/statistics/comments/pvopu/in_spss_when_running_post_hoc_tests_following/,1.0,1.0,,en
1109035,2012-02-19 00:25:58,statistics,Basic (perhaps metaphysical) question about histograms and differences in the way they are plotted by various R packages,pvptw,kiwipete,1171323108.0,https://www.reddit.com/r/statistics/comments/pvptw/basic_perhaps_metaphysical_question_about/,2.0,4.0,"My interest in statistics is mainly from an applied perspective. I haven't generally given a lot of thought to histograms other than to use them to eyeball distributions.

Recently, I was plotting a dataset wherein the lowest value is zero. When I plot the dataset using ggplot2, the zero values show up to the left of zero on the X axis. Plotting the same histogram using hist() puts those values to the right of the x axis. And finally histogram() (from the lattice package) does yet a different thing.

If I standardize the binwidths, hist() and histogram() return a similar plot, but ggplot2 still differs.

Here is an example in R, [and the rendered examples](http://imgur.com/a/OfHab):

    # Load ggplot2 and lattice libraries
    library(ggplot2)
    library(lattice)
    
    # A bogus dataset
    foo &lt;- c(rep(0, 5), rep(.5, 5), rep(1, 5))
    
    # plot using ggplot2, default binwidths
    ggplot() + geom_histogram(aes(foo))

    # plot using hist(), default binwidths
    hist(foo)
    
    # plot using lattice, default binwidths
    histogram(foo)
    
    
    # plot using ggplot2, explicit binwidths
    ggplot() + geom_histogram(aes(foo), binwidth = .1)
    
    # plot using hist(), explicit binwidths
    hist(foo, breaks = 10)
    
    # plot using lattice, explicit binwidths
    histogram(foo, breaks = 10)


I believe hist() must have some ruleset to include zero in the first bin when there are no values below zero (bin1 &gt;= 0). ggplot2 seems to consistently include only values greater than zero in the first bin (bin1 &gt; 1). Finally, I'm not exactly sure what lattice is doing when you let it pick its own bin widths.


The ggplot2 appears to have the most consistent handling of bins by not making an exception for zero. However, it offends my visual sensibilities. The histogram makes me think there are values below zero, when there are not.

Is there a right answer to this?",en
1109036,2012-02-19 02:17:59,AskStatistics,Need help creating a weighted value,pvuav,vellyr,1316144731.0,https://www.reddit.com/r/AskStatistics/comments/pvuav/need_help_creating_a_weighted_value/,1.0,0.0,"So I've recently begun collecting cologne, and there's this great site called basenotes.com that offers user reviews of pretty much every cologne on the market, so I can tell which ones are worth the money before I buy. They allow you to rate the cologne either positive, neutral or negative. 

What I want to do is create a weighted value that takes ratio of positive to negative reviews into account, but corrects for small sample sizes. For example, there are some with 30 positives to 1 negative, but only 40 reviews total. I would like a value that would allow me to compare that in a meaningful way to say, a cologne with a 5.0 ratio and 350 reviews.",en
1109037,2012-02-19 16:38:26,statistics,“False-positive psychology”,pwfws,Professor_IR,1310512226.0,https://www.reddit.com/r/statistics/comments/pwfws/falsepositive_psychology/,11.0,1.0,,en
1109038,2012-02-19 16:42:16,statistics,Standardized writing styles and standardized graphing styles,pwg03,Professor_IR,1310512226.0,https://www.reddit.com/r/statistics/comments/pwg03/standardized_writing_styles_and_standardized/,1.0,0.0,,en
1109039,2012-02-19 21:00:45,statistics,Who doesn't know,pwojj,[deleted],,https://www.reddit.com/r/statistics/comments/pwojj/who_doesnt_know/,1.0,0.0,,en
1109040,2012-02-19 23:41:35,MachineLearning,"Machine Learning and the Brain - The Neuroscience of Reinforcement Learning, a video lecture.",pwve7,MasCapital,1290915566.0,https://www.reddit.com/r/MachineLearning/comments/pwve7/machine_learning_and_the_brain_the_neuroscience/,25.0,6.0,,en
1109041,2012-02-20 11:19:08,statistics,"TheoryOfReddit user ScrapeAndAnalyze wrote a script to get some statistis about the commenting distribution of frontpage posters vs. commenters, for example: /r/starcraft users tend to post only in that sub, etc.",pxlyr,lebigz,1258900628.0,https://www.reddit.com/r/statistics/comments/pxlyr/theoryofreddit_user_scrapeandanalyze_wrote_a/,24.0,0.0,,en
1109042,2012-02-20 13:28:45,statistics,Need your opinion on open source stat. programs,pxof7,Spunsh,1305650066.0,https://www.reddit.com/r/statistics/comments/pxof7/need_your_opinion_on_open_source_stat_programs/,1.0,5.0,"Hi there,
I'm currently looking for a new open source stat. software (primarily to perform some regression analysis) that is easy to use (i.e. where no complex ""programing"" is requiered). Does anybody have a recommendation/experiences in this field?
Thx in advance
Spunsh",en
1109043,2012-02-20 17:03:45,statistics,Upcoming Statistics.com courses on the use of R,pxtlq,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/pxtlq/upcoming_statisticscom_courses_on_the_use_of_r/,4.0,0.0,,en
1109044,2012-02-20 17:03:56,MachineLearning,Upcoming Statistics.com courses on the use of R,pxtm0,[deleted],,https://www.reddit.com/r/MachineLearning/comments/pxtm0/upcoming_statisticscom_courses_on_the_use_of_r/,0.0,0.0,,en
1109045,2012-02-20 17:04:02,rstats,Upcoming Statistics.com courses on the use of R,pxtm3,talgalili,1271226645.0,https://www.reddit.com/r/rstats/comments/pxtm3/upcoming_statisticscom_courses_on_the_use_of_r/,6.0,1.0,,en
1109046,2012-02-20 22:57:02,MachineLearning,Writing de-anonymizer achieves 80% accuracy with 50% recall on 2.4m posts from 100k blogs,pya0m,vebyast,1288302570.0,https://www.reddit.com/r/MachineLearning/comments/pya0m/writing_deanonymizer_achieves_80_accuracy_with_50/,48.0,14.0,,en
1109047,2012-02-20 23:15:51,statistics,Any advice for an undergraduate on internships?,pyaxo,Hawki2013,1305044257.0,https://www.reddit.com/r/statistics/comments/pyaxo/any_advice_for_an_undergraduate_on_internships/,6.0,4.0,I'm an undergraduate junior looking at several potential internships for the upcoming summer. My two top picks right now are Mayo Clinic for biostatistics or interning in data management for a promising startup. Would either of these really help a resume for either grad school (Statistics or Biostatistics) or finding a job?,en
1109048,2012-02-21 00:52:46,statistics,"I have no experience in data modeling, but I want to build a customer churn model for work. How do I get started?",pyfp8,Nicos111,1242848562.0,https://www.reddit.com/r/statistics/comments/pyfp8/i_have_no_experience_in_data_modeling_but_i_want/,1.0,2.0,"I'm a very recent college graduate working in loyalty marketing at a major US retailer. My job is not technical in nature, but I see a major opportunity. We have tens of millions of customers with loyalty profiles (shopping cadence, basket size and about 1,000 other metrics). I'd like to use this data to create a model for customer defection. With that model, I would like to flag certain behavior patterns in customers and allocate additional resources to these high-risk customers. 

I've taken some basic stats classes, but I don't know where to start an undertaking of this scope. I want to use this project as a way to teach myself the fundamentals of data modeling. I just need some advice as to where I should begin (books, textbooks, videos, etc). Anyone have advice?  ",en
1109049,2012-02-21 01:23:26,statistics,"Is the number of words in a book considered ""random""? Is the number of instances of a single word in a book ""random""?",pyh6j,ellimist,1230497140.0,https://www.reddit.com/r/statistics/comments/pyh6j/is_the_number_of_words_in_a_book_considered/,0.0,1.0,"I'm having a debate with a friend, and I say no to both questions, as there seem to be initial conditions, and it seems more chaotic than random.

e.g. there are 71 instances of the word ""house"" in a book; is that a random number?

Can anyone help me out here?

Thanks!",en
1109050,2012-02-21 03:18:51,rstats,Grouping Dummy Variables ,pymh9,Iamthelolrus,1251613077.0,https://www.reddit.com/r/rstats/comments/pymh9/grouping_dummy_variables/,3.0,1.0,"I have panel data that varies by state. 

I have been using the factor command to generate state dummies but I expect that my continuous independent variable will have a different impact in different parts of the country. I'd like to lump the states into 5 or 6 broader regions and interact these regions with my continuous variable. 

Thoughts?
",en
1109051,2012-02-21 05:08:24,statistics,What am I doing wrong?,pyrmn,ano1114490,1315259159.0,https://www.reddit.com/r/statistics/comments/pyrmn/what_am_i_doing_wrong/,2.0,18.0,"So I'm trying to do a back-of-the-envelope calculation on how likely it is that I get a certain sophisticated computer virus. I started with an estimated number of hackers (10,000 hackers globally), and then multiplied that by the chance that they fit a certain characteristic (e.g., are out to get some financial information). 

After my calculations, I realized that I got a number above 1. What am I doing wrong?",en
1109052,2012-02-21 13:38:07,statistics,Need help: modeling interactions in logistic regression,pz7ju,nowthatsweird,1329823214.0,https://www.reddit.com/r/statistics/comments/pz7ju/need_help_modeling_interactions_in_logistic/,7.0,2.0,"Hi r/statistics,
I would really appreciate it if someone here could give me any advice on how to model interaction effects in logit models. I'm of course aware of the multiplicative option, but it seems that this is not the optimal way to go if I need to compare coeficients.

From what I can gather, coefficients are best compared using marginals, meaning that I use postestimation commands to get them. Afterwords, I compute the significance of several linear combinations of said marginal effects, using the interaction patterns I want ot observe. I do this using Stata by first entering the margins command, and then using lincom, subtracting the effects by setting the covariates of interest to 1 or 0 (all dummy vars). So, if I want to model interactions, I would calculate a pattern like 0/1/1 - 0/0/1.

So my first question is: is this procedure correct?

Second question: being that I want to compare different patterns, which are in some way interaction effects, do I need to enter multiplicative interaction terms in my logit model in the first place, and then run postestimation based on these results? Or can I leave them out? Thing is, there is quite a discrepancy regarding standard errors between these to proceedings...

I hope this isn't all too confusing, there might be some unfortunate wording in there, since English is not my native language...

Anyway, I hope someone here can point me into the right direction!",en
1109053,2012-02-21 13:51:02,artificial,"AGI Summer School lectures from China, featuring Goertzel,de Garis, Pei Wang, Joscha Bach, and more. ",pz7sm,moscheles,1316266003.0,https://www.reddit.com/r/artificial/comments/pz7sm/agi_summer_school_lectures_from_china_featuring/,6.0,0.0,,en
1109054,2012-02-21 18:12:05,statistics,"Hi, new to r/stats and I'm highly motivated into data gathering and analysis (drawing conclusions), but I'm fairly a newbie.",pzfuq,jinnyjuice,1288922715.0,https://www.reddit.com/r/statistics/comments/pzfuq/hi_new_to_rstats_and_im_highly_motivated_into/,2.0,3.0,"[Here is my attempt at determining ""athleticism level by countries.""](https://docs.google.com/open?id=0ByMpA5SsDIDmOWYxMjU4NjktZThiNC00MGFmLWFkYWItYWRlNDlkMGU2YzIw) -- you should *probably* download the original.

I already know that chart is pretty bad and it doesn't tell you anything about athleticism--it almost seems racist. Anyway, with its apparent flaws, I found that countries with high GDP per-capita are athletic.

What other statistics should/could I have included to make this a bit more of a reliable data?",en
1109055,2012-02-21 21:21:20,statistics,Opinions on simple main effects from mixed design ANOVA..?,pzosi,brainguy,1318624997.0,https://www.reddit.com/r/statistics/comments/pzosi/opinions_on_simple_main_effects_from_mixed_design/,1.0,7.0,"Hey [/r/statistics](/r/statistics), I was wondering what you guys think about simple main effects since they were recently introduced to me and I find it odd (and don't quite understand why) SPSS won't calculate them for you (unless you edit the syntax). 

Do you guys like them? What are some of the caveats or criticisms? I don't know that much about statistics (I'm a neuroscientist) but they seem like a pretty good way to examine where your significant differences lie when you have a very large ANOVA like I ran on my recent data; 2 between subject's variables with 2 levels each and one within subjects variable with 16 levels.

To be clear I am talking about simple main effects as described in [Howell's Psych Stats text book](http://www.amazon.com/Statistical-Methods-Psychology-CD-ROM-Howell/dp/053437770X), which if you don't have access to the book is a procedure where you calculate a pooled error term and then calculate F scores with that error term and the mean square error for each specific comparison. Hope that description made sense. I'm interested to hear your opinion!
",en
1109056,2012-02-21 21:55:27,analytics,Not breaking your Google Analytics (like a pro) | PagerDuty Blog,pzqje,devopsninja,1328663169.0,https://www.reddit.com/r/analytics/comments/pzqje/not_breaking_your_google_analytics_like_a_pro/,1.0,0.0,,en
1109057,2012-02-21 22:51:35,statistics,Interpreting the confidence interval of a  periodogram,pztge,Damark81,1274898720.0,https://www.reddit.com/r/statistics/comments/pztge/interpreting_the_confidence_interval_of_a/,1.0,0.0,"I am running R's plot.spec() to produce a periodogram. How can I word the interpretation of the confidence interval (ci) as it is drawn in the graph?
My understanding of this ci is that there is 95% chance that the hidden period will be within the confidence interval? 
if anyone has a beginner literature about this that could point me to, I would greatly appreciate it. ",en
1109058,2012-02-22 04:46:24,statistics,Choosing a number,q0bce,ponavis,1286155872.0,https://www.reddit.com/r/statistics/comments/q0bce/choosing_a_number/,1.0,0.0,,en
1109059,2012-02-22 04:56:53,MachineLearning,Can-we-fit-model-when-normalized-data-has-NaNs,q0buv,harit66,1277617617.0,https://www.reddit.com/r/MachineLearning/comments/q0buv/canwefitmodelwhennormalizeddatahasnans/,2.0,1.0,,en
1109060,2012-02-22 05:29:12,statistics,Confused biologist not sure the best way to represent her data. Is this box plot appropriate?,q0dgh,essentialparadoxes,1307764605.0,https://www.reddit.com/r/statistics/comments/q0dgh/confused_biologist_not_sure_the_best_way_to/,9.0,5.0,"If this is the wrong subreddit, please direct me to the right one!

Okay, so my PI loves notched box plots and wants them in our paper, but I'm not sure it's the most appropriate way to represent the data, given how widely the number vary and the small sample size. Also, we had been using whiskers of 1.5 SD in all the graphs for consistency, but the mean +/- 1.5 SD extends beyond the actual range of values, which my other collaborator doesn't like. None of us know much about statistics though. What are your thoughts?

This is o[ne particular data](http://tinypic.com/r/5wmerq/5) set which comes out looking weird in  box plots (I did one separate one for each time point), but all our data has similar sample size.",en
1109061,2012-02-22 17:04:32,statistics,"Help with associations with count data, matching images to feelings. Loglinear Model?",q0yfb,[deleted],,https://www.reddit.com/r/statistics/comments/q0yfb/help_with_associations_with_count_data_matching/,3.0,1.0,"I have a question, if I have survey data on people who were given a stack of images then asked to write the number of the image next to the emotion or feeling they feel it represents bests, which would then kind of look like this 
http://i.imgur.com/80Ics.png 
what would be the best approach to identify the image that is most strongly associated with each emotion, with a preference for images that are strongly associated with only one emotion.

Thanks for the help.",en
1109062,2012-02-22 19:00:48,statistics,Can of Clouds | Controversial Questions &amp; Statistics,q13nk,joerocca,1329929944.0,https://www.reddit.com/r/statistics/comments/q13nk/can_of_clouds_controversial_questions_statistics/,1.0,0.0,,en
1109063,2012-02-22 19:03:11,statistics,Help with Satistics HW! Probability a Casino will be up. ,q13ru,baydestrian510,1329929418.0,https://www.reddit.com/r/statistics/comments/q13ru/help_with_satistics_hw_probability_a_casino_will/,0.0,1.0,,en
1109064,2012-02-22 20:00:52,statistics,Frustrated: How can I - concretely - test if two (or more) data sets are independent. ,q16ii,Ayakalam,1310746049.0,https://www.reddit.com/r/statistics/comments/q16ii/frustrated_how_can_i_concretely_test_if_two_or/,9.0,25.0,"
Hi all, 

This is my first post to r/statistics. Coming from a EE background, we always hear about this signal being independent of this signal. Algorithms abound that say ""We can do this because signal x1 and signal x2 are independent"".

I have researched this topic but keep getting hit over the head with it. Everything I have read about independence gives me the text book definition but no way of really utilizing it. 

I know that for two random variables to be independent, the product of their marginal PDFs should equal the joint PDF. p(x,y) = p(x)*p(y). I see this everywhere I go, in every wiki, book, and link I have seen. Thats great. Thats fantastic. But frankly I dont see how it helps me 'on the ground'.

So, my questions are as follows: 

1) I have two data vectors. a[n] and b[n]. Each is length N. Lets say N = 1000. Lets say that a[n] measured temperature every day. And lets say that b[n] measured, oh, I dont know, the number of newborns born at some hospital for each day. I know intuitively those two are independent... I have those two vectors... _what do I type into my MATLAB to show they are independent?_ My fingers are on the keyboard with ML open ready to go. 


2) Again I have two data vectors, a[n] and b[n]. But lets say that this time, the two vectors are heavily dependent. Perhaps a[n] measures the temperature everyday, and b[n] (help me out here) might be umm, the temperature squared? I hesitate to use humidity because I want b[n] to be uncorrelated to a[n] but still dependent. If you can think of a better example by all means use it, but the over arching question is, again, given those two vectors, _what do I do to the data here on my computer in code, to show their degree of dependence_.

Thanks in advance everyone, and sorry for my frustration, I am trying to really grasp this idea. Thanks. ",en
1109065,2012-02-22 20:34:10,datascience,Designing Easy to Understand Dashboards,q185i,CANWorkSmart,1329935518.0,https://www.reddit.com/r/datascience/comments/q185i/designing_easy_to_understand_dashboards/,0.0,1.0,,en
1109066,2012-02-22 22:17:40,statistics,Help me design a prediction tool in R,q1de0,firstcity_thirdcoast,1308864047.0,https://www.reddit.com/r/statistics/comments/q1de0/help_me_design_a_prediction_tool_in_r/,7.0,29.0,"I work at a company that collects *enormous* amounts of sales data. Each transaction includes exact location, date/time, dollar amounts for cost and revenue, margins, customer names, and a host of other information.

I've been tasked with figuring out a way to predict the volume of sales in any given regional market in a particular future time period based on: 1) our historical data, and 2) macroeconomic trends (industrial production figures, employment, inventories, etc). I have all the data I need for both those, so now it's time to design how this will work in R.

I've done some cursory research into a lot of different prediction packages, but I need some basic guidelines as to how I should begin. Is it multiple regression? Machine learning? ARIMA? ANOVA? Holt-Winters? Is this a time series? I'm self-taught in R and have no problem learning whatever I need to complete the task, but I need a jumping-off point. Thank you in advance!",en
1109067,2012-02-22 23:21:39,statistics,Assessing the fitness of a probabilistic system.,q1gps,equalx,1291409916.0,https://www.reddit.com/r/statistics/comments/q1gps/assessing_the_fitness_of_a_probabilistic_system/,1.0,4.0,"I'm only recently beginning to learn some interesting statistical methods, but I've come across a situation where I know statistics would be helpful, yet I can't come up with good keywords to even google for an approach to make.

I'm attempting to sample a system for fitness.  The system contains binary events (outcome 1 or 0), and a probability function based on two associated values.  Because of this, we know the outcome of an event, and the probability of that outcome occurring.

The two values associated with the event will change the predicted probability.  They may be different for any given event.  It might be safe to assume events are independent, but I suspect that a rigorous proof of independence would fail.  Because these values can change, we'll have different probabilities floating around for each event, so we can't say that there's a constant theta like in a binomial distribution.  Importantly, the system I'd like to test assumes an estimate of the values for each event, but also believes that there is an actual realization of the values.  The difference between the predictions coming from these sets of values should account for an error in the visible probabilities, and as the values become more accurate over time, the error should be reduced.

So what I'm looking for (in a sort of TL:DR) is a way to assess the accuracy of many event predictions, where the predictions are variable and the divergence I'm looking to detect is the difference between the prediction accuracy in a series of realized events, and the actual prediction accuracy.  Is there something out there to do that?  I've looked a little at Bayesian approaches, but haven't come up with a solution yet.

If this is horrible and convoluted, tell me and I'll try to explain another way.",en
1109068,2012-02-23 02:14:50,statistics,Help on replications for RCBD,q1pep,[deleted],,https://www.reddit.com/r/statistics/comments/q1pep/help_on_replications_for_rcbd/,1.0,0.0,"I need to replicate my design for a random complete block design. Let's say I have 4 nuisance factors and I need to replicate the treatments more than just one time for a given block. Is there a more general model that allows each treatment(3 treatments, if this is necessary) to be tested more than once for each blocking factor. Maybe,could I have say 8 blocks and blocks 1 and 2 are the same blocking factor level, blocks 3 and 4 are the same factor level, etc..Wondering what sort of model I can use, due to expenses I can't afford more than 4 blocking factors.",en
1109069,2012-02-23 03:22:50,MachineLearning,Naive Bayes and the Test/Training Set,q1ska,hntd,1301600241.0,https://www.reddit.com/r/MachineLearning/comments/q1ska/naive_bayes_and_the_testtraining_set/,5.0,10.0,"Hi, I'm using a naive bayes for some basic text classification of some twitter feeds and I had a question.

If I have a training set and a test set each of 1000 randomly instances from a group of 70000. Would there be a problem if I had say instance X in the training set and then the same instance (X) appeared in the test set?

I'm curious if this would cause significant skewing or just general issues such that I want to go and ensure that there is 0 overlap between the 2 sets.

If there is a problem could someone explain to me what it might be? My reasoning behind why I think it might not be a problem is that I think the naive bayes is very simplistic in it's classification approach. Even if it's seen this exact instance in the past it's still going to calculate it's probabilities and classify based on those probabilities and not the fact it's seen this instance in the past, which it doesn't even remember anyways.

So if someone could help me clear this up I'd really appreciate it. Thanks for any insight.",en
1109070,2012-02-23 07:16:59,MachineLearning,Unpacking the Elo rating system,q23rk,diffeomacx,,https://www.reddit.com/r/MachineLearning/comments/q23rk/unpacking_the_elo_rating_system/,4.0,0.0,,en
1109071,2012-02-23 07:28:35,data,"Little startup Infochimps has a platform for big data

Read more: http://news.cnet.com/8301-32973_3-57383313-296/little-startup-infochimps-has-a-platform-for-big-data/#ixzz1nBDVTHXr",q248t,[deleted],,https://www.reddit.com/r/data/comments/q248t/little_startup_infochimps_has_a_platform_for_big/,1.0,0.0,,en
1109072,2012-02-23 07:33:57,artificial,Invertuality - An interesting piece of AI,q24gf,[deleted],,https://www.reddit.com/r/artificial/comments/q24gf/invertuality_an_interesting_piece_of_ai/,4.0,15.0,,en
1109073,2012-02-23 15:44:58,statistics,"Can I reject the null if my bootstrapped mean's confidence intervals lie outside zero, even when the original distribution was not normal?",q2gsb,quaternion,1252017693.0,https://www.reddit.com/r/statistics/comments/q2gsb/can_i_reject_the_null_if_my_bootstrapped_means/,7.0,10.0,"If I have two non-normal distributions and I want to test if they are different from one another, can I sample with replacement from the original dataset many times in order to get a confidence interval for the difference, and see whether it includes zero?

I think permutation testing, not bootstrapping, is the technique I need to use here, since the use of confidence intervals will be assuming a normal distribution.  Permutation testing obviously does not involve this same assumption.  amirite?
",en
1109074,2012-02-23 17:11:02,MachineLearning,Computers are very good at the game of Go,q2jtu,farful,1249628056.0,https://www.reddit.com/r/MachineLearning/comments/q2jtu/computers_are_very_good_at_the_game_of_go/,34.0,19.0,,en
1109075,2012-02-23 19:22:48,artificial,tutorial for game playing AI?,q2puz,[deleted],,https://www.reddit.com/r/artificial/comments/q2puz/tutorial_for_game_playing_ai/,7.0,7.0,"I want to make an AI for a game like gomoku. My worry is that my code will run too slow and the AI will take ages to move.

Is there a tutorial that guides you through it to make a fast program?",en
1109076,2012-02-23 21:17:10,statistics,Help with Uniform Binomial Distribution Problem.,q2vqa,[deleted],,https://www.reddit.com/r/statistics/comments/q2vqa/help_with_uniform_binomial_distribution_problem/,1.0,6.0,"[Link to Problem
](http://i.imgur.com/rjmC2.png)

disclaimer: this is for a study guide for an exam tomorrow. I'm not trying to get you to do my homework, I actually need to figure out why I can't get it right...


That's the problem. My thought process is going like this (sorry if its messy):

Need to use correction factor. 

part a: Correction factor of +.5. Use (X-mean)/stnddev to find the z factor. 

(20.5-15)/3.5 = 1.57. 1.57 = P(.4418 for X) It wants more than 20, so you do (.5 - .4418) = .0582% is greater than 20 minutes. 

Is that logic correct? It doesn't seem to like that answer. 

Cheers",en
1109077,2012-02-23 23:40:39,computervision,Spoonfeeding request: Compression Sensing with L1 Minimization,q330k,mcandre,1158766748.0,https://www.reddit.com/r/computervision/comments/q330k/spoonfeeding_request_compression_sensing_with_l1/,5.0,1.0,"I read an interesting article in [Wired](http://www.wired.com/magazine/2010/02/ff_algorithm/) describing a process for recovering high quality data from low quality samples using a mathematical technique called l1 minimization. I would like to use this technique to create command line programs in Haskell for recovering pictorial and audio files. There is a MATLAB library, [L1-MAGIC](http://users.ece.gatech.edu/~justin/l1magic/), that provides scripts implementing l1 minimization, but the code is disorganized. One reason I would like to port this from MATLAB to Haskell is to take advantage of Haskell's automatic parallelization and CUDA capabilities. I'd like to create an l1 API that is both practical and generic: you call a `recover` function over any signal and recover it, for images, audio, video, whathaveyou.

I'm not highly versed in linear algebra, otherwise I would take the research papers and implement l1 from scratch. Could someone describe an algorithm for recovering a high quality picture from a low quality sample picture using l1 minimization? The L1-MAGIC library is too disorganized and impractical (imperative, global variables, no concrete examples) for me to sort out.",en
1109078,2012-02-24 00:29:34,MachineLearning,Weakest Link Pruning in a Decision Tree: definition of an internal node,q35id,dogstongueupmynose,1327882068.0,https://www.reddit.com/r/MachineLearning/comments/q35id/weakest_link_pruning_in_a_decision_tree/,3.0,5.0,"I'm implementing Decision Trees in python, eventually to become Gradient Boosted Decision Trees.  
My question is about pruning a decision tree using Weakest Link Pruning.

In Elements of Statistical Learning (ESL) p 308 (pdf p 326), it says: ""we successively collapse the internal node that produces the smallest per-node increase in [error].""  My understanding is that an internal node is *any* non-terminal node.

Then, I open Programming Collective Intelligence (p 155) and while the text therein makes it sound like we're looking at *any* non-terminal node, the function they define ('prune') only tests entropy reduction on nodes that are *direct parents of terminal nodes*.

Did I understand ESL correctly, and I should be looking at *every* non-terminal node, not just those that are direct parents of terminal nodes?
",en
1109079,2012-02-24 08:16:18,statistics,N. Koreans' life expectancy ranks 151st in world,q3qt8,koreastory,1305868915.0,https://www.reddit.com/r/statistics/comments/q3qt8/n_koreans_life_expectancy_ranks_151st_in_world/,1.0,0.0,,en
1109080,2012-02-24 09:33:25,statistics,Maximum Likelihood Confusion...,q3t6p,Ayakalam,1310746049.0,https://www.reddit.com/r/statistics/comments/q3t6p/maximum_likelihood_confusion/,0.0,1.0,"
Hi all!

So I have been studying up on Maximum Likelihood Estimation (MLE), and I have consulted a number of sources. 

Funnily enough I actually understand the process of estimating the parameters for MLE, but I actually do not understand (or buy) how MLE 'starts'.

Here is where my confusion is: 

I do not know if its something deeper or just general slop, but I cannot seem to get consensus among the many sources that I read / listen to on what _exactly_ the x_i's represent from a vector of observations. 

In other words - MLE is maximizing a 'likelihood function', that is made up of a product of PDFs. Ok...I can buy that. How that is maximized I completely understand, but I am not sure I understand WHY this likelihood function is a product of PDFs, because I dont understand exactly WHOS pdfs we are multiplying all together. 

In most examples I see, they start with a vector of N (iid) observations. x_1, x_2, x_3 ... x_N. 

After that, this is where the confusing begins: Those are possible outcomes of ONE RANDOM VARIABLE ('big' X as they say), which to me, means there is ONLY ONE PDF out there whos parameters I am trying to estimate. One PDF. Not N PDFs. 

In contrast to 'big X', representing the name of the random variable, I have here a list of N 'small x' _observations_. 

To my knowledge, observations do not have PDFs - random variables like 'big X' from which 'small x' observations are sired have a PDF, but NOT the observations themselves. 

So as far as I am concerned, there is only ONE random variable, ONE pdf. So how is it that I am now beginning by multiplying N pdfs together?

So which one is it?... Am I estimating 1 PDF? Or am I estimating N PDFs that all magically happen to have the same parameters??

--------------------------------------

In other lectures, I have seen each 'small x' (x_1, x_2, etc) be referring to as EACH being a random variable. Well, I suppose this might make more sense...but technically to me they are still observations - numbers - ....

So, I am very confused. Different lectures have different names for this. Would someone please shed some light on this? I am looking for consistency and understanding of how to view those x's - are they random variables in their own right? Or are they observations from ONE random variable? ... very confused.

Thanks..",en
1109081,2012-02-24 13:06:08,statistics,A redditor collects data from comments and runs a statistical analysis of desired sex frequency in r/sex. (xpost),q3xp6,paulginz,1261162522.0,https://www.reddit.com/r/statistics/comments/q3xp6/a_redditor_collects_data_from_comments_and_runs_a/,39.0,8.0,,en
1109082,2012-02-24 16:16:16,data,Data for the public good,q422w,d1235,1330091512.0,https://www.reddit.com/r/data/comments/q422w/data_for_the_public_good/,1.0,0.0,,en
1109083,2012-02-24 18:11:15,statistics,Any Biostatisticians here that can answer some questions? Any general advice about grad school?,q46fs,Distance_Runner,1293552667.0,https://www.reddit.com/r/statistics/comments/q46fs/any_biostatisticians_here_that_can_answer_some/,1.0,1.0,"I'm in my junior year as a Biology undergraduate and I've developed a strong interest in Statistics... In the fall I'll be applying to various graduate schools in Biostatistics for an MS, though I may possibly pursue a PhD in the future... Here are some questions I have for the career biostatisticians:

- What is your work week like? How many hours do you work?
- What is each day like? For example, what do you spend most of your time doing?
- I'd like to go work get into Clinical trials after I graduate... For this, would you advise PhD, or will a MS be sufficient?
- Are there rankings published for Biostat programs (because I can't seem to find any)?... School's I'm looking at include Columbia, Johns Hopkins, Michigan, Colorado (at Denver), UCLA, Pittsburgh, and Florida State.
- I've seen that most schools require at least 2 semesters of Calculus and linear algebra (all of which I will have completed prior to applying next year)... Though I don't have time to take it before I start applying next Fall, will it still be beneficial (and how much so) will it be for me to take Calc 3 (multivariable) the summer before I start grad school?
- Any other general advice you can give me?

Here are the relevant classes I've taken: Elementary Statistics (credited from AP class in High School), Biostatistics (A), Calculus I (A), Calculus II (currently taking), and Computer Application of Statistics (currently taking)... Of course, I've also taken plenty of Biology courses.... In my computer Application class, I'm learning SPSS, SAS and R....I'll be taking Linear Algebra this coming Fall Semester.... With that said, I have a GPA of 3.5. I have yet to take the GRE, but I've taken a few practice test and I actually got a perfect school on the quantitative section on 2 of the 3 practice tests.... With all that, what type of school should I be applying to?",en
1109084,2012-02-24 18:12:26,statistics,"Which two-letter initials are most common? (J.S., J.B., J.M., and so forth). How popular are YOUR initials?",q46hk,RickWicklin,1327333524.0,https://www.reddit.com/r/statistics/comments/q46hk/which_twoletter_initials_are_most_common_js_jb_jm/,8.0,4.0,,en
1109085,2012-02-24 18:47:34,statistics,"Think Complexity part five: Self-organized criticality, long tailed-distributions, and normal accident theory.",q483i,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/q483i/think_complexity_part_five_selforganized/,4.0,0.0,,en
1109086,2012-02-24 18:48:49,MachineLearning,"Think Complexity part five: Self-organized criticality, long tailed-distributions, and normal accident theory [xpost statistics]",q485p,AllenDowney,1300587223.0,https://www.reddit.com/r/MachineLearning/comments/q485p/think_complexity_part_five_selforganized/,9.0,0.0,,en
1109087,2012-02-24 19:13:23,statistics,Maximum Likelihood Confusion,q49a5,Ayakalam,1310746049.0,https://www.reddit.com/r/statistics/comments/q49a5/maximum_likelihood_confusion/,0.0,3.0,"
Hi all, 

So I am wrapping my head around Maximum Likelihood Estimation (MLE), and ironically, I understand everything about the algorithm, except the starting point. 

What confuses me is the following:

All the examples I have seen of course, start with some vector of *observations*, x[1], x[2], ... x[N]. The observations are iid. The objective is to estimate parameters (theta) of the PDF - **ONE PDF** - from which sired such observations. This part I get. We are allowed to estimate ONE PDF because X ('big X') is a random variable from which all those x's (small x's) are sired. 

As far as I know, big X's have PDFs. Small x's dont. 

Anyway, so a PDF is written like this: 

p(x[1], x[2], x[3]... x[n] | theta) = p(x[1]|theta) * p(x[2]|theta) * ... p(x[N]|theta).

Here all the sudden I now have **N pdfs**. This confuses me a lot:

1) Like I said before, as far as I know, _observations_ (like x[5]) dont have PDFs. Random variables do. So in this case the observations are 'small x', where as a random variable is 'big X'. We are trying to estimate the pdf of 'big X' - X. How is it that I now have an individual PDFfor each and every observation 'small x'?

Every lecture video/article seem to use contradictory terms among themselves of 'random variable' and 'observation' while simultaneously taking the effort to point out that they are not the same and shouldnt be confused - so which one is it??  

Thanks in advance! 

",en
1109088,2012-02-24 19:45:13,statistics,Manipulating Census Data,q4arb,jvonhalle,1220281814.0,https://www.reddit.com/r/statistics/comments/q4arb/manipulating_census_data/,1.0,6.0,"Hey R/Statistics,

I am trying to analyze demographic info, however, my standard skewness and kurtosis are above the range for normality.

Does anyone have any suggestions how to transform the census data to make it more normal? or is that unnecessary? 

I am trying to do correlation analyses, and I want to make sure I am doing this correctly.

Thanks!",en
1109089,2012-02-24 22:14:49,MachineLearning,"Graduate Summer School: Deep Learning, Feature Learning (July 9-27)",q4i11,blind_swordsman,1318716377.0,https://www.reddit.com/r/MachineLearning/comments/q4i11/graduate_summer_school_deep_learning_feature/,17.0,1.0,,en
1109090,2012-02-24 22:27:44,statistics,What would I need to do to get into a statistical analytics line of work?,q4in9,[deleted],,https://www.reddit.com/r/statistics/comments/q4in9/what_would_i_need_to_do_to_get_into_a_statistical/,3.0,8.0,"I have a a BA in Econ. I took a Stats 101 and an Econometrics class, but that was not my focus and barely remember anything. I've tried reading some Stats stuff online, but end up getting more confused when I get down into the details of how to do something. 

Any tips on where I should get started? Also, how strong should your math be for being half way decent in statistics? ",en
1109091,2012-02-25 05:41:59,statistics,"I'm sure the answer to this problem is trivial, but not for me. ",q51bt,hsfrey,1152763001.0,https://www.reddit.com/r/statistics/comments/q51bt/im_sure_the_answer_to_this_problem_is_trivial_but/,6.0,5.0,"Is it acceptable to post specific problems on r/statistics?

If so:

If you put 2 points at random on the line (0,1) what is the distribution of the distance between them?

My gut feeling would be that the average would be 1/3 and the std. dev. I don't know.

That's the question. Here's the context, if you're interested:

I wanted to get a rough idea of the correlation between 18 different kinds of measurements on a population of about 3600, and didn't want to do all the correlation calculations, so I came up with an ad hoc non-parametric method. For all I know, someone may have described it already.

I just figured the deciles for each criterion, and made a matrix of the averages over the population of the differences of the deciles. 

EG: if a member is in the 3d decile of criterion 1, and the 6th decile of criterion 2, the distance measure is 3. I averaged those up for each pair of criteria.

Almost all of the average distances between pairs of criteria are between 3 and 4. The extremes are 2.04 and 4.14.

With 18x17 pairs, what is the probability that distances that large could occur by chance?


",en
1109092,2012-02-25 07:55:27,statistics,Ohio University's Popularity Trending Downward =/ ,q565s,jc215910filmdude,1326388630.0,https://www.reddit.com/r/statistics/comments/q565s/ohio_universitys_popularity_trending_downward/,0.0,1.0,,en
1109093,2012-02-25 08:47:02,statistics,Personal statistics libraries - must-haves?,q57sq,Neurokeen,1307713928.0,https://www.reddit.com/r/statistics/comments/q57sq/personal_statistics_libraries_musthaves/,23.0,8.0,"I just borrowed a copy of Agresti's Categorical Data Analysis from a professor of mine while working on a consulting project, and was surprised by how well organized it is as a reference. It had me wondering: what general reference volumes could possibly be considered ""must-haves"" for people working in statistics? Opinions?

I understand this would vary between subfields to a degree, so if you know of a great source that might only be relevant for a subfield, go ahead and note it.",en
1109094,2012-02-25 19:40:31,MachineLearning,Penny Auction Scam Uncovered By PhD Students,q5m9m,DrNewton,1284858454.0,https://www.reddit.com/r/MachineLearning/comments/q5m9m/penny_auction_scam_uncovered_by_phd_students/,53.0,7.0,,en
1109095,2012-02-25 21:03:47,MachineLearning,Brainwave-controlled skateboard,q5pjc,H4L9000,1297796030.0,https://www.reddit.com/r/MachineLearning/comments/q5pjc/brainwavecontrolled_skateboard/,1.0,0.0,,en
1109096,2012-02-26 01:59:17,statistics,Lapsed engineer needs help calculating BMR. ,q61mu,kchoudhury,1322273781.0,https://www.reddit.com/r/statistics/comments/q61mu/lapsed_engineer_needs_help_calculating_bmr/,3.0,3.0,"One of my new years resolutions was to (re)learn statistics. Since learning is usually best driven by real world applications, I'm going to try to calculate my [basal metabolic rate](http://en.wikipedia.org/wiki/Basal_metabolic_rate) in a statistically correct fashion. 

(Google it, you say? The online calculators I've seen peg it at 1800 to 2600 for me, so I'm leery of trusting them...)

Let's start with the following (extremely) simple relationship: 

    BMR - C_in + C_out = (W_t-1 - W_t) * 3500 

where: 

    BMR = basal metabolic rate
    C_in = calories eaten
    C_out = calories expended
    W_t-1 = weight in pounds yesterday
    W_t = weight in pounds today

I am in the process of collecting statistics for C_in, C_out, W_t-1 and W_t. After two or three days of data, however, I've noticed that the calculated value of BMR jumps around a LOT. What is the statistically correct way of ""smoothing"" the calculated BMR numbers? ",en
1109097,2012-02-26 04:36:24,artificial,Why can't I find anything about the PARADISE chess AI? and other stuff?,q67qt,[deleted],,https://www.reddit.com/r/artificial/comments/q67qt/why_cant_i_find_anything_about_the_paradise_chess/,7.0,3.0,"In my AI book AIMA it mentioned a chess AI by David E. Wilkins which uses knowledge and reason instead of searching. I can't find any of his publications on it though.

It sounds really interesting but also do you have information about game playing AI that doesn't just do search?",en
1109098,2012-02-26 04:49:40,statistics,"I need a good SAS Reference-not ""The little SAS book""",q688t,BirthDeath,1295140816.0,https://www.reddit.com/r/statistics/comments/q688t/i_need_a_good_sas_referencenot_the_little_sas_book/,7.0,12.0,"I'm working on a project for someone that requires the use of SAS.  I'm primarily an R user, but I've used SAS in the past during an internship.

This project calls for a relatively complicated automation routine, and I am not that great with macros in SAS.  I have ""The Little SAS Book,"" but it is not very useful for more advanced questions.  I'm looking for a reference text or website that offers a good explanation of macros and some of SAS's more advanced data processing functionality.",en
1109099,2012-02-26 13:48:41,MachineLearning,"Ask ml: Probability of someone knowing an English word, given a small sample list of his/her known words ",q6mfz,9diov,1237475660.0,https://www.reddit.com/r/MachineLearning/comments/q6mfz/ask_ml_probability_of_someone_knowing_an_english/,3.0,4.0,"The other day after visiting this site: http://testyourvocab.com/, I got an idea of implementing an application where the input is a list of English words, similar to the site mentioned. A user can indicate the words he/she knows among the list and the application will calculate the probability that he/she knows ***any word specified***. 

For example, the input list has 100 words, John knows words in the list such as  basic words like, think, go, work, etc and harder words like sedulous, vibrissae, uxoricide, etc. From there, the app can estimate with probability 0.7 that he knows  the word ""parsimonious"" as well.

I only have basic understanding of machine learning after taking the online Stanford AI class (I did write a simple spam filter as a result). Thus, I need some pointers to know how I can solve the said problem.

Thank you for reading.
Cheers",en
1109100,2012-02-26 21:03:14,statistics,What kind of time series analysis is optimal for this data?,q6yar,MyNameCouldntBeAsLon,1311784142.0,https://www.reddit.com/r/statistics/comments/q6yar/what_kind_of_time_series_analysis_is_optimal_for/,2.0,0.0,"I am working with a database that covers 30 years for a number of countries (130). Each observation is a year for a particular country with a number of variables.



The problem is that the matrix is not complete: I do not have observations for all the countries, all the years. For instance, country 2 only has observations for 1982-1990, while country 3 has observations for 1972, 1973, 1980.



What kind of model should I use? I originally considered a Vector Autoregressive, because the nature of the variables indicate that behavior. But seeing how each vector (for each country) is 'incomplete' I am now unsure of the proper methodology.",en
1109101,2012-02-26 21:57:26,statistics,How do you determine a set of coefficients? (I think I'm missing something really basic here...),q70of,[deleted],,https://www.reddit.com/r/statistics/comments/q70of/how_do_you_determine_a_set_of_coefficients_i/,2.0,1.0,"Working my way through a stats book and one of the exercises is to look at a set of data and determine a set of coefficients that will contrast the 5 sets into 2. In an early part of the chapter it said that when determining a set of coefficients for a linear combination, they should all add up to zero. So something like Group 1 &amp; 2 = .5 each and Group 3, 4 &amp; 5 = -1/3 each. Is that right? Can I just give them any value? Is there a reason not to have them add up to zero? Sorry, I'm really new at all this stuff.",en
1109102,2012-02-26 22:47:21,MachineLearning,Vectorizing words to use a Machine Learning algorithm,q72wa,nadie854,1295106489.0,https://www.reddit.com/r/MachineLearning/comments/q72wa/vectorizing_words_to_use_a_machine_learning/,2.0,5.0,"I'm testing an idea to vectorize any secuence of symbols into fixed size vector in R^n using a hierarchical combination of echo state networks. The objective is to classify these sequences as vector (there is a lot of Machine Learning algorithms to use with fixed sized real vectors).

In particular, i'm testing this algorithm with english words, trying to classify them as nouns or adjectives. My dataset is here: http://www.ashley-bovan.co.uk/words/partsofspeech.html

Using a SVM to classify, i'm getting 9% of error, somebody please can point me to relevant papers or results to compare?

Thanks!",en
1109103,2012-02-26 23:28:38,artificial,Write up on the winning combat Ants AI,q74u4,[deleted],,https://www.reddit.com/r/artificial/comments/q74u4/write_up_on_the_winning_combat_ants_ai/,1.0,0.0,,en
1109104,2012-02-26 23:46:36,MachineLearning,Is there any research into algorithms for identifying provocative quotes in news articles?,q75od,sanity,1149365629.0,https://www.reddit.com/r/MachineLearning/comments/q75od/is_there_any_research_into_algorithms_for/,1.0,2.0,,en
1109105,2012-02-27 02:27:13,statistics,ELI5: penalized logistic regression models for probability,q7cvc,ZZ_Bay,,https://www.reddit.com/r/statistics/comments/q7cvc/eli5_penalized_logistic_regression_models_for/,0.0,3.0,"I came across this concept when reading this article. http://comp.social.gatech.edu/papers/cscw12.hierarchy.gilbert.pdf 
If anybody could explain it to me in a way that would at least allow me to BS understanding, that would be great. ",en
1109106,2012-02-27 10:21:02,statistics,"Stats test tomorrow - don't know how to do this: If A and B are two events where P(A) = .3, P(B) = .2, and P(A and B) = .1, find P(A or B), P(A|B) and P(B|A).",q7w4r,TBizzcuit,1270020303.0,https://www.reddit.com/r/statistics/comments/q7w4r/stats_test_tomorrow_dont_know_how_to_do_this_if_a/,0.0,3.0,,en
1109107,2012-02-27 15:11:14,statistics,can i use ANOVA to test dependencies between nominal variables and  interval scaled variables?,q81wi,beepup,1298111588.0,https://www.reddit.com/r/statistics/comments/q81wi/can_i_use_anova_to_test_dependencies_between/,5.0,17.0,"hey, statistics beginner here,

i have a set of nominal variables and a set of interval variables (rating 1-5). i know that anova can be used for nominal and metric variables. i dont know if i can interpret my rating scale as metric and just do the ANOVA like this. I'd just like to do that, and simply assume (it is not for scientific purposes or anything) equal distances between the ratings, just to explain something.
i know contingency analysis with loglin models can handle a couple of variables, but then those are for nominal scaled variables, and are not appropriate for my rating scale. i dont know any ""dependency testing methods"" particlular for this case, regression analysis seems to be for 2 sets of metric variables. any ideas, comments?

edit: Problem description:
50 probants rate productcharacteristics (1-7) in 4 different situations. I basically want to see if the preference of the productcharacteristic is dependant on the situation. now, each proband is giving his preference 4 times SAS is puking out some stuff, but im actually not sure of im actually doing something that makes sense. 

01 13

01 24

01 35

01 47

02 13

02 24

02 35

02 46

03 13

03 24

03 32

and so on

proband 1-2 situation 4 preference 5",en
1109108,2012-02-27 16:21:23,MachineLearning,Best way to get Reuters and stock data?,q83um,imissyourmusk,1317942820.0,https://www.reddit.com/r/MachineLearning/comments/q83um/best_way_to_get_reuters_and_stock_data/,13.0,14.0,I saw someone mention good tools for pulling down timely stock and news information but I can't seem to find the post.  Does anyone have any advice for this?,en
1109109,2012-02-27 19:26:13,statistics,Significance for pretest/posttest measures for an intervention group - with control available for seasonal changes.,q8ba7,janerikz,1328892851.0,https://www.reddit.com/r/statistics/comments/q8ba7/significance_for_pretestposttest_measures_for_an/,2.0,3.0,For a group questionnaire measures were made pre and post intervention. Same two measures were made for a control group to check for seasonal changes. I have seen that control have some (non-significant) changes in opposite direction as intervention group. How do I do significance testing on this?,en
1109110,2012-02-27 19:43:45,MachineLearning,MUSIC IR,q8c3m,imbenzene,1253279624.0,https://www.reddit.com/r/MachineLearning/comments/q8c3m/music_ir/,0.0,0.0,,en
1109111,2012-02-27 20:19:48,statistics,To jitter or not to jitter: That is the question,q8dtx,RickWicklin,1327333524.0,https://www.reddit.com/r/statistics/comments/q8dtx/to_jitter_or_not_to_jitter_that_is_the_question/,2.0,2.0,,en
1109112,2012-02-27 20:57:50,MachineLearning,Structured Approach to learning ML. How to?,q8fnb,sharmilasiwa,1330342476.0,https://www.reddit.com/r/MachineLearning/comments/q8fnb/structured_approach_to_learning_ml_how_to/,13.0,6.0,"Hi,
   I'm a CS undergrad, currently working.  I attended Prof. Andrew Ng's machine learning class and Prof. Thrun's Artificial Intelligence class. I am really interested in furthering my knowledge in machine learning.  Currently I'm going through Prof. Ng's advanced machine Learning class in Stanford SEE.  I have started working on trying out some algorithms on my own.  
Supervised Learning Spam classification http://www.minvolai.com/blog/2012/01/email-classification-supervised-learning/
Supervised Learning grid search http://www.minvolai.com/blog/2012/02/grid-search-for-email-classification/

I'm not sure how to proceed further from there.  So I'm appealing to your knowledge and experience to guide me.

How can I proceed from here in a structured way, to understand more advanced topics and algorithms?  What approach is best for each type of media (text, image, video, numerals) ? Is my direction correct?   Did you learn ML on your own?  If so what pitfalls did you face? Your guidance and support would mean so much to me.


Thank you.",en
1109113,2012-02-27 21:42:52,MachineLearning,Building Predictive Models in R Using the Caret Package [pdf],q8hsi,willis77,1188267725.0,https://www.reddit.com/r/MachineLearning/comments/q8hsi/building_predictive_models_in_r_using_the_caret/,3.0,1.0,,en
1109114,2012-02-27 22:32:47,MachineLearning,Help build a machine learning system to predict college basketball,q8kaw,danger_t,1249412021.0,https://www.reddit.com/r/MachineLearning/comments/q8kaw/help_build_a_machine_learning_system_to_predict/,19.0,4.0,,en
1109115,2012-02-28 00:36:29,MachineLearning,New tools bring Machine Learning to the masses,q8qq7,sbc1906,1241279011.0,https://www.reddit.com/r/MachineLearning/comments/q8qq7/new_tools_bring_machine_learning_to_the_masses/,5.0,2.0,,en
1109116,2012-02-28 01:48:49,statistics,Self-study for stats n00b in political science?,q8ufi,DarumaRed,1265395210.0,https://www.reddit.com/r/statistics/comments/q8ufi/selfstudy_for_stats_n00b_in_political_science/,6.0,4.0,"My political science graduate program is quant. heavy. Unfortunately, I've come in with much better qualitative skills than quantitative skills. The department offers a lot of statistical and econometric options, but so far the methods courses have left me feeling like I have a lot of methodological gaps. It's only the 1st year of my PhD, but I'd like to get a head start and develop some strong quant. skills with what's left of my very scant free time.

What self-study materials can r/statistics recommend for me? I've supplemented the core statistics class here with DeVeaux's 'Intro Stats' book, but that stops before any real down dirty stats stuff. Any good self-study stuff for R and Stata too? Heck, I'll even take recs for using LaTeX like a stats ninja.

Thanks in advance.",en
1109117,2012-02-28 04:26:43,MachineLearning,Google's cloud-based machine learning tools,q928i,incomodo_a_la_gente,1326686227.0,https://www.reddit.com/r/MachineLearning/comments/q928i/googles_cloudbased_machine_learning_tools/,13.0,1.0,,en
1109118,2012-02-28 13:17:24,statistics,Professor David Spiegelhalter of Cambridge University wants to know about your coincidences!,q9ki3,0B0A1113001A,1282244481.0,https://www.reddit.com/r/statistics/comments/q9ki3/professor_david_spiegelhalter_of_cambridge/,15.0,1.0,,en
1109119,2012-02-28 14:01:16,computervision,Ask r/CV: best cameras for visual people-tracking system in an office environment?,q9ldi,astebbin,,https://www.reddit.com/r/computervision/comments/q9ldi/ask_rcv_best_cameras_for_visual_peopletracking/,6.0,5.0,"Background: I'm currently attempting to design a multi-camera people tracking system for an office-style environment (halls, classrooms, common rooms, un-surveyed offices/stairwells/bathrooms, etc). Having taken several computer vision courses and familiarized myself with the people-tracking literature, I've got the software side of this project pretty well mapped-out. However, I'm still hazy on the other side of the coin: hardware.

My hardware needs are straightforward: Linux-compatible, at least VGA resolution (preferably SVGA), not too laggy in low-light conditions, capturing at a minimum of 15 frames per second. For the specific application I have in mind, I'd be looking at acquiring 20-30 units of whichever camera model best meets these needs. I've basically got an infinite supply of new-ish IBM Thinkpads to which these cameras will be connected, ideally in clusters of 3 or 4, which will feed all video streams back to a server cluster for processing. I've used a variety of Logitech, Microsoft, and no-name brand webcams for computer vision applications in the past, but with recent apparent advances in camera quality, I'm no longer sure which manufacturer might be best. I've only used USB cameras in the past, as they're easy to interface with my laptop and OpenCV, but I'd be open to considering non-USB cameras (ex. Firewire, component) if they're better on bandwidth, resolution, or FPS. Price is not a huge concern, but I'd really like to stay below $100 per camera if possible.

Anyone here have experience with this type of problem or hardware setup? If so, I'd greatly appreciate any advice, tips, or insight you might have!",en
1109120,2012-02-28 14:17:13,statistics,Bitstream Question,q9lpc,[deleted],,https://www.reddit.com/r/statistics/comments/q9lpc/bitstream_question/,2.0,0.0,"If I have a bitstream A such that the bits are 1 with probability a and likewise a bitstream B and probability b, is it the case that the probability of a bit being 1 in C = A | B is 1-((1-a)\*(1-b)) and the probability of a bit being 1 in D = A &amp; B is a\*b?

Also how would I calculate the error and certainty of mathematical results from this?",en
1109121,2012-02-28 15:43:02,MachineLearning,"Run time of an svm model fitted on 10,000 records with 60 vars?",q9nur,eyyochen,1291655782.0,https://www.reddit.com/r/MachineLearning/comments/q9nur/run_time_of_an_svm_model_fitted_on_10000_records/,3.0,6.0,"I'm using R's caret package and trying to fit an Linear SVM on a training dataset of size 10,000x60. Does anyone have any idea how long this is supposed to run for? It's being run on a macbook air with 4gb of memory and a 1.7Ghz i7 processor. I am new to ML and have no idea how long an SVM takes to build. Any help would be much appreciated.",en
1109122,2012-02-28 17:22:05,statistics,Estimated-d &amp; SPSS,q9r3q,blundboy,,https://www.reddit.com/r/statistics/comments/q9r3q/estimatedd_spss/,3.0,3.0,"Hey I was wondering if theres any way to calculate the estimate-d (cohen's-d from pooled variance in independent t testing) in SPSS. Since whe have to use the output to support our numbers. I've already calculated the d, but can't find the spss function for it.
Thanks in advance,
Blundboy ",en
1109123,2012-02-28 17:44:29,rstats,Statistics with R,q9ryl,sean_the_geek,1328097574.0,https://www.reddit.com/r/rstats/comments/q9ryl/statistics_with_r/,5.0,1.0,,en
1109124,2012-02-28 19:53:03,MachineLearning,Stanford Unsupervised Feature Learning and Deep Learning Tutorial,q9xsg,zettaphone,1326144309.0,https://www.reddit.com/r/MachineLearning/comments/q9xsg/stanford_unsupervised_feature_learning_and_deep/,28.0,8.0,,en
1109125,2012-02-28 21:24:05,datascience,CAN has tried to address the challenges of hiring smart people.,qa27e,CANWorkSmart,1329935518.0,https://www.reddit.com/r/datascience/comments/qa27e/can_has_tried_to_address_the_challenges_of_hiring/,1.0,0.0,,en
1109126,2012-02-28 22:14:03,rstats,"Demoing R: Maximum ""wow"" with fewest lines of code?",qa4ph,ScientistDaddy,1320395809.0,https://www.reddit.com/r/rstats/comments/qa4ph/demoing_r_maximum_wow_with_fewest_lines_of_code/,13.0,11.0,"I am giving a demonstration of R to psychology graduate students tomorrow. This is a non-programmer crowd and I need some help convincing them to start using R over SPSS/PASW.

I'd like to run scripts to show off R's capabilities, but the code would need to be very short and easy to understand. Any ideas?
",en
1109127,2012-02-29 04:50:25,AskStatistics,Why is it so important that a sample is I.I.D.? (independently and identically distributed),qap6b,ntsprtn7,1326095968.0,https://www.reddit.com/r/AskStatistics/comments/qap6b/why_is_it_so_important_that_a_sample_is_iid/,9.0,8.0,"Sorry if this is a little too basic. I'm kinda new to stats.

EDIT: Thanks for the awesome replies, everyone. For me, this is one of those basic concepts that I just went along with but didn't think too much about. I can see why assuming i.i.d. is so helpful now!",en
1109128,2012-02-29 05:52:48,statistics,"Don't mean to bring politics into this subreddit, but there are some people looking for statistical analysis help regarding possible election fraud in GOP primary.",qasef,[deleted],,https://www.reddit.com/r/statistics/comments/qasef/dont_mean_to_bring_politics_into_this_subreddit/,46.0,21.0,,en
1109129,2012-02-29 07:27:26,MachineLearning,Learning Structure of Bayes Networks with K2 ,qawzz,erUserName,1290733949.0,https://www.reddit.com/r/MachineLearning/comments/qawzz/learning_structure_of_bayes_networks_with_k2/,11.0,23.0,"So, I recently implemented Bayes Networks for discrete variables, and implemented the K2 algorithm for learning the structure of a network ( http://web.cs.wpi.edu/~cs539/s05/Projects/k2_algorithm.pdf ) . 

Has anyone else implemented this? I ask, because its results are just horrible. It will usually infer only one or 2 variables as having a relationship with the target class, with very few nodes ending up in the Markov blanket. Naive Bayes often ends up giving better results then the inferred graph. Has anyone else observed this? ",en
1109130,2012-02-29 09:31:34,statistics,"Finding confidence interval, big sample size but most results are zero. Need some direction...please help.",qb1uc,klik,1186720029.0,https://www.reddit.com/r/statistics/comments/qb1uc/finding_confidence_interval_big_sample_size_but/,2.0,6.0,"I am not a statistician, but at work a lab turned to me to try to come up with an upper limit (95 or 99% confidence) and if that limit is reached it would require a written report to explain that high result.  I took a year worth of data, about 4000-5000 samples, but most results are zero.  Out of 5000 samples, you might have 15-25 nonzero results, results are usually 1 or 2, but a few can go as high as 10 or 15 or higher.  The results are recorded as integers.  I'd greatly appreciate any help on what would be the best way to interpret these zero values.",en
1109131,2012-02-29 13:41:05,statistics,O'Reilly is offering 50% discount in data related e-books and videos.,qb77f,phaxsi,1318846049.0,https://www.reddit.com/r/statistics/comments/qb77f/oreilly_is_offering_50_discount_in_data_related/,5.0,1.0,,en
1109132,2012-02-29 16:24:42,MachineLearning,Question about Cross Validataion.,qbbeo,eyyochen,1291655782.0,https://www.reddit.com/r/MachineLearning/comments/qbbeo/question_about_cross_validataion/,4.0,14.0,"Hi,

I'm trying to fit a logistic regression model regularized by the L1 norm. I am using the glmnet package in R. I have a dataset of a 100k records with roughly 60 regressors (binary response). Here is my method:
1. Cut the original dataset into a test dataset and a train dataset (stratified).
2. Run glmnet with 10-fold cross validation on the train dataset.
3. Fit on the test dataset using the predicted model from step 2 and calculate the mean square error of prediction.

Is this the correct approach? Any help would be much appreciated.",en
1109133,2012-02-29 17:00:06,statistics,Quantization of Continuous Data,qbcpj,RobMagus,1292332875.0,https://www.reddit.com/r/statistics/comments/qbcpj/quantization_of_continuous_data/,2.0,10.0,"A student has numeric ratings on a variety of items in some questionnaires and some sports performance data and would like to model the performance on questionnaires.

He wanted to do an analysis where some of the data are split into quantiles so he can compare high, medium, and low performance groups. I suggested he do a linear regression instead so that he wouldn't lose power by throwing away the continuous data - but I realized that the only papers I've seen on this being bad were about doing median splits on continuous data.

Intuitively, it still seems that quantizing your data will sacrifice power - but how is this affected by the number of quantiles you choose to use? Are there circumstances where using, say, quartiles isn't much worse than doing a regression?

I'm still going to tell the student they should do a regression - I was just curious about when quantization of continuous data is bad, and when it's ok.",en
1109134,2012-02-29 17:37:15,statistics,Why/how is the p-value uniformly distributed under the null hypothesis? ,qbe9y,[deleted],,https://www.reddit.com/r/statistics/comments/qbe9y/whyhow_is_the_pvalue_uniformly_distributed_under/,3.0,9.0,"I don't understand how this works. Under the null I expect 5% of the p-values to be below the 5% threshold (duh), 20% of the values to be under a 20% threshold, and so forth. To me, this is a bell curve. There's something I'm missing. Can anyone elaborate? Thanks",en
1109135,2012-02-29 18:05:51,statistics,Statistics Review,qbfmc,profevilj,1318538882.0,https://www.reddit.com/r/statistics/comments/qbfmc/statistics_review/,2.0,4.0,I have recently moved back into an analytics role for a firm after many years of running my own business. I was hoping to review but lost most of my previous texts in Hurricane Irene. Can anyone recommend a few titles or journals that I can review to brush up on my basic statistics knowledge (like correlation coefficients and poisson etc etc)? I know this is a basic question but help is appreciated!,en
1109136,2012-02-29 20:55:50,statistics,UIUC Masters in Statistics Program Perceptions ,qbny7,[deleted],,https://www.reddit.com/r/statistics/comments/qbny7/uiuc_masters_in_statistics_program_perceptions/,1.0,5.0,"Does anyone here have any information or perception of the value of a MS Stat degree, or the MS Stat Analytics degree from UIUC?  I know its a top 40 Grad program, but I don't know how important rank is when searching for Stat jobs out of grad school. ",en
1109137,2012-02-29 22:22:29,statistics,Can someone please explain what statistics I need to do for my project?,qbsb8,Quinny86,,https://www.reddit.com/r/statistics/comments/qbsb8/can_someone_please_explain_what_statistics_i_need/,1.0,3.0,"My lecturer said I need t-test, anova, and tukey test, but I've never done stats or used minitab before.

Basically I've done experiments on how pressure affects the stretch reflex, so I've got ten results at each pressure. I've tried various types of anova on the averages of my results but it just throws errors back at me, what am I doing wrong? Should I be using all my data rather than just the averages?
 

",en
1109138,2012-02-29 23:21:57,statistics,Why everyone thinks they’re above average.,qbvdx,KvanteKat,1171046085.0,https://www.reddit.com/r/statistics/comments/qbvdx/why_everyone_thinks_theyre_above_average/,2.0,0.0,,en
1109139,2012-03-01 03:02:38,statistics,Using statistics in a court of law: Prosecution statistician says white and black jurors seated at same rate,qc6mj,prhodes,1328233759.0,https://www.reddit.com/r/statistics/comments/qc6mj/using_statistics_in_a_court_of_law_prosecution/,5.0,1.0,,en
1109140,2012-03-01 03:46:59,statistics,What metrics can one use to say if a logistic model is a good fit?,qc8pk,benjeye,1292457824.0,https://www.reddit.com/r/statistics/comments/qc8pk/what_metrics_can_one_use_to_say_if_a_logistic/,5.0,8.0,"I found this on wiki:
[http://en.wikipedia.org/wiki/Logistic_regression#Model_accuracy](http://en.wikipedia.org/wiki/Logistic_regression#Model_accuracy)

But, the response variable in the model I'm building isn't 0 or 1 but rather probabilities between 0 and 1. To do the above, do I make a cut off point and classify the probabilities below that point as 0 and ones above as 1? If so, what should the cut of be? Merely .5? Or are there other metrics out there.",en
1109141,2012-03-01 08:14:51,statistics,ANOVA quirk in R,qclgu,thavi,1264864567.0,https://www.reddit.com/r/statistics/comments/qclgu/anova_quirk_in_r/,11.0,5.0,"Hey guys, when I want to do an ANOVA in R and the classification variable has been categorized by a number (i.e., Brand 1 being classified simply as ""1"", Brand 2 classified simply as ""2""), the ANOVA always spits out 1 degree of freedom.

When I change all the numbers to letters (i.e. ""1"" becomes ""a"", ""2"" becomes ""b"", for example), THEN the ANOVA runs properly and produces results with the correct degrees of freedom.  This is fine if I have a small amount of data to re-code, but is there a (simple) way to just declare the category as a qualitative classifying variable from the start?",en
1109142,2012-03-01 17:45:51,statistics,Proposal for categorizing types of randomness,qd0d6,cornlog,1140470264.0,https://www.reddit.com/r/statistics/comments/qd0d6/proposal_for_categorizing_types_of_randomness/,6.0,6.0,,en
1109143,2012-03-01 20:17:45,statistics,Could someone please help me understand how to interpret the results in logistic regression? ,qd7e1,Pooma__,1283789912.0,https://www.reddit.com/r/statistics/comments/qd7e1/could_someone_please_help_me_understand_how_to/,5.0,8.0,"I feel really stupid, but i just cant get my head around it. Perhaps Im making it more complicated than it is. It's just something about odds that confuses me.

So. If i make up an example here:
Trying to predict y=weather or not someone passes a test , using x= hours of study.
The coefficent (odds ratio?) becomes 1.17. Does this mean that for every hour of study the individual is 17% more likely to pass the test than the one studying one hour less? Or what is the correct interpretation?

And that damn term odds ratio, what does it mean?

It really really annoys me not being able to get this straight, pleas help me out!",en
1109144,2012-03-02 00:16:14,artificial,Computer Go demystified: An interview with Martin Müller,qdjg4,[deleted],,https://www.reddit.com/r/artificial/comments/qdjg4/computer_go_demystified_an_interview_with_martin/,1.0,0.0,,en
1109145,2012-03-02 01:01:49,MachineLearning,Review: Machine Learning: An Algorithmic Perspective by Stephen Marsland,qdltz,robintw,1148653350.0,https://www.reddit.com/r/MachineLearning/comments/qdltz/review_machine_learning_an_algorithmic/,17.0,7.0,,en
1109146,2012-03-02 01:42:28,statistics,Inspiration for a simple experiment.,qdnry,ortl,1321578118.0,https://www.reddit.com/r/statistics/comments/qdnry/inspiration_for_a_simple_experiment/,2.0,8.0,"Currently I'm in a Design of Experiment class.  We are suppose to come up with a simple, inexpensive 2 setting 6-8 factor experiment.  I was trying to avoid baking cookies and varying the quantity of ingredients since that one seems to have been picked by a few people.",en
1109147,2012-03-02 02:30:37,statistics,Help with R.,qdq3i,[deleted],,https://www.reddit.com/r/statistics/comments/qdq3i/help_with_r/,8.0,10.0,"I posted this over on r/homeworkhelp, but I´m posting it here too, since I think this is a more appropriate forum.
...

I have trouble working with R. I have never learned programming, so I have nothing to build on.

Here goes: I have a dataset, from my teacher, so I believe the setup of the data set is alright. I can't read from the set however.

I write: Where x is one of the columns in the dataset: x &lt;- (data$x)

table(x)

Which should give me a count of each value in the column. But it gives me ""character(0)"". Anyone have any advice?
",en
1109148,2012-03-02 04:30:33,MachineLearning,"Does anyone have any experience with Segaran's ""Programming Collective Intelligence"" ?",qdvsh,Wonnk13,1287022683.0,https://www.reddit.com/r/MachineLearning/comments/qdvsh/does_anyone_have_any_experience_with_segarans/,3.0,5.0,"All of my coursework is with Bishop's pattern matching book and other more theoretical texts. Although they're well written i'm finding it easy to get lost in all of the proofs and struggling to learn how implement things with real world data. 

Segaran's book looks to be more of an applied ""cookbook"" style, however it is nearly five years old. Are the techniques still applicable, and is any of the code (Python 2.4) broken or replaced by better libraries? Any input appreciated :) ",en
1109149,2012-03-02 07:15:28,MachineLearning,The people who believe we are living in a simulation...,qe36h,[deleted],,https://www.reddit.com/r/MachineLearning/comments/qe36h/the_people_who_believe_we_are_living_in_a/,0.0,7.0,"I didnt know where to post this but I am looking for links or videos or names related to the following quote...if you can help....or tell me which subreddit to post in:

""Because of the rate computing power is increasing there is actually a small but growing fringe religion/spiritual belief out there that believes that we as humans live within a simulation and that ""god"" is the one who programmed us. A lot of it is based on the simulation argument/hypothesis. I've seen a few interviews about it but I'm not sure what it's called.""",en
1109150,2012-03-02 10:57:30,statistics,Nielsen TV ratings scandal in Turkey.,qe9yw,srkiboy83,1299056137.0,https://www.reddit.com/r/statistics/comments/qe9yw/nielsen_tv_ratings_scandal_in_turkey/,2.0,0.0,,en
1109151,2012-03-02 15:30:17,statistics,Need to test data for multivariate normality? Here are some graphical and analytical goodness-of-fit tests.,qefgz,RickWicklin,1327333524.0,https://www.reddit.com/r/statistics/comments/qefgz/need_to_test_data_for_multivariate_normality_here/,5.0,0.0,,en
1109152,2012-03-02 17:54:29,statistics,SPSS Help: Syntax for re-naming variables?,qekfy,Brain_Doc82,1304194023.0,https://www.reddit.com/r/statistics/comments/qekfy/spss_help_syntax_for_renaming_variables/,2.0,3.0,"I'm working with some data from an existing database (aka, I did not crreate this nightmare), here is the issue.

I have data from hundreds of subjects and each subject has a unique identifier (i.e., 9000, 9001, 9002, etc).  However, each subject has a T1 and T2, and for each visit their data is coded in the database as if it were a unique subject (i.e., 9000_1, 9000_2, 9001_1, 9001_2, etc. where _X is the scan number for that subject).  I have over 600 variables for each subject and in the existing database (not SPSS) the variables all have the same name (i.e., age, sex, handedness, etc).  I need to do a comparison of the data at T1 and T2, but since the variables share the name, I can't merge the SPSS files I've created.  All I need to do is rename the variables so that they are unique (ideally just adding _T2 to all the time 2 variables) but I can't seem to find any syntax or any way of doing this?  Does anyone have any suggestions?  This is really holding up my research and I'm getting incredibly frustrated, and I really don't want to waste the time by manually renaming nearly 700 variables.  Thanks in advance for any help!  

P.S. I have access to other programs (i.e., R, SAS, NCSS, Matlab, etc), but very little experience with R and SAS.",en
1109153,2012-03-02 18:40:34,statistics,"Statistician ranks Maryland basketball ""lucky"" because they've won many close games, but coach's comments indicate it's not luck at all.",qemez,RickWicklin,1327333524.0,https://www.reddit.com/r/statistics/comments/qemez/statistician_ranks_maryland_basketball_lucky/,7.0,5.0,,en
1109154,2012-03-02 19:25:56,statistics,BBC News - Go Figure: Watching out for Wimbledon-washing machine links,qeoht,simonjp,1127707200.0,https://www.reddit.com/r/statistics/comments/qeoht/bbc_news_go_figure_watching_out_for/,4.0,0.0,,en
1109155,2012-03-02 20:28:22,computervision,"Public DB of Haar detection patterns (not just faces, but hands and fingers)?",qerfi,mcandre,1158766748.0,https://www.reddit.com/r/computervision/comments/qerfi/public_db_of_haar_detection_patterns_not_just/,3.0,2.0,,en
1109156,2012-03-02 22:30:42,MachineLearning,new subreddit for reinforcement learning!,qexeq,lpiloto,1310021871.0,https://www.reddit.com/r/MachineLearning/comments/qexeq/new_subreddit_for_reinforcement_learning/,2.0,3.0,,en
1109157,2012-03-03 01:17:20,statistics,T-test test statistic ti-89,qf5ko,nick1792,1317615478.0,https://www.reddit.com/r/statistics/comments/qf5ko/ttest_test_statistic_ti89/,3.0,2.0,"How can I calculate the test statistic for a 2 sample t-test on a ti-89, I did the 2SampT test but don't see it on there.",en
1109158,2012-03-03 03:14:52,statistics,Statcounter,qfapo,owensmith151,1330672454.0,https://www.reddit.com/r/statistics/comments/qfapo/statcounter/,2.0,0.0,,en
1109159,2012-03-03 04:20:10,statistics,significance of a LARGE standard deviation help?,qfddf,isballs,,https://www.reddit.com/r/statistics/comments/qfddf/significance_of_a_large_standard_deviation_help/,0.0,9.0,"I recently took an exam for my college statistics course.  The professor graded the exam and gave us our personal grades as well as two pieces of information after he ""curved"" the exam:

* The curved mean was 85 
* The curved standard deviation was 14 

I know that being in an advanced stats class right now, I probably shouldn't really be needing somebody else to answer this question for me but...

what exactly do these parameters say about the spread of the grade distribution?  Did a lot of people do really well or did people generally get around the mean of 85?  

* the class has about 40 people in it. ",en
1109160,2012-03-03 09:16:18,MachineLearning,What is Mahalanobis Distance? An accessible and intuitive introduction.,qfo82,lpiloto,1310021871.0,https://www.reddit.com/r/MachineLearning/comments/qfo82/what_is_mahalanobis_distance_an_accessible_and/,17.0,2.0,,en
1109161,2012-03-03 15:57:52,rstats,Must-Have R Packages for Social Scientists,qfvzu,srkiboy83,1299056137.0,https://www.reddit.com/r/rstats/comments/qfvzu/musthave_r_packages_for_social_scientists/,15.0,0.0,,en
1109162,2012-03-03 15:58:03,artificial,Philosophy of Artificial Intelligence:  Neats vs. Scruffies,qfvzz,moscheles,1316266003.0,https://www.reddit.com/r/artificial/comments/qfvzz/philosophy_of_artificial_intelligence_neats_vs/,11.0,2.0,,en
1109163,2012-03-03 20:25:11,computervision,Small project: Teach Japanese using Haar feature detection,qg4e8,mcandre,1158766748.0,https://www.reddit.com/r/computervision/comments/qg4e8/small_project_teach_japanese_using_haar_feature/,1.0,1.0,,en
1109164,2012-03-04 00:06:43,statistics,Help with simple statistics homework,qgdl6,optinet101,1326587882.0,https://www.reddit.com/r/statistics/comments/qgdl6/help_with_simple_statistics_homework/,1.0,0.0,,en
1109165,2012-03-04 02:24:00,statistics,Quick stats project survey!,qgj6p,burnthiscity,1329416846.0,https://www.reddit.com/r/statistics/comments/qgj6p/quick_stats_project_survey/,5.0,18.0,"Long time lurker, this is my first post, I hope this is the right place, feel free to redirect me if i'm wrong :)
I have a stats project i'm currently working on, and need 100 responses, facebook only gave me 45...so I need 55 more, I figured redditors are nice, they'll help me out!
The question: How many dogs did you adopt from a shelter, of all that you own?
Example: I have 3 dogs, 1 of whom is adopted from a shelter, so my answer would be: 1/3
I would be incredibly grateful! Thank you!

EDIT: Did pretty good on my project, got a lot of answers from both here and r/dogs, thanks to everyone who answered/gave tips :D",en
1109166,2012-03-04 14:46:00,statistics,how to arrange data?,qh45i,rutcrex,,https://www.reddit.com/r/statistics/comments/qh45i/how_to_arrange_data/,1.0,6.0,"I am quite a new-be to stats. My GF is doing some theoretical work on climbing and had some kids doing some climbing. Then on two different occasions she had them climb a specific climbing route and rated them to see how much they have improved. My question now is: What would be better to arrange the data: 

1) Climber 1, Speed 1st attempt: 30s, Speed 2nd attempt: 20s

or

2) Climber 1, 1st attempt, Speed 30s

   Climber 1, 2nd attempt, Speed 20s


I am planning to visualize the data with ggobi. Thanxs for the help. ",en
1109167,2012-03-04 19:28:20,statistics,The most interesting statistical paradoxes,qhbik,[deleted],,https://www.reddit.com/r/statistics/comments/qhbik/the_most_interesting_statistical_paradoxes/,38.0,6.0,,en
1109168,2012-03-04 22:18:27,datasets,Does anyone know of any data sets where I can apply data mining techniques towards customer relationship management?,qhil4,aguyfromucdavis,1279525151.0,https://www.reddit.com/r/datasets/comments/qhil4/does_anyone_know_of_any_data_sets_where_i_can/,1.0,0.0,I'm looking for data sets that show customer behavior or activity and whether or not they bought a certain product. I'm looking to apply techniques like k-means clustering or decision trees to profile the customer population and learn more about their propensity to purchase a product. Thanks! (x-post from r/machinelearning),en
1109169,2012-03-04 22:57:00,statistics,Can i get into Graduate program in statistics?,qhkbf,zack12,1286224807.0,https://www.reddit.com/r/statistics/comments/qhkbf/can_i_get_into_graduate_program_in_statistics/,3.0,9.0,"I am doing a degree in accounting and finance from university of london external program. I have taken couple of statistics courses. One is basic level, while the other is advanced statistics. I really like to study statistics on the graduate level. Now i cant take anymore statistics units in my course but i am reading a lot of statistics books these days. I am also learning python, and intend to learn R once i am done with python. Is it possible for me to get into a graduate program. What do you guys recommend? I really interested in university somewhere in Canada or somewhere in Europe. ",en
1109170,2012-03-05 02:54:48,rstats,Efficiency of R-loop,qhum3,Honglang,1321811248.0,https://www.reddit.com/r/rstats/comments/qhum3/efficiency_of_rloop/,9.0,9.0,,en
1109171,2012-03-05 05:16:08,computervision,papers on how to apply machine learning to object recognition?,qi11v,waspinator,1202264096.0,https://www.reddit.com/r/computervision/comments/qi11v/papers_on_how_to_apply_machine_learning_to_object/,2.0,2.0,"I'm doing a project in a machine learning class and I'd like for it to be in computer vision, but I'm not sure where to start. Are there any good papers about how to use machine learning for 2D object recognition? I don't really mind which technique is used for machine learning or object recognition, or even what the object is. Which ever one is easiest to get started with I guess.

Thanks.",en
1109172,2012-03-05 16:31:44,statistics,Testing if groups are similar enough,qiks6,Freebush,1303177243.0,https://www.reddit.com/r/statistics/comments/qiks6/testing_if_groups_are_similar_enough/,2.0,15.0,"Strange question, I know. I'm more or less just testing if two groups are equal with their improvement paths, only problem is that one of the groups is just all the data I have (lots missing), and the other is where I have all the data (not missing anything at all) so there is overlapping in this. Sorry, that was a terrible explaination. 
So for where I have complete data I have about 600 people, and for the other group where there is missing and complete, I have about 1150. I am tracking progress on a clinical outcome measure, so I am using a repeated measures design, but clinicians often forget to complete the test. 
My question is am I able to use the missing and complete data set, and how can I prove that it is similar enough to use. Would it be something such as using Fmax to test if the variances are statistically insignificant? That's what originally came into my mind, but since the complete data is within the group with all the data, didn't know if this was applicable. 
Cheers for this",en
1109173,2012-03-05 18:16:36,datasets,"ESPN Starts Opening The Doors To Its Data With Developer Center, First API Program",qiot8,wundie,1170872617.0,https://www.reddit.com/r/datasets/comments/qiot8/espn_starts_opening_the_doors_to_its_data_with/,14.0,1.0,,en
1109174,2012-03-05 19:54:17,MachineLearning,The data science debate: domain expertise or machine learning?,qitet,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/qitet/the_data_science_debate_domain_expertise_or/,21.0,1.0,,en
1109175,2012-03-05 20:23:07,datasets,x-post from Open Data- Searching for end-user tools to format data to Open Data standards,qiuvt,gingerjoyce,,https://www.reddit.com/r/datasets/comments/qiuvt/xpost_from_open_data_searching_for_enduser_tools/,3.0,2.0,"I have been searching around through Google and can’t seem to find any good tools for end-users to assist them with converting existing data formats to a more open standard.  

Moving spreadsheets to a CSV format is easy, but what about converting MS Word documents to a functional, and clean, XML format?  MS Word can save into an XML format, but it is not clean.  
Can anyone recommend tools/applications that would be easily usable by end-users that would allow them to either format MS Word into usable XML or plain text to XML?  

We are trying to get some traction on the Open Data movement but the largest obstacle we are facing is from day-to-day users.  An easy-to-use tool/application to help them put their data into a semantic format for Open Data would really help break that barrier.  They must exist but I am just not finding them.
",en
1109176,2012-03-05 23:37:07,statistics,How does one meaningfully cluster categorical free association data?  (xpost from psychology),qj4dg,clusteringstuff,1330639477.0,https://www.reddit.com/r/statistics/comments/qj4dg/how_does_one_meaningfully_cluster_categorical/,1.0,0.0,,en
1109177,2012-03-05 23:51:09,MachineLearning,how does computer vision use machine learning?,qj529,waspinator,1202264096.0,https://www.reddit.com/r/MachineLearning/comments/qj529/how_does_computer_vision_use_machine_learning/,5.0,13.0,"I'm reading [Szeliski's book](http://szeliski.org/Book/) about computer vision, and I can't always see a connection between computer vision and machine learning in many places. Especially in instance recognition. Does it exist? 

is there any machine learning going on in instance recognition? Is [SIFT](http://en.wikipedia.org/wiki/Scale-invariant_feature_transform) the only thing required to accomplish this? Does any learning take place?

Is there a paper or other document explaining how a machine learning approach is used in finding objects in an image?",en
1109178,2012-03-06 02:05:22,MachineLearning,Why generic machine learning fails.,qjc26,qkdhfjdjdhd,1211920583.0,https://www.reddit.com/r/MachineLearning/comments/qjc26/why_generic_machine_learning_fails/,15.0,6.0,,en
1109179,2012-03-06 04:14:18,artificial,"Boston University scientists are developing robots modeled on the human brain that can learn, make decisions, and adapt to their environments",qjid0,kamoylan,1254047922.0,https://www.reddit.com/r/artificial/comments/qjid0/boston_university_scientists_are_developing/,23.0,4.0,,en
1109180,2012-03-06 04:59:55,statistics,Is there statistical evidence of fraud in the Russian election data?,qjkl9,brews,1263715301.0,https://www.reddit.com/r/statistics/comments/qjkl9/is_there_statistical_evidence_of_fraud_in_the/,10.0,5.0,,en
1109181,2012-03-06 10:08:31,MachineLearning,Confusion matrix with leave-one-out cross validation,qjx7z,thrope,1275749746.0,https://www.reddit.com/r/MachineLearning/comments/qjx7z/confusion_matrix_with_leaveoneout_cross_validation/,3.0,3.0,"I have started working on a project where we are using a nearest mean classifier on a noisy data set to evaluate different features. 

We do leave one out - where we leave out one sample, fit the classifier to the remaining data, and then predict for the left out sample. We repeat this with each data point and build the confusion matrix in this way. 

I am trying to get a deeper understanding of the theory and I get the impression the usually the confusion matrix is generated from a whole set of test data (not used for fitting) and is for a single classifier - whereas here we are building the matrix from all the many different (leave-one-out) classifiers. 

Is this still called a confusion matrix or is there any other terms for this procedure (I am a beginner in the ML field). Is there any reference I could read for more details of the theory behind this kind of procedure or how it could be improved. 

",en
1109182,2012-03-06 12:17:20,MachineLearning,I need your help finding a suitable algorithm for an incremental learning task. (details inside),qjzxq,huepfburg,1289891304.0,https://www.reddit.com/r/MachineLearning/comments/qjzxq/i_need_your_help_finding_a_suitable_algorithm_for/,6.0,7.0,"Dear reddit,

I am a compsci student from Germany and currently working on a project that will most likely be part of my bachelors thesis. I am having troubles finding an algorithm/method that allows me to do the following:

There will be a set of objects (up to 5,000 per Set) which will be presented to the user. The user will select exactly one of these objects from the set depending on the context – he will choose the object which fits best into the current context.

The system should learn over time what object the user selected within what context and later present the most suitable object for the current context at the top of the list of available objects.

Now here is an example: The user has loaded a resource containing numerous items (Strings) into his the system (a texteditor). After the user initiated a special auto-complete mode, the system should present a list of these items (Strings) and should order them depending on how suitable the Strings are for the current context (before and after the cursor position). The system has learned what Strings are most suitable for the context.

I was first thinking of using an naïve bayes approach by using the tokenized context as featureset and the items from the resource as available classes. I am not sure if this is a valid approach. Here are the problems I am facing:

* The number of available classes (items) can be very large.
* The context will be different every time. It is most unlikely, that the exact same featureset will occure more than once.
* The algorithm will have to be suitable for incremental learning: The system will start off without any data about the user's preferences and learn them over time, while the user is working with the system.

Do you have any suggestions as to what methods would be applicable to the task I've just described? Obviously performance is an issue – the system should not take too long for the user to notice, before the ordered list of items is presented.

Thanks in advance",en
1109183,2012-03-06 14:54:27,rstats,Visualization of Data Using R,qk37b,srkiboy83,1299056137.0,https://www.reddit.com/r/rstats/comments/qk37b/visualization_of_data_using_r/,11.0,0.0,,en
1109184,2012-03-06 18:29:32,AskStatistics,Stats brain-teasers,qkanx,gnujosh,1265392804.0,https://www.reddit.com/r/AskStatistics/comments/qkanx/stats_brainteasers/,1.0,1.0,"I had a friend who applied for a job at a investment company, and they asked him a couple interesting stats questions that take a similar form to the coding questions that some companies ask.

One question was:
A plane has 100 seats, all in a single line.  Every person is assigned a random seat, and they board in order from 1 to 100.  The first person who boards picks a random seat.  When anyone else boards, if their seat is taken, they will sit at a random open seat.  Question: what is the probability that person 100 will be able to sit in seat 100?  More generally, is it the same for all n?

Another:
There is a barrel with N ropes in it.  This means there are 2 * N ""ends.""  You randomly reach in and grab two ""ends,"" tie them together, and put it back into the barrel.  The process continues until there are no more ""ends.""  Question: what's the expected number of loops?  Question: what's the expected loop length?",en
1109185,2012-03-06 21:07:59,datasets,Request: game logs archive with timestamp?,qki6t,melipone,1292611081.0,https://www.reddit.com/r/datasets/comments/qki6t/request_game_logs_archive_with_timestamp/,0.0,0.0,I am looking for game logs where moves are recorded with a timestamp attached. One-person games will work too. ,en
1109186,2012-03-06 21:12:09,MachineLearning,Has any one tried torch7? How does it compare to matlab or numpy?,qkif3,DoorsofPerceptron,1276810914.0,https://www.reddit.com/r/MachineLearning/comments/qkif3/has_any_one_tried_torch7_how_does_it_compare_to/,8.0,12.0,,en
1109187,2012-03-06 22:33:36,MachineLearning,Stanford NLP class is live!,qkmd0,HughJorgan1986,,https://www.reddit.com/r/MachineLearning/comments/qkmd0/stanford_nlp_class_is_live/,40.0,7.0,,en
1109188,2012-03-06 23:11:53,datasets,NCAA college football strength of schedule data,qko6b,johnnymo08,1325984192.0,https://www.reddit.com/r/datasets/comments/qko6b/ncaa_college_football_strength_of_schedule_data/,4.0,0.0,"Hey reddit, 
I'm doing an econometric paper and I need some help. I cant seem to find a good consistent site with college football strength of schedules for the past 12 years. I have looked on NCAA.com but cant seem to find any data with SOS. I have found this website http://www.jhowell.net/cf/cfindex.htm but I'm not sure how accurate the data is. 
any help would be appreciated
",en
1109189,2012-03-07 00:10:07,statistics,standard error why? when to use over SD?,qkr00,[deleted],,https://www.reddit.com/r/statistics/comments/qkr00/standard_error_why_when_to_use_over_sd/,5.0,13.0,"I have an old asshole of stats professor on my thesis defense, which is first week of next month. 

That is the kind of analysis that I will be doing. But I can hear him saying ""what are the benefits of using the standard error over the standard deviation"" in my thesis defense. My answer would be something like ""well, i think that because of the large sample size, using the sampling distribution of the mean is just as informative and its a matter of preference, but I also wanted to make "". Do you think think thats an acceptable answer?

what would you say in that situation?

honestly, im using standard errors because it was easy when spss spat them out. 

For example:
  *the standard errors are in parenths. 
Grade 3: Math
Poverty
High
189.10 (1.10)
188.08 (1.01)
201.79 (1.01)
200.26 (1.01)
Med
195.78 (1.19)
193.58 (1.01)
207.32 (1.01)
204.21 (1.01)
Low
197.90 (0.95)
197.09 (1.00)
210.37 (1.00)
209.04 (1.01)
 
The averaged scores for the third grade math condition evidenced an initial difference between high (189.10), medium (195.78), and low (197.78) poverty groups that continues along the other three testing dates. It is important to note that during the Spring ’09, the medium group has a higher standard error (1.19) compared to the other schools suggesting greater variability in that sample. However in the later tests that variability decreased to around normal. Also the low poverty group in Spring ‘09 had a lower standard error (0.95) compared to the other groups suggesting less variability than those in that sample. However in later tests that increases to normal. ",en
1109190,2012-03-07 01:26:54,statistics,What exactly is a Confidence Interval?!,qkuo9,MacBelieve,1291955283.0,https://www.reddit.com/r/statistics/comments/qkuo9/what_exactly_is_a_confidence_interval/,5.0,14.0,"I've been told a CI has nothing to do with probabilities of containing the true parameter, I've been told it has to do with hypothetical distribution of samples. I'm not sure if I'll ever understand it.

Does anyone have links to good explanations? Wikipedia was little help. Also, I'm not afraid of some heavier reading",en
1109191,2012-03-07 01:28:54,statistics,Question about distributions with combinatorics.,qkurm,[deleted],,https://www.reddit.com/r/statistics/comments/qkurm/question_about_distributions_with_combinatorics/,1.0,2.0,"Statistics noob here. 

So I have a set of 10 paths, uniformly distributed in length between 5 and 20 meters. What would be the distribution of total length given that I pick 5 of those paths at random?

Thanks in advance.",en
1109192,2012-03-07 06:29:33,statistics,Hey Redditors I'm collecting information about the beliefs of the community here! ,ql9ie,jngo12,1320631833.0,https://www.reddit.com/r/statistics/comments/ql9ie/hey_redditors_im_collecting_information_about_the/,1.0,0.0,,en
1109193,2012-03-07 06:56:44,MachineLearning,What Lang/Platforms to use for working with datasets?,qlash,zigzagp,1330961236.0,https://www.reddit.com/r/MachineLearning/comments/qlash/what_langplatforms_to_use_for_working_with/,5.0,15.0,"Hi, I'm a final year student in CS and is on the verge of completing the lectures of Artificial Intelligence (Stanford Online) and i have already covered Machine Learning (Stanford Online). Now, i want to work with some data sets and from what i read from quora or from here, i see that people generally recommend either R or NumpY or specific libraries for specific algorithms. What i would like to know, is if there are different situations/purposes for which each of them are used and if there are, please let know. Also, if you would like to recommend new platforms, pls do so but also tell us as to why it is being used?",en
1109194,2012-03-07 06:57:49,statistics,"Stata Help, Quick Question ",qlau6,FK99,1328685549.0,https://www.reddit.com/r/statistics/comments/qlau6/stata_help_quick_question/,1.0,0.0,,en
1109195,2012-03-07 12:48:17,statistics,Cochran-Mantel-Haenszel statistics &amp; uneven sample sizes,qllcx,[deleted],,https://www.reddit.com/r/statistics/comments/qllcx/cochranmantelhaenszel_statistics_uneven_sample/,1.0,0.0,"Hi all,
I have a case-control study with cases/patients and controls sampled from two geographic populations. I perform a meta-analysis to look at the effect of genetic variants on the disease, irrespective of the population. Using the CMH test to compensate for potential differences between the two populations is pretty much the standard procedure.

My problem or question relates to what happens if you have very uneven sample sizes between the two populations. In population A you have 5x the number of controls than cases, in population B you have 3x the number of cases than controls.

I was under the impression that what the CMH test is doing is pretty much make the two analyses separately and then make a joint test for them. This would eliminate any potential sample size biases between the populations, right? (I know that the uneven sample sizes are not optimal but they are what we have). 

I have now talked to a colleague that maintains that there would still be problems in using such unequal sample sizes, despite the usage of the CMH test. If this is true, I'd love to know why. If the CMH does ""fix"" the unequal sample size issue, I'd love to get help in finding a reference where this is specifically stated.

My apologies for the long post.
/zorglubb",en
1109196,2012-03-07 18:41:52,analytics,link adsense to analytics NEW INTERFACE,qlwas,MINECRAFTdotNET,1295797501.0,https://www.reddit.com/r/analytics/comments/qlwas/link_adsense_to_analytics_new_interface/,1.0,0.0,Anyone know how to link in the NEW interface ,en
1109197,2012-03-07 18:47:13,datasets,"Introducing r/BioDatasets (xpost from r/bioinformatics, r/biology)",qlwjd,Derpscientist,,https://www.reddit.com/r/datasets/comments/qlwjd/introducing_rbiodatasets_xpost_from/,8.0,3.0,"[/r/BioDatasets](/r/BioDatasets) is a subreddit with the goal of sharing information about the ever expanding curated biologically-relevant databases. Its creation was inspired by the successes of [/r/Datasets](/r/datasets). It is baffling to me how many large well-curated and maintained biologically relevant datasets exists, that relate to nearly every niche of modern molecular biology. This is primarily targetted towards the bioinformatics and molecular biology community, but it should serve as a nice index for toy data to be used in other disciplines. I envision the subreddit will have three types of posts:

* Links to biologically-relevant datasets.
* News about well known biologically-relevant datasets.
* Requests for specific biologically-relevant datasets.

Thanks!",en
1109198,2012-03-07 19:29:15,MachineLearning,"Reduce excise on textile machinery, TMMA tells FinMin",qlykm,hhind,1331141137.0,https://www.reddit.com/r/MachineLearning/comments/qlykm/reduce_excise_on_textile_machinery_tmma_tells/,0.0,0.0,,en
1109199,2012-03-07 20:45:36,data,Innovation data?,qm2ac,dmdude,1250681632.0,https://www.reddit.com/r/data/comments/qm2ac/innovation_data/,1.0,0.0,"Does anyone have any suggestions on data that would correlate with a companies' innovation capacity? I'm less interested in survey-based measure like Business Week's 25 Most Innovative Companies and more interested in things like product redesigns, or mentions in Popular Science.",en
1109200,2012-03-07 22:31:58,datasets,Innovation data?,qm7m9,dmdude,1250681632.0,https://www.reddit.com/r/datasets/comments/qm7m9/innovation_data/,1.0,0.0,"Does anyone have any suggestions on data that would correlate with a companies' innovation capacity? I'm less interested in survey-based measure like Business Week's 25 Most Innovative Companies and more interested in things like product redesigns, or mentions in Popular Science.",en
1109201,2012-03-08 00:42:11,statistics,How do I find the probability ,qme7k,escrocs,1306676258.0,https://www.reddit.com/r/statistics/comments/qme7k/how_do_i_find_the_probability/,1.0,0.0,,en
1109202,2012-03-08 07:14:30,statistics,Can anyone provide an example of when you'd use each of the following error metrics? [x-post with answers and math],qmxku,dylan89,1290533803.0,https://www.reddit.com/r/statistics/comments/qmxku/can_anyone_provide_an_example_of_when_youd_use/,0.0,0.0,"Minimize the absolute error?

Minimize the squared error?

Maximize R-Squared?

Maximize the correlation coefficient?

Minimize the maximum error?

",en
1109203,2012-03-08 07:21:51,statistics,Simple Linear Regression : How to Find MSE without a data set provided (through formulaic means ) ,qmxwe,JackoWacko11,1321859773.0,https://www.reddit.com/r/statistics/comments/qmxwe/simple_linear_regression_how_to_find_mse_without/,0.0,0.0,"
edit: i realized this is not homework help, sorry ",en
1109204,2012-03-08 07:38:41,AskStatistics,Help with probability question! (Cumulative distribution.),qmymb,[deleted],,https://www.reddit.com/r/AskStatistics/comments/qmymb/help_with_probability_question_cumulative/,1.0,1.0,"I'm working on an assignment and I'm stuck on this question. I'm not necessarily looking just for someone to answer it, but to help point me in the direction of where to start or how to do it. Thanks!

**The question is here: http://puu.sh/jTJi**",en
1109205,2012-03-08 11:40:51,computervision,[Ask CV] affordable high-ish-speed cameras?,qn5lf,vrld,,https://www.reddit.com/r/computervision/comments/qn5lf/ask_cv_affordable_highishspeed_cameras/,7.0,10.0,"Motivated by the other thread about [people tracking](http://www.reddit.com/r/computervision/comments/q9ldi/ask_rcv_best_cameras_for_visual_peopletracking/) I'd like to ask your opinion about affordable mid-to-high-speed cameras. With that I mean cameras able to capture images with at least 60 fps and with a reasonable resolution.

Why do I need it? The general is to build a micro-expression recognition system. Micro-expressions are interesting because they mirror spontaneous basic emotions (i.e. anger, disgust, fear, joy, sadness, surprise) and in contrast to regular expressions they cannot be suppressed easily. As the name suggests they have a very short duration - usually about 0.4s to 0.6s - and are thus very hard to recognize, even for humans.

The best (and so far only) option seems to be the PLAYSTATION Eye camera, which was also mentioned in another thread. It can do both 60fps and 120fps. The only drawback is the relatively low resolution - at least 800x600px would be nice. On the other hand the camera is very affordable so it's probably worth a purchase anyway.

Do you have any other suggestions or point me in the right direction?",en
1109206,2012-03-08 16:45:49,statistics,Test of equality for two population means,qnci4,[deleted],,https://www.reddit.com/r/statistics/comments/qnci4/test_of_equality_for_two_population_means/,1.0,0.0,"I need to test for equality, not difference. I am wondering if anybody knows of any tests parametric or non-parametric.",en
1109207,2012-03-08 18:19:15,statistics,New programming languages and new statistical techniques: Irrelevant or necessary?,qngau,RickWicklin,1327333524.0,https://www.reddit.com/r/statistics/comments/qngau/new_programming_languages_and_new_statistical/,5.0,1.0,,en
1109208,2012-03-08 19:10:23,MachineLearning,Library to Strip Wiki Markup from Wikipedia?,qninc,ltltltlt,1331224991.0,https://www.reddit.com/r/MachineLearning/comments/qninc/library_to_strip_wiki_markup_from_wikipedia/,6.0,19.0,"I was wondering if there's a better resource for stripping the natural language text, doing appropriate substitutions of things like
    {{convert|3|lbs|kg}}
but ignoring things like
    {{cite book|bla|bla|bla}}
and replacing [[Cat|Cat]] style links but removing [[fr:Chat]] style links, preferably in Ruby, Python, or even C.

All I can find is the PHP that is part of Mediawiki itself and a number of sources that strip the marked-up text from XML, which is trivial.

If not, would anyone be interested in collaborating on writing (a solid begining to) such a parser over the weekend? I have been doing some experimenting and the difficulty is really just recognizing what templates are syntactically relevant (convert, et al) and which are information that doesn't belong/I don't want in the natural language portion (see, cite, et al)",en
1109209,2012-03-08 19:53:23,AskStatistics,What is the probability that a date formatted MM/DD/YYYY will be confused with DD/MM/YYYY if the day of the week is provided.,qnkpq,bob_at_hotmail,1331228630.0,https://www.reddit.com/r/AskStatistics/comments/qnkpq/what_is_the_probability_that_a_date_formatted/,3.0,3.0,"When applicants submit forms we don't always know if they're using DD/MM/YYYY vs MM/DD/YYYY.  Wondering how much we could really clear up by adding a day of the week field.  Adding Month (please print) hasn't worked as not all applicants are proficient in English.

Example of a the case we're trying to identify: in 2012 3/12 and 12/3 both fall on a monday, so the form would still be ambiguous. However, there are no overlapping Tuesdays in 2012.

Thanks!",en
1109210,2012-03-08 20:14:22,datasets,Free Your Metadata - Polish and publish your metadata with Google Refine,qnlpc,mhermans,1169219262.0,https://www.reddit.com/r/datasets/comments/qnlpc/free_your_metadata_polish_and_publish_your/,1.0,0.0,,en
1109211,2012-03-08 21:57:28,statistics,"anyone know of a free, easy-to-use implementation of the fisher test for periodicity?",qnqui,weinerjuicer,1240768234.0,https://www.reddit.com/r/statistics/comments/qnqui/anyone_know_of_a_free_easytouse_implementation_of/,1.0,2.0,otherwise i will have to slog through some old stats papers and write it in mathematica,en
1109212,2012-03-08 22:21:49,statistics,Senior Project,qns3v,gurkab,1278004899.0,https://www.reddit.com/r/statistics/comments/qns3v/senior_project/,1.0,0.0,"I am currently an undergraduate Junior who has decided to do his senior project (Capstone) next fall so I can, first, get it out of the way, and second, be able to present it on grad school applications in the spring. So here's my dilemma...

I really want to do it on something operations research based using linear and non-linear optimization, linear programming, simplex algorithm etc. But I am somewhat restricted. I would like to involve statistics in my senior project because then these two credits that represent the senior project will be able to work toward my statistics minor. All in all, kill two birds with one stone. I would also like to incorporate, if possible, a little finance as well. Let me know what you're thinking of!

Any suggestions on how I can go about this? I don't know as much math topics as plenty of people around here so that's why I need your help! Thanks guys! Cheers!
",en
1109213,2012-03-08 22:51:52,statistics,"Is there a test whose null hypothesis is that there is a difference between the means of two populations? (In contrast, the H0 of T tests is that the populations are equal)",qntme,lukis100,1232427503.0,https://www.reddit.com/r/statistics/comments/qntme/is_there_a_test_whose_null_hypothesis_is_that/,3.0,23.0,"I want to conduct a statistical test that will allow me to say that two populations are similar rather than different. As statisticians know, I can't do this with tests like the T test. Hence my question.",en
1109214,2012-03-08 23:21:49,rstats,"Montreal R workshop: Plyr, reshape and other data manipulation goodies",qnv9y,likelihoodtprior,1318629271.0,https://www.reddit.com/r/rstats/comments/qnv9y/montreal_r_workshop_plyr_reshape_and_other_data/,3.0,0.0,,en
1109215,2012-03-09 01:00:16,datascience,What's the mix of software used in r/datascience?,qo09q,suitablyvague,1309115259.0,https://www.reddit.com/r/datascience/comments/qo09q/whats_the_mix_of_software_used_in_rdatascience/,9.0,2.0,Getting big into Rapidminer lately and CPLEX/R mostly for work,en
1109216,2012-03-09 03:45:18,artificial,AI and Language Responses?,qo886,[deleted],,https://www.reddit.com/r/artificial/comments/qo886/ai_and_language_responses/,2.0,3.0,"Hi /r/artificial, 

I'm writing a Chatbot / Siri kinda robot in PHP, and so far I've been using Regular Expression to pigeon-hole the responses in the right direction.

I know this cannot be the most optimal way, and I come to you and ask for any help in writing better algorithms.

I'm really interested in knowing how software can discern whether I want to ask for a Wikipedia article for the band +44 (for example), or whether to apply the term in a mathematical context.

Sorry if this isn't the right subreddit, it seemed like a good fit.
 ",en
1109217,2012-03-09 15:34:30,statistics,JRS'12 data mining contest: multi-label classification of biomedical papers; last chance to join,qoukx,datt,1276003335.0,https://www.reddit.com/r/statistics/comments/qoukx/jrs12_data_mining_contest_multilabel/,4.0,0.0,,en
1109218,2012-03-09 17:37:49,statistics,This is how my statistics professor draws the normal distribution curve.  It annoys me every time he draws it,qoyu0,[deleted],,https://www.reddit.com/r/statistics/comments/qoyu0/this_is_how_my_statistics_professor_draws_the/,1.0,0.0,,en
1109219,2012-03-09 17:51:48,artificial,"Building Lego 10230 while explaining how the brain works; schemas, adaptation, synchronization, and more",qozft,wacco,1259079118.0,https://www.reddit.com/r/artificial/comments/qozft/building_lego_10230_while_explaining_how_the/,7.0,6.0,,en
1109220,2012-03-09 18:38:45,MachineLearning,Irfan’s Taxonomy of Predictive Modeling,qp1hj,willis77,1188267725.0,https://www.reddit.com/r/MachineLearning/comments/qp1hj/irfans_taxonomy_of_predictive_modeling/,6.0,0.0,,en
1109221,2012-03-09 20:36:16,rstats,"Is there any R package or function for this form of weighted average: more closely a data point is to the median, more heavily it's weighted. (Also please kindly tell me what this is called.)",qp6xl,randombozo,1266016617.0,https://www.reddit.com/r/rstats/comments/qp6xl/is_there_any_r_package_or_function_for_this_form/,3.0,10.0,,en
1109222,2012-03-09 22:01:33,MachineLearning,Peter Norvig - The Unreasonable Effectiveness of Data,qpb0w,vrld,,https://www.reddit.com/r/MachineLearning/comments/qpb0w/peter_norvig_the_unreasonable_effectiveness_of/,5.0,0.0,,en
1109223,2012-03-10 05:21:34,statistics,R Tutorials from Universities Around the World,qpuej,brews,1263715301.0,https://www.reddit.com/r/statistics/comments/qpuej/r_tutorials_from_universities_around_the_world/,18.0,3.0,,en
1109224,2012-03-10 05:22:41,rstats,R Tutorials from Universities Around the World,qpug2,brews,1263715301.0,https://www.reddit.com/r/rstats/comments/qpug2/r_tutorials_from_universities_around_the_world/,6.0,0.0,,en
1109225,2012-03-10 16:26:19,statistics,What kind of statistics can I use for western blots?,qqa2v,Banko,1196210985.0,https://www.reddit.com/r/statistics/comments/qqa2v/what_kind_of_statistics_can_i_use_for_western/,2.0,35.0,"I have western blot data where each lane represents a sample pooled from a number of individuals.  I can assume a normal distribution of values among the individuals, however, the value for each individual would be too low to detect, so I cannot actually measure the individuals, only the pools.  The question is, what kind of statistics can I do on this?

The data might look like this, where A to D are each pool: A:10 (n=31), B:17 (n=15), C:8 (n=25), D:12 (n=26).

Can I in any way use statistics to show that the value for B is greater than the others?",en
1109226,2012-03-10 21:57:10,statistics,"Looking for a ""Cool"" Dataset for Multivariate Analysis Project",qqlks,beggarsvelvet,1331408783.0,https://www.reddit.com/r/statistics/comments/qqlks/looking_for_a_cool_dataset_for_multivariate/,8.0,7.0,"I'm a 4th year Statistics major and for my final project for my Multivariate Course I am supposed to independently undertake some analysis (methods we've learned include Factor Analysis, Discriminant Analysis, Logistic Regression, Cluster Analysis) on any dataset of my choosing. I've done a bit of searching on my own for data but am trying to nail down something super interesting. Anyone have any suggestions of datasets that might be interesting for this project and that might lend themselves well to this type of analysis? Thanks!",en
1109227,2012-03-10 23:38:21,statistics,Testing Occupancy Differences Between Seasons,qqpr2,confused_stats,1331412098.0,https://www.reddit.com/r/statistics/comments/qqpr2/testing_occupancy_differences_between_seasons/,2.0,1.0,"I am working on a stats project for a retirement home (also a project for a Stats course). Overall, I am wondering what test I need to run and how I should run it in Minitab. Information below:

What I am trying to test/prove is that there is a significant difference in occupancy during the winter months in comparison to fall, spring and summer (reason: climates change ""drastically"" in this state throughout seasons). I have data for occupancy percentage totals for each week of every month January 2008 - December 2011. Due to my lack of comprehension of the material or incorrectly entered data, I have been unsuccessful in finding the correct test (the professor has provided little help in guiding me towards the right answer).

What I have tried: ANOVA one-way test, Chi-square Goodness of fit, and Chi-square test of independence. One of these still may be correct, however the data may be entered incorrectly.

I have entered the data as follows: There are four columns, Fall, Spring, Summer, Winter. Under each column are occupancy percentages for each week of each respective month for every year 2008-2011. For example, under the column Fall there is occupancy for 9/1/08, 9/8/2008...11/24/08...9/7/09, 9/14/2009...11/23/09.

If anyone has knowledge of statistics and can help with this, it would greatly appreciated. Please let me know if you have any questions or need clarity on anything mentioned above. Again, I am looking to find what test I need to run.",en
1109228,2012-03-11 03:50:53,statistics,Survival Analysis in Finance,qqzs0,sortizo,1288834170.0,https://www.reddit.com/r/statistics/comments/qqzs0/survival_analysis_in_finance/,4.0,4.0,I have a question for anyone related to survival analysis. I am currently taking a course in this subject but the only example we are looking at are applied on life of persons or machines. I was wondering if there is some research papers or books about this branch of statistics but applied to finance. Thanks,en
1109229,2012-03-11 07:12:34,statistics,Understanding Copulas and a Copula Toy Problem,qr73u,caffeine314,1193595677.0,https://www.reddit.com/r/statistics/comments/qr73u/understanding_copulas_and_a_copula_toy_problem/,6.0,4.0,"Can someone please help me understand Copulas better with a toy problem? I think I understand the purpose of a copula. Here it is; hopefully it's correct:

Suppose we have a r.v. named X with a certain distribution, and another r.v. Y with a different distribution. Because we know the distribution of X, we know how to calculate things like:

P(X &lt; x)

and similarly, because we know the distribution of Y, we know how to calculate things like:

P(Y &lt; y)

But just because we know the cdf/pdf of X and Y individually, we don't necessarily know their joint distribution. That is, in general, we don't know how to calculate:

P(X &lt; x and Y &lt; y)

However, we do know how to calculate their marginal distributions:

P(X &lt; x | Y = y)

P(Y &lt; y | X = x)

At this point, we need to consider two cases.

Case 1: X and Y are independent: In this case, knowing the marginals P(X &lt; x | Y = y) and P(Y &lt; y | X = x) does allow us to compute the joint distribution of X and Y. The information contained in the marginals is equivalent to the information in the joint distribution.

Case 2: X and Y are dependent. In this case, the joint distribution contains much more information than the marginals give, so for this case, knowing the marginals is not enough -- they do not give the joint distribution.

How am I doing so far?

A copula is a class of functions that gives us a ""proxy"" joint distribution of X and Y for the case where X and Y are dependent. A few points:

    I suppose you could use a copula to calculate the joint distribution even if X and Y are independent. It'll be the wrong joint distribution in the sense that it'll give you something other than what the marginals give, but you can still calculate it nevertheless.

    If X and Y are dependent, the marginals don't contain enough information to compute the joint distribution, so you have to ""do your best"" to fill in the missing info. This is why a copula is a class of functions. If I give you a set of 10 points, there are many, many, many ways you can connect the dots to form a curve. That's why there's a tremendous number of curve fitting methods. Similarly, there are many different kinds of copulas because the ""fill in"" the missing info in different ways, just like how we can connect two point using a spline, a linear line, an exponential curve, etc.

    I wrote that a copula gives us a ""proxy"" joint distribution because we can't hope to exactly fill in info missing from the marginal distributions in the exactly correct method. The copula gives a method to try to approximate the joint distribution, not to obtain the joint distribution. Hence, it's a proxy.

Did I do ok? If yes, how about a toy problem?

I'd like to find the simplest imaginable toy problem. I need to find two correlated random variables. How about this:

X = Normal(2,3)

Y = X + Normal(4,5)

X and Y are clearly dependent. Can someone pick a copula and show me how to find the joint distribution of X and Y using a copula method?
",en
1109230,2012-03-11 23:29:35,statistics,Anyone familiar with using AIC in stata to determine the best ARMA model?,qryno,not_a_creative_alias,1323759510.0,https://www.reddit.com/r/statistics/comments/qryno/anyone_familiar_with_using_aic_in_stata_to/,7.0,12.0,"I'm new to stata and my instructor didn't really specify how to do this. I'm supposed to build an ARMA(p,q) model and determine p and q using the AIC in stata. Really unfamiliar on how to approach this and I've been googling for the last little while. Can anyone help?",en
1109231,2012-03-12 01:11:37,statistics,This question concerns calculating selection probabilities.,qs375,cherizzle,1331506917.0,https://www.reddit.com/r/statistics/comments/qs375/this_question_concerns_calculating_selection/,1.0,9.0,,en
1109232,2012-03-12 06:10:54,statistics,Probability question related to the subset-sum problem,qsge0,shaggorama,1233555004.0,https://www.reddit.com/r/statistics/comments/qsge0/probability_question_related_to_the_subsetsum/,7.0,1.0,"X-posted to [/r/math](http://www.reddit.com/r/math/comments/qsgfo/probability_question_related_to_the_subsetsum/)

The [subset sum](http://en.wikipedia.org/wiki/Subset_sum_problem) problem can be stated pretty simply as:

*given a set of integers, confirm whether or not there exists a subset that sums to a target value. If this subset exists, what is it?*

For example, if we're looking for a subset that sums to 0 in the set {-1, 3, 5, -4} we find: {5, -1, -4}, but if we look for a subset that sums to -3 we won't find one. Here's what I'm interested in determining: 

***given a range [-m, m], what is the probability that within a set of k randomly drawn ~~values~~ integers from the range (with replacement) there exists a subset that adds up to 0?***

This isn't homework or anything. I'm not even a student. I wrote a python script that solves subset sum problems and stumbled onto this question speed testing it. I've been brushing up on my prob/stats and am curious to see how you guys approach this (assuming this can even be solved analytically).

Since I have the code I can to validate any intuitions you guys might come up with via monte carlo (within reason: the program runs slower as k gets bigger). I've been meaning to make the code available on github; I'll let you know when that code's up if anyone's interested

EDIT: clarified that the set is comprised of integers drawn from the range. Per [G-Brain](http://www.reddit.com/r/math/comments/qsgfo/probability_question_related_to_the_subsetsum/c406hpz), I should probably have defined the interval as: {-m, -m+1, ..., 0, ..., m-1, m}",en
1109233,2012-03-12 07:22:02,statistics,Looking for masters level biostatistics programs,qsj5p,ftcnt,1196008909.0,https://www.reddit.com/r/statistics/comments/qsj5p/looking_for_masters_level_biostatistics_programs/,1.0,6.0,I am looking to start a masters in biostatistics online.  I am looking for recommendations of programs.  Are there any that can be completed in 12 months ?  I also prefer a course based program.,en
1109234,2012-03-12 07:43:47,statistics,What are the most attractive sucker-bets you can think of?,qsjx2,lllusionOfSecurity,1328732365.0,https://www.reddit.com/r/statistics/comments/qsjx2/what_are_the_most_attractive_suckerbets_you_can/,1.0,0.0,,en
1109235,2012-03-12 11:57:27,statistics,Has anyone ever used the programming language Mata in Stata?,qsqfr,adizzan5220,1313616651.0,https://www.reddit.com/r/statistics/comments/qsqfr/has_anyone_ever_used_the_programming_language/,2.0,0.0,"Hi r/statistics

I have this very large dataset that's text rich and contains mostly string variables. My PI is a health economist and wants to do an analysis on this dataset using Stata. However, that's basically impossible to do because of Stata's 244 string character limit. Many of the string variables have observations with a lot more characters and thus the dataset cannot be read in completely. 

A programmer told me about a feature within Stata called Mata. It is a very powerful programming language that compiles code and is based in C. It has a string character limit of 2,147,437,647. That's a lot more than enough. However, the resources online are limited and I am having trouble reading in the dataset into Mata.

The dataset is in a text file that is in pipe delimited (vertical bar) format. I know that I must write a function in Mata to do this, but I have no idea how to approach it in terms of syntax and execution. I was wondering if anyone has had experience with Mata that could provide help/advice.

Thanks for the help.",en
1109236,2012-03-12 18:48:02,MachineLearning,Personalization vs Privacy,qt3ak,towaway1234,1326833129.0,https://www.reddit.com/r/MachineLearning/comments/qt3ak/personalization_vs_privacy/,0.0,2.0,"
I use AdBlock in Chrome and I'm subscribed to the following lists:

Fanboy tracking list, Fanboy annoyance list, Easylist privacy, the main Fanboy list, the main Easylist.

Am I still able to receive customized and personalized content when I use various web apps, such as Google Search? Or does Adblock block webapp attempts to collect data about me, therefore are they unable to offer customized and personalized content to me?

Google Search personalizes content for every user. They do this by collecting as much data about the user as possible. After that they run machine learning algorithms on the data which in the end outputs the user's preferences. In the end, they feed this into their search algorithm that modifies the displayed results in order to suit the preferences of the user more, therefore the user will find the search results more relevant to his interest.

Example: I regularly visits my university's homepage. For example, the university uses Google Analytics to track page views and visitors. The data is sent to the university Analytics account and it is used by Google too. This way, Google can compare my IP address to the IP address that I use to log into their services. Therefore, they can see that I visit my university's homepage a lot, so I must attend it. Then next time I do a search on Computer Programming 101, Google Search will return many results from many universities. However, because Google now knows that I attend that one university, it will modify the search results and display that one on top. Therefore, I get more relevant result for my interests.

Adblock blocks the Google Analytics tracker plugin, which means Google cannot collect data about me. Therefore, they won't know that I go to that one university, so when I search for Computer Programming 101, they will display a lot of results that I'm not looking for. They won't be able to personalize and customize the search results to my interests, since they don't have enough data about me, because Adblock blocks their data collection attempts.

I hope it is clearer now what I meant. There may be a myriad of different scenarios where this happens, I just wrote one off the top of my head.",en
1109237,2012-03-12 21:37:22,datascience,Google Refine query,qtbus,suitablyvague,1309115259.0,https://www.reddit.com/r/datascience/comments/qtbus/google_refine_query/,2.0,2.0,"So Google Refine is pretty amazing. I've been using it on testing datasets and I really like it

Q: I would love to use this for some datasets in work, but because I'm uploading files I'm unclear on the security of my data. Can Google use this data, or can it be accessed by any third party?",en
1109238,2012-03-12 21:56:01,statistics,A Julia version of the multinomial sampler,qtctu,[deleted],,https://www.reddit.com/r/statistics/comments/qtctu/a_julia_version_of_the_multinomial_sampler/,1.0,0.0,,en
1109239,2012-03-12 21:56:50,statistics,A Julia version of the multinomial sampler [r-bloggers],qtcvc,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/qtcvc/a_julia_version_of_the_multinomial_sampler/,1.0,1.0,,en
1109240,2012-03-12 23:34:37,statistics,"Made a subreddit: Statistics of people on omegle, you can suggest surveys!
In need of suggestions :).",qti63,omegle-statistics,1331581941.0,https://www.reddit.com/r/statistics/comments/qti63/made_a_subreddit_statistics_of_people_on_omegle/,2.0,0.0,,en
1109241,2012-03-13 01:23:44,statistics,"Statistical facepalm in the Lancet, of all places (humorous)",qtnrz,BillyBuckets,1315418176.0,https://www.reddit.com/r/statistics/comments/qtnrz/statistical_facepalm_in_the_lancet_of_all_places/,49.0,12.0,,en
1109242,2012-03-13 01:53:24,statistics,"Q: Can we test whether a time series follows a nontrivial conditional distribution with a given form, such that the distribution of each value is conditional on the previous?",qtpb3,dopplerdog,1213964526.0,https://www.reddit.com/r/statistics/comments/qtpb3/q_can_we_test_whether_a_time_series_follows_a/,2.0,8.0,"The Kolmogorov-Smirnov test is a well known test to compare a sample of iid data with a reference distribution, or two samples of iid data.  Unfortunately, one of the requirements is that this data is iid.

Supposing we have a discrete time series of data so that the distribution at any time is conditional on the value at the previous time.  My hypothesis is that the conditional distribution takes a specific form.  However, since the distribution is conditional, the data won't be iid, hence I wouldn't be able to use K-S to test whether my hypothesis should be rejected (i.e. I don't have one reference distribution for all of the data, but a conditional distribution, i.e. a different distribution for each and every value).

Is there any test I could use to verify whether I need to reject my hypothesis?  I'd need something like K-S but where the data is not iid, but where each value is distributed according to a conditional distribution function which depends on the previous value. 

Many thanks in advance.

",en
1109243,2012-03-13 02:40:45,MachineLearning,Ask /r/ML: NLP system for laws relevant to given activity?,qtrp9,astebbin,,https://www.reddit.com/r/MachineLearning/comments/qtrp9/ask_rml_nlp_system_for_laws_relevant_to_given/,3.0,1.0,"Does anyone know whether a natural language-processing system exists to list all laws relevant to an activity specified in a textual query, such as buying a car, owning a pet, going fishing, etc.? I know that a lot of research has done in legal NLP recently, specifically in the field of patent redundancy checking, but I can't seem to find a system fitting this particular description. Thanks!",en
1109244,2012-03-13 03:09:18,computervision,I need guidance on computer-vision techniques for 'detecting' this image. ,qtt3x,Ayakalam,1310746049.0,https://www.reddit.com/r/computervision/comments/qtt3x/i_need_guidance_on_computervision_techniques_for/,2.0,20.0,"
Hi /r/computervision!

I am very glad I found this sub-reddit, and this is my first post here. Please bear in mind, I am a CV noob. 

So, basically this is a machine-vision question, and what I need to do is quite 'simple' - I have this 2-D template, and I would like to 'detect' it if it is present using machine-vision techniques. 

Here is what [I am looking for](http://i.imgur.com/Mhyes.jpg). (ie, 'perfect' image).

Here are the myriad ways in which it can be [received](http://imgur.com/a/AjIYb#0), just to give you an idea.

As you can see:

* Image can be shifted around horizontally, and this will be cyclical. (ie, Fall over the right side and re-appear on the left side, and vice versa).

* Image can be shifted vertically, but will NOT cycle around. (Cant shift up anymore with first ping hitting the top, same for bottom). (This makes the problem easier).

* The 'pings' in the image can vary in thickness. 

In machine-vision lingo I believe this means that the algorithm needs to be:

* Shift Invariant
* Scale Invariant

The good news is that I dont need or care about rotational invariance.

Anyway, I am hoping to get some feedback from others who have experience in the machine-vision field, if you give me some guidance I will most likely be able to take it from there, but I would like to know how best to approach/tackle the problem. For what its worth I will be using open c-v to as well. (I am also an open c-v noob as well). 

Thanks so much in advance - any insights will help! 

P.S. In reality, I will NOT have colors as you see here - I will only be given a matrix of numbers, so I suppose this means 'greyscale'. Thanks.",en
1109245,2012-03-13 05:49:00,MachineLearning,Wondering if someone could point me in the right direction...,qu14e,Rotten194,1298766458.0,https://www.reddit.com/r/MachineLearning/comments/qu14e/wondering_if_someone_could_point_me_in_the_right/,4.0,8.0,"Not sure if this is the right place for this, but here goes...

I'm not very experienced with ML, never taken a class or anything, but am currently working on a project that I wanted to use it in to learn some more about it.

In this project I'm looking to take a smallish set of numbers (~10-20) and tweak them based on binary good/bad feedback. I was wondering if this is possible, and if so if someone could point me at a good book/paper/website to work on something like this? Thanks!",en
1109246,2012-03-13 06:13:19,statistics,What is the difference between binompdf and binomcdf?,qu2a2,TBizzcuit,1270020303.0,https://www.reddit.com/r/statistics/comments/qu2a2/what_is_the_difference_between_binompdf_and/,1.0,1.0,,en
1109247,2012-03-13 12:15:15,artificial,"When creative machines overtake man
",qucyh,marijnfs,1292406668.0,https://www.reddit.com/r/artificial/comments/qucyh/when_creative_machines_overtake_man/,17.0,11.0,,en
1109248,2012-03-13 13:47:54,statistics,Q: Confused about interaction terms. Why don't we square root them?,queyx,[deleted],,https://www.reddit.com/r/statistics/comments/queyx/q_confused_about_interaction_terms_why_dont_we/,5.0,12.0,"Regarding linear regression with interaction terms, for example:

y    =    x1 * b1    +    x2 * b2    +     x1 * x2 * b3

I've thought about interaction terms in linear regression on several occasions and find it difficult to understand their behaviour precisely. I understand that they enable two factors to collectively have either a positive or negative effect on the dependent variable, however, the fact that we are multiplying the values together confuses me. Would this not create a quadratic? Should we always plot say x1*x2 against y to see what the shape is and should we ever use sqrt(x1*x2)? Apologies if this question is wooly. My reason for asking it is that I don't feel entirely comfortable and in control when including interaction terms and want to be aware of any assumptions I am making when using them. Thank for you time.",en
1109249,2012-03-13 19:44:04,statistics,difference between moment-generating-functions and convolutions,qut7z,citrusvanilla,1283309625.0,https://www.reddit.com/r/statistics/comments/qut7z/difference_between_momentgeneratingfunctions_and/,2.0,11.0,"this is introductory prob and stat class with calculus for first year graduate students:

from what i understand, MGFs help us find the 'distributions', specifically the 'moments' that we can interpolate as exp vals and variances.

then, a convolution helps us find the 'density' of a sum of two random variables.

is it not possible to find the density from the MGF as well?  or do the MGFs only give us moments (as in, an unknown density in which we might apply chebyshev's to or something)... thanks brahs",en
1109250,2012-03-13 20:11:29,statistics,"Simple, silly SAS question",quumb,hypermonkey2,1301460132.0,https://www.reddit.com/r/statistics/comments/quumb/simple_silly_sas_question/,2.0,4.0,"Hi All!

Sorry for the noobish question. I'm looking at SAS code for CRD designs with variables Plot and trt (plot and treatment).
in the model statement of PROC GLM, i see use of Plot(trt).
What does this mean exactly?

Thanks very much!",en
1109251,2012-03-13 20:59:45,MachineLearning,"Looking to scrape contact info from mixed raw HTML pages, for indexing into a central database. Anybody faced this problem (or similar) before and have a lead?",qux3f,[deleted],,https://www.reddit.com/r/MachineLearning/comments/qux3f/looking_to_scrape_contact_info_from_mixed_raw/,6.0,15.0,"Title is a pretty good summary. I've got a large number of files I'm responsible for extracting name, email, address, phone number from to build a database for work, and data scraping is a little out of my wheelhouse. The layout of the pages is not all the same, but there are indicators to help locate the pertinent data - the name or an abbreviation of the name is part of the filename, and the phone numbers seem to universally be in the xxx-xxx-xxxx or (xxx) xxx-xxxx format. I wouldn't presume to barge in here and ask y'all to do my work for me - just a point in the right direction would be a blessing. Without prior knowledge, I'm using the shotgun research approach so far, and several hours of Googling has thusfar produced... bupkiss. So, any ideas?
",en
1109252,2012-03-13 22:06:34,statistics,A question of step-wise variable selection in multiple regression...,qv0ph,brews,1263715301.0,https://www.reddit.com/r/statistics/comments/qv0ph/a_question_of_stepwise_variable_selection_in/,1.0,8.0,"I've started to model for the sake of prediction, and so I've started to think about this a bit more than I have in a long time.

How do you guys feel about the various means of selecting terms in simple multiple regression? Any passionate feelings to a particular style? Step-wise AIC seems to get much love (esp. in R), but I still see others using ""F-to-enter; F-to-exit"", and etc.

TL;DR: (Geeze, you're lazy) Anyone have compelling arguments for or against a particular method of automated model fitting?",en
1109253,2012-03-13 23:48:08,statistics,"a sample size of 24 is collected with sample average 47.1. if the population standard deviation is 12, what is the value of the test statistic to test if the population mean equals 50",qv616,[deleted],,https://www.reddit.com/r/statistics/comments/qv616/a_sample_size_of_24_is_collected_with_sample/,1.0,1.0,,en
1109254,2012-03-14 01:24:28,statistics,DAE here work in surveys?,qvazk,[deleted],,https://www.reddit.com/r/statistics/comments/qvazk/dae_here_work_in_surveys/,5.0,15.0,"I may have landed myself an interview with a place I very much would like to work. However, the job description seems to be very survey-centric. While I'm no stranger to opening my big mouth on anything related to statistics, I'm suddenly starting to doubt my ability to work with surveys specifically.

**basics**
Does anyone here work on surveys? What does the work week look like? Are you on a team or mostly alone? 

**stat side**
Do you data clean? What kind of models do you use? Do you feel it's more of a ""data mining"" and exploratory analysis position or are you expected to present more confirmatory type results and reports? Do you see production from the beginning to the end? What's your technical training?

**comp side**
Do you set up online survey tools, if so how is that done? (I do only a marginal amount of webdev, but I understand it can be very tricky to set these things up correctly). What learning tools are there? Any recommended books / websites / forums? ",en
1109255,2012-03-14 03:27:14,statistics,Path analysis versus SEM,qvgva,[deleted],,https://www.reddit.com/r/statistics/comments/qvgva/path_analysis_versus_sem/,2.0,0.0,"Hi!

I'm currently learning SEM, and I was hoping I could get some feedback on the issue of doing a path analysis versus a SEM in a project I'm working on. First, I should note that I understand that a path analysis is a SEM without a measuring model.

So, I developed a CFA model for a theoretical construct I'm working on that is supposed to be associated with a number of variables. These later variables come from published research, and for the most part are themselves factors of models that were found by using either EFAs or CFAs. So, I am considering three options regarding how to proceed (although there are at least one more):

1. Do a path analysis. That is, use the means of the items that load onto the factors I found with my own CFA as well as the means of the items that load onto the factors that other people have discovered in their research.
2. Do a SEM. That is, use my CFA as well as replicate the factors found by other researchers in CFAs without altering these factors at all, and then associate the different variables.
3. Mix of 1 and 2. That is, do a SEM such that I use my CFA, but use the means of the items that load onto the factors that other people have discovered in their research.

I should note that all of these approaches would be consistent with the social sciences theories I'm using, and as such, my current consideration is mostly statistical.

As always, I appreciate whatever feedback I can get.

Best.",en
1109256,2012-03-14 07:32:51,statistics,Got called up for a job involving statistical analysis,qvsjx,fizziks,1277232356.0,https://www.reddit.com/r/statistics/comments/qvsjx/got_called_up_for_a_job_involving_statistical/,9.0,8.0,Hey r/statistics. I got called up today after applying for a job that involves statistical analysis on financial and marketing data as well as price modelling. Anyone out there familiar with this type of work? I'm a recent physics and math graduate and I've had about a year's worth of statistics courses so I covered the basics. I'd like to perhaps refresh/learn some of the material before I go in for an face-to-face interview so I'm more prepared. Any tips on what I should focus on? I'd appreciate your help. ,en
1109257,2012-03-14 08:12:25,statistics,"Distributed data from across the web, mapped and linked for your use!",qvu0m,daledinkler,1199898909.0,https://www.reddit.com/r/statistics/comments/qvu0m/distributed_data_from_across_the_web_mapped_and/,2.0,1.0,,en
1109258,2012-03-14 09:02:23,MachineLearning,[AskML] ML algorithm to grade transcripts of counseling sessions?,qvvkt,hillset,1318127882.0,https://www.reddit.com/r/MachineLearning/comments/qvvkt/askml_ml_algorithm_to_grade_transcripts_of/,1.0,6.0,"I've recently been wondering what it would take to create some sort of program to grade transcripts of students practicing a specific counseling technique in a video taped counseling session.  Basically, I have ~150 videos of students practicing a 10 minute counseling session with a practice client, and I also have a structured form we use to grade these taped sessions.  The form asks the evaluator to rate the student on 14 items (such as expressing empathy or addressing the client's concerns adequately).  Each of these items can be graded as a + (if the student says something that reflects appropriate use of that item, for ex. if the student appropriately expresses empathy), or a - (if the student says something that reflects inappropriate use of that item).  These final + and - counts for each item are then assessed by the evaluator to arrive at an overall score (out of 100) for the video.  I am interested in designing an NLP program that could predict + and - counts for each item on the evaluation form based on transcripts of these counseling sessions, and also in an ML algorithm that could predict an overall score for the video based on the distribution of counts over the 14 items.  I was wondering if anyone on /r/machinelearning could recommend any good books/papers/websites to point me in the right direction or if anyone on reddit was interested in working on this project with me?  Since the data is from students I cant share it but I would love the opportunity to collaborate on the code with a fellow redditor.  Thanks for the help /r/machinelearning!",en
1109259,2012-03-14 16:33:37,artificial,"The great historical debates in AI, mapped onto 7 giant posters.",qw75x,moscheles,1316266003.0,https://www.reddit.com/r/artificial/comments/qw75x/the_great_historical_debates_in_ai_mapped_onto_7/,71.0,13.0,,en
1109260,2012-03-14 16:56:47,statistics,Stats journals are too slow - here's an idea for a fast one,qw83r,t_rex_tullis,1318255134.0,https://www.reddit.com/r/statistics/comments/qw83r/stats_journals_are_too_slow_heres_an_idea_for_a/,12.0,2.0,,en
1109261,2012-03-14 17:15:07,rstats,π Day Special! Estimating π using Monte Carlo,qw8xu,likelihoodtprior,1318629271.0,https://www.reddit.com/r/rstats/comments/qw8xu/π_day_special_estimating_π_using_monte_carlo/,6.0,3.0,,en
1109262,2012-03-14 19:22:49,statistics,Statistics question regarding the proportion of African-American athletes in professional sports,qweyc,Shovelbum26,1328134221.0,https://www.reddit.com/r/statistics/comments/qweyc/statistics_question_regarding_the_proportion_of/,3.0,7.0,"So there was a question [here](http://www.reddit.com/r/Fitness/comments/qw6lm/physical_anthropology_and_fitness/) in [/r/fitness](http://www.reddit.com/r/Fitness/) about whether there are racial advantages in some types of fitness.  I, being an Anthropologist, chimed in about the vaguaries of racial vs. cultural bias towards something (nature vs. nurture).  Someone, rather flippantly, pointed out in the comments that the NBA and NFL are dominated by African-Americans and that actually kind of got me thinking, and my statistics skills began to fail me when trying to answer this issue.  Here is my reasoning:

The issue is that African-Americans are disproportionatly represented in professional sports.   I thought at first that it's not necessary for African-americans to have an innate physical advantage for this to be true.  It could be because of physical **or** cultural bias (or some combination). I'm not an African-american, so I can't speak intelligently about the culture, but let's hypothesize.  It's possible that the African-American culture is very athletically oriented, especially with basketball.  Maybe African-american kids are more likely to grow up watching and loving basketball than caucasian kids.  If so, we'd expect a disproportianate amount of African-americans to be good at basketball compared to caucasians.

In other words, if you pick two kids, one African-american and one caucasian, then the one that is better at basketball will be the one who has practiced more, exercises more, etc.  Because of a cultural bias towards basketball, there is a better than 50% chance that the African-american kid will be better at basketball than the caucasian kid.

However, when I started thinking about professional sports my logic got a little more difficult.  When you look at the NBA, there are only a little over 400 players in the NBA.  When you narrow the sample size from the however many 100's of millions of kids there are in America down to 400 basketball players shouldn't the population statistics overwhelm the cultural bias towards skill at basketball?  Basically, for every one of those 400 NBA players there are tens of thousands who didn't have the physical skills, drive, ambition, body proportions, etc., who didn't ""make it"" in the pros.  

Let me put it this way.  Think of it a pyramid.  At the bottom are all the kids in America.  The next level are the ones who like basketball and play some.  There are fewer of those.  The next level are the ones who really like it and want to get good.  That's even fewer.  The next level are the ones who not only want to be good, but have the drive and determination to practice in a regimented way and push themselves.  Even fewer.  The next level will be the ones that have all that stuff, and also have the correct body proportions to be competitive (long arms, tall, etc.)  At the top is the last group that is the ""whole package"".  They have the drive, ambition, body composition, and physical skill to be professionals.  

Now, because there are so many more caucasian kids, shouldn't the cultural variables (drive, desire, work ethic) be moot?  Those factors remove you from contention early in the process.  Since you have much fewer professional jobs than people who want them you *should* be able to find 10 qualified people (physically and mentally) for every spot, and thus the actual numbers should reflect those in the general population (unless there was some selection preassure in favor of one group or another, like for instance racism against African-americans keeping them out of sports for a long time).

So it seems to me that the disproportiate amount of African-Americans in professional sports show that either 

1) They are being disproportionatly selected from the pool of equally qualified athletes, which would mean there is actually selection *in favor* of African-Americans in sports.  In other words, when scouts pull a marble from the bag of qualified atheletes, there is some factor making the ""African-American"" marbles more likely to be pulled.

Or

2) African-Americans have, as a population, a slightly higher statistical capacity to the skills necessary for basketball.  Basically, they can climb ""higher"" on the pyramid.  In RPG terms the idea would be that while most people cap out at a ""10"" in basketball skills on their character sheet, some people can get an ""11"" and those people tend to be African-Americans.  Thus when you start winnowing down that original pool of a couple hundred million kids when you get to the best of the best, African-Americans have a slightly higher potential physical skill set and thus a higher-than-expected portion of the pro-athlete population

I know statistical reasoning doesn't necessarily follow knee-jerk logical reasoning, so I definitely didn't want to make any claim without getting my statistical logic straight.  

So, is my reasoning off?  Is my idea of statistics incorrect here?  Or does the number of African-Americans in sports, because of statistics, actually prove that there is some physical difference that gives them an advantage at the top of athletic competition?",en
1109263,2012-03-14 21:55:35,AskStatistics,"What are the odds of making a ""perfect rack"" (i.e. all the balls in the correct places for a game to begin) in 8-ball pool without paying attention to what you are doing?",qwmv1,[deleted],,https://www.reddit.com/r/AskStatistics/comments/qwmv1/what_are_the_odds_of_making_a_perfect_rack_ie_all/,2.0,2.0,"So, to be clear: you are playing pool in your local hall, you are racking up for a new game, you are just grabbing balls and stuffing them in the triangle haphazardly. What are the chances of getting (what I am calling) a perfect rack.
You can imagine you are playing with Stripes and Solids or Reds and Yellows, whatever. I am NOT talking about getting the ball numbers in any special 1-15 order, just about getting a correct rack. Here's an example in [spots and stripes](http://www.logosoftwear.com/cgi-images/SP0507.JPG).",en
1109264,2012-03-14 22:06:22,statistics,Bias and covariance explained to an 11-year-old,qwneg,RickWicklin,1327333524.0,https://www.reddit.com/r/statistics/comments/qwneg/bias_and_covariance_explained_to_an_11yearold/,24.0,4.0,,en
1109265,2012-03-14 23:04:32,rstats,Montreal R User Group,qwqez,likelihoodtprior,1318629271.0,https://www.reddit.com/r/rstats/comments/qwqez/montreal_r_user_group/,3.0,0.0,,en
1109266,2012-03-14 23:19:56,MachineLearning,Learning From Data - Online Course from Caltech,qwr8o,lightcatcher,1275521943.0,https://www.reddit.com/r/MachineLearning/comments/qwr8o/learning_from_data_online_course_from_caltech/,45.0,5.0,,en
1109267,2012-03-14 23:55:41,statistics,That’s NOT How the “Law of Large Numbers” Works [r-bloggers],qwt3k,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/qwt3k/thats_not_how_the_law_of_large_numbers_works/,8.0,2.0,,en
1109268,2012-03-15 02:13:36,statistics,"If a redditor can finish this quiz, I will buy them a year of reddit gold + $20",qwzw8,dukesmushbrain,1322562153.0,https://www.reddit.com/r/statistics/comments/qwzw8/if_a_redditor_can_finish_this_quiz_i_will_buy/,0.0,0.0,,en
1109269,2012-03-15 04:21:52,statistics,"Video from my PyCon tutorial on Bayesian statistics (it's three hours, but it flies by!)",qx6ak,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/qx6ak/video_from_my_pycon_tutorial_on_bayesian/,27.0,3.0,,en
1109270,2012-03-15 05:41:53,artificial,Pictures Of Flower Arrangements: 15 Fresh Flower Arrangements,qxa6a,humasaleem,1318048022.0,https://www.reddit.com/r/artificial/comments/qxa6a/pictures_of_flower_arrangements_15_fresh_flower/,1.0,0.0,,en
1109271,2012-03-15 15:04:58,artificial,"UCLA's Judea Pearl, a big brain behind artificial intelligence, wins Turing Award (""Nobel Prize in Computing"")",qxprg,gnikrowten,1145909787.0,https://www.reddit.com/r/artificial/comments/qxprg/uclas_judea_pearl_a_big_brain_behind_artificial/,5.0,1.0,,en
1109272,2012-03-15 15:11:54,MachineLearning,Bayesian statistics tutorial: video from PyCon 2012,qxpzt,AllenDowney,1300587223.0,https://www.reddit.com/r/MachineLearning/comments/qxpzt/bayesian_statistics_tutorial_video_from_pycon_2012/,24.0,0.0,,en
1109273,2012-03-15 16:20:27,statistics,"Why does centering (e.g., mean centering) a variable produce less multicollinearity than not centering?",qxsgf,[deleted],,https://www.reddit.com/r/statistics/comments/qxsgf/why_does_centering_eg_mean_centering_a_variable/,1.0,5.0,"I'm learning about regression right now and have learned that centering predictors reduce the multicollinearity associated with higher order terms.

My professor told us that it's because with an uncentered variable, say, X, there is a high correlation with X^2. However, with a centered variable, say, CX, there is a low correlation between CX and CX^2.

I don't understand how centering the variables would lead to a reduction in correlation between the two variables. Can someone dumb this down and explain it to me? 

Thanks so much for all of your help :)",en
1109274,2012-03-15 17:18:39,statistics,How should relative changes be measured?,qxuxz,RA_Fisher,1299707119.0,https://www.reddit.com/r/statistics/comments/qxuxz/how_should_relative_changes_be_measured/,5.0,8.0,"In my econometrics education I was told about the *trick* of finding relative change by using natural logs (ln(value a) - ln(value b)).  I was generally told that the difference between this method and the ordinary method ((value a - value b)/(value b)) would be small if the values were small, however I shouldn't use the log method if values are large.

However, recently my interest was peaked when I came to a different conclusion in some statistical work than another person that was using the log method (I was using the ordinary method) on the same data.

So then, I set out to inform myself of the difference.

It seems to me that there is a significant flaw in using the ordinary method.  It's asymmetric, 10 is 100% greater than 5, but 5 is only 50% less than 10 using the ordinary method, but 10 is 69 percent greater than 5, and 5 is 69% less than 10 using the log method.

It's clear to me that if, for example, you're averaging relative change of a series over years that the ordinary method will bias the resultant higher.  I came across this paper, which is apparently the seminal paper dealing with the issue: http://www.jstor.org/stable/2683905 . I've also found some other references that analyze the behavior of the two methods when constructing indexes, and it seems as if the issue is settled for indexes.

Does anyone else have comments, thoughts, is there something that I'm overlooking?  Are there instances where the ordinary method is better than the log method?",en
1109275,2012-03-15 18:35:07,statistics,Converting heavy tailed residuals into a normal distribution?,qxyhp,cotacota,1262817707.0,https://www.reddit.com/r/statistics/comments/qxyhp/converting_heavy_tailed_residuals_into_a_normal/,1.0,2.0,"Is there an easy way to fix this issue so I can use normal tests?  Time series data, the original data looks somewhere between a t-distribution and a uniform distribution.",en
1109276,2012-03-15 22:31:14,statistics,Undergrad Research Ideas,qyarq,[deleted],,https://www.reddit.com/r/statistics/comments/qyarq/undergrad_research_ideas/,2.0,9.0,"I'm an undergrad senior studying Linguistics and Computer Science, and I'm hoping to do some stats-related research in the Spring.

The problem is, my stats knowledge is very limited. I've only taken AP Stats (way back in high school) and I'm currently taking an upper-division Intro to Probability Theory course. I find the subject really interesting though and would like to find a way to apply what I've learned.

I was thinking of doing something with [WordNet](http://wordnet.princeton.edu/), or collecting acoustic data to explore something interesting about human speech, but I'm not sure what research questions would be feasible given my background. Any ideas? All suggestions are welcome and greatly appreciated!",en
1109277,2012-03-15 23:08:43,statistics,Trying to understand dummy variables [HELP],qycrd,statsn3wb23,1331845413.0,https://www.reddit.com/r/statistics/comments/qycrd/trying_to_understand_dummy_variables_help/,2.0,12.0,"Hi, I'm attempting to do a regression with categorical predictors. This regression will have multiple categorical predictors, but for the sake of brevity, here is what im trying to do:

y = B*[State] + error

Y is what i am trying to predict (a continuous variable .. which is the dollar change in premiums)
B is the coefficient
State can be any 50 states. CA, AR, AZ, etc. .. This is the challenge. I just dont understand how to do this. Any help or hints would be greatly appreciated!!

Thanks!
",en
1109278,2012-03-16 00:18:51,statistics,Class notes and journal articles from Hadley Wickham's Data Visualization class at Rice University. A good list of resources for anybody wishing to learn more about the subject.,qyge8,bubbles212,1294796168.0,https://www.reddit.com/r/statistics/comments/qyge8/class_notes_and_journal_articles_from_hadley/,23.0,2.0,,en
1109279,2012-03-16 01:09:05,artificial,This is my final paper for a graduate level AI class in which I redefine the Singularity and Intelligence. What do you think? [x-post from r/singularity],qyisj,[deleted],,https://www.reddit.com/r/artificial/comments/qyisj/this_is_my_final_paper_for_a_graduate_level_ai/,2.0,5.0,,en
1109280,2012-03-16 09:46:06,statistics,Anyone recommend any statistics books?,qz4kl,KidLogic,1291849024.0,https://www.reddit.com/r/statistics/comments/qz4kl/anyone_recommend_any_statistics_books/,1.0,0.0,"I am graduating college but really want to be able to manipulate statistics in every day life - of course this requires deeper knowledge of statistics.


I am wondering, is there any ""relaxed"" statistics books that exist that gives tips and tricks into calculating statistics in everyday life? When I say ""relaxed"" I mean a light-hearted, fun book that isnt something I'd be require to buy by one of my professors.

example: http://www.amazon.com/Statistics-You-Cant-Trust-Friendly/dp/0966617150/ref=sr_1_1?ie=UTF8&amp;qid=1331883917&amp;sr=8-1",en
1109281,2012-03-16 15:08:17,MachineLearning,"University researchers are developing a system to help identify people who are behind offensive comments posted on the internet.
",qzbet,fishandchips,1161341942.0,https://www.reddit.com/r/MachineLearning/comments/qzbet/university_researchers_are_developing_a_system_to/,15.0,6.0,,en
1109282,2012-03-16 17:13:19,statistics,How to ask a statistics question,qzg6i,plf515,1271028320.0,https://www.reddit.com/r/statistics/comments/qzg6i/how_to_ask_a_statistics_question/,3.0,0.0,,en
1109283,2012-03-16 17:40:33,rstats,Tutorial for dates and times in R using the {lubridate} package,qzhga,talgalili,1271226645.0,https://www.reddit.com/r/rstats/comments/qzhga/tutorial_for_dates_and_times_in_r_using_the/,6.0,0.0,,en
1109284,2012-03-16 18:40:47,statistics,What should I use to estimate conversion rate based on data?,qzk9b,[deleted],,https://www.reddit.com/r/statistics/comments/qzk9b/what_should_i_use_to_estimate_conversion_rate/,1.0,4.0,"If I I see that from a specific traffic source, visitors convert into registered users at a rate of x%, what is the best method for making a statistical estimate for an interval where to true conversion rate lies?

There are many methods for creating confidence intervals, but most of them rely on several samples from a population. But what if I simply know that say, overall there has been 350 visitors from a particular source, and from that I got 42 registered users - what method do I best use for an estimate for an interval for the true conversion rate? Do I use the standard deviation for the sample somehow? I can provide actual numbers if needed...





",en
1109285,2012-03-16 19:48:21,AskStatistics,"A line in front of movie theater, a free ticket is to given to the first person whose birthday is the same as someone who has already bought a ticket. You choose the position in this line, which position have the largest chance of getting the free ticket?",qznmx,theUENtv,1279767597.0,https://www.reddit.com/r/AskStatistics/comments/qznmx/a_line_in_front_of_movie_theater_a_free_ticket_is/,1.0,4.0,"This came up as a question for a quantitative analyst interview. The answer is 19.61 or 20, but can someone please explain to me how to get this answer? 

I for the life of me cannot figure it out.
Thanks",en
1109286,2012-03-16 22:21:28,statistics,I pick 1000 apples. When I check them I find 120 are bad. What's a 95% estimate for the rate of bad apples and how do I calculate it?,qzv3y,[deleted],,https://www.reddit.com/r/statistics/comments/qzv3y/i_pick_1000_apples_when_i_check_them_i_find_120/,0.0,5.0,"I.e. I want to calculate the rate for the entire population based off of one sample (though the sample can be large - 1000 apples). What's the formula?

Thanks",en
1109287,2012-03-16 22:55:27,MachineLearning,Computer matching wits with humans in crossword tournament,qzwuu,joshdick,1150941224.0,https://www.reddit.com/r/MachineLearning/comments/qzwuu/computer_matching_wits_with_humans_in_crossword/,10.0,0.0,,en
1109288,2012-03-17 00:27:10,MachineLearning,Ask ML: Mean parameterization of exponential families,r01an,the_mind_is_a_sponge,1329345730.0,https://www.reddit.com/r/MachineLearning/comments/r01an/ask_ml_mean_parameterization_of_exponential/,11.0,7.0,"Hello all, I was wondering if someone could help me understand a concept that I just came across when reading a graphical model text by Wainwright and Jordan: http://www.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf

What I'm confused about is how the mean parameters paramaterize a distribution. Do they just replace the canonical parameters in the regular exponential family density function? This concept is introduced in section 3.4 of the text

",en
1109289,2012-03-17 04:37:55,statistics,Player activity &amp; statistics,r0bpb,reseph,1217622280.0,https://www.reddit.com/r/statistics/comments/r0bpb/player_activity_statistics/,4.0,5.0,"Let's say I want to do statistics on a TF2 server. I have the data (the # of players on at one time &amp; the current map, pulled say every 10min) and I want to show some statistics.

I can do statistics on player data easy enough (like the average per hour). But what about map statistics (like which map is unpopular)?

The issue is that the server could be empty for half the day, because people are at work etc. So any maps player during that time could end up ""unpopular"" if I did a straightforward statistic on player-map relations. And at the same time, I can't simply ignore 0 player data because what if a map scares everyone away and 0 players are on for just during that specific map (say a 20min period)?

Suggestions? Or any tips on some nice other statistics I could use?",en
1109290,2012-03-17 05:33:21,statistics,Advice setting up a server for high performance statistical computing,r0dun,BirthDeath,1295140816.0,https://www.reddit.com/r/statistics/comments/r0dun/advice_setting_up_a_server_for_high_performance/,15.0,22.0,"My department's statistical computing facilities are greatly in need of an overhaul.  We currently spend almost $10,000 per year renting space on a computing cluster that is poorly maintained (the administrator for some reason refuses to update R, leading to numerous incompatibility issues with packages).

My classmates and I are trying to convince our department head to invest in our own server.  My department is very small and a good portion of the students are theorists, so only about 10-15 students would be actively submitting jobs. 

I'm trying to put together a proposal to bring to the committee with purchasing power, but my knowledge on servers is rather limited and my numerous web searches have not proven fruitful.

What are the kind of specs I should look for in a server and where are some good places to get price quotes from (I assume I can get some sort of educational discount)?  I want something comparable to the current cluster, which has 8 nodes and 64gb of RAM.  We also have a scarcity of computer access for graduate students, so we'd like to order several thin-clients (or possible use older computers as dummy terminals).    

We would also like the server to be self-administered (i.e. graduate students have administrator privileges), because we have had so many issues in the past with system administrators outright refusing to install updates.  Are there any good web resources for setting up and maintaining such a server?  Is it reasonable that graduate students with no experience in system administration, but a decent knowledge of linux could maintain a server.  Also, is there a preferred distro for server management?

I would also appreciate input on any other alternatives or ideas implemented by other departments.",en
1109291,2012-03-17 05:41:44,statistics,Help with Midpoint,r0e60,[deleted],,https://www.reddit.com/r/statistics/comments/r0e60/help_with_midpoint/,1.0,1.0,,en
1109292,2012-03-18 05:14:30,datasets,20-30 years worth of news articles,r1lqn,skadamat,,https://www.reddit.com/r/datasets/comments/r1lqn/2030_years_worth_of_news_articles/,8.0,4.0,"I'm collaborating with a few other students / a professor to do a financial sentiment research project and we're looking to go back a few decades and observe market / media patterns. I know it'll be hard to find news articles from before 2000 since articles were distributed mostly on paper, but if anybody knew where to start, I'd love to hear!",en
1109293,2012-03-18 05:45:59,statistics,Using SEM with binary dependent variable,r1mxk,[deleted],,https://www.reddit.com/r/statistics/comments/r1mxk/using_sem_with_binary_dependent_variable/,1.0,0.0,"Hi!

I want to build a model whose dependent variable is binary. I know there are some commercial software that can deal with this type of issue, but I would like to use the R sem package to deal with it (This question is not limited to R though). In particular, I read [here](http://faculty.chass.ncsu.edu/garson/PA765/structur.htm) the following:

""Logit and probit scores. With any SEM package, it is possible to calculate logit or probit regression scores for each categorical variable, then use these scores as input to SEM. This method assumes that each such regression is well specified so the categorical variables are well fitted. For further discussion see McCulloch (1994).""

I wonder whether anybody has any thoughts on whether the above approach is likely to work. In particular, what might I want to look at to make sure that it actually works?

As always, thank you in advance.

~Kino",en
1109294,2012-03-18 06:19:04,statistics,Can anyone explain statistical significance? ,r1o3g,Surf_is_up,1332043289.0,https://www.reddit.com/r/statistics/comments/r1o3g/can_anyone_explain_statistical_significance/,1.0,8.0,"I'm just trying to get some guidance on which areas I need to study and brush up on.  I have some survey data and I need to explain whether the differences in some variables are statistically significant.  In detail: I've done a survey and have about 100 observations (of about 20 questions, lip pert scale 1-5).  I've also data on the same question from last year's survey.  So if last years mean was e.g. 3.75 and this years mean is 4.2, how do I determine if this difference is statistically significant? I'll need to do the is calculation for each of the questions, but for purposes of this discussion, we can limit it to just one.

As a part 2 to the question, I also have some scoping data, i.e. the 100 responses are from 2 different sub groups, e.g. Managers and non-managers.  Again, how do I determine if the difference between the two groups is statistically significant?

Edit1: Likert scale, not lippert
First time poster here, any help will be greatly appreciated :)",en
1109295,2012-03-18 08:41:36,artificial,pond instagram scenery beautiful ,r1sl9,wdsfseddd,1332052728.0,https://www.reddit.com/r/artificial/comments/r1sl9/pond_instagram_scenery_beautiful/,0.0,0.0,,en
1109296,2012-03-18 13:05:31,statistics,"This post is dedicated to my beloved professor, Mam Belinda V. De Castro. Please naman oh.. T__T 

Dapat “MAM” yan eh! :))

",r1xd7,ewkywa,1332068541.0,https://www.reddit.com/r/statistics/comments/r1xd7/this_post_is_dedicated_to_my_beloved_professor/,1.0,0.0,,en
1109297,2012-03-18 14:40:35,computervision,This is my first post in ComputerVision and I've just finished my work about Sign language recognition. Hope you like it!,r1yvx,[deleted],,https://www.reddit.com/r/computervision/comments/r1yvx/this_is_my_first_post_in_computervision_and_ive/,6.0,2.0,,en
1109298,2012-03-18 16:21:16,datasets,"Austin, TX Data Portal",r20vh,tinio,1227203752.0,https://www.reddit.com/r/datasets/comments/r20vh/austin_tx_data_portal/,12.0,0.0,,en
1109299,2012-03-18 18:41:03,statistics,Can someone explain the kalman filter to me in layman's terms like im 5?,r25b8,ultraspeedz,,https://www.reddit.com/r/statistics/comments/r25b8/can_someone_explain_the_kalman_filter_to_me_in/,17.0,4.0,I've been using it but I have no idea what im doing...,en
1109300,2012-03-18 18:55:53,statistics,Finding Distribution Function from a data set?,r25uu,loganfrederick,1252032115.0,https://www.reddit.com/r/statistics/comments/r25uu/finding_distribution_function_from_a_data_set/,4.0,5.0,"Hey r/statistics, this isn't a homework problem but is for a research problem at school.

I have a vector of 4000 numbers that are non-normally distributed. My question is, how do I find the function that describes the curve/ the probability density function?

The only technique I really know of for finding the curve of an oddly shaped distribution is polynomial interpolation, but I am concerned about the tail effects and accuracy. I have a friend who also mentioned a Monte Carlo Method might be able to work, but I don't know enough about implementing them.

Any help would be greatly appreciated!",en
1109301,2012-03-18 20:24:17,statistics,SEM with binary dependent variable,r29an,[deleted],,https://www.reddit.com/r/statistics/comments/r29an/sem_with_binary_dependent_variable/,1.0,0.0,"Hi!

Much like with regression, handling binary dependent variables in SEM requires special considerations. In particular, some of these are noted [here](http://faculty.chass.ncsu.edu/garson/PA765/structur.htm) and include:

""1. Polychoric correlation. LISREL/PRELIS uses polyserial, tetrachoric, and polychoric correlations to create the input correlation matrix, combined with ADF estimation (see below), for variables which cannot be assumed to have a bivariate normal distribution.""

* ""Sample size issue. ADF [Asymptotically distribution-free] estimation in turn requires a very large sample size. Yuan and Bentler (1994) found satisfactory estimates only with a sample size of at least 2,000 and preferably 5,000. Violating this requirement may introduce problems greater than treating ordinal data as interval and using ML estimation. This is also a reason cited for preferring the Bayesian estimation approach to ordinal data taken by Amos since Bayesian estimation can handle smaller samples than ML or ADF. ""

I'm currently trying to use the package sem in R to test my model and the author of the model suggests using polychoric correlations [here](http://r.789695.n4.nabble.com/Link-functions-in-SEM-td859182.html). The problems are: 1) I don't know what estimation method is being used with these correlations (i.e., ADF or ML), 2) My sample size is small N = 173, and 3) I'm not familiar with how to interpret polychoric associations (in the case that it is appropriate for me to use them). All the other variables in my model are continuous in nature. Any help and/or links would be greatly appreciated. I'm also considering using other software like OpenMX, but I'm still reading about how it handles binary data. Help with what other software I might want to use would also be appreciated.
",en
1109302,2012-03-19 02:44:34,AskStatistics,Choosing the optimal number in a guessing game,r2qaz,JimboMonkey1234,1316293491.0,https://www.reddit.com/r/AskStatistics/comments/r2qaz/choosing_the_optimal_number_in_a_guessing_game/,7.0,6.0,"Say we have N people who are competing for a prize, where the winner is determined by whoever guesses closest to a hidden number between 0 and 999.  Each player writes down their guess and hands it in at the same time; if there is a winning tie, everyone plays again.

We're one of the players, and we get to choose a strategy.  Everyone else acts as perfect random number generator.

If N = 2, that is, it is us against one other person, the optimal number for us to choose is clearly 500.

If N = 1000, there is no optimal number to choose.  Everyone's on an even playing field.

What about for each N between 3 and some reasonable number, like 20?  What number should we choose in each case for the best odds of winning?

I ran some simulations for this, but I wasn't able to make much sense of the results, other than noticing that, for small N &gt; 2, as N increased the middle position become less advantageous.",en
1109303,2012-03-19 16:51:22,statistics,"Statistical Laws Governing Fluctuations in Word Use
from Word Birth to Word Death",r3hrq,scientologist2,1201635654.0,https://www.reddit.com/r/statistics/comments/r3hrq/statistical_laws_governing_fluctuations_in_word/,6.0,0.0,,en
1109304,2012-03-19 17:29:48,datascience,SAP is the latest addition to the list of vendors integrating with R,r3je6,[deleted],,https://www.reddit.com/r/datascience/comments/r3je6/sap_is_the_latest_addition_to_the_list_of_vendors/,0.0,0.0,,en
1109305,2012-03-19 17:31:28,statistics,Clarifying the two parameter exponential distribution.,r3jgy,topheroly,1245562759.0,https://www.reddit.com/r/statistics/comments/r3jgy/clarifying_the_two_parameter_exponential/,7.0,12.0,"Hello /r/statistics fellows, I usually don't post questions here but this one has me a bit stumped.  I have a client who used miniTAB (lol) to generate scatter plots against a number of different distributions.  The best fit being a two parameter exponential distribution. 


After some initial digging, I haven't found a good resource detailing the MLE, moment-generating-function or other similar properties of what you would expect for any other distribution.  I was wondering if any of you could point me in the right direction.


For those curious why I need these properties.  I was wondering if it was possible to transform a two-parameter exponential to an erlang and run simulations from there in SAS (what the client was wanting to do).  Thanks for any insight.",en
1109306,2012-03-19 17:39:47,datascience,SAP is the latest addition to the list of vendors integrating with R,r3jub,[deleted],,https://www.reddit.com/r/datascience/comments/r3jub/sap_is_the_latest_addition_to_the_list_of_vendors/,0.0,0.0,,en
1109307,2012-03-19 21:50:18,datasets,Request: dataset for detection of community structure,r3wb4,sindrome198,1306525765.0,https://www.reddit.com/r/datasets/comments/r3wb4/request_dataset_for_detection_of_community/,2.0,3.0,"Hi,

I am trying to use a few community detection algorithms to try and analyze the different run times of these algorithms. But before I do that, I want a dataset where I can try and predict the ""communities"" A priori.

Would anyone have an idea where to find a dataset/have one? Any help would be appreciated!

Thanks ",en
1109308,2012-03-19 22:08:52,datascience,SAP is the latest addition to the list of vendors integrating with R,r3xae,[deleted],,https://www.reddit.com/r/datascience/comments/r3xae/sap_is_the_latest_addition_to_the_list_of_vendors/,0.0,0.0,,en
1109309,2012-03-19 22:11:33,statistics,"An interesting contribution to the Bayesian Vs. Frequentist debate 

[Revisiting the Berger location model: Fallacious confidence interval or a rigged example?
Aris Spanos]",r3xfw,anonemouse2010,1262468532.0,https://www.reddit.com/r/statistics/comments/r3xfw/an_interesting_contribution_to_the_bayesian_vs/,7.0,1.0,,en
1109310,2012-03-19 23:26:52,statistics,"Solving ""find the next number in the sequence"" problem using R and stats [r-bloggers]",r41gw,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/r41gw/solving_find_the_next_number_in_the_sequence/,12.0,1.0,,en
1109311,2012-03-19 23:27:13,MachineLearning,"Solving ""find the next number in the sequence"" problem using R and stats [r-bloggers]",r41ho,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/r41ho/solving_find_the_next_number_in_the_sequence/,1.0,0.0,,en
1109312,2012-03-20 00:29:36,statistics,Simple functional programming in R,r44q2,michaeldbarton,1283997895.0,https://www.reddit.com/r/statistics/comments/r44q2/simple_functional_programming_in_r/,12.0,1.0,,en
1109313,2012-03-20 04:57:52,statistics,Whenever I do my statistics homework (X-Post from r/funny),r4imn,DopamineDomain,1269293819.0,https://www.reddit.com/r/statistics/comments/r4imn/whenever_i_do_my_statistics_homework_xpost_from/,1.0,0.0,,en
1109314,2012-03-20 05:17:42,rstats,Regression by means of polyserial correlations in R,r4jn7,lukis100,1232427503.0,https://www.reddit.com/r/rstats/comments/r4jn7/regression_by_means_of_polyserial_correlations_in/,2.0,4.0,"Hi!

I have two continuous independent variables and a binary dependent variable, and I want to use regression by means of polyserial correlation, but I can't find any information on how to do this in R. I know the alternative is to use logistic regresssion, but the problem that I'm trying to solve necessitates that I use polyserial correlation. Any help would be much appreciated.

Best,

~Kino",en
1109315,2012-03-20 05:59:36,statistics,"Reddit, please help me out with what statistical test to use...",r4lps,[deleted],,https://www.reddit.com/r/statistics/comments/r4lps/reddit_please_help_me_out_with_what_statistical/,3.0,4.0,"Hello /r/statistics! I am (somewhat of) a statistics newbie and I need some help with a good test to use for my problem. Try to help me out if you can! Here it is:

I am trying to run Monte Carlo simulations for an experiment where I choose a sub-sample (n=100) randomly from a total of 1000 observational data points (repetition allowed) in a time series. I do the same for a different (independent) portion of the time series (n=100, total=1000). 

**What is a good test to compare the std. dev. and means of the 100 chosen values and tell if they are significantly different (within error) for each MC simulation?**

Thanks!",en
1109316,2012-03-20 10:08:08,statistics,Stuck at a simple analysis,r4u01,throwawaynever,1298748938.0,https://www.reddit.com/r/statistics/comments/r4u01/stuck_at_a_simple_analysis/,3.0,9.0,"Hi r/statistics!
I have been struggling to figure out how to work out the statistics for an experiment, and have spent fruitless days scouring the web with no reward. I turn to you in my hour of need.

My experiment is very simple, and I need to analyse the results. I grew a selection of plants at 5 concentrations of a specific nutrient solution. I measured a range of variables- leaf number, plant height etc.

I'm using Statistica to work out which of the treatments had the greatest effects upon the plants. So far I have used  one way ANOVA. When I run the analysis I get an F value and a P value. What do I do with those? What does the F- value mean, and how do I report it? Same with the P value?

I have spent days on the interwebs looking around, but am just confusing myself more and more. any clarity would be greatly appreciated.

EDIT: Thanks all for your input- I can see its going to take a while before I'm completely confident in my statistics!",en
1109317,2012-03-20 11:50:28,statistics,SAS vs R: Which one is more useful and why?,r4w0q,SlowBullets,1325766081.0,https://www.reddit.com/r/statistics/comments/r4w0q/sas_vs_r_which_one_is_more_useful_and_why/,17.0,26.0,"I am currently debating on whether or not I want to start learning R.

I'm somewhat familiar with SAS and have done fairly well in any assignments/projects whenever given in school. As far as I know, the field where you work depends on whether SAS or R is used. I feel like R would be useful to learn, but the only downside is that unlike SAS, it has no certification program.

Anyways, what are the pros and cons of both? Furthermore, should I master both programs or just focus on one?

What are good ways to self study either SAS or R? Did you use books? The Internet? Prep classes? In my university classes, my teacher did a rather poor job explaining the material to us. Not only that, the material was easy to say the least. I want to study more challenging stuff! I know SAS is not this easy. :P 


Thanks a lot!",en
1109318,2012-03-20 13:42:56,statistics,"Looping over space delimited elements of a user 
defined string vector in SPSS",r4ybx,kotzwagon,1313137652.0,https://www.reddit.com/r/statistics/comments/r4ybx/looping_over_space_delimited_elements_of_a_user/,3.0,10.0,"Hello /r/statistics,

I seldom use SPSS (preferring to use Stata). I am unfamiliar with the syntax and I have no desire to invest the time required to use SPSS for this single task. It is for this reason that I turn to you.

I have downloaded a set of raw data files (with fixed column widths) and no dictionary files, but I have SPSS code that converts those files into *.sav format individually. This code looks a bit like this.

    data list file =  [data file type][country].dat
    [variable labelling]
    [value labelling]
    [coding missing values]
    [etc.]
    save outfile = [data file type][country].sav

The data is from the [TIMSS](http://nces.ed.gov/timss/index.asp). Each country has up to three ""populations"" for each wave, each with seven or so different data file types which have their own variable and value labelling conventions. Each program provided by ""the TIMSS people"" has its own year and datafile type specific convention. I don't mind manually running each type of program for each population, but I really **DO** mind manually entering in the three word country-code infixes that specify the data files. That is, I would like to loop the same program over the entire set of countries for each data file type, population and wave.

In Stata, I would find this quite easy to do. I would simply pull a list of the files in the folder, extract the country code infix from those filenames, transpose the list of country codes in excel and copy them into a do-file to define a local Stata ""macro"" (over which I would loop the provided command). Like this...

    local countries = ""AUS AUT BFL BFR BGR CAN CHE ...""
    foreach country of local countries{
        use [datafiletype]`country'.dat, clear ***(if opening a *.dat file like this was possible)
        [data processing code]
        save [datafiletype]`country'.dta, replace
    }

How can I do something similar in SPSS?

BELOW IS AN EXAMPLE OF ONE OF THE SPSS PROGRAMS PROVIDED:

     /* Codebook, Date:                  15.09.97  File: ACTCODE.SDB */ 
    
    *NOTE: Assignment of variable names. 
    data list file =""ACT[country]1.DAT""
    /         VERSION   1    -  2
              IDCNTRY   3    -  5
              IDPOP     6    -  6
              IDSTRAT   7    -  9
              IDSCHOOL  10   -  15
              ITROTAT   16   -  16
              ITPAS1    17   -  17
              ITPAS2    18   -  18
              ITPAS3    19   -  19
              ITPAS4    20   -  20
              ITPAS5    21   -  21
              ITPAS6    22   -  22
              ITPAS7    23   -  23
              ITPAS8    24   -  24
              DPCDATE   25   -  30  .
    
    *NOTE: Assignment of variable labels. 
    variable labels        VERSION   ""*FILE VERSION*""
                          /IDCNTRY   ""*COUNTRY ID*""
                          /IDPOP     ""*POPULATION ID*""
                          /IDSTRAT   ""*STRATUM ID*""
                          /IDSCHOOL  ""*SCHOOL ID*""
                          /ITROTAT   ""*ROTATION SCHEME*""
                          /ITPAS1    ""*ADEQUACY OF ROOM*""
                          /ITPAS2    ""*TIME ALLOTED PER STATION*""
                          /ITPAS3    ""*ADEQUACY OF EQUIPEMENT AND MATERIAL*""
                          /ITPAS4    ""*MISSING INFORMATION FROM MANUAL*""
                          /ITPAS5    ""*CLEARNESS OF INSTRUCTIONS*""
                          /ITPAS6    ""*DIFFICULTY OF PERFORMANCE TASKS*""
                          /ITPAS7    ""*STUDENTS ATTITUDE TOWARDS ASSESSMENT*""
                          /ITPAS8    ""*STAFFS ATTITUDE TOWARDS ASSESSMENT*""
                          /DPCDATE   ""*FILE CREATION DATE AT DPC HAMBURG*""  .
    
    
    VALUE LABELS
         IDPOP          1    ""population 1""
                        2    ""population 2""
                        /
         ITROTAT        1    ""rotation 1""
                        2    ""rotation 2""
                        9    ""missing""
                        8    ""not admin.""
                        /
         ITPAS1         1    ""inadequate""
                        2    ""adequate""
                        3    ""excellent""
                        9    ""missing""
                        8    ""not admin.""
                        /
         ITPAS2         1    ""too short""
                        2    ""just about right""
                        3    ""too long""
                        9    ""missing""
                        8    ""not admin.""
                        /
         ITPAS3         1    ""inadequate""
                        2    ""adequate""
                        3    ""excellent""
                        9    ""missing""
                        8    ""not admin.""
                        /
         ITPAS4         1    ""yes""
                        2    ""no""
                        9    ""missing""
                        8    ""not admin.""
                        /
         ITPAS5         1    ""unclear""
                        2    ""clear""
                        3    ""very clear""
                        9    ""missing""
                        8    ""not admin.""
                        /
         ITPAS6         1    ""easy""
                        2    ""just about right""
                        3    ""difficult""
                        9    ""missing""
                        8    ""not admin.""
                        /
         ITPAS7         1    ""positive""
                        2    ""neutral""
                        3    ""negative""
                        9    ""missing""
                        8    ""not admin.""
                        /
         ITPAS8         1    ""positive""
                        2    ""neutral""
                        3    ""negative""
                        9    ""missing""
                        8    ""not admin.""
                        /  .
    
    
    *RECODE of sysmis-values. 
    RECODE      IDSTRAT    (999             = sysmis).
    RECODE      ITROTAT    (9               = sysmis).
    RECODE      ITPAS1     (9               = sysmis).
    RECODE      ITPAS2     (9               = sysmis).
    RECODE      ITPAS3     (9               = sysmis).
    RECODE      ITPAS4     (9               = sysmis).
    RECODE      ITPAS5     (9               = sysmis).
    RECODE      ITPAS6     (9               = sysmis).
    RECODE      ITPAS7     (9               = sysmis).
    RECODE      ITPAS8     (9               = sysmis).
    RECODE      DPCDATE    (999999          = sysmis).
    
    
    *RECODE of other missing-values. 
    MISSING value  IDSTRAT   (998 ).
    MISSING value  ITROTAT   (8 ).
    MISSING value  ITPAS1    (8 ).
    MISSING value  ITPAS2    (8 ).
    MISSING value  ITPAS3    (8 ).
    MISSING value  ITPAS4    (8 ).
    MISSING value  ITPAS5    (8 ).
    MISSING value  ITPAS6    (8 ).
    MISSING value  ITPAS7    (8 ).
    MISSING value  ITPAS8    (8 ).
    MISSING value  DPCDATE   (999998 ).
    
    
    SAVE OUTFILE = ""ACT[country]1"".
    ",en
1109319,2012-03-20 15:23:27,datasets,The China Study,r513d,napperc,1331140995.0,https://www.reddit.com/r/datasets/comments/r513d/the_china_study/,3.0,0.0,,en
1109320,2012-03-20 16:36:27,statistics,ggplot2 chart for Senior Thesis --- I think I've got it!,r53ri,cvilhelm,1206136191.0,https://www.reddit.com/r/statistics/comments/r53ri/ggplot2_chart_for_senior_thesis_i_think_ive_got_it/,13.0,11.0,,en
1109321,2012-03-20 16:36:48,statistics,808 video hockey games completed - have a lot of stats compiled from these games and yearning for new ideas...,r53rz,randude,1286197010.0,https://www.reddit.com/r/statistics/comments/r53rz/808_video_hockey_games_completed_have_a_lot_of/,9.0,0.0,"http://randude.com/ps3/2_buds_hockey_stats.xlsx

Current tabs are:

NHL12 Summary

NHL12 Stats

NHL12 Games

NHL 12 Summary - last X games (change the value in cell B1 to recalculate the whole sheet - PS, there's still one section on this tab i cant get to calculate out right)

",en
1109322,2012-03-20 21:33:04,computervision,How would you detect him?,r5ign,CmdrSammo,,https://www.reddit.com/r/computervision/comments/r5ign/how_would_you_detect_him/,18.0,24.0,,en
1109323,2012-03-20 22:09:38,MachineLearning,Modeling and Reasoning with Bayesian Networks,r5keg,chris_nh82,1297997827.0,https://www.reddit.com/r/MachineLearning/comments/r5keg/modeling_and_reasoning_with_bayesian_networks/,8.0,5.0,"I just purchased Modeling and Reasoning with Bayesian Networks by Adnan Darwiche. So far I'm finding it very well written and readable. I'd like to do some of the exercises to confirm my understanding, but I can't seem to find solutions so I can check my answers. Anyone happen to know if they are available anywhere? A teacher's guide perhaps?

Thanks",en
1109324,2012-03-20 23:20:19,statistics,What's a good way to brush up on some Stats?,r5o7b,wakipaki,1321843628.0,https://www.reddit.com/r/statistics/comments/r5o7b/whats_a_good_way_to_brush_up_on_some_stats/,3.0,7.0,"My boss has told me to incorporate more statistical analysis at my position.  I'm fairly rusty on a lot of my subjects.  Any suggestions as to where I can brush up on it?

I guess it's important to mention I learn best visually so instructional youtube videos are super helpful.",en
1109325,2012-03-21 02:15:12,statistics,"ANOVA prayer. (AKA, satire on this 0.05 threshold)",r5xmp,SigmaStigma,1323358797.0,https://www.reddit.com/r/statistics/comments/r5xmp/anova_prayer_aka_satire_on_this_005_threshold/,18.0,16.0,"&gt;Oh blessed ANOVA.
We pray that thine p-value might be below mine threshold and not lay victim to the devil's error.

As one whose views don't lie firmly within the frequentist camp, I often find it tiring to hear people pooh-pooh findings when they see that a p value is not below 0.05. In ecological sciences this is especially true, where 0.1 is generally accepted, and even then, a 0.12 is viewed with only a bit of skepticism. It's near impossible to control for everything, and while math may follow strict rules, biotic/abiotic interactions may not.

So hopefully it gave someone a chuckle, and next time, remember 0.05 is not a law. It's a guideline.",en
1109326,2012-03-21 03:52:41,MachineLearning,"""Second Chance"" Machine March Madness Competition",r62q7,danger_t,1249412021.0,https://www.reddit.com/r/MachineLearning/comments/r62q7/second_chance_machine_march_madness_competition/,0.0,0.0,,en
1109327,2012-03-21 07:46:38,computervision,anyone knows how to choose among different L1-minimization methods?,r6dtv,katahe,1319467861.0,https://www.reddit.com/r/computervision/comments/r6dtv/anyone_knows_how_to_choose_among_different/,2.0,0.0,"I am recently dealing with sparse coding problems. In order to recover the sparse representation of a certain vector y we have to apply some special algorithms here called L1-minimization.
    **y=A*x**

But there are many algorithm to apply here:


*Orthogonal Matching Pursuit*

*Primal-Dual Interior-Point Method*

*Gradient Projection*

*Homotopy*

*Polytope Faces Pursuit*

*Iterative Thresholding*

*Proximal Gradient*

*Primal Augmented Lagrange Multiplier*

*Dual Augmented Lagrange Multiplier*


and even more.
But I don't know which to choose!
any one can help me get through this?",en
1109328,2012-03-21 08:20:21,statistics,Question about sample correlation between two stochastic processes.,r6ewu,brownck,1268955199.0,https://www.reddit.com/r/statistics/comments/r6ewu/question_about_sample_correlation_between_two/,5.0,5.0,"Suppose  have two discrete stochastic processes, or two random vectors if you like, [;X(t,\omega);] and [;Y(t,\omega);] for [;t=1,...,N;], where [;\omega;] is a sample event. I only get to see one realization (one sample event [;\omega_1;]),  from each stochastic process [;X(t,\omega_1)=x(t);] and  [;Y(t,\omega_1)=y(t);] for [;t=1,...,N;]. How do I compute the correlation between [;X(t);] and [;Y(t);] from only  [;x(t);] and  [;y(t);] ?

The exact correlation is defined by 

[;E[(X(t)-\mu_t)(Y(t)-\mu_t)]/(\sigma_X \sigma_Y);].

Is the sample correlation just

[; \sum_{t=1,..,N} ( x(t)-\bar{x(t)})(y(t)-\bar{y(t)})/(\sigma_x \sigma_y);]

Is this a biased or unbiased estimator?",en
1109329,2012-03-21 10:34:42,MachineLearning,Proper method for Calculating Similarity Between Two Similarity Matrices?,r6i99,thornpyros,1315576672.0,https://www.reddit.com/r/MachineLearning/comments/r6i99/proper_method_for_calculating_similarity_between/,19.0,8.0,"We want to calculate how similar two *similarity* matrices are. First of all I clarify some points. We have two similarity matrices which contain same set of items for which we gather information from two different sources.

You can assume that similarity matrices contain pairwise closeness of two people. These closeness values come from two different sources such as Facebook and MySpace.

Let's give you an example to illustrate what I want to say.

We have 4 people: John, Paul, George, Ringo. All they have Facebook and MySpace accounts.

From their Facebook profiles, we made some processing and somehow we evaluated the closeness of these people as follows.

* Paul - John = 0.5
* Paul - George = 0.4
* Paul - Ringo = 0.6
* John - George = 0.6
* John - Ringo = 0.5
* George - Ringo = 0.6

From MySpace,

* Paul - John = 0.2
* Paul - George = 0.3
* Paul - Ringo = 0.4
* John - George = 0.2
* John - Ringo = 0.1
* George - Ringo = 0.15

From informations above, we can easily create two matrices both of which are symmetric with respect to the main diagonal. Our ultimate goal is to calculate how similar these two matrices are.

1- What if I align all those values (of course only one triangular for each matrix) in one row (like below) and simply evaluate Pearson Correlation Coefficient between
these rows?

* facebook = [0.5, 0.4, 0.6, 0.6, 0.5, 0.6]
* myspace  = [0.2, 0.3, 0.4, 0.2, 0.1, 0.15]

You may have noticed that these similarities are **not independent** of each other. So, is it enough or appropriate method to calculate similarity between two matrices?
According to an article named Mandel Test on [Wikipedia](http://en.wikipedia.org/wiki/Mantel_test) this simple procedure is not appropriate.

&gt;Because distances are not independent of each other – since changing the ""position"" of one object would change n − 1 of these distances (the distance from that object to each of the others) – we can't assess the relationship between the two matrices by simply evaluating the correlation coefficient between the two sets of distances and testing its statistical significance. The Mantel test deals with this problem.

2- How about the rank correlation techniques such as Kendall?

3- Is there more specific or proper technique to evaluate this? 

4- Mantel Test calculates correlation between two matrices. It seems it is proper for our purpose. I will read [Spatial Analysis in Ecology - Mantel's Test](http://www.nceas.ucsb.edu/scicomp/Dloads/SpatialAnalysisEcologists/SpatialEcologyMantelTest.pdf) which can be reached by Wikipedia but could anyone who use this method for same reason share his/her experience?",en
1109330,2012-03-21 10:44:50,statistics,Proper method for Calculating Similarity Between Two Similarity Matrices?,r6igq,thornpyros,1315576672.0,https://www.reddit.com/r/statistics/comments/r6igq/proper_method_for_calculating_similarity_between/,5.0,3.0,"We want to calculate how similar two *similarity* matrices are. First of all I clarify some points. We have two similarity matrices which contain same set of items for which we gather information from two different sources.

You can assume that similarity matrices contain pairwise closeness of two people. These closeness values come from two different sources such as Facebook and MySpace.

Let's give you an example to illustrate what I want to say.

We have 4 people: John, Paul, George, Ringo. All they have Facebook and MySpace accounts.

From their Facebook profiles, we made some processing and somehow we evaluated the closeness of these people as follows.

* Paul - John = 0.5
* Paul - George = 0.4
* Paul - Ringo = 0.6
* John - George = 0.6
* John - Ringo = 0.5
* George - Ringo = 0.6

From MySpace,

* Paul - John = 0.2
* Paul - George = 0.3
* Paul - Ringo = 0.4
* John - George = 0.2
* John - Ringo = 0.1
* George - Ringo = 0.15

From informations above, we can easily create two matrices both of which are symmetric with respect to the main diagonal. Our ultimate goal is to calculate how similar these two matrices are.

1- What if I align all those values (of course only one triangular for each matrix) in one row (like below) and simply evaluate Pearson Correlation Coefficient between
these rows?

* facebook = [0.5, 0.4, 0.6, 0.6, 0.5, 0.6]
* myspace  = [0.2, 0.3, 0.4, 0.2, 0.1, 0.15]

You may have noticed that these similarities are **not independent** of each other. So, is it enough or appropriate method to calculate similarity between two matrices?
According to an article named Mandel Test on [Wikipedia](http://en.wikipedia.org/wiki/Mantel_test) this simple procedure is not appropriate.

&gt;Because distances are not independent of each other – since changing the ""position"" of one object would change n − 1 of these distances (the distance from that object to each of the others) – we can't assess the relationship between the two matrices by simply evaluating the correlation coefficient between the two sets of distances and testing its statistical significance. The Mantel test deals with this problem.

2- How about the rank correlation techniques such as Kendall?

3- Is there more specific or proper technique to evaluate this? 

4- Mantel Test calculates correlation between two matrices. It seems it is proper for our purpose. I will read [Spatial Analysis in Ecology - Mantel's Test](http://www.nceas.ucsb.edu/scicomp/Dloads/SpatialAnalysisEcologists/SpatialEcologyMantelTest.pdf) which can be reached by Wikipedia but could anyone who use this method for same reason share his/her experience?",en
1109331,2012-03-21 12:46:55,analytics,"My first app for real-time stats of your online activity. Track visits on any sites you want (you don't have to own them), provided that you can paste image from url on them - Need feedback from bloggers, webdevelopers, guys who sell stuff online, and those who put a lot of content online.",r6kui,MichaelStar,1332102409.0,https://www.reddit.com/r/analytics/comments/r6kui/my_first_app_for_realtime_stats_of_your_online/,0.0,0.0,,en
1109332,2012-03-21 16:08:41,statistics,Model distribution for overdispersed count data?,r6qid,thrope,1275749746.0,https://www.reddit.com/r/statistics/comments/r6qid/model_distribution_for_overdispersed_count_data/,1.0,0.0,"I am looking for a model distribution to use for overdispersed count data (fano's between 1.1-3), that can be fit efficiently (ideally from basic statistics without needing a numerical optimisation). I am using negative binomial at the moment but the maximum likelihood optimisation is going to be a real bottleneck for me.

Any ideas?

[EDIT: this is for use within leave-one-out crossvalidation for a classifier so if it was easy to update the parameters for a missing data point that would be very handy too!]",en
1109333,2012-03-21 17:28:42,statistics,Repeated Measure Anova Question,r6tyv,ScientiaNeuro,1304536174.0,https://www.reddit.com/r/statistics/comments/r6tyv/repeated_measure_anova_question/,4.0,15.0,"I am in the process of analyzing one of my research projects for my dissertation.  Right now I have 4 separate groups measured over 12 time points (~12 weeks).  I have been trying various post-hoc tests in SPSS but I am having trouble finding a test to find significance between groups at each time point.  I.e. Is group two significantly different than group one, two, or three at 2 weeks.  
Should this comparison be done on a week by week based approach with a one way ANOVA? Is there another approach or post-hoc test I should be looking into?",en
1109334,2012-03-21 18:44:37,statistics,The Joys Of Stats (Documentary),r6xpx,Sahio,1320404634.0,https://www.reddit.com/r/statistics/comments/r6xpx/the_joys_of_stats_documentary/,27.0,2.0,,en
1109335,2012-03-21 20:21:41,MachineLearning,Question about gradient descent,r72vp,radicality,1278153627.0,https://www.reddit.com/r/MachineLearning/comments/r72vp/question_about_gradient_descent/,0.0,5.0,"Hi guys! I have an exam in ML tomorrow, and thought you guys maybe might help me with a question. It's not a specific problem from an exercise sheet, but I'm a bit confused nevertheless. I've already typed it out, [here it is](http://math.stackexchange.com/questions/122977/batch-vs-incremental-gradient-descent) Thanks",en
1109336,2012-03-21 23:00:16,computervision,Ask CV: How to approach the problem of detecting similarly-shaped structures in microscope slide images?,r7b7u,astebbin,,https://www.reddit.com/r/computervision/comments/r7b7u/ask_cv_how_to_approach_the_problem_of_detecting/,3.0,4.0,"An archaeology professor at my university would like to at least partially automate the process of finding certain structures in images of microscopic slides. He has hundreds if not thousands of slides, and it takes perhaps ten minutes to perform a cursory human visual analysis of each slide, so the benefit of even a semi-accurate CV system for this task would be enormous. The specific type of object in question is phytoliths (examples: https://www.google.com/search?q=phytoliths&amp;tbm=isch), which as I understand them, are basically empty cell wall structures left behind by decomposed plant matter. These phytoliths are somewhat shape-variable within several well-defined categories of shape (e.g. sharply rectangular, dumbbell-shaped, circular, etc.), are generally distinguishable from the background by their high-contrast edges, and tend to fall within a fairly well-defined range of bounding box sizes. Also, the slide images are RGB color images, and phytoliths tend to be a certain shade of brown whereas the background is generally transparent.

My question is this: what sort of general CV techniques would apply to this problem? I'm more familiar with techniques and problems in other specific areas of CV (ex. people tracking, object tracking in video, and OCR), and not so familiar with the problem of general object detection in single images. My current plan is to use a sliding-window approach across all parts of images within a narrow portion of scale space (where the slide magnification is known, allowing me to tailor window size), and build a simple object-of-interest detector with a linear SVM using edge response features and color histograms. I'll likely also give SIFT and SURF a shot, as implementation-wise they're essentially free to try with OpenCV. My training and testing data sets are fairly small at the moment, but will hopefully grow in the future as more slide images are collected. I'd eventually hope to package this detector in a GUI program where the user could rapidly evaluate and label potential phytolith structures picked out by the algorithm, for online algorithm accuracy enhancements through re-training.

What does /r/CV think? Am I on the right track? Are there other features or detection methods which could be more useful here? Features would ideally be rotation-invariant, but not necessarily scale-invariant (though I don't see how scale-invariance would hurt). Any feedback would be appreciated. Thanks!",en
1109337,2012-03-22 00:21:13,datasets,Looking for Diabetes Public Database.,r7fc6,i_314,1332093544.0,https://www.reddit.com/r/datasets/comments/r7fc6/looking_for_diabetes_public_database/,1.0,2.0,"Hello, 
Do you know about any available and public Diabetes database?
Something similar the NCI is doing with Cancer: (http://www.cancer.gov/statistics/tools)
I´m developing an Analytics tool (hopefully Open Source) with real data at a hospital, and I would like to compare and improve it against a more complet dataset.
The UCI Machine Learning Repository: Pima Indians Diabetes Data Set is not suitable for my purpose.
Thank you very much.
",en
1109338,2012-03-22 02:12:30,AskStatistics,reddit fact check request. please and thank you.,r7ktl,[deleted],,https://www.reddit.com/r/AskStatistics/comments/r7ktl/reddit_fact_check_request_please_and_thank_you/,0.0,1.0,"[http://www.reddit.com/r/funny/comments/r70ou/kulula_airlines_doesnt_take_themselves_very/c43ipen](http://www.reddit.com/r/funny/comments/r70ou/kulula_airlines_doesnt_take_themselves_very/c43ipen)

""Did you know if you shuffle a deck of playing cards randomly, it is highly probable that that combination of cards has never existed in any other deck of cards before?""",en
1109339,2012-03-22 03:14:28,statistics,JMP and Graeco-Latin Squares,r7nxj,ortl,1321578118.0,https://www.reddit.com/r/statistics/comments/r7nxj/jmp_and_graecolatin_squares/,0.0,1.0,"For an exercise we are suppose to generate Graeco-Latin squares (and hope for orthogonality), specifically 10x10 ones.  Is it possible to get JMP to only generate orthogonal 10x10 Graeco-Latin squares?  I am able to generate non-orthogonal ones.

Edit: clarity.",en
1109340,2012-03-22 05:19:44,statistics,Question about probability of not getting your significant other pregnant. (Serious nerdy question),r7u7g,[deleted],,https://www.reddit.com/r/statistics/comments/r7u7g/question_about_probability_of_not_getting_your/,3.0,7.0,"So I learned a while back that if you wanted to calculate the probability of rolling a ""2"" on a die 6 times in a row it would be (1/6)^6

My friend recently impregnated his girlfriend who is on birth control, and being the good friend that I am, I decided to tell him what his odds were of getting her pregnant. Problem is something in my gut is telling me I calculated it wrong. Here it is: 

Assume birth control is taken effectively, yadda, yadda and is 99% effective. They've been dating for 12 months. Have sex an average of 12 times a month. So I did (.99)^144 which gives about 23.5% chance of her not being pregnant after 144 fun encounters. Which scares the living hell out of me. 

I know every time the event takes place it doesn't consider the previous event and the odds are still 99% 

Is this right? Thanks!

**tl;dr** chance of not getting your gf pregnant after 144 sexual encounters with 99% protection efficiency=23.5%  **Is this right?**

",en
1109341,2012-03-22 09:43:54,MachineLearning,The Rupp Report: Successful Two Product Lines Strategy For Karl Mayer,r83rv,hhrotery,1331529952.0,https://www.reddit.com/r/MachineLearning/comments/r83rv/the_rupp_report_successful_two_product_lines/,1.0,0.0,,en
1109342,2012-03-22 13:14:08,statistics,"Card party in Villareal.
I prefer to leave the referee’s performance uncommented.
Unbelievable!",r87xa,eujppo,1332414704.0,https://www.reddit.com/r/statistics/comments/r87xa/card_party_in_villareal_i_prefer_to_leave_the/,1.0,0.0,,en
1109343,2012-03-22 14:43:18,statistics,Calculating the odds of winning a raffle?,r8a5p,armitages,1171457931.0,https://www.reddit.com/r/statistics/comments/r8a5p/calculating_the_odds_of_winning_a_raffle/,4.0,11.0,"Hi guys; was wondering if you could help me here? Im trying to calculate the odds of winning [this raffle](http://www.lottery.ie/en/easter-millionaire-raffle/).

Could someone help me out or direct me please?

After Googling I came across the [HYPGEOMDIST funstion](http://office.microsoft.com/en-us/excel-help/hypgeomdist-function-HP010335647.aspx) in Excel in the following format, is this correct? ::

=HYPGEOMDIST(0,&lt;#of_tickets_held&gt;,&lt;total#_prizes&gt;,&lt;#tickets_total&gt;)

So this gives the chance of loosing, after buying one ticket, as 0.996783333
=HYPGEOMDIST(0,1,579,180000)


Accurate?",en
1109344,2012-03-22 17:29:57,statistics,What are the odds - My wife's parents anniversary is my birthday and my parents anniversary is my wife's birthday.  ,r8gpp,[deleted],,https://www.reddit.com/r/statistics/comments/r8gpp/what_are_the_odds_my_wifes_parents_anniversary_is/,1.0,0.0,"As the title indicates, my wife's parents anniversary is my birthday and my parents anniversary is my wife's birthday.  What are the odds?  I haven't taken a statistics class in a long time.  I figured that the odds of an aniversary being the same as a birthday are 1 in 365.  Since it happens for both my wife and I, I multiplied it and figured it was 1 in 133,225.  I am really not sure how right I am.  ",en
1109345,2012-03-22 18:09:15,statistics,"Out of 6 die rolls, what is the probability I will get 3 odds and 3 evens?",r8ik6,[deleted],,https://www.reddit.com/r/statistics/comments/r8ik6/out_of_6_die_rolls_what_is_the_probability_i_will/,1.0,2.0,I keep getting 1/64 but I know that's not right.,en
1109346,2012-03-22 22:33:53,statistics,Regression capabilities in the latest release of Matlab's Statistics Toolbox look absolutely awesome!,r8vqb,bitethemuffin,,https://www.reddit.com/r/statistics/comments/r8vqb/regression_capabilities_in_the_latest_release_of/,2.0,7.0,,en
1109347,2012-03-22 22:36:51,AskStatistics,What is the difference between sigma and sigma squared in statistics/econometrics?,r8vut,dummyvariable,1332432557.0,https://www.reddit.com/r/AskStatistics/comments/r8vut/what_is_the_difference_between_sigma_and_sigma/,0.0,3.0,,en
1109348,2012-03-22 23:44:11,statistics,Question about which type of correlation to run with one nominal variable (not dichotomous) and a quantitative variable,r8zh4,[deleted],,https://www.reddit.com/r/statistics/comments/r8zh4/question_about_which_type_of_correlation_to_run/,2.0,2.0,"I'm trying to do a correlation between a nominal variable (race) and a continuous variable. Everything I keep reading about correlations between nominal and interval/ratio data mentions the point biserial correlation, but that seems to be just for nominal variables that are dichotomous. Do any of you know what correlation is used for a non-dichotomous nominal variable and a continuous variable, and if I can run that in SPSS?

Thanks :)
",en
1109349,2012-03-23 00:05:33,rstats,Montreal R Workshop: Introduction to Bayesian Methods. March 26 14h.,r90ji,likelihoodtprior,1318629271.0,https://www.reddit.com/r/rstats/comments/r90ji/montreal_r_workshop_introduction_to_bayesian/,4.0,0.0,,en
1109350,2012-03-23 01:46:35,artificial,Free shipping on realistic artificial silk Palm Trees. http://www.mypalmbeachtrees.com/Palm-Trees,r95my,cchic,1312853008.0,https://www.reddit.com/r/artificial/comments/r95my/free_shipping_on_realistic_artificial_silk_palm/,1.0,0.0,,en
1109351,2012-03-23 02:03:31,statistics,Help required with Social Network/Sentiment/Behavioural change analysis,r96hj,tren,1190453602.0,https://www.reddit.com/r/statistics/comments/r96hj/help_required_with_social/,1.0,0.0,,en
1109352,2012-03-23 05:25:05,MachineLearning,Free Stanford online class:  Probabilistic Graphical Models (registration ends Fri.),r9gkh,contrarianism,1263224785.0,https://www.reddit.com/r/MachineLearning/comments/r9gkh/free_stanford_online_class_probabilistic/,14.0,8.0,,en
1109353,2012-03-23 05:37:46,statistics,"Some thoughts on Omitted Variable Bias.

I'm hoping for a nice discussion.",r9h4u,econometrician,1323865912.0,https://www.reddit.com/r/statistics/comments/r9h4u/some_thoughts_on_omitted_variable_bias_im_hoping/,2.0,12.0,"One of the most common statistical critiques to many applied empirical results is ""omitted variable bias""; it's one of the most frequent claims I've heard in my four years of experience with statistics. While I do not feel I am expert, I do believe that I know enough to be dangerous (*ha*). I'm writing this in the hope of a good debate on the matter. But, allow me to give my two cents on the matter:

Omitted variable bias is a theoretical precept; typically, researchers are interested in some set of variables **X**, and how they *affect* an input **Y** (In this case, **X** can be an n by k vector, and **Y** an n by 1 vector). Suppose we're interested in this relatoinship, and we specify the model:

[1] **Y**= **X**'b+**e**

Typical refutes are that  some vector **Z** may be affecting **Y**, and, thus, the effect of **X** will be misleading; that is, the effect will be *biased* by some magnitude and direction.

This is simple, and quite frequently shouted out by researchers, but, at times, I disagree.

Allow me to give an example:

The labor economics literature has been raging about the causal effects of schooling for over fifty years, but no one tends to assess the fact that, when looking at a sample of data, estimating a model with a simple Mincer [1967] specification (i.e., regressing experience, experience^2 , and years of education on wages) simply yields the estimated average returns to experience and education in your sample. In effect, it answers the question, ""What's the average wage when I control for education and experience?"" 

Yet, the literature has been raving about omitted variable bias and self-selection, but I *really* feel that that is a different question, do you agree, or disagree? Please, feel free to leave your thoughts and comments, I'd really like to know the thoughts and opinions of r/statistics.

Post Script: While this example is clearly economic in nature, it can be extended to non-economic cases.",en
1109354,2012-03-23 06:16:31,MachineLearning,"Ask ML: any (realtime) background removal (or motion segmentation, generally) algorithms which actually works for real world applications?",r9it2,[deleted],,https://www.reddit.com/r/MachineLearning/comments/r9it2/ask_ml_any_realtime_background_removal_or_motion/,0.0,1.0,"opencv has a couple of algorithms for background removal, butt they are not of high performance. which algorithms do you know which would work decently for background removal of a typical webcam video, excluding the naive methods like the use of chroma key.",en
1109355,2012-03-23 15:11:26,statistics,"The curse of dimensionality: All points are far from the mean, yet it is still possible to define outliers in high-dimensional data",r9wp6,RickWicklin,1327333524.0,https://www.reddit.com/r/statistics/comments/r9wp6/the_curse_of_dimensionality_all_points_are_far/,4.0,1.0,,en
1109356,2012-03-23 15:34:11,computervision,Free Computer Vision class from Berkeley starts about now !,r9xgb,zeushammer,1298950424.0,https://www.reddit.com/r/computervision/comments/r9xgb/free_computer_vision_class_from_berkeley_starts/,15.0,6.0,,en
1109357,2012-03-23 15:55:13,artificial,wayne white,r9y7p,vxqepf,1332510854.0,https://www.reddit.com/r/artificial/comments/r9y7p/wayne_white/,0.0,0.0,,en
1109358,2012-03-23 16:34:37,statistics,LOL!statistics men 9outof10,r9ztl,ctalgv,1332513118.0,https://www.reddit.com/r/statistics/comments/r9ztl/lolstatistics_men_9outof10/,1.0,0.0,,en
1109359,2012-03-23 17:52:14,MachineLearning,Question about linear SVMs and libsvm,ra3cj,[deleted],,https://www.reddit.com/r/MachineLearning/comments/ra3cj/question_about_linear_svms_and_libsvm/,5.0,3.0,"Hi guys! I have a quick question about linear SVMs. 

I was always under the impression that a linear SVM splits the original feature space into two parts using a hyperplane in the original feature space (i.e. does not need to be mapped to higher dimensional space). If this is true, then why would I need support vectors or a kernel at all? Shouldn't my output just be a vector of weights (one for each feature dimension) and a bias term to define the hyperplane? When I try to use a linear SVM in libsvm, I'm getting lots of support vectors out and support vector weights. Something like 9000 support vectors, with a weight coefficient for each. I should really only need 1100 weights (the number of dimensions of my feature space) plus a bias term, right? Am I missing something obvious?

Thanks!",en
1109360,2012-03-23 20:49:46,statistics,"Worst Statistics exam question ever, can you top it?",raca9,deanzamo,1325540877.0,https://www.reddit.com/r/statistics/comments/raca9/worst_statistics_exam_question_ever_can_you_top_it/,16.0,12.0,"I am a community college math/instructor and I was helping a student try to understand a question in an intro Stat course (from a different instructor.) The test was on Hypothesis Testing and none of the questions had context. However, I have no clue why the instructor asked this question:

""Let the random variable X be Student t5 distributed. Find the 5th percentile of X minus P( X &lt; 1.48 )""

When the student (who recognized it makes no sense to take the difference between a  percentile and a probability) asked the instructor if there was a typo, the instructor said just answer the question. 

Does anyone else have examples of really bad stat questions?",en
1109361,2012-03-24 00:44:26,computervision,Detecting text in an image: Is Stroke Width Transform the right way to do this?,rankh,zorlack,1158152852.0,https://www.reddit.com/r/computervision/comments/rankh/detecting_text_in_an_image_is_stroke_width/,5.0,7.0,"I've recently embarked on a project to extract and read text in images, but I'm completely out of my depth in the world of computer vision, so I was wondering if I'm going about this the right way.

Some google-fu lead me to the topic of using Stroke Width Transforms to detect text in images, I've found an open source implementation of this and have started testing with it, but I wanted to see if anyone knows a better way to approach this problem.

Thanks!",en
1109362,2012-03-24 08:12:53,statistics,Need statistics help for business lead model,rb4s6,tren,1190453602.0,https://www.reddit.com/r/statistics/comments/rb4s6/need_statistics_help_for_business_lead_model/,2.0,0.0,"Hi, I'm hoping a statistician can help one of the members on our site. He needs someone to do some calculations relating to closing business leads.
He's willing to pay $150 for a few hours work, you can find out more details here: http://zombal.com/zomb/scientific-design/business-lead-model or feel free to PM me.",en
1109363,2012-03-24 12:11:05,statistics,Python for Econometrics - Kevin Sheppard,rb9gp,bashtage,1332583821.0,https://www.reddit.com/r/statistics/comments/rb9gp/python_for_econometrics_kevin_sheppard/,28.0,5.0,,en
1109364,2012-03-24 21:37:55,statistics,Proofiness: The Dark Arts of Mathematical Deception ,rbqd0,SoftwareJudge,1332350131.0,https://www.reddit.com/r/statistics/comments/rbqd0/proofiness_the_dark_arts_of_mathematical_deception/,9.0,0.0,,en
1109365,2012-03-25 01:19:55,statistics,Math/Stats pros:  I'm stumped on interpreting something trivial in a paper.  Please help.,rbzk9,starkrampf,1249950631.0,https://www.reddit.com/r/statistics/comments/rbzk9/mathstats_pros_im_stumped_on_interpreting/,1.0,4.0,"So I've been reading up on optimal betting strategies (e.g. kelly criterion) and this [paper by Thorp](http://web.williams.edu/Mathematics/sjmiller/public_html/341/handouts/Thorpe_KellyCriterion2007.pdf) is quite interesting.  I'm trying to play around with his solutions in Matlab but I'm out of luck with interpreting what 'N' stands for.


Specifically, on page 8, line 20, he defines Eq. 3.1 which has the *term* 'N' in it. Now, he doesn't define N as a variable in that section, but does use it as the number of trials earlier in the Introduction section.  However, N being the number of trials doesn't seem to make sense, since he uses 'n' for that purpose.


I've gone on to assume he means that N(.) is a function, like the Normal distribution, and that m and S^2 are used for that.  This does not lead to his results either.


So, this is where I am.  What does 'N' stand for?  Any clues?  This is likely to be an embarrassment to me, but whatever.


Thanks!",en
1109366,2012-03-25 01:48:38,statistics,I made this problem a few years ago for an introductory statistics course. This may very well be the only community able to appreciate it.,rc0ry,[deleted],,https://www.reddit.com/r/statistics/comments/rc0ry/i_made_this_problem_a_few_years_ago_for_an/,1.0,0.0,,en
1109367,2012-03-25 05:02:00,AskStatistics,Help increase the rate of organ donation by giving LifeShare your opinion on this survey! Thanks!,rc64s,daniellew,1332449086.0,https://www.reddit.com/r/AskStatistics/comments/rc64s/help_increase_the_rate_of_organ_donation_by/,0.0,1.0,,en
1109368,2012-03-25 06:41:48,statistics,"it appears the expectation value of the pretax 
payout for this week's Megamillions lottery ticket is 
well above the cost of the ticket [XLS w/calculations]",rca04,jmdugan,1221770863.0,https://www.reddit.com/r/statistics/comments/rca04/it_appears_the_expectation_value_of_the_pretax/,5.0,10.0,,en
1109369,2012-03-25 11:27:52,statistics,A statistics question about fast food!,rcibq,magicjamesv,1321166313.0,https://www.reddit.com/r/statistics/comments/rcibq/a_statistics_question_about_fast_food/,4.0,6.0,"So, a couple years ago my friend created what he calls the ""In 'n Out Challenge"". The challenge is to get one of every order number (1-100). He's up to about 50 unique numbers right now. While we were enjoying our Double-Double's today, we wondered: How many times would you have to come to In 'n Out to, statistically, get every number possible. After a few minutes, we realized that finding the answer would prove to be a much more daunting task than we first imagined. After a few hours, we finally gave up, so I decided to turn to Reddit! First correct answer wins a free burger!

TL;DR: How many times would you have to randomly select a number (1-100) before having all the unique whole numbers between 1 and 100?",en
1109370,2012-03-25 14:49:24,statistics,God takes look at variance [funny],rclb3,palanoid,,https://www.reddit.com/r/statistics/comments/rclb3/god_takes_look_at_variance_funny/,15.0,0.0,,en
1109371,2012-03-25 16:18:15,statistics,"I've been asked to make a ""guideline"" for quantitative research, namely interviews. Is that more than the sketch of the interview, with potential questions? (sorry if I'm in the wrong subreddit)",rcmv9,erikhun,1280500725.0,https://www.reddit.com/r/statistics/comments/rcmv9/ive_been_asked_to_make_a_guideline_for/,1.0,0.0,,en
1109372,2012-03-25 19:31:45,statistics,"New to Time Series and R; trying to simulate an ARIMA(0,2,2) process in R. ",rcsms,origamispiderman,1296704658.0,https://www.reddit.com/r/statistics/comments/rcsms/new_to_time_series_and_r_trying_to_simulate_an/,4.0,4.0,"Like the title says, I am new to Time Series, as well as R. I'm trying to simulate 45 values of an ARIMA(0,2,2) process, with the coefficients of the MA parts to be theta_1 = 1.0, theta_2 = -0.75.

So I have the following code in R:

&gt; ts.sim &lt;- arima.sim(model=list(order=c(0,2,2), ma=c(1.0,-0.75)) ,n=45)

However, when I try to find the values of the thetas using Maximum Likelihood Estimates method, I get values way off from 1 and -0.75.

For example,

    &gt; ts.sim &lt;- arima.sim(model=list(order=c(0,2,2), ma=c(1.0,-0.75)) ,n=45)

    &gt; arima(ts.sim, order=c(0,2,2), method=""ML"")
    
    Coefficients:
            ma1      ma2
          0.1772  -0.4462
    s.e.  0.1401   0.1342

Just wondering if anyone can help me out. Maybe I messed up the code somewhere? Shouldn't the estimated coefficients be close to 1 and -0.75? I've tried repeating it with a larger n, but the estimates were still way off.",en
1109373,2012-03-25 23:07:38,statistics,Simple stats question that my boss told me to research for him on the internet. Can you help /r/stats?,rd2nm,smcedged,1210555463.0,https://www.reddit.com/r/statistics/comments/rd2nm/simple_stats_question_that_my_boss_told_me_to/,1.0,6.0,"
He's running a biochem experiment. I'm currently 3000 miles from him, so I'm not entirely sure what the experiment background is, but he has sent me this in an email:


""I have a mathematical question for you. Maybe the statistics Dept.
At your school can help. OK?   I have a regression line on a semi-log
Scale... The x axis being linear, the y scale, two cycle log.

If I have a line with a 0.9999 correlation coef. And I interpolate a value
For a given sample of 90%. It's 90% +/- what?

If that curve has a correlation coif. Of only 0.9700 and I come up with
90%.  What is the upper and lower limits of this value based on the degree
Of scatter associated with a curve that has a coef. Of 0.9700?""

I've never actually taken a single stats class, so I really am not too sure what to do with this. Help?
",en
1109374,2012-03-26 00:02:32,statistics,Median of two uniform distributions added together?  ,rd5bj,[deleted],,https://www.reddit.com/r/statistics/comments/rd5bj/median_of_two_uniform_distributions_added_together/,2.0,3.0,"Hello all,

I am trying to find the general way of finding the median of two continuous uniform distributions. Lets say I am trying to find the median height for all high school students in my town, and there are two high schools. The heights in high school 1 are uniformly distributed between a and b (so the median height in high school 1 is (a+b) / 2. The heights in high school 2 are uniformly distributed between 0 and b (so the median height in high school b is b/2). 

To make things complicated, suppose high school 1 is x times larger than high school 2 (and lets normalise the size of high school two to 1).  So the mass of this new distribution is 1 + x. 

Is there a general way of finding the median height of high schoolers in this case? Please let me know if these needs clarification and any help is greatly appreciated,

Cheers,

Nick",en
1109375,2012-03-26 00:37:14,statistics,Confidence Intervals of Restricted Cubic Splines,rd70a,Iamthelolrus,1251613077.0,https://www.reddit.com/r/statistics/comments/rd70a/confidence_intervals_of_restricted_cubic_splines/,3.0,3.0,"I have data, y, that is a function of x and z where x is continuous and z is a bunch of dummy variables.

My model is: y = f(x) + z where f(x) is a restricted cubic spline.

I regress y on the design matrix of x and the dummy variables, z, and I get coefficients corresponding to each column of my design matrix. 

I'm interested in the shape of f(x) rather than the fitted values of y. I can create a sequence spanning my range of x values and evaluate f(x) over the sequence. This gives me an idea of the overall shape of f(x).

I would like to have confidence bands around f(x). I have standard errors for the components of f(x) (the coefficients between knots) but I'm not sure how to get from the standard error of the knots to the standard error of the entire function. The point estimates are additive but because the standard errors are positive, the confidence bounds would blow up as x gets larger. 

Thoughts?",en
1109376,2012-03-26 11:17:38,statistics,"Hi r/statistics, I've just finished the book ""Moneyball"" and I'm very interested to find out more about statistics. Are there any books or websites you can link me to?",rdxwl,d23durian,1318600263.0,https://www.reddit.com/r/statistics/comments/rdxwl/hi_rstatistics_ive_just_finished_the_book/,21.0,25.0,Im also a college freshman. I have a stats class I'm about to fail badly because at the start of the semester I thought statistics was the most boring subject ever. I'm starting to change my mind. ,en
1109377,2012-03-26 15:40:29,rstats,Modeling time series data in R,re42l,statsarefun,1328780296.0,https://www.reddit.com/r/rstats/comments/re42l/modeling_time_series_data_in_r/,3.0,8.0,"Hi!
I have market prices for chicken, and I want to examine whether the prices of chicken bits go up just as fast as they go down when the price of chicken changes. What kind of statistical testing method would be recommended for doing such an analysis, and how does one go about coding this in R? Im doing this analysis as part of an introduction to R. (Not an assignment, just for fun.)",en
1109378,2012-03-26 16:45:42,statistics,Visual correlations question,re698,steelerman82,1318489674.0,https://www.reddit.com/r/statistics/comments/re698/visual_correlations_question/,0.0,3.0,"Not homework, I promise. **If I am in the wrong place, by all means downvote me to oblivion**

I'm trying to correlate sets of data on US Maps to drive home a point about religious fervor, poverty, education, etc, etc.   What would be the best way to display this data visually for the most impact?  I was thinking breaking it up like:

* 100 - 80 
* 70-79
* 60-69
* 50-59
* 40-49
* 30-39
* 20-29
* 0-19

but what do you think would be the best color scheme for this?  I tried white to bule on a 5 point scale; and a red to blue on a ten point, with purple in the middle.  neither were spectacular.

",en
1109379,2012-03-26 18:08:42,MachineLearning,Factual’s Gil Elbaz Wants to Gather the Data Universe,re9o5,jdw25,1220557205.0,https://www.reddit.com/r/MachineLearning/comments/re9o5/factuals_gil_elbaz_wants_to_gather_the_data/,22.0,0.0,,en
1109380,2012-03-26 18:36:37,AskStatistics,"Question about coin tossing, whether the next outcome is dependent on past outcomes.",reazd,aloknrao_1985,1332035324.0,https://www.reddit.com/r/AskStatistics/comments/reazd/question_about_coin_tossing_whether_the_next/,7.0,8.0,"Let say I have a coin, which I don't know if its a fair or unfair coin. I toss it a large number of times, say 1 million times, after which my results are 800,000 heads and 200,000 tails. For the 1000001th attempt, if I were to bet on a head or tail, would it be optimal for me to use the historical results and bet on a head (since it has turned up 80% of the time) or is the next outcome independent of all past outcomes?",en
1109381,2012-03-26 19:05:54,statistics,Finding an optimal configuration for a system. When is a good time to stop?,recdj,MisterWanderer,1275298154.0,https://www.reddit.com/r/statistics/comments/recdj/finding_an_optimal_configuration_for_a_system/,9.0,8.0,"Ok so I have a multi dimensional (7-10) problem in which I want to find a ""good"" solution. I have a test function that converts a set of values into a number (lower is better). Currently what I am doing is pulling random values in the range that makes sense and then doing gradient descent to find the local minimum related to that value.

This is working out just fine since the problem isn't all that big. We get a nice set of solution candidates. My question is ... When do we know when to stop? I don't need a perfect solution. Just some knowledge that after a certain iteration X we will have a Y% chance of having the minimum value possible would be plenty.

",en
1109382,2012-03-26 21:32:37,MachineLearning,Question about Radial Basis Function Kernels for SVM,rejn0,[deleted],,https://www.reddit.com/r/MachineLearning/comments/rejn0/question_about_radial_basis_function_kernels_for/,6.0,5.0,"There are a couple things about kernels (specifically the RBF kernel) that I am wondering about.

First, I know that for a kernel to be valid, it most be symmetric and positive semi-definite. How can you prove that the RBF kernel has those qualities?

Second, for the RBF in form: 

K(xi,xj) = exp(-B||xi-xj||^2) 

How can you prove that any dataset is separable for large enough values of B?

Thanks for any help or insights on these inquiries!",en
1109383,2012-03-26 21:35:45,statistics,What do you do while you are waiting for your computer to run calculations?,rejt7,CosinQuaNon,1270804031.0,https://www.reddit.com/r/statistics/comments/rejt7/what_do_you_do_while_you_are_waiting_for_your/,3.0,7.0,"I recently ran into a lot more data for my work and now all my calculations take much longer to process. While I am exploring different options for speeding my specific task up, sometimes things do just take a long time. So what do you do in the downtime to be more efficient?",en
1109384,2012-03-26 21:57:26,AskStatistics,Mediation with a Categorical IV,rekxb,chucko326,1290533115.0,https://www.reddit.com/r/AskStatistics/comments/rekxb/mediation_with_a_categorical_iv/,1.0,0.0,"Could someone explain (in plain English) how to test for mediation when the IV is categorical (2 levels in an experimental treatment) and the MV and DV are continuous?

I understand the concept of mediation, and how to do it with linear regression (when all variables are continuous), but I am blanking on what to do with my categorical IV.",en
1109385,2012-03-26 23:08:31,MachineLearning,Model to predict drop-in attendance at a tutoring center?,reom6,esg43,1276979138.0,https://www.reddit.com/r/MachineLearning/comments/reom6/model_to_predict_dropin_attendance_at_a_tutoring/,0.0,3.0,"Hello, r/MachineLearning.  
I am undertaking a project to try to predict attendance at a tutoring center (unlimited drop-in).  I have about 1.5 years of data with the following fields: Student ID, Arrival Time, Duration of stay, Grade, School.

My ultimate goal is to predict the days and times of attendance of each student.  I have trained a model which estimates a probability of attendance on a particular day based on

1. What percentage of times the student has been present that day of the week.

and

2. What percentage of the time the student has been present on a particular day in the previous 2 weeks.

The model then comes up with a weighted probability of attendance based on the aforementioned statistics, and then estimates the time of arrival as the first time interval that we have seen the student more than 50 percent of the times on that day.  What I mean here is that we maybe see the student 50 percent of the time between 3PM and 3:30PM on Wednesday, and so we estimate that they will arrive at that time.

What other models might work in such a situation?  Is there any literature on this that I am missing?  I have been considering Neural Networks, Logistic regression (by deriving some more features about student behavior).  
Thank you in advance for your input!",en
1109386,2012-03-26 23:52:13,MachineLearning,High Performance machine learning ... the numbers are impressive.,rer18,gfuzzball,1316710424.0,https://www.reddit.com/r/MachineLearning/comments/rer18/high_performance_machine_learning_the_numbers_are/,0.0,6.0,,en
1109387,2012-03-27 00:31:44,statistics,Google summer of code 2012 – and R – a call for students (please help to share),ret7k,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/ret7k/google_summer_of_code_2012_and_r_a_call_for/,19.0,0.0,,en
1109388,2012-03-27 02:48:07,statistics,Stats question here. So I need to know if there is a significant difference between the forest type of real data points and random data points.,rf09o,[deleted],,https://www.reddit.com/r/statistics/comments/rf09o/stats_question_here_so_i_need_to_know_if_there_is/,1.0,4.0,"I obtained 30 data points, and have found each forest type that the point falls within (on ArcGIS). I then generated 30 random points and did the same. What I am having trouble with is determining if it is random, or if by preference (selection preference from a specific group of animals). The forest type is given in codes such as PIRE, PIST, and etc. What kind of test can I do to compare the real and random data set forest types? It has been too long since I took my stats class. ",en
1109389,2012-03-27 03:01:31,statistics,How can I estimate the confidence intervals for Ripley's cross-K function (in 1D)?,rf0yi,[deleted],,https://www.reddit.com/r/statistics/comments/rf0yi/how_can_i_estimate_the_confidence_intervals_for/,1.0,0.0,"Or rather, the L(r) - r function (depending on your definition).

My goal is examine the relationship (correlations) between two sets of point data.

Basically I have two 1D vectors that I'm computing the cross-K/L function for, but I don't know how to find the confidence intervals. I read something about randomly shifting the data and doing Monte Carlo simulations, but I'm not entirely sure how/why that works.

Any help would be appreciated, thanks!",en
1109390,2012-03-27 03:38:44,statistics,How can I estimate the inference interval for hypothesis testing with Ripley's cross-K function (in 1D)? ,rf2uo,roger_,1178076247.0,https://www.reddit.com/r/statistics/comments/rf2uo/how_can_i_estimate_the_inference_interval_for/,4.0,0.0,"Or rather, the L(r) - r function (depending on your definition).

My goal is examine the relationship (correlations) between two sets of point data.

Basically I have two 1D vectors that I'm computing the cross-K/L function for, but I don't know how to find the intervals for inference testing (clustering vs. dispersion). I read something about randomly shifting the data and doing Monte Carlo simulations, but I'm not entirely sure how/why that works.

Any help would be appreciated, thanks!",en
1109391,2012-03-27 09:25:09,MachineLearning,Packaging for the Pharmaceutical Industry,rfj0h,gustavoduhamel,1323925005.0,https://www.reddit.com/r/MachineLearning/comments/rfj0h/packaging_for_the_pharmaceutical_industry/,1.0,1.0,,en
1109392,2012-03-27 10:47:18,datasets,Request: Daily crude oil prices and price of gasoline dating back around 1990.,rfl8f,statslab,1332834220.0,https://www.reddit.com/r/datasets/comments/rfl8f/request_daily_crude_oil_prices_and_price_of/,0.0,0.0,"Hi!
Im going to work some statistical magic on refined oil prices. Does anybody know where I can find the data in a manageable format? Preferred frequency would be day to day and preferred time period would be over the past 10 years.",en
1109393,2012-03-27 11:10:19,MachineLearning,Looking for efficient LOO implementation of linear discriminant classifier,rflq7,thrope,1275749746.0,https://www.reddit.com/r/MachineLearning/comments/rflq7/looking_for_efficient_loo_implementation_of/,2.0,0.0,"I am looking to do leave-one-out crossvalidation with a basic linear or diagonal linear classifier (assume normal distribution etc.)

I have an efficient version for template matching (means only), but I am sure there must be some tricks for updating the (co)variances efficiently for each leave out sample (and maybe updating the inverse of the covariance matrix - I saw Sherman-Morrison mentioned).

Any implementation I could look at to get a head start or any other tips would be great.",en
1109394,2012-03-27 15:55:38,statistics,Even You Can Learn Statistics: A Guide for Everyone Who Has Ever Been Afraid of Statistics (2nd Edition) - Free Kindle Book,rfs5s,scohoe,1218732082.0,https://www.reddit.com/r/statistics/comments/rfs5s/even_you_can_learn_statistics_a_guide_for/,16.0,9.0,,en
1109395,2012-03-27 16:25:48,MachineLearning,The sunrise problem: a classic Bayes's theorem problem (from 1763).,rft3d,AllenDowney,1300587223.0,https://www.reddit.com/r/MachineLearning/comments/rft3d/the_sunrise_problem_a_classic_bayess_theorem/,28.0,0.0,,en
1109396,2012-03-27 17:51:02,statistics,Thoughts on census data... (not US Census data),rfwdn,[deleted],,https://www.reddit.com/r/statistics/comments/rfwdn/thoughts_on_census_data_not_us_census_data/,3.0,14.0,"Say I do a study where we send out a survey to the entire population(obviously not everyone will respond, but they are given a chance to respond). What is your opinion on reporting a ""sampling error"" and doing significance testing?

Generally I don't because I've been taught that conceptually neither really apply.  But I have had clients request these metrics be reported.",en
1109397,2012-03-27 21:16:05,statistics,"Need Questions from Applied Statistics and the SAS Programming Language, help?",rg6ga,Sauter,1263069059.0,https://www.reddit.com/r/statistics/comments/rg6ga/need_questions_from_applied_statistics_and_the/,2.0,8.0,"I know this is a shot in the dark but here it goes...  
I bought the 4th edition of this book trying to save money but I need the 5th edition for the correct homework problems. Can anyone either scan me the problems if you have the book or direct me to a link online to view them while I wait for my book to come in the mail? Thanks in advance!",en
1109398,2012-03-27 23:12:10,computervision,"OpenTLD, aka 'Predator', on github",rgcnp,protein_bricks_4_all,1325957555.0,https://www.reddit.com/r/computervision/comments/rgcnp/opentld_aka_predator_on_github/,1.0,0.0,,en
1109399,2012-03-28 04:18:05,statistics,Did I answer these problems right?,rgsnw,TBizzcuit,1270020303.0,https://www.reddit.com/r/statistics/comments/rgsnw/did_i_answer_these_problems_right/,1.0,1.0,,en
1109400,2012-03-28 05:55:16,MachineLearning,Probabilistic Counting,rgxvj,willis77,1188267725.0,https://www.reddit.com/r/MachineLearning/comments/rgxvj/probabilistic_counting/,8.0,1.0,,en
1109401,2012-03-28 06:39:46,statistics,"Inversion of Error Matrix in Linear Regression, suggestions?",rh06c,MikotoMisaka,1326383674.0,https://www.reddit.com/r/statistics/comments/rh06c/inversion_of_error_matrix_in_linear_regression/,4.0,14.0,"I am doing a standard multivariable regression on a data set.  I calculate the linear least squares fit to a data set for 5 different parameters.  I also construct the a 5 x 5 convariance matrix.  I would like to ortogonalize the covariance matrix but can't find any literature on how to go about this.  It is simple for the matrix on slopes since they are just numbers, but errors don't add the same ..... so I have a matrix ofo errors I would like to invert.

The end product would be: I take matrix of least squared fits and and orthogonalize it, I would like to propagte the error I calculate pre-inversion to the newly calculate values.  Anyone have any experience with this? or able to point me to some good literature about inverting a matrix full of errors?


note:  I have already tried to building a class in C++ that knows how to add errors and then invert using LUDecompsition to try and calculate the inverse while propagting errors properly.  The code works if the matrix is full of numbers, but fails if you use the error class.

tldr; I use a least square fit build a 5x5 matrix of slopes and associated errors.  The slopes are correlated.  I want to orthogonalize this matrix.  I don't know how to propagate/get errors on the newly orthogonalized values.

I would appreciate any thoughts or recommendations of books or other reddits if this is not a correct topic for here.

thanks.",en
1109402,2012-03-28 18:29:16,statistics,"In a logit regression model, does it matter how the yes/no response variable was created?",rhn1l,sketcher7,1292804677.0,https://www.reddit.com/r/statistics/comments/rhn1l/in_a_logit_regression_model_does_it_matter_how/,5.0,8.0,"My understanding of the logit model is that we assume the response variable is the result of a Bernoulli trial. Now, I am reading a paper that constructs the response in a way that makes me think they cheated on this assumption.

The authors are trying to model passage rates for a national exam (the Bar exam), and they are only interested in the *eventual* passage rate of examinees. Like many other exams, people can take the Bar exam as many times as they wish, so an examinee may pass on the first, second, ..., or the nth trial. The authors of the paper simply record a pass if there was an eventual pass.  Given that this outcome was the result of n Bernoulli trials where nth trial resulted in a pass, Does it still count as a Bernouli variable? Can this be modeled using logit, or have they violated the assumptions for the model with this way of constructing the response variable? I think they did. Does [r/statistics](http://reddit.com/r/statistics) agree?

Here's a [link](http://www.unc.edu/edp/pdf/NLBPS.pdf) to the paper. ",en
1109403,2012-03-28 23:44:31,rstats,Chicago R Tutor Needed,ri3wj,stat-throwaway,1332966721.0,https://www.reddit.com/r/rstats/comments/ri3wj/chicago_r_tutor_needed/,1.0,0.0,"Hey guys,

I am interested in having someone tutor me / help build a mini-course to help me learn R and, if things go well, MATLAB as well. I am fairly open about the structure, compensation, etc., but the person needs to be very well versed in R and ideally in financial mathematics / quantitative finance. I expect the person to be able to have a coherent structure in their teaching and have the ability to provide some sort of homework/problem sets that will let me practice between sessions.

About me: I work in finance and have a decent quantitative background/mindset for a liberal arts major (linear algebra, multi-variate calc, basic 200 level statistics and probability, some work in econometrics, etc.). I am however quite rusty, so presume you're going to be dealing with the equivalent of a smart high schooler / college freshman.

If you have any interest, please post in this thread or send a private message. Thanks. ",en
1109404,2012-03-28 23:56:39,statistics,How to determine sample size to confirm database transfer worked properly. ,ri4jy,overstood,1297269315.0,https://www.reddit.com/r/statistics/comments/ri4jy/how_to_determine_sample_size_to_confirm_database/,2.0,11.0,"I'm transferring data from one database to another (for those interested, its a Learning Management System for hospital staff). We need to confirm that previously completed courses are properly mapped from the old system to the new one. 

My question is, how do I determine the sample size to be 95% certain that all previously completed courses are reflected in the new database. There are 36,000 completed courses - any thoughts on how many I need to manually verify were transferred to be satisfied that the rest were, too. 

Any insights you might have would be helpful. Googling brought up only sample size calculators, but I'm not certain they were geared to my data. I've taken uni stats and research courses, but years ago, so I'm pretty rusty. 

Thanks! ",en
1109405,2012-03-29 00:25:56,statistics,The statistics of Hardy-Weinberg Equilibrium,ri66d,conkledonkle,,https://www.reddit.com/r/statistics/comments/ri66d/the_statistics_of_hardyweinberg_equilibrium/,2.0,5.0,"Hey r/statistics!

I'm doing a presentation on the Hardy-Weinberg equilibrium. I'm pretty well versed on the biological side and its basic uses, but I'm having a tough time finding some good resources about the statistics of it. 

I'm not asking for you guys to do my presentation. I was just wondering if anyone could point me in the right direction, perhaps some links to some resources I could use. Thanks!",en
1109406,2012-03-29 06:00:48,MachineLearning,Choosing a Machine Learning Classifier,rinw9,mycall,1183346313.0,https://www.reddit.com/r/MachineLearning/comments/rinw9/choosing_a_machine_learning_classifier/,23.0,25.0,,en
1109407,2012-03-29 08:24:43,MachineLearning,Diamond Blade: A Tool For Perfection And Elegance,riucn,dimit0948,1313116124.0,https://www.reddit.com/r/MachineLearning/comments/riucn/diamond_blade_a_tool_for_perfection_and_elegance/,1.0,1.0,,en
1109408,2012-03-29 15:07:29,MachineLearning,Book on the implementation of ML algorithms?,rj4a2,cpa,1180284933.0,https://www.reddit.com/r/MachineLearning/comments/rj4a2/book_on_the_implementation_of_ml_algorithms/,15.0,19.0,"Hi there,

I'm quite familiar with machine learning and I'm looking for a book that covers all the implementation details of ML algorithms (conditioning, numerical stability, scalability, parallel implementations...)

Does such a book exist?",en
1109409,2012-03-29 16:01:54,artificial,i want to live here,rj5w6,msnork,1333026068.0,https://www.reddit.com/r/artificial/comments/rj5w6/i_want_to_live_here/,1.0,0.0,,en
1109410,2012-03-29 16:34:35,artificial,The most realistic artificial Plants,rj713,cchic,1312853008.0,https://www.reddit.com/r/artificial/comments/rj713/the_most_realistic_artificial_plants/,1.0,0.0,,en
1109411,2012-03-29 18:17:10,MachineLearning,The Drivetrain Approach: A four-step process for building data products,rjbic,willis77,1188267725.0,https://www.reddit.com/r/MachineLearning/comments/rjbic/the_drivetrain_approach_a_fourstep_process_for/,12.0,0.0,,en
1109412,2012-03-29 19:03:22,statistics,SAS resources,rjdue,mathsuu,1197264006.0,https://www.reddit.com/r/statistics/comments/rjdue/sas_resources/,2.0,6.0,"A friend of mine from Brazil wants to learn SAS but he is poor and can't afford paying for a course. He doesn't have SAS on his computer either. So reading books alone won't help. He needs to practice.

This could get him a job and change his life forever. I'm going to contact SAS and see if I can get a discount so I can pay for his course.

But besides that, how can I help him?

EDIT: This is not for a stats job, but rather an entry level reporting-type job.",en
1109413,2012-03-29 20:20:11,AskStatistics,How many six-sided dice would one need to maximize the probability of rolling a sum of 4 billion?,rjhw7,Arandanos,1303688376.0,https://www.reddit.com/r/AskStatistics/comments/rjhw7/how_many_sixsided_dice_would_one_need_to_maximize/,3.0,13.0,,en
1109414,2012-03-29 20:28:58,statistics,$500 Million Jackpot: Calculating Your Odds - Forbes,rjidm,Daveinla,1332906899.0,https://www.reddit.com/r/statistics/comments/rjidm/500_million_jackpot_calculating_your_odds_forbes/,10.0,11.0,,en
1109415,2012-03-30 07:05:19,AskStatistics,How many times would one need to flip a coin to ensure at least a 95% probability of at least one heads?,rkflt,Eldoritoz,1295397326.0,https://www.reddit.com/r/AskStatistics/comments/rkflt/how_many_times_would_one_need_to_flip_a_coin_to/,3.0,7.0,"A friend asked me for help for his homework and I haven't taken probability in a while so I'm quite rusty. I'm fairly sure that the answer is 5, but I have no idea how to get that without using a guess and check method, and there's a bonus for not using G and C. Any help would be greatly appreciated.",en
1109416,2012-03-30 07:44:26,statistics,n gram analysis of database text fields,rkh91,squakky,1249088473.0,https://www.reddit.com/r/statistics/comments/rkh91/n_gram_analysis_of_database_text_fields/,1.0,3.0,"I'm wondering if anyone out there has done this, or something similar.
I have a bunch of oracle database records containing email bodies, and I'm looking to analyze patterns of 'n' reoccurring words, where n is my input (3,4,5 etc).

Sorta like this.
Ref: http://www.codeproject.com/Articles/20423/N-gram-and-Fast-Pattern-Extraction-Algorithm

Has anyone done or seen this implemented in sql? Can I get pointers on the algorithm? The output I'm shooting for would ideally contain two columns: phrase and frequency.

Thanks!",en
1109417,2012-03-30 08:22:39,statistics,Had surgery and missed some class. Do you think you guys could help me on a problem?,rkirb,TBizzcuit,1270020303.0,https://www.reddit.com/r/statistics/comments/rkirb/had_surgery_and_missed_some_class_do_you_think/,1.0,1.0,,en
1109418,2012-03-30 15:53:23,MachineLearning,R 2.15.0 is released,rkuak,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/rkuak/r_2150_is_released/,0.0,0.0,,en
1109419,2012-03-30 15:53:33,rstats,R 2.15.0 is released,rkuar,talgalili,1271226645.0,https://www.reddit.com/r/rstats/comments/rkuar/r_2150_is_released/,1.0,0.0,,en
1109420,2012-03-30 15:53:48,statistics,R 2.15.0 is released [r-bloggers],rkub6,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/rkub6/r_2150_is_released_rbloggers/,20.0,12.0,,en
1109421,2012-03-30 17:01:54,MachineLearning,What is Support Vector Machine? : xpost from explainlikeimfive,rkwvq,imissyourmusk,1317942820.0,https://www.reddit.com/r/MachineLearning/comments/rkwvq/what_is_support_vector_machine_xpost_from/,25.0,7.0,,en
1109422,2012-03-30 19:54:09,analytics,Go to programming languages to learn for web analytics?,rl4pm,ctdf,1309629100.0,https://www.reddit.com/r/analytics/comments/rl4pm/go_to_programming_languages_to_learn_for_web/,5.0,12.0,"Hi,

Are there any go to programming languages that someone should learn for when working in analytics?",en
1109423,2012-03-30 21:32:10,computervision,Registering Point Clouds from 2 Kinects,rl9ky,mathnathan,1277574833.0,https://www.reddit.com/r/computervision/comments/rl9ky/registering_point_clouds_from_2_kinects/,2.0,0.0,,en
1109424,2012-03-31 10:23:23,MachineLearning,Does it make sense to perform canonical correlation analysis on two covariance matrices?,rm5ir,[deleted],,https://www.reddit.com/r/MachineLearning/comments/rm5ir/does_it_make_sense_to_perform_canonical/,1.0,0.0,"I'm doing basic classification and I'd like to make my training data as similar as possible to my testing data.  The idea being that I would transform my training data and train my classifier on the transformed data and then predict on data that was similarly transformed.  A thought I had, which may or may not make sense when you get down to the details, was to perform CCA on the covariance matrices of my training and testing data and then generate new data according to the result of my CCA which I'd like to interpret as a covariance matrix which maximizes the correlation between the training and testing data.  There are several potential problems with this, but I figured I'd ask here before trying to find the answers in the mathematics myself.  However, here are some things I think may be a problem:

It's quite likely that the CCA projection of two symmetric matrices is not symmetric itself and thus I wouldn't be able to interpret the result as a covariance matrix.  Also, how should I use the covariance matrix to modify my training data?  Would it even be meaningful/helpful to do so?",en
1109425,2012-03-31 10:25:06,MachineLearning,AskML: Does it make sense to perform canonical correlation analysis on two covariance matrices?,rm5k7,lpiloto,1310021871.0,https://www.reddit.com/r/MachineLearning/comments/rm5k7/askml_does_it_make_sense_to_perform_canonical/,6.0,5.0,"I'm doing basic classification and I'd like to make my training data as similar as possible to my testing data.  The idea being that I would transform my training data and train my classifier on the transformed data and then predict on data that was similarly transformed.  A thought I had, which may or may not make sense when you get down to the details, was to perform CCA on the covariance matrices of my training and testing data and then generate new data according to the result of my CCA which I'd like to interpret as a covariance matrix which maximizes the correlation between the training and testing data.  There are several potential problems with this, but I figured I'd ask here before trying to find the answers in the mathematics myself.  However, here are some things I think may be a problem:

It's quite likely that the CCA projection of two symmetric matrices is not symmetric itself and thus I wouldn't be able to interpret the result as a covariance matrix.  Also, how should I use the covariance matrix to modify my training data?  Would it even be meaningful/helpful to do so?",en
1109426,2012-03-31 11:20:49,MachineLearning,TN textile industry unhappy with power tariff hike,rm6p1,hhrotery,1331529952.0,https://www.reddit.com/r/MachineLearning/comments/rm6p1/tn_textile_industry_unhappy_with_power_tariff_hike/,1.0,0.0,,en
1109427,2012-03-31 20:47:07,datascience,Looking up Images Trademarked By Companies Using OpenCorporates and Google Refine,rmlgl,mhermans,1169219262.0,https://www.reddit.com/r/datascience/comments/rmlgl/looking_up_images_trademarked_by_companies_using/,4.0,0.0,,en
1109428,2012-03-31 21:10:45,artificial,A robotic fovea and a laser/camera duo can greatly enhance Object Recognition; presented by Andrew Ng,rmmi3,moscheles,1316266003.0,https://www.reddit.com/r/artificial/comments/rmmi3/a_robotic_fovea_and_a_lasercamera_duo_can_greatly/,8.0,3.0,,en
1109429,2012-03-31 23:58:39,statistics,Statistics Project Ideas,rmtwa,mwdeuce,1318980995.0,https://www.reddit.com/r/statistics/comments/rmtwa/statistics_project_ideas/,3.0,3.0,"I'm not 100% sure if this should be here or /r/homeworkhelp, so I'll go both routes.  I have about 3 weeks to submit an abstract for some sort of statistical project.  This is the 3rd course in my stats series, so I'm still fairly new to the discipline.  To give you an idea of where I stand capability-wise, last quarter for my final I did a small paper (11 pg) on the recession from 2005 to 2009 and used imported data in SPSS from IPUMS database to look at income trends, job displacement, inflation, etc.  Normally I would not ask for help but this presentation is in front of faculty and the student body (I don't know wtf my professor was thinking, none of us are graduate students and our grasp of the formulas and concepts is tenuous at best). Any ideas or at least links to interesting data sets would would be greatly appreciated, ty.",en
1109430,2012-04-01 02:59:37,statistics,Need help selecting test in SPSS,rn1rb,[deleted],,https://www.reddit.com/r/statistics/comments/rn1rb/need_help_selecting_test_in_spss/,3.0,4.0,"I have a simple data set, but can't figure out how to analyze it using SPSS . 

What I have-

-3 categorical independent variables

~50 binomial responses per category, ~150 data points total

I did a chi-squared test and got a p&lt;0.01. What I still want to determine is if one of the categories has a significantly higher proportion of positive responses than the other two, which serve as controls. When doing the same for my normally distributed data, I was looking for the one-tailed p-value using ANOVA orthogonal contrasts, if that helps.",en
1109431,2012-04-01 11:50:06,rstats,Draw your Breasts with R,rnpmc,[deleted],,https://www.reddit.com/r/rstats/comments/rnpmc/draw_your_breasts_with_r/,1.0,0.0,,en
1109432,2012-04-01 12:11:58,artificial,"Computer outperforms humans at detecting lies, by watching the eyes",rnq8g,roboploats,1333271460.0,https://www.reddit.com/r/artificial/comments/rnq8g/computer_outperforms_humans_at_detecting_lies_by/,1.0,1.0,,en
1109433,2012-04-01 17:37:36,statistics,Do any students actually know how to debug code?,rnyba,[deleted],,https://www.reddit.com/r/statistics/comments/rnyba/do_any_students_actually_know_how_to_debug_code/,15.0,16.0,"This is in [/r/statistics](/r/statistics) because I'm mostly asking this question with respect to R and SAS code.

I understand that sometimes an outside observer may be able to catch errors in code that are not immediately apparent to the coder, but every time someone asks me to look at their code because it isn't working, it feels like they just don't understand how to debug code. I don't know what's wrong with their code, but I know a reasonable set of steps to take to figure out what's going wrong and where.

Does anyone else experience this? Or am I the only one?

**EDIT**: Obviously many students actually do know how to debug their code. It was a bad choice for the title on my part.",en
1109434,2012-04-01 17:44:23,datasets,Question about the National Longitudinal Survey of Youth,rnykm,econometrician,1323865912.0,https://www.reddit.com/r/datasets/comments/rnykm/question_about_the_national_longitudinal_survey/,2.0,0.0,"Does anyone know if the NLSY79 data set offers gpa? I have grade outcomes for undefined courses, but I don't see gpa available. Does anyone know if this is something I'd have to calculate myself? Or is it something available at the NLSY?",en
1109435,2012-04-01 18:40:53,computervision,Help making template matching work better,ro0xx,eternal512,1162418599.0,https://www.reddit.com/r/computervision/comments/ro0xx/help_making_template_matching_work_better/,7.0,6.0,"Sorry for the wall of text, i'll post a picture of a cat dancing later :)

I'm building an openCV/IPCam based garage door monitor.  I'm using openCV + python.  

Here is my [setup](http://i.imgur.com/VT5QJ.jpg).

Basically I periodically poll the IP Camera, convert to grayscale, perform histogram equalization, then matchTemplate to look for the 'X' in the upper left (this indicates the camera is active,and correctly pointed), and then the 'A' on the door.  If i can see both X &amp; A, then the door is closed. 

I'm using openCV's matchTemplate, but my results aren't amazing.  I've tried the different matchingMethods (DIFF, CORR,etc), but the min/max values in RESULTS are so varried, i started checking the min/max locations in results against the known position of X and A.   This works pretty well for most of the day (and night. the [IR on the camera](http://i.imgur.com/aS9Kk.jpg) is surprisingly good).

It works well for most the day, the windows allow for lots of lighting changes, and it handles that well, mostly, (**when the windows are very brightly illuminated, things go wrong**, (as well as the **lights on the garage lift**, at night).  here is a [failure](http://imgur.com/1UWbv). )

**So on to my question:Outside of tutorials which show the syntax for performing matchTemplate, are there any good resources that outline what works well?** 

Ie: 

should my template be a stark white A on a black background, or more like the A in the grabbed images? [currentTemplate](http://imgur.com/qSuj8)

Should i blur something?  is equalization burning me?

Should I be using something other than template matching? what would you suggest? It seems like templateMatching should be a slam dunk, since the X and A aren't changing AT ALL... I could mask out the troubling areas (the lights and windows, but feels hackish).  So i'm surprised to see so much failure.    I'm currently archiving training images, may pursue something more ML/AI.. but I'm surprised I would have to go to such lengths.

Thanks for any advice.  (i'm happy to share code, training images, etc, if anyone wants them.. I'll post something to a blog or some DIY website once i get more robust results going ******). 

As promised, [dancing cat](http://www.earlann.net/images/120_dancing_cat.gif)",en
1109436,2012-04-01 20:00:23,MachineLearning,x/post. Will this influence the design of Neural networks?,ro4s8,strategosInfinitum,1279064470.0,https://www.reddit.com/r/MachineLearning/comments/ro4s8/xpost_will_this_influence_the_design_of_neural/,6.0,4.0,,en
1109437,2012-04-01 22:22:20,statistics,Explaining MANOVA linearity violation,roc41,McFace82,1294450675.0,https://www.reddit.com/r/statistics/comments/roc41/explaining_manova_linearity_violation/,5.0,3.0,"I apologize in advance for what may be a basic question, but I am at the end of my nerves!

I am preparing to present on my research. I used an independent sample t-test to test for differences between 2 groups. I did this instead of a 2-group MANOVA because my variables weren't ""very"" correlated (out of 9 variables, there was only one pair that was about .3-.4). 

I know that not having strong correlations with my independent variables will cause a problem with power in MANOVA (and other problems may ensue from that). I need help clearly articulating this to someone though; someone on my committee is very hung up on the fact that I opted for a t-test instead of MANOVA. I justify this decision because one of the assumptions for MANOVA was violated, but I don't know how else I can explain this. 

How would you explain the problem of not having linear variables in a MANOVA.",en
1109438,2012-04-01 22:26:58,statistics,Statistics project,roccu,salineinjectedballs,1330222913.0,https://www.reddit.com/r/statistics/comments/roccu/statistics_project/,1.0,5.0,"I have a quarter long project for my stats class this quarter.  The idea is to analyze a data set, either gathered from personal research or found in a journal, in a way that uses all the techniques learned throughout the 3 class sequence, (probability, regression, anova, hypothesis testing; nothing too advanced just the meat and potatoes)  Any interesting ideas?",en
1109439,2012-04-02 04:31:16,computervision,Scilab and SIP for Image Processing - arXiv preprint,rou7z,rfabbri,1263964548.0,https://www.reddit.com/r/computervision/comments/rou7z/scilab_and_sip_for_image_processing_arxiv_preprint/,3.0,1.0,,en
1109440,2012-04-02 07:54:56,AskStatistics,What is the probability of a positive assertion being true? ,rp3n9,[deleted],,https://www.reddit.com/r/AskStatistics/comments/rp3n9/what_is_the_probability_of_a_positive_assertion/,1.0,0.0,"A while ago I wrote a paper on skepticism. In it, I talked about how I thought it is statistically impossible for a positive assertion to be true, And that it is statistically impossible for a negative assertion to be false. 

I wrote this back in 11th grade for extra credit, so I didn't really put much effort into it. 

After re-reading the paper I realized that I made the mistake of arbitrarily assigned .000001% to be ""statistically impossible"", and that I assumed that there are a finite number of positive assertions that are true, and an infinite number of negative assertions that are true.

Now, what I mean by positive and negative assertion. I use positive assertion to mean an assertion that claims something to be true. For example, saying ""new laws are needed to regulate alcohol"" and ""humans are affecting the global climate""  and ""one plus one equals two"" are all positive assertions. If it says this is true, we should do this, so on, it is a positive assertion. 

Whereas a negative assertion is an assertion that claims that a something to be false. For example, saying ""we do not need laws to regulate alcohol"" or ""humans are affecting the global climate"" or ""one plus one does not equal two"" are all negative assertions. If it says this is false, we should not do this, so on, it is a negative assertion.

Now, my question is was I right? Is it statistically impossible for a positive assertion to be true, considering the infinite ways it can be false? Is there a way of proving this? 

I don't know if it is possible to determinant the statistical probability between degrees of infinity. 

If you need clarification about the question, please ask, because I know I am using slightly different definitions of positive assertion and negative assertion. 

For example, it seems whenever I talk to people about this, they think that ""I know this is wrong"" is a positive assertion, but it isn't.

Thanks for your time.",en
1109441,2012-04-02 08:12:36,MachineLearning,How to write a news search engine for the local language,rp4a5,jestinjoy,1207330851.0,https://www.reddit.com/r/MachineLearning/comments/rp4a5/how_to_write_a_news_search_engine_for_the_local/,8.0,8.0,"I am trying to write a news search engine for my local language. The problem is that different sites use different fonts.

I am just a beginner on this and I know how to use python. I am looking for some suggestions as to how should I proceed to write a search engine that lists related news for a given query.
",en
1109442,2012-04-02 18:57:34,statistics,1940 U.S census data for individuals now in public domain,rpn8w,blind_swordsman,1318716377.0,https://www.reddit.com/r/statistics/comments/rpn8w/1940_us_census_data_for_individuals_now_in_public/,17.0,1.0,,en
1109443,2012-04-02 20:39:37,datasets,Archives bends under rush for 1940 census records; share your family stories,rps5p,berlinbrown,1135573200.0,https://www.reddit.com/r/datasets/comments/rps5p/archives_bends_under_rush_for_1940_census_records/,0.0,0.0,http://openchannel.msnbc.msn.com/_news/2012/04/02/10972767-archives-bends-under-rush-for-1940-census-records-share-your-family-stories?chromedomain=usnews,en
1109444,2012-04-03 01:42:27,statistics,Do people choose birth dates more often in MegaMillions? Data and analysis (Google Docs),rq85b,sheppa28,1274041127.0,https://www.reddit.com/r/statistics/comments/rq85b/do_people_choose_birth_dates_more_often_in/,8.0,7.0,"**(Sorry about formatting; tried to clean it up)**

Posting this for discussion, debate, and hopefully someone can help with a more full analysis.

Data: [Google Docs spreadsheet](https://docs.google.com/spreadsheet/ccc?key=0AhkRh3HV6t8pdFZRR0t6bnl5eVA1bk9rR01YeWwtV0E)

I wanted to test the hypothesis if people chose to play the numbers 1-31 disproportionally higher than 32-56, due to birth dates or other reasons, as is commonly reported on the news. I couldn't find any archive of actual distributions of all tickets purchased, but then realized historical [prize tables](http://www.megamillions.com/numbers/pastdrawings.asp) might give some insight. If people do pick 1-31 more often (or some other set of numbers), then there should be more winners whenever those numbers appear in the winning drawing. 



First tab is **DrawDetails**, with columns:

* Index of Game, from 6/24/2005 (start of [current format](http://en.wikipedia.org/wiki/Mega_Millions#Basic_game) of the game)
* Date of Game
* Winning Numbers + MegaBall
* Prize
* Number of Winning Tickets

*(3+1) means they match 3/5 of the regular numbers and the MegaBall. Odds of winning given [here](http://www.megamillions.com/howto/). A total of 707 games have been held with the current format.*



The next tab, **Specific Numbers**, might take some explaining of what I was trying to do:

For each number (1-56) I counted the total number of games in which it appeared in the winning drawing (excluding megaball), as well the total sum of each type of winning combination.

*Example: The 14 listed for specific number 2, under (5+1) winning combination means that there were 14 total times in which someone matched (5+1) in a game in which ""2"" was a number in the winning drawing.*

I then tried to estimate the total number of actual winning tickets that contained that number. If 3,381 total people won with (4+1) then there should be (4/5) chance any specific number (out of the 5 winning numbers) are in that group of 4. Likewise for (4+0), etc. Multiplied each total amount by the probability that specific number is contained in that winning combination (4+1 or 5+0, etc.) and added them up. This is column M.

Column N is the number we would expect to see for Column M, given the total number of tickets and chance a specific number appears in a winning ticket. Although I did compute it, I don't think I may have been right to use a Chi Square Test; as the ""OBS"" column is actually an estimate based on probability that number may appear in the winning combination for each column.


**TL;DR: Posting data and analysis to see if anyone can delve further, or have other fun analysis.**


",en
1109445,2012-04-03 05:36:14,statistics,Biostatistical suggestions,rqkn2,Marzipan86,1333260693.0,https://www.reddit.com/r/statistics/comments/rqkn2/biostatistical_suggestions/,2.0,9.0,"I am testing the effect of photoperiod and temperature on metabolic rate in a species of freshwater mussel. I used a 3X3 experimental design. More specifically, I had 3 tanks in each of 3 different photoperiod enclosures. Within each photoperiod, I had a ""hot"" tank, a ""medium"" tank and a ""cold"" tank. 

Anyway, I have over 12000 data points for each of the 45 mussels I used. So far I have performed a linear regression to determine rate of oxygen consumption and corrected this rate for soft tissue mass. This leaves me with one number for each of the 45 mussels.

Now I want to do three different things: Compare the nine treatments, compare the three photoperiods and compare the three temperatures. I've been told to use an ANOVA, but is that the best test to use? If so, how do I perform this test? I have access to Excel and SAS. 

EDIT: Just ran PROC UNIVARIATE by treatment (tank) and found my data to be non-normal... So.... Now what?
Any input would be appreciated.",en
1109446,2012-04-03 06:55:56,statistics,How do I compute correlation between two categorical variables?,rqors,statguy,1271026910.0,https://www.reddit.com/r/statistics/comments/rqors/how_do_i_compute_correlation_between_two/,14.0,7.0,"Maybe correlation is not the correct term here, but essentially I have file with say lots of columns of categorical data and I am interested in knowing which ones closely related.

For example one column can be type of food and another column can be color so we can have something like

 * chicken, white
 * chicken, white
 * chicken, white
 * chicken, white
 * tomato, red
 * tomato, red
 * tomato, red
 * apple, red
 * apple, red

So is there a way to determine the degree of correlation between the two columns. Essentially if I were to build a model with both these features (among many others) I could essentially just do away with color and only keep the type of food (For a linear model)

So is there a way for me to identify highly correlated categorical variables?

I looked into hellinger distance and bhattacharya coefficient but not sure if they apply here.",en
1109447,2012-04-03 10:13:04,MachineLearning,Optimization Algorithms in Machine Learning,rqw4j,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/rqw4j/optimization_algorithms_in_machine_learning/,12.0,1.0,,en
1109448,2012-04-03 15:58:13,statistics,Passive voice in technical writing: $100 bounty for a style guide that recommends it.,rr3t6,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/rr3t6/passive_voice_in_technical_writing_100_bounty_for/,18.0,23.0,,en
1109449,2012-04-03 16:43:40,MachineLearning,How can I get started in Predictive Analytics?,rr5e8,yougottawanna,1305139209.0,https://www.reddit.com/r/MachineLearning/comments/rr5e8/how_can_i_get_started_in_predictive_analytics/,6.0,11.0,"I have a BS in Finance from about 15 years ago but have mainly used it for basic accounting, modeling and financial reporting roles.  I am interested in learning how to build predictive models related to customer behavior, but my last math class was an entry level calculus course in college.

I have a basic understanding of statistics (as in, took a 101 level stats course and have read the wikipedia articles about regression analysis.)  I would say my algebra &amp; trig are extremely rusty and would probably need to be learned from scratch, and my calc is non-existent.

So, what math do I need to really excel in this field?  Mainly stats?  Do I need to relearn the algebra and trig before I proceed to higher level math?  What else do I need to know?

I have 10 hours / week to dedicate to self-study and I really want to learn.

Help!

**Edit: Apologies if I've ruffled some feathers by not looking hard enough for the FAQ or the many weekly identical posts asking for information.**",en
1109450,2012-04-03 17:36:45,statistics,The Stata Blog » The Penultimate Guide to Precision,rr7l5,Professor_IR,1310512226.0,https://www.reddit.com/r/statistics/comments/rr7l5/the_stata_blog_the_penultimate_guide_to_precision/,5.0,1.0,,en
1109451,2012-04-03 18:18:12,MachineLearning,Computing solutions for crunching large datasets?,rr9h9,[deleted],,https://www.reddit.com/r/MachineLearning/comments/rr9h9/computing_solutions_for_crunching_large_datasets/,5.0,7.0,"Hey all, I'm starting to work on an indpendent project or two with some engineering and economist friends of mine, and we're realizing that we don't really have the computing power at our disposal to make the most of really large datasets (ie, we own three laptops between us). 

Do you guys have any useful (and hopefully economical solutions) for us? What do you guys use? Dedicated computer(s)? Fancy pants cloud solution? Any help is appreciated.",en
1109452,2012-04-03 18:23:22,MachineLearning,AskML: How much confidence can one have in nested cross-validation results?,rr9q6,hbweb500,,https://www.reddit.com/r/MachineLearning/comments/rr9q6/askml_how_much_confidence_can_one_have_in_nested/,8.0,8.0,"I posted this on the Kernel Machines forum, but I thought I would try my luck here, too:

In short, I am wondering how much confidence to have in the results of nested cross-validation after noticing some weird results.

I have a small amount of data (~115 examples), so I am using nested, leave-one-out cross-validation to come up with an estimate of accuracy and to do parameter selection for the RBF kernel. 

Now, I have developed a set of 128 different ways to generate feature vectors, and I'd like to pick the one that will give me the best performance. To do so, I test each of the 128 different methods using the nested cross-validation approach.

The results are a bit puzzling. The majority of the feature generators produce accuracy that is around chance, or reasonably close to it. My best results have accuracy of around 70%, which is pretty good for the data being classified. A typical confusion matrix for such cases looks like:

     ( 36 18 )
     ( 14 47 )

where the (i,j) entry is the number of examples that are actually class i but were classified as class j.

This is all well and good. My problem is that there are some choices of features which produce accuracy near 20%. For example, this is a confusion matrix from one of these cases:

    (  1 53 )
    ( 42 19 )


This poses a problem, because I could just use this choice of features and flip the classification and get ~83% accuracy!

So this worries me. Is it just that the classifier is performing at chance, and that I have so few examples that performing at 20% is just bad luck? This would also mean that my ""good"" results of ~70% accuracy may also be due to luck. Or is there another explanation, perhaps, for obtaining a confusion matrix that is almost entirely off-diagonal?

Also: is there a better measure than accuracy to judge the performance of the classifier? I am currently using the parameters which give the best accuracy in classifying the training set in cross-validation, but it occurs to me that there may be other, better measures (ROC curve, etc...). 

Thanks!
",en
1109453,2012-04-03 18:27:32,MachineLearning,"Operations Research, Machine Learning, and Optimization",rr9xq,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/rr9xq/operations_research_machine_learning_and/,17.0,6.0,,en
1109454,2012-04-03 19:07:08,statistics,"Time series basics?

",rrbte,[deleted],,https://www.reddit.com/r/statistics/comments/rrbte/time_series_basics/,1.0,0.0,"Can someone post a link to a page that would teach me the basics of time series? I'm in my first year of university and I need only the very basic stuff. The topics in my syllabus are:

    Time Series

10.1 Meaning of Time Series

10.2 Various components of a time series (Explanation and illustrations of each component)

10.3 Additive and multiplicative methods for analysis of a time series.

10.4 Methods of estimating trends (i) Freehand or graphical method (ii) Method of least square (iii) Method of semi-averages (iv) Method of moving averages.

10.5 Simple Numerical Problems.
",en
1109455,2012-04-03 19:14:07,datasets,1940 census records released by the US National Archives,rrc5c,lpiloto,1310021871.0,https://www.reddit.com/r/datasets/comments/rrc5c/1940_census_records_released_by_the_us_national/,13.0,0.0,,en
1109456,2012-04-03 19:52:07,statistics,Repeated Measures Statistics question,rre25,[deleted],,https://www.reddit.com/r/statistics/comments/rre25/repeated_measures_statistics_question/,5.0,13.0,"Hi there,

I am in the process of making some changes for a manuscript that I just had returned to me. This study analyzed the effects of an increase in alcohol provided to a group of mice on both their body weights and their liquid consumption. I have 4 different alcohol values provided (0%, 5%, 10%, 15%, and 20% w/v). All of the alcohol% have data over ~7 days, and the last (20%) for ~5weeks. We originally utilized a t-test at the different time points we increased the alcohol but need a different statistics test as per reviewer comments.

I took an intro stats class 3 years ago, and in searching it appears as though I will need to use a repeated measures ANOVA. I saw this post ( http://www.reddit.com/r/statistics/comments/r6tyv/repeated_measure_anova_question/ ) but I don't have any statistics software. Hopefully someone can point me in the right direction at a somewhat novice level. 

Any help would be great. Thanks!",en
1109457,2012-04-03 22:41:38,rstats,Can anyone tell me what cranvas is?,rrmst,approximist,1325038443.0,https://www.reddit.com/r/rstats/comments/rrmst/can_anyone_tell_me_what_cranvas_is/,5.0,0.0,"I know it is a package for creating interactive plots. But their github wiki is opaque to me. From what it appears, it does not seem to be a package as the installation procedure is all about qt based libraries? I just completed the instructions but have no idea how to use it. Does anyone have experience with this package?",en
1109458,2012-04-04 00:28:36,rstats,What are the best online forums for R?,rrslq,internetrageguy,1332700653.0,https://www.reddit.com/r/rstats/comments/rrslq/what_are_the_best_online_forums_for_r/,1.0,0.0,"I'm a pretty new R user, but I'm very familiar with MATLAB.  I'm trying to translate lots of my MATLAB code to R so that people who don't want to spend $$$ on software can use it.  I could really use some help beyond the documentation that exists for normal R functions.  

For example, today, I discovered that in order to create a real column vector, you have to do the following in R:

x = t(t(1:5));

     [,1]

[1,]    1

[2,]    2

[3,]    3

[4,]    4

[5,]    5

Yet, when you type 1:5 in R, it appears as a row vector:
[1] 1 2 3 4 5

How does double transposing [1] 1 2 3 4 5 create a column vector?  

Back in the MATLAB world where shit actually makes sense, it would be x = transpose(1:5); or: x= 1:5'.

Thanks for the suggestions.",en
1109459,2012-04-04 01:48:40,statistics,What are the best forums for R?  (X-post from rstats) ,rrwtu,internetrageguy,1332700653.0,https://www.reddit.com/r/statistics/comments/rrwtu/what_are_the_best_forums_for_r_xpost_from_rstats/,10.0,7.0,"I'm a pretty new R user, but I'm very familiar with MATLAB. I'm trying to translate lots of my MATLAB code to R so that people who don't want to spend $$$ on software can use it. I could really use some help beyond the documentation that exists for normal R functions.

For example, today, I discovered that in order to create a real column vector, you have to do the following in R:

x = t(t(1:5));

 [,1]

[1,] 1

[2,] 2

[3,] 3

[4,] 4

[5,] 5

Yet, when you type 1:5 in R, it appears as a row vector: [1] 1 2 3 4 5

How does double transposing [1] 1 2 3 4 5 create a column vector?

Back in the MATLAB world where shit actually makes sense, it would be x = transpose(1:5); or: x= 1:5'.

Thanks for the suggestions.
",en
1109460,2012-04-04 13:18:13,statistics,How do i calculate the rating of a player in a multiplayer contest over a large span of time. I came up with something but looking for something more standard and proven. ,rsomx,[deleted],,https://www.reddit.com/r/statistics/comments/rsomx/how_do_i_calculate_the_rating_of_a_player_in_a/,7.0,3.0,,en
1109461,2012-04-04 15:50:31,computervision,Free online CV courses?,rssj7,hey_what_happen,1333543639.0,https://www.reddit.com/r/computervision/comments/rssj7/free_online_cv_courses/,7.0,2.0,Are there any free CV courses out there? ,en
1109462,2012-04-04 19:29:20,statistics,How R Searches and Finds Stuff [r-bloggers],rt2sl,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/rt2sl/how_r_searches_and_finds_stuff_rbloggers/,7.0,2.0,,en
1109463,2012-04-04 21:07:26,artificial,OMax is a software environment which learns in real-time typical features of a musician's style and  plays along with him,rt86c,gallais,1274961186.0,https://www.reddit.com/r/artificial/comments/rt86c/omax_is_a_software_environment_which_learns_in/,15.0,1.0,,en
1109464,2012-04-05 00:00:46,datasets,The 70 Online Databases that Define Our Planet - Technology Review,rtins,TheyCallMeRINO,1248704491.0,https://www.reddit.com/r/datasets/comments/rtins/the_70_online_databases_that_define_our_planet/,27.0,1.0,,en
1109465,2012-04-05 01:07:35,MachineLearning,High False Negative Rate in Binary Text Classification,rtmj9,LADataJunkie,1327342831.0,https://www.reddit.com/r/MachineLearning/comments/rtmj9/high_false_negative_rate_in_binary_text/,10.0,13.0,"I am looking for some advice on how to accomplish a lower false negative rate in a binary classifier. I was taught to always start with Naive Bayes (if it has any hope of being decent) and then move on from there so that's why I have not tried anything more complex like LDA or sLDA etc.

I have a bunch of web pages (3500) to train a classifier for determining safe for work content vs NSFW. I have parsed them into text/unigrams, removed punctuation, stop words, words that only appear once etc. and initially used all of unigrams (37500)  as features in a Naive Bayes model. About 65% of the examples are safe for work (negative) and 35% are not safe for work (positive). Each data point is 0 or 1: 1 if the feature appears in the document, 0 else. Then I use the chi-squared feature extraction method to rank features in order of usefulness. Using 5, ..., 2000 features in steps of 5, I used 10-fold cross validation on Naive Bayes to attempt to find the best number of features to use. Unfortunately, the lowest false negative rate I can achieve is 40%, whereas the false positive rate is only 10%.

In this case, a false negative is much much worse as I am classifying pages as safe for work or NSFW. I also tried upweighting the NSFW examples so that based on weights, 50% of the data is safe and 50% is not safe but this did not help.

What are some techniques I can use to get a lower false negative rate? Am I wrong to be starting with Naive Bayes and should I try some other method instead?",en
1109466,2012-04-05 03:03:22,statistics,Help! Need to demo geo. wgtd. regression in a package,rtsos,prawer,1299174928.0,https://www.reddit.com/r/statistics/comments/rtsos/help_need_to_demo_geo_wgtd_regression_in_a_package/,2.0,3.0,"Hi Everybody,

I need to demo geographically weighted regression to a group of stats grad students.  I am an urban policy grad student and am planning to start the demo in arcgis, but the computers in their classroom don't have it (naturally), so I'd like to take an export of the data and finish or repeat the demo in a package that they have access to.

I am personally familiar with SAS and SPSS, and the stats computers have SAS, SPSS, R, and a few other things that I don't really recognize (Maple? Matlab? Minitab?)  It would be nice to do the demo in a package I know, but it would probably be more nice if the package I use has built in modules for GWR (a proc in SAS?).

I guess I'm wondering:

1. Does SAS have built-in GWR procedures? Or are there third-party macros that have been built to facilitate this?
2. I have a feeling one university statistician or another has implemented GWR in R, but I know nothing of how to use R / get started.  My presentation is on April 16th -- if anyone can recommend a tutorial and/or a GWR module for R, that would be great.

Thanks in advance for your help!",en
1109467,2012-04-05 05:06:46,statistics,Stats opinion request: LMM,rtzc3,ecocurious,1333591179.0,https://www.reddit.com/r/statistics/comments/rtzc3/stats_opinion_request_lmm/,1.0,2.0,"So I have a design where my sample unit is individuals of a species. These samples are distributed across a gradient of sites. Each site has a value that represents environmental characteristics of that site.   
.  
My question is looking at whether the behavioral relationships of these individuals changes depending on the environmental characteristics. Specifically, whether the relationship between territory quality and territorial behavior changes when these environmental variables change (so I'm most interested in the interaction term)  
.  
My LMM model is territorial behavior ~ territory quality + environ + territory quality*environ  
.  
All are continuous variables    
It was suggested to me to use site as a random effect in this model, because there are multiple individuals from each site. However, since the environ variable is a unique value for each site (i.e each individual from site A has the same value), is this redundant? My gut tells me the random effect isn't necessary. Thoughts?

",en
1109468,2012-04-05 05:25:53,statistics,zeros,ru0db,chirpychirp,1277776610.0,https://www.reddit.com/r/statistics/comments/ru0db/zeros/,5.0,10.0,I have data that is zero-inflated but not count data. What is the most appropriate distribution? ,en
1109469,2012-04-05 07:39:24,statistics,"Hi r/statistics, just a friendly reminder to provide very specific information when asking a stats question!",ru6rc,cvet,1270519045.0,https://www.reddit.com/r/statistics/comments/ru6rc/hi_rstatistics_just_a_friendly_reminder_to/,19.0,1.0,"R/statistics—I like you guys &amp; gals: question-askers want to learn and question-answerers want to help. However, I've noticed that far too many questions recently necessitate responses of ""please provide more information so we can start discussing the question.""

As a late ""convert"" to statistics (English major now doing economic research), I admit to poorly phrasing both questions and answers. It's something I've tried to curtail as my knowledge of statistics has grown. I just want to remind other learners here to try and do the same: conversation is most productive when we've got a good idea of the discussion right up front. 

Thanks &amp; you may now return to your software packages!",en
1109470,2012-04-05 09:56:59,MachineLearning,Machine Learning with Python,rubew,Faleira,1323241077.0,https://www.reddit.com/r/MachineLearning/comments/rubew/machine_learning_with_python/,20.0,13.0,"(xpost from [/r/learnprogramming](/r/learnprogramming) because I hadn't known this subreddit existed)

Hi, so I'm trying to learn how to use Orange with python to do data comparisons, and machine learning to make educated predictions and would be grateful if i could be pointed into the right direction for what I wanted to do. 

So to begin with, i'm trying to compare 2 sets of data, and see if one affects the other. For example, if i had the following data sets:

A = [1,2,3,2,1] 

B = [2,5,8,1,3] 

C = [10,14,16,14,10]

Is there some kind of analysis that could be done, where Orange could figure out that A and C are similar, as they increase and decrease at the same points? I'm hoping to be able to identify that C is closer to A than B in that regard, and then, say if C had another value given, be able to predict what the new value in A was.

I'm currently still following the tutorials found on the Orange site , but, I feel like i still may not know how to approach my problem at the end.
If anyone knows how to go about this, do you think you could point out some functions or classes I should be looking into?

Would it be better to do this with something other than Orange? such as PyBrain or PyML?",en
1109471,2012-04-05 12:24:26,computervision,Intro to Gestural Interfaces with the Microsoft Kinect,rueta,cavedave,1128052800.0,https://www.reddit.com/r/computervision/comments/rueta/intro_to_gestural_interfaces_with_the_microsoft/,7.0,0.0,,en
1109472,2012-04-05 17:11:31,computervision,Ball Tracking with OpenCV.. need suggestions,rumdi,hey_what_happen,1333543639.0,https://www.reddit.com/r/computervision/comments/rumdi/ball_tracking_with_opencv_need_suggestions/,7.0,10.0,"This is a ball tracking project that I started to get into computer vision.   Currently goes by like this.  

convert to HSV color space --&gt; threshold --&gt; hough transformation for the ball


Any suggestions about how I can improve this? Perhaps some algorithms that I should use?


EDIT: forgot the link


http://www.youtube.com/watch?v=g1n1U5qoUNc

Edit 2: Modified program thanks with tips from Jdban and artard. I would appreciate some feedback if possible. Thank you

http://www.youtube.com/watch?v=KtcFCDXVkXc


",en
1109473,2012-04-05 20:26:32,statistics,What do you do in Statistics Club?,ruwcf,LittleBigBen1,1321615729.0,https://www.reddit.com/r/statistics/comments/ruwcf/what_do_you_do_in_statistics_club/,5.0,12.0,,en
1109474,2012-04-05 21:14:04,MachineLearning,What are some metrics I can use to measure the similarity/distance between a pair of variables - each variable consisting of two different types of data(i.e. each variable has spatial and temporal data)?,ruyvm,lpiloto,1310021871.0,https://www.reddit.com/r/MachineLearning/comments/ruyvm/what_are_some_metrics_i_can_use_to_measure_the/,1.0,4.0,"EDIT: Just to clarify, the basic solution I can come up with is to do a comparison between each time series of the two different items and a comparison between the spatial values of the same items and measure the similarity between the two items as a some average of the two similarity results.  However, I was hoping to see if there was anything more sophisticated, for instance, I think mutual correlation coefficient might be applicable.

Edit: Added much more specificity.

I have a list of N components, X_1 ... X_n, for each of my K subjects, S_1...S_k. Each X component has two features: feature1 = a time series consisting of ~200 points of data. feature2 = a set of continuous variables.
Given this setup, I would like to compare two components, X_a and X_b, to each other by comparing X_a.feature1 against X_b.feature1 and X_a.feature2 against X_b.feature2. I would like to use these two comparisons to produce a single value measuring the similarity between component X_a and component X_b.
The goal of analyzing similarity between components is to say that out of all components from subject S_m, component X_x best corresponds to component X_y from subject S_n.
I hope these details help.",en
1109475,2012-04-05 23:44:25,statistics,"Fast R question: does lm store a vector of p-values that I can quickly access somehow, and if so, how?",rv735,Crotchfirefly,1289629047.0,https://www.reddit.com/r/statistics/comments/rv735/fast_r_question_does_lm_store_a_vector_of_pvalues/,4.0,5.0,"Let's say that I have a linear model called amod.  Is there some function maybe that returns the column of p-values that are returned when I use ""summary(amod)"", preferably in a usable vector?  Or do I just have to recalculate them?",en
1109476,2012-04-06 02:40:01,statistics,Help with post-hoc testing?,rvg1c,Jacqland,1294597312.0,https://www.reddit.com/r/statistics/comments/rvg1c/help_with_posthoc_testing/,3.0,4.0,"Hello kind ladies and gentlemen. For my Honours thesis (undergrad) I have been running some experiments, and now that I've run my ANOVAs I have completely forgotten / don't know how to determine what direction my significant results are slanting. 

I think I have to do some kind of t-tests between all the levels and factors, but I really don't know and I've been tearing my hair out trying to make sense of the textbook I used for Quantitative Methods of Linguistics a few years ago.

Basically, I have the following significant factors and interactions: TrainedVoice, Test, StimType:Test, TrainedVoice:StimType:Test

Here's the levels breakdown:

TrainedVoice|StimType|Test
:--:|:--:|:--:
W|N|0
E|T|1
 |S|2

So... 

1. How do I determine the direction of significance? (IE -Is TrainedVoice W causing response times to be slower or faster then TrainedVoice E?)

2. How do I run my post-hoc tests, or determine my Bonferroni Correction?

3. Helllllllp. If you're familiar with R, that would also help very much, since it's what I've been using thus far. Would R result help? I ran 3 ANOVAs, looking at the factors Time1, UCW, and UFW:

UCW

                                 Df Sum Sq Mean Sq F value    Pr(&gt;F)    

        TrainVoice                 1  1.716  1.7162 16.3723 5.669e-05 ***

        Test                       1  6.229  6.2295 59.4283 3.476e-14 ***

        TrainVoice:Test            1  0.192  0.1916  1.8278  0.176744    

        StimType:Test              2  1.104  0.5519  5.2654  0.005334 ** 

        TrainVoice:StimType:Test   2  1.137  0.5685  5.4230  0.004565 ** 

        Residuals                866 90.777  0.1048                      

        Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 `


UFW

                                  Df Sum Sq Mean Sq F value    Pr(&gt;F)    

        TrainVoice                 1  1.186 1.18554 11.5992 0.0006902 ***

        Test                       1  0.852 0.85189  8.3348 0.0039860 ** 

        TrainVoice:Test            1  0.135 0.13537  1.3244 0.2501212    

        StimType:Test              2  0.444 0.22209  2.1729 0.1144648    

        TrainVoice:StimType:Test   2  0.076 0.03786  0.3704 0.6905525  
  
        Residuals                866 88.513 0.10221                      

        Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 


Time1

                                   Df     Sum Sq   Mean Sq F value   Pr(&gt;F)   

        TrainVoice                  1 2.4787e+07  24787476  0.7878 0.374915   

        Test                        1 2.7428e+08 274281780  8.7178 0.003207 **

        TrainVoice:Test             1 6.1439e+07  61439033  1.9528 0.162524   

        StimType:Test               2 2.1658e+08 108292313  3.4420 0.032292 * 

        TrainVoice:StimType:Test    2 2.0517e+08 102584593  3.2606 0.038679 * 

        Residuals                1306 4.1090e+10  31462218                 
   
        Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

Let me know if there's any other information I can give you. (This was also posted in homeworkhelp, but I messaged the mods here and they said they think its appropriate).
",en
1109477,2012-04-06 04:33:48,statistics,"When calculating an expected value, remember E=1.",rvlmf,kstein1110,1235593061.0,https://www.reddit.com/r/statistics/comments/rvlmf/when_calculating_an_expected_value_remember_e1/,1.0,0.0,,en
1109478,2012-04-06 06:13:07,statistics,A couple quick STATA questions regarding frequency tables,rvqcm,thisguyukno,1285797089.0,https://www.reddit.com/r/statistics/comments/rvqcm/a_couple_quick_stata_questions_regarding/,3.0,5.0,"I have two fairly simple questions I hope someone here can help me with. I'm working with a dataset that has a lot of dummy variables and I'm trying to create a two-way frequency table of sorts (something like tab, sum() I suppose), but instead of it comparing just one variable to another, I want to compare a LIST of variables to another variable. To put it another way, I want to show what the frequency is of one variable having the value of 1 (the only other possible value is zero) if some other variable is also one (demographic indicator/dummy variables or what have you). So, for instance I have dummy variables for MALE, FEMALE, BLACK, WHITE HISPANIC, ASIAN, etc where an observation (individual) has this characteristic if the value is one, and I want to be able to say ""Male's show 45% adoption of this indicator variable, Females show 32% of this indicator variable"", etc, but just a big table of these figures. I really don't want to have to tab, sum() for each variable I want to analyze, and I'm fairly sure it's possible but I don't know how to. :(

Secondly, the variable in question is coded as 1 ""Yes"" 2 ""No"". If I only care about the frequency of Yes's, should I recode all the 2's as 0 ""No"" and sum the variable in question when I tabulate, or is there a better way to get just the frequency of yes's (1's) that I am not aware of? I really need your help, reddit, and I'll be forever greatful. :B",en
1109479,2012-04-06 06:50:51,MachineLearning,X-post from r/statistics: How to rank products based on user input,rvs55,[deleted],,https://www.reddit.com/r/MachineLearning/comments/rvs55/xpost_from_rstatistics_how_to_rank_products_based/,1.0,0.0,,en
1109480,2012-04-06 07:53:15,statistics,Question about Math Subject GREs for Statistics PhD programs,rvuye,hep_th,1303620608.0,https://www.reddit.com/r/statistics/comments/rvuye/question_about_math_subject_gres_for_statistics/,7.0,3.0,"How required is the math subject gres? I get the impression sometimes when reading about programs that it is _very_ helpful, but I'm worried that because I'm not very experienced in some of the higher level topics (topology etc), the score could be detrimental to my efforts. Any advice? Thanks.",en
1109481,2012-04-06 09:30:27,statistics,Question about regression modeling using SAS. All tips/ideas much appreciated. ,rvyh9,[deleted],,https://www.reddit.com/r/statistics/comments/rvyh9/question_about_regression_modeling_using_sas_all/,1.0,0.0,"I am looking for any help/guidance building a regression model using SAS. This is college level undergraduate work for a freshman interested in statistics. 

The data I am using concerns employees  from one job category of a bank that was sued for sex discrimination. The employees were hired between 1965 and 1975. In my model Y is log(beginning salary). The other variables are:

• 1977 salary
• Sex (Female = 1)
• s = Seniority (months since time of hire)
• a = Age (months)
• e = Education (years)
• x =Experience (months of experience prior to hiring)

Seniority may seem like an odd variable to include, but it serves as a proxy for how long before 1977 the person was hired.

I want  to predict Y using all predictors except for sex. I was thinking about a  first order model then maybe exploring other more complex models that include quadratic terms of the predictors and  pairwise interactions between the 4 main variables. 

My questions are:

1) Any tips on  how to do this quickly and easy on SAS?

2) How do I go about finding the best model?

Any suggestions, comments are appreciated. ",en
1109482,2012-04-06 10:05:06,statistics,Does PhD program matter for an academic position? (UFL vs Ohio State vs UNC Bio),rvzfg,fraulein_x,1333695601.0,https://www.reddit.com/r/statistics/comments/rvzfg/does_phd_program_matter_for_an_academic_position/,2.0,1.0,"Does PhD program matter for an academic position? Do only your publications matter? What is the benefit of a higher-ranked program? Can one be hired for a good position from any of (UFL, Ohio State, UNC Bio)?",en
1109483,2012-04-06 11:36:16,rstats,A short introduction to R.,rw1nk,srkiboy83,1299056137.0,https://www.reddit.com/r/rstats/comments/rw1nk/a_short_introduction_to_r/,18.0,1.0,,en
1109484,2012-04-06 13:40:18,statistics,Co-Vary Or Die: How correcting for confounders saved lives,rw428,greenrd,1195738321.0,https://www.reddit.com/r/statistics/comments/rw428/covary_or_die_how_correcting_for_confounders/,17.0,1.0,,en
1109485,2012-04-06 15:03:24,statistics,Extending reddit's ranking system to rate users in a game. ,rw5rw,fruiapps,1332186636.0,https://www.reddit.com/r/statistics/comments/rw5rw/extending_reddits_ranking_system_to_rate_users_in/,0.0,0.0,,en
1109486,2012-04-06 16:45:27,statistics,Stuck between grad school for statistics and actuarial track,rw8qm,negative_epsilon,1321570073.0,https://www.reddit.com/r/statistics/comments/rw8qm/stuck_between_grad_school_for_statistics_and/,15.0,15.0,"Hey guys, I've been fighting with myself over the last four semesters over what I want to do with my life, and it's time to shit or get off the pot (Two semesters until I get my B.S. in stats with a math minor). I'm torn between the track of being a statistics professor and the track to become an actuary. I was really unhappy as a mathematics major for quite a few semesters, so my GPA tanked to around 2.8 and now that I'm a stats major, I'm pretty I sure I could pull it up to a 3.0 before I'm done but that's about it. I go to UCF and my school has a graduate statistics program desperate for students, so I'm sure I could at least get into the program here, but with a 3.0 I'm not sure I'd be able to get into many schools. Without a school with name recognition, how hard is it to get into Ph.D programs? Also, I think I want to get more into college level teaching than researching, is that common for statistics doctorates? 

On the other hand, I passed Exam P on my first go and feel like I have the ability to pass FM on my first go if I took it in August like I was originally planning. I know it would again be a bit of a challenge getting into the field with such a low GPA, but if I did I'd have it in for the rest of my career. I just feel like I wouldn't be as fulfilled as I would be going to grad school and eventually becoming a professor. I don't know.

I guess my question to you guys is: Were any of you stuck between this decision? What swayed you? How is the market for statistics professors, even at community colleges? Am I pretty much SOL for any half-way decent grad school with a piss-poor GPA? ",en
1109487,2012-04-06 18:49:09,MachineLearning,More Precious than Gold?,rwe8f,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/rwe8f/more_precious_than_gold/,6.0,0.0,,en
1109488,2012-04-06 18:49:54,statistics,More Precious than Gold?,rwe9y,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/rwe9y/more_precious_than_gold/,8.0,0.0,,en
1109489,2012-04-06 20:08:29,statistics,Suggestions for sample sizes?,rwia7,ecocurious,1333591179.0,https://www.reddit.com/r/statistics/comments/rwia7/suggestions_for_sample_sizes/,4.0,7.0,"I'm looking for papers that give suggestions for minimum sample sizes to run multiple regression models. I think I remember reading somewhere that 3, 5 or 10 samples per fixed effect was necessary, but I'm having difficulty finding a good source to cite. Also, does the same hold for interaction terms as well? I have a n=50 sample that I want to run a model with 4 fixed terms and 6 interaction terms. Is my sample size absurdly low? I'm confused as to how to run an appropriate model that doesn't eat up all my df and give spurious results. Can help me understand? thanks!",en
1109490,2012-04-07 00:35:35,statistics,PLEASE HELP!!! - Probability distribution for the sums of rolling three standard die,rwxui,[deleted],,https://www.reddit.com/r/statistics/comments/rwxui/please_help_probability_distribution_for_the_sums/,1.0,1.0,I'm EXTREMELY lost here. All I can find online or in my text book is how to do a probability distribution for the sums of rolling 1 standard die. Can someone PLEASE walk me through how to create a probability distribution for the sums of rolling three standard die??,en
1109491,2012-04-07 01:52:58,statistics,I need a 10 - 15 minute activity for high school / young college students to draw interest in statistics.,rx1ht,OctopusGasm,1326149945.0,https://www.reddit.com/r/statistics/comments/rx1ht/i_need_a_10_15_minute_activity_for_high_school/,13.0,13.0,"I have been asked to prepare a 10-15 minute activity for a general advanced mathematics fair for high school seniors and college freshman.  I would like an activity related to statistics and/or probability that has every student participate. I think my groups will be small, but each field is given a short amount of time for activities.

So far I've been thinking about estimating pi with Buffon's needle experiment and some form of Deal or No Deal.  Could you volunteer any ideas that you know of or have tried with success?  Thanks in advance.",en
1109492,2012-04-07 08:48:22,MachineLearning,Homework advice - Clustering w/o K-Means?,rxjdt,groundshop,1274017460.0,https://www.reddit.com/r/MachineLearning/comments/rxjdt/homework_advice_clustering_wo_kmeans/,19.0,50.0,"Hello all,
I'm working on a data mining project that ends with me clustering bag-of-words type data. 

The majority of the project so far has been pre-processing (the data is an awesome web-crawled data set of tweets from middle eastern countries during the arab spring!). I have a dictionary made of word counts, so I can assign some sort of weight to each word. 

I'm getting to the point now where I need to actually cluster the data. The vectors are very sparse (each feature is a word :/ Maybe I should try something else for this? Kernel method to map it onto some subspace??) After alllll the work I've done preprocessing rough, incomplete, arabic/french/english mixtures of tweets I feel like I've got to find SOME algorithm that's more complicated than the k-means that the professor spoon fed us. 

Any thoughts? If anyone knows of an algorithm that's particularly good on sparse data, I will upvote you and your family.",en
1109493,2012-04-07 13:59:22,artificial,The Hunt for AI [x-post from r/Documentaries],rxqrz,CopyofacOpyofacoPyof,1324911721.0,https://www.reddit.com/r/artificial/comments/rxqrz/the_hunt_for_ai_xpost_from_rdocumentaries/,41.0,8.0,,en
1109494,2012-04-07 17:36:51,statistics,Regression Analysis using a spatial covariate?,rxvg9,denacioust,1294348977.0,https://www.reddit.com/r/statistics/comments/rxvg9/regression_analysis_using_a_spatial_covariate/,1.0,11.0,"I'm currently working on modelling some data for which the latitude and longitude are covariates. Upon fitting a simple linear model their coefficients don't appear to be significant, however something about using them as they are just doesn't feel right to me.

I'm just wondering if anyone has come across any method of using latitude and longitude data in a regression model in a more appropriate manner?",en
1109495,2012-04-07 23:01:19,statistics,Any advice on learning to use R?,ry8wi,sg187,1333569514.0,https://www.reddit.com/r/statistics/comments/ry8wi/any_advice_on_learning_to_use_r/,20.0,16.0,"I taken some programming (java and c++) so I have a general knowledge of that and quite a bit of stats. I'm looking to learn R, are there any resources that you all have found especially helpful?",en
1109496,2012-04-08 01:08:18,statistics,T-Test to compare two ratios of averages?,ryeps,TheDisreputableDog,1320804647.0,https://www.reddit.com/r/statistics/comments/ryeps/ttest_to_compare_two_ratios_of_averages/,3.0,9.0,"Okay, so I haven't taken a stats class for five years, but I'm analyzing data and am at a loss of what to do. My data is the ratio of two averages.  I know how to get the std dev of the ratio of two averages, but I don't know how to get the p value of two ratios of averages. I don't even know if what I'm saying makes sense, but I've tried looking for an answer on the internet and can't find what I'm looking for. Any help will be great!

Edit: Someone let me know I wasn't being clear enough with a link, so I'll give you an example of what I am doing. I can't use exactly what I'm doing because it may be proprietary. Okay,say I have a two step process, with step A happening before step B. In this system, it is better for step B to run faster than step A.  I want to test a treatment that will make the process run better.  This can happen in one of two ways: the treatment makes A slower or B faster. It doesn't matter which. So, I have my untreated process, and I measure how fast A runs and how fast B runs 3 times. I take the averages of the speeds of the 3 As and the 3 Bs and then make a ratio of those averages, A/B. This ratio is my data. I then run my test treatment and take the same measurements. I now have two ratios. Say the test ratio is lower than the control (thus the test is better). How do I show that the test is signification better? ",en
1109497,2012-04-08 01:32:21,statistics,Advice on generalized linear model in R,ryfsh,r_newbie,1333837570.0,https://www.reddit.com/r/statistics/comments/ryfsh/advice_on_generalized_linear_model_in_r/,1.0,11.0,"Hello, I am new to R and not a statistics guru and would really appreciate advice here. I am trying to determine the relationship between a dependent variable S and three independent variables, which we can call U, X and Y. The relationship, as far as I can guess, is:

S = kU + min(aX, bY) + ""noise""

where k, a and b are the constants I need to determine. The reason for min is this: X and Y are the flow rates of two ""machines"" in series (roughly) and S is the flow rate of the final output. The slower of the two machines determines the throughput. U refers to the difficulty of the job itself.

I have about 50+ data points, and can generate some more, though for larger job sizes, it takes a while to generate a data point. Since min is not a nice polynomial, I would really appreciate any advice on how to model it. My real goal is to determine optimal values of X and Y for a given size U. That is, for a given U, I want to determine X and Y so that S is maximized. Thanks!",en
1109498,2012-04-08 01:56:51,datasets,Looking for property sales and assessment data sets for property tax sales ratio study. Any ideas?,rygvr,metafork,1306272358.0,https://www.reddit.com/r/datasets/comments/rygvr/looking_for_property_sales_and_assessment_data/,1.0,4.0,,en
1109499,2012-04-08 03:42:30,statistics,"ICC(1,2) Question",ryl86,Palmsiepoo,1315548686.0,https://www.reddit.com/r/statistics/comments/ryl86/icc12_question/,2.0,6.0,"I'm working with a large data set of factory workers that are grouped into teams. They all rated their team leader using various leadership scales. I have individual data that needs to be aggregated to the group level. But before I do this I need to make sure there is enough consensus among the group to aggregate (i.e., does everyone view their leader the same way? If so, aggregation is appropriate). 

In order to do this I am calculating Rwg, ICC(1), and ICC(2) statistics. So far, I have found that average Rwg values are within the acceptable range (&gt;.7) and ICC(1) values are relatively small (&lt;.5) but ICC(2) values are extremely low (around .3ish). Why is this happening? It seems that there is agreement among ratings; why is this value so low? 

Thanks, I really appreciate any help on this. ",en
1109500,2012-04-08 04:56:40,statistics,Can someone explain the difference (if there is one) between hierarchical linear modeling and linear mixed-effects models?,ryodf,emsoni,1306717148.0,https://www.reddit.com/r/statistics/comments/ryodf/can_someone_explain_the_difference_if_there_is/,9.0,19.0,"I'll be analyzing fMRI data with two continuous and two categorical variables (predictors), and have been told to use LME by one lab and HLM by another. Trying to figure out which would be best!",en
1109501,2012-04-08 08:15:43,statistics,What are the chances of this happening?,rywfb,[deleted],,https://www.reddit.com/r/statistics/comments/rywfb/what_are_the_chances_of_this_happening/,1.0,0.0,,en
1109502,2012-04-08 18:18:51,AskStatistics,My friend has an IQ of &gt;= 135. What percentage of people did he score higher than?,rza30,[deleted],,https://www.reddit.com/r/AskStatistics/comments/rza30/my_friend_has_an_iq_of_135_what_percentage_of/,0.0,3.0,"My friend took an IQ test that gave him a score of 135. Unfortunately the test said that it wasn't sensitive to scores higher than that, so his IQ might be higher.

I know that IQs are measured on a normal distribution. My question is, what percentage of people is my friend ""smarter"" than? And how does one do the calculation? I had a look at the Wikipedia page for normal distributions but I'm kind of overwhelmed by the mathematical notation.

(PS I know that the IQ test isn't the be-all and end-all of intelligence)

Thanks!",en
1109503,2012-04-08 18:30:39,statistics,Probability Question Database,rzaj6,RSeafood,,https://www.reddit.com/r/statistics/comments/rzaj6/probability_question_database/,13.0,15.0,"I am looking for a database of fun and challenging probability questions. Sort of like project euler, but of probability problems. Does anyone know of one that exists?

I have tried Google, but most of the 'challenging probability' problems aren't interesting, too easy, etc. That is why I am more interested in something like projecteuler, something with hundreds of problems of varying difficulty. 

*Edit*
Grammar. Also I guess I am in the process of starting one. Any thoughts here would be nice. 

*Edit^2*
Site will be up. If anyone (I don't think anyone will respond to this :D) wants to partake in any way, message me. It will take me time as, well, I have a life and whatnot. ",en
1109504,2012-04-08 19:19:57,MachineLearning,"Ask ML: While training sparse autoencoders, how do you decide values for the sparsity parameter and it's associated weight used in the cost-function?",rzcfh,strayadvice,,https://www.reddit.com/r/MachineLearning/comments/rzcfh/ask_ml_while_training_sparse_autoencoders_how_do/,14.0,33.0,,en
1109505,2012-04-08 21:13:35,MachineLearning,An R programmer looks at Julia,rzhgj,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/rzhgj/an_r_programmer_looks_at_julia/,1.0,0.0,,en
1109506,2012-04-08 21:40:35,rstats,An R programmer looks at Julia,rzirv,utcursch,1175517803.0,https://www.reddit.com/r/rstats/comments/rzirv/an_r_programmer_looks_at_julia/,1.0,0.0,,en
1109507,2012-04-09 09:05:34,datasets,All TIME Magazine covers (March 1923 to March 2012).,s0fld,trexmatt,1312368694.0,https://www.reddit.com/r/datasets/comments/s0fld/all_time_magazine_covers_march_1923_to_march_2012/,15.0,3.0,"Thought someone might find this useful even though it's not exactly a dataset...  Here's all (or almost all) of the TIME Magazine covers from March 1923 to March 2012.

~250 MB total, with 4,517 JPEGs

EDIT: Fixed link - [link](http://minus.com/mpsP9r6jk/)",en
1109508,2012-04-09 15:40:43,artificial,There is a proposal to cover part of New York with artificial mountain and then use it for skiing ang hiking trips!,s0p3n,sadcxcv,1333975179.0,https://www.reddit.com/r/artificial/comments/s0p3n/there_is_a_proposal_to_cover_part_of_new_york/,0.0,2.0,,en
1109509,2012-04-09 16:49:56,statistics,'The Dawning of the Age of Stochasticity' - paper/lecture by David Mumford on the role of  statistics in Science / AI,s0rhq,blind_swordsman,1318716377.0,https://www.reddit.com/r/statistics/comments/s0rhq/the_dawning_of_the_age_of_stochasticity/,28.0,6.0,,en
1109510,2012-04-09 16:56:29,computervision,help with Hough transfrom?,s0rr1,waspinator,1202264096.0,https://www.reddit.com/r/computervision/comments/s0rr1/help_with_hough_transfrom/,5.0,5.0,"I'm having trouble getting the simple hough transform to work/understand. Could anyone see whats wrong with my code?

Thanks

http://dsp.stackexchange.com/questions/1958/help-understanding-hough-transform


**EDIT:** I'm also interested in the [generalized Hough transform](http://en.wikipedia.org/wiki/Generalised_Hough_transform). Where can I find a readable (non optimized) implementation of it?; preferably in matlab/octave

I'm stuck again trying to implement the Generalized version this time. Can't figure out how to [Φ(x)](http://i.stack.imgur.com/2Omxv.gif).
http://stackoverflow.com/questions/10176157/generalized-hough-r-table",en
1109511,2012-04-09 20:14:30,statistics,List of Websites to find Internet Usage Statistics,s11fg,pagetron,1328742785.0,https://www.reddit.com/r/statistics/comments/s11fg/list_of_websites_to_find_internet_usage_statistics/,1.0,0.0,,en
1109512,2012-04-09 20:59:57,statistics,To sample or not to sample: That is the question,s140v,RickWicklin,1327333524.0,https://www.reddit.com/r/statistics/comments/s140v/to_sample_or_not_to_sample_that_is_the_question/,3.0,0.0,,en
1109513,2012-04-09 21:10:52,statistics,Greater gender equity in math/stats than in other STEM disciplines,s14m2,RickWicklin,1327333524.0,https://www.reddit.com/r/statistics/comments/s14m2/greater_gender_equity_in_mathstats_than_in_other/,4.0,1.0,,en
1109514,2012-04-09 22:20:22,MachineLearning,Extracting Structure on a Very Complexly Interacting Feature Space,s18je,CPlusPlusDeveloper,1317051268.0,https://www.reddit.com/r/MachineLearning/comments/s18je/extracting_structure_on_a_very_complexly/,16.0,12.0,"Consider the case where you have a dataset with fairly low dimensionality and a very high number of data points. One such that in most supervised learning approaches you'd be more concerned with minimizing bias rather than variance.

However in this problem the relatively low dimensionality is somewhat deceptive because the variables have very complex high dimensional interactions. I'll give you an example of what I'm talking about:

Suppose you were looking at randomly sampled points in chess games between evenly matches players. The data you had was which pieces each player had lost and you wanted to predict whether the game would ultimately end up in win/lose/draw.

Your feature space in this case is 30 0/1 variables: 8 pawns, 2 rooks, 2 knights, 2 bishops, 1 queen, (no king because if he's missing the game's over) multiplied by 2 sides. Assume that you have access to hundreds of millions of independent data points (you can always run AI-AI games and sample new positions).

The idiosyncrasies of the interactions here runs far deeper than standard linear/quadratic variable interactions, for example:
---------------------------------------------------------------------------------------------------------------------------

* Early game when a lot of material's on the board knights tend to be better than bishops. When there's not much material bishops tend to be better.

* If a player is missing a few pawns from ranks far away from each other that's not as bad as a cluster of pawns missing which opens up a hole in his defense.

* At the end of the game if the other side has two bishops, but the difference between having one bishop and no bishop is the difference between a guaranteed draw and win.

* A queen is about equal to two rooks early game, but becomes much less powerful late game.

* The difference of one side having 1 pawn and the other side having no pawns is far more than the difference between having 7 pawns and 8 pawns (because of the chance to convert the pawn to another piece).

As you can see there are endless variations of how the variables can interact. Many of the interactions go along an early/late game dimension. I.e. fewer material more material interactions, which basically involve some derived metric from the underlying 30 variables (more pieces that are captured off the board the ""later game"" it is, so a rough approximation might just be a sum of the 30 0/1 variables).

This makes it very challenging because it's almost like you need a combined supervised-unsupervised learning process that can discover interesting projections of the variables to interact on (i.e. discover the early/late game metric and start interacting other variables on it). Then plug those into a supervised method. This brings me to the various approaches that I'm considering:
---------------------------------------------------------------------------------------------------------------------------

* Boosted trees: The problem is the underlying weak learners don't have any chance to capture any of the interesting structure in the feature space. For trees to work they need to have many nodes to discover the complex interactions, and boosting tends not to do well with deep trees.

* Random forests: This has a better chance of working. But I would have to use very deep trees to capture the interaction, and even though random forest does pretty well with deep trees I've never heard of trees 20+ levels deep which is what you would need to discover some of the early/late game interactions. (At least you would need to go 20+ deep if you were splitting nodes based on single variables).

* SVMs: This seems somewhat appealing. I'm not too experienced with SVMs, have much more experience with trees. But from what I understand they have a good reputation from capturing non-parametric interactions like these. My only concern here is that a Gaussian kernel wouldn't work here. But again I'm inexperienced so any input on this in particular would be greatly appreciated.

* Logistic regression: This one I have to throw out right away because the problem is so non-linear, and I would have to include 32! interactions (an interaction for every possible piece).

* Neural nets: I'm always somewhat skeptical of these, but the universal function feature does seem to be what I'm looking for here. It would have to be high layer though, and even though the data set's large, with ANNs over fitting is always a problem IMO. Maybe bagging many-layered ANNs here might be a good approach, but I've never heard of anyone doing this so I'm not sure if there are good arguments not to.

* Manifold learning: This is the most ""far out"" approach, but it seems like the only thing that has the direct capability of recovering what I want, i.e. low-material high-material variable projection. Of course I don't know how exactly to translate an unsupervised method into a supervised problem...

Anyway this problem has me tied up in knots. So any interesting approaches or good advice would be greatly appreciated. Thanks.
---------------------------------------------------------------------------------------------------------------------------
",en
1109515,2012-04-09 23:03:41,artificial,"Eccerobot, A Robo",s1azp,kondrat1983,1320575383.0,https://www.reddit.com/r/artificial/comments/s1azp/eccerobot_a_robo/,5.0,4.0,,en
1109516,2012-04-09 23:40:55,rstats,Comparing Julia and R’s Vocabularies,s1d2u,talgalili,1271226645.0,https://www.reddit.com/r/rstats/comments/s1d2u/comparing_julia_and_rs_vocabularies/,1.0,0.0,,en
1109517,2012-04-10 04:40:18,statistics,"Just finished reading this book, highly recommend it for those interested in the history of mathematical statistics.",s1tcq,sheppa28,1274041127.0,https://www.reddit.com/r/statistics/comments/s1tcq/just_finished_reading_this_book_highly_recommend/,8.0,6.0,,en
1109518,2012-04-10 07:27:10,statistics,"Dear r/statistics, I need your help: regression and time-series (x-post r/math)",s22a2,tentaclelove,1329868354.0,https://www.reddit.com/r/statistics/comments/s22a2/dear_rstatistics_i_need_your_help_regression_and/,6.0,8.0,"Hey guys, so here is:

I have no math skills whatsoever and I got started on a project not knowing what I was getting myself into. Anyway, I have a huge data set coming my way and I need to perform some sort of analysis on it.

I am getting in-situ measurements (lets call it var1) taken every hour over 4 months. Aside from that, I have two other variables (var2 and var3) being measured across the same time span. I need to analyze changes in var1 (statistically) over the collection time-span and to show if any changes in var2 or var3 (or both combined) correlate to changes in var1 (across time). How would I go about doing this? Any 3D way of looking at this? Would you guys know any books that I can read that would be relevant or any Matlab stuff I can look up?

Thank you very much for your help!!",en
1109519,2012-04-10 08:07:52,MachineLearning,"Can someone explain the Perceptron Learning Algorithm to me?  Like I'm a 5-year-old, please.",s243g,autodidact4life,1324101605.0,https://www.reddit.com/r/MachineLearning/comments/s243g/can_someone_explain_the_perceptron_learning/,18.0,11.0,"I'm trying to slog through the Caltech online course called ""Learning from Data.""  We haven't even turned in the first homework yet, and I'm lost.  We're supposed to code our own PLA:

Create 100 random points above and below a random line and designate those that are above as +1 and those that are below as -1.  Got it. I can do that pretty easily.

I understand initializing all of the weights to zero.

But, I get really lost when it comes to how the algorithm iterates.  I know that I'm supposed to multiply the weights across the inputs to create an output for each point.  I'm supposed to compare it to the desired output and then iterate.  How do I calculate desired output?  

To help myself, I choice a random line y=2x and then points above and below the line.  (1,1,3), (1,3,7), (1,2,3), (1,4,7) which map to  +1, +1, and -1, -1, respectively.

Can anyone on Reddit walk me through the actual steps that the computer would take with these data points as it iterates?  It's easy enough to copy a script from the net, but I still don't quite understand what it all does, and the mathematical notations aren't as helpful as I would have hoped.

Any help would be VERY much appreciated.  ",en
1109520,2012-04-10 17:36:00,MachineLearning,Trying to understand Matlab's classify.m,s2kig,[deleted],,https://www.reddit.com/r/MachineLearning/comments/s2kig/trying_to_understand_matlabs_classifym/,3.0,0.0,"I am trying to understand Matlab's classify.m which is a basic linear/quadratic discriminant implementation found as part of the statistics toolbox. 

From my machine learning textbook, for the quadratic discriminant the -0.5log(det(sigma)) term and the quadratic -0.5(x-m)'S ^-1 (x-m) term should have the same sign (both negative, with only the log prior the only positive term).

But in the matlab function it does the following:

    D(:,k) = log(prior(k)) - .5*(sum(A .* A, 2) + logDetSigma(k));

So the quadratic term and the logdet term have different signs. I would expect:

    D(:,k) = log(prior(k)) - .5*(sum(A .* A, 2) - 0.5*logDetSigma(k));

Could this be a bug in Matlab or am I missing something? The determinant is calculate as 2*sum(log(s)) where s are the singular values of R from QR decomposition of centered training data. I am not sure if this could explain it?
",en
1109521,2012-04-10 20:12:48,statistics,Help with understanding the purpose of this analysis from a journal paper (paired t-test),s2sl3,randombabble,1319191466.0,https://www.reddit.com/r/statistics/comments/s2sl3/help_with_understanding_the_purpose_of_this/,2.0,9.0,"I came across a paper where they conducted Principal Component Analysis and found 4 factors that influence the outcome. One of the statistical analysis the author chose to use was paired t-test which ""is used to determine the mean differences for each of the 4 factors"". From the paired t-test, the author said the mean of Factor1 (4.48) and the mean of Factor2 (4.57) were found to be more influential to the outcome than the mean of the Factor3 (3.39) and the mean of Factor4 (3.18). However, the analysis of this was not shown in the paper and I have been trying to understand how did the author come up with that claim from a paired samples t-test. I have been testing it out with some imaginary data and pairing them as follows:        
1) Factor1 paired with Factor2    
2) Factor1 paired with Factor3    
3) Factor1 paired with Factor4    
4) Factor2 paired with Factor3    
5) Factor2 paired with Factor4    
6) Factor3 paired with Factor4    
    
But even after doing that, I still can't seemed to understand how this can determine which 2 factors are more influential than any other 2? So perhaps I am misunderstanding this and would like to hear any suggestions.            
The paper is available at the link below and the paired t-test paragraph is on P.9 at the lower left hand corner if its any help.http://www.osra.org/itlpj/dillonreiffall2004.pdf
",en
1109522,2012-04-10 20:26:44,AskStatistics,Correlating Multiple Variables Year by Year?,s2tdh,boomfoom,1318908720.0,https://www.reddit.com/r/AskStatistics/comments/s2tdh/correlating_multiple_variables_year_by_year/,1.0,0.0,"If one has a number of variables that they want to be able to show the correlation over a set number of years, how would one show how closely linked these are?  Is there a good way to do time series correlation?",en
1109523,2012-04-10 22:08:02,statistics,Small business Market analysis insights,s2z8v,MrPeel11,1329231475.0,https://www.reddit.com/r/statistics/comments/s2z8v/small_business_market_analysis_insights/,6.0,17.0,"Hi guys,

This is the closest subreddit I could find that relates to my question. Basically coming out of university economics, I'm starting to see the value in starting my own small business. Looking into something with low startup costs, just in case it doesn't work out.
So my idea is to target local small, growing businesses and offer data analysis at a rate they can afford (less than a consultant) 
I have a few concerns on the feasibility of it though. Firstly, most retail companies will take some data... postal codes, name etc. I want to take their existing data and look for relationships such as key segments, and target markets for those companies with a tight marketing/advertising budget. 
My worry is the variety of data these places may or may not collect. Am I doomed to failure because companies may want the service but have a body of data too small to offer any valid insight?
What is a good book or resource on learning the best methods... I'm thinking of learning and dabbling in some cluster analysis, but I understand that for more validity I'll need a series of tests and methods. 
Also, what is a good stats platform to use... ideally I'd go for SAS, but that'll be out of budget for a while... I really like stata... is R a good platform to learn?

Basically anything you have for me, I'd be more than appreciative to learn and hear what you guys think

Thanks in advance!",en
1109524,2012-04-11 01:17:19,computervision,All TIME Magazine covers (March 1923 to March 2012). X-post from r/datasets and r/datamining,s3a51,rightname,1265921832.0,https://www.reddit.com/r/computervision/comments/s3a51/all_time_magazine_covers_march_1923_to_march_2012/,1.0,0.0,"I found this in [/r/datasets](/r/datasets) posted by ""trexmatt""
[Post Link](http://www.reddit.com/r/datasets/comments/s0fld/all_time_magazine_covers_march_1923_to_march_2012/)
[Download Link](http://nonavoid.tumblr.com/).

I have an idea for a data mining exercise with these covers. I would like to get some guidance on how to go about it. 

I would like to extract just the text in each cover and get as output just the cover quotes. I would like to know what kind of character training sets are available for this and what are some good imaging library in python that can be used for this exercise. 
",en
1109525,2012-04-11 01:19:53,statistics,Am I the only one frustrated by the use of unexplained acronyms in replies?,s3a9l,lpiloto,1310021871.0,https://www.reddit.com/r/statistics/comments/s3a9l/am_i_the_only_one_frustrated_by_the_use_of/,9.0,14.0,"A lot of replies to posts/questions involve acronyms that go entirely unexplained.  I'm relatively new to formal stats (I've never taken a stats class, although I've taken several machine learning classes), so it may be the case that some of these acronyms are pretty common knowledge.  However, I still don't think that should make it okay to use them in a post without explaining them.  A lot of people who read or post on this subreddit are newbies and to try and help them without explaining your acronyms is a bit backwards.  Can we maybe add something to the guidelines for this subreddit on the sides?  Or am I completely alone in my frustration?

EDIT: Adding an example: Most recently, the acronyms ""ACF"" and ""PACF"" were recently used in a post - not to single out the particular person, unexplained acronyms happen all the time on this subreddit.",en
1109526,2012-04-11 02:49:21,statistics,Speeding up R code using a just-in-time (JIT) compiler,s3f4k,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/s3f4k/speeding_up_r_code_using_a_justintime_jit_compiler/,3.0,0.0,,en
1109527,2012-04-11 02:54:10,MachineLearning,Speeding up R code using a just-in-time (JIT) compiler,s3fds,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/s3fds/speeding_up_r_code_using_a_justintime_jit_compiler/,1.0,0.0,,en
1109528,2012-04-11 02:54:17,rstats,Speeding up R code using a just-in-time (JIT) compiler,s3fe4,talgalili,1271226645.0,https://www.reddit.com/r/rstats/comments/s3fe4/speeding_up_r_code_using_a_justintime_jit_compiler/,1.0,0.0,,en
1109529,2012-04-11 05:03:54,statistics,"Economics student here, I've been studying statistics for a while now, but I still don't understand one-sided testing. Does r/statistics have any insights?",s3mox,newboob,1334108578.0,https://www.reddit.com/r/statistics/comments/s3mox/economics_student_here_ive_been_studying/,2.0,9.0,"Basically, why and when do you use it? It seems that when doing a one-sided test it's definitely possible that your assumptions were wrong and you could get a result that is statistically significant in the other direction, perhaps on an extreme confidence level. The thing that is tempting, and seems very reasonable to me is to say that this result is statistically significant.

Yet, if you were doing the original test on a let's say .05 confidence level, and you count this extreme result in the other direction as significant, the chance of a type I error is clearly greater than .05.

To do the opposite and say the test was not significant, seems to be wrong too, because you might be ignoring a very significant result.

So from where I see it, a one-sided test could only ever inflate the significance of your test, or alternatively have you ignoring very significant results in the unexpected direction.

Yet one-sided tests are still so very widely used, so I feel there has to be something I'm missing.",en
1109530,2012-04-11 05:06:50,statistics,How can I generate a set of random normally distributed variables with a given correlation based on a variance covariance matix?,s3mvb,jewsicle,1302110001.0,https://www.reddit.com/r/statistics/comments/s3mvb/how_can_i_generate_a_set_of_random_normally/,4.0,7.0,"I would prefer a technique based in Stata or excel because that is what I am most familiar with but any advice is welcome.

thanks.",en
1109531,2012-04-11 05:33:37,statistics,confidence band for sample covariance matrix,s3oek,FailedLifeForm,1299165736.0,https://www.reddit.com/r/statistics/comments/s3oek/confidence_band_for_sample_covariance_matrix/,2.0,2.0,"I have a data set that is iid. it could be multivariate normal, or not.  Is it possible to get a confidence band for the sample covariance matrix, or should I simply calculate confidence intervals for pairwise correlations?",en
1109532,2012-04-11 06:34:57,statistics,"High school senior weighing options in statistics career, seeking advice",s3rlo,Epicwarren,1277061390.0,https://www.reddit.com/r/statistics/comments/s3rlo/high_school_senior_weighing_options_in_statistics/,5.0,15.0,"Hello everyone! For a while I've been looking at majoring in actuarial science (I've discussed the matter with /r/actuary as well) but I'm starting to think that I want a more diverse field of employment options. I am considering majoring in statistics and studying for the actuarial exams on my own, so that if actuary doesn't work out I can drive for a career as a statistician (I fell in love with stats after taking AP Stats in high school). I am debating between UIUC and Drake University (so if any of you have experience with the schools, it'd be appreciated. Drake doesn't have statistics as a major but they do have a great actuarial science program).

First question: Is it a recommended path to major in statistics, possibly minor in something like finance, and study for exams separately? What kinds of internships should I be looking for during college with this setup, and what burden would I be facing with my exam attempts and schooling combined?

Second question: what career options are there with just a bachelors degree in statistics, and what is the outlook on such careers? I was told that if I study statistics, I should go for a Masters... but I don't even want to think about grad school right now. Also would greatly appreciate input on the current employment levels at the bachelors/masters thresholds.

Third question: What things should I be doing in college besides trying exams and working on GPA/internships to display myself as a quality stats student? I want an outstanding resume after graduating so I'm not stuck in the unemployment pool.

I know I asked a lot, but I'm just weighing my options right now and want to make sure my decision will set me towards success. Thanks guys!",en
1109533,2012-04-11 06:39:05,statistics,Panel analysis vs. repeated measures ANCOVA.,s3rr5,RA_Fisher,1299707119.0,https://www.reddit.com/r/statistics/comments/s3rr5/panel_analysis_vs_repeated_measures_ancova/,5.0,5.0,My background in statistics began and is heavily oriented towards econometrics.  So lately I've been wondering what the differences between panel data analysis and repeated measures ANCOVA are and when either has advantage of use.  Are they essentially the same thing?  I have a friend that I'm helping with some medical research.  My understanding is that the difference really lies in the lingo and variable selection.  Any insight would be appreciated.,en
1109534,2012-04-11 10:06:54,statistics,Barnard’s exact test – a powerful alternative for Fisher’s exact test (implemented in R),s40eb,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/s40eb/barnards_exact_test_a_powerful_alternative_for/,3.0,2.0,,en
1109535,2012-04-11 13:10:14,MachineLearning,I tried to solve a puzzle using Eureqa. Here's the result...,s44p5,szza,1196514433.0,https://www.reddit.com/r/MachineLearning/comments/s44p5/i_tried_to_solve_a_puzzle_using_eureqa_heres_the/,27.0,12.0,,en
1109536,2012-04-11 14:26:44,artificial,20 Cool Illustrator Icon Tutorials And Free Illustrator Icon Design,s46eg,sanabaig,1312551860.0,https://www.reddit.com/r/artificial/comments/s46eg/20_cool_illustrator_icon_tutorials_and_free/,1.0,0.0,,en
1109537,2012-04-11 15:58:00,MachineLearning,Clever Algorithms: Statistical Machine Learning Recipes,s4942,reidhoch,1209656850.0,https://www.reddit.com/r/MachineLearning/comments/s4942/clever_algorithms_statistical_machine_learning/,17.0,8.0,,en
1109538,2012-04-11 20:03:15,statistics,"Statistics problem help
",s4l4a,[deleted],,https://www.reddit.com/r/statistics/comments/s4l4a/statistics_problem_help/,1.0,0.0,,en
1109539,2012-04-11 20:51:55,statistics,Does anyone have a highlighting package for gedit and SAS?,s4nwa,Secret_Identity_,1250274504.0,https://www.reddit.com/r/statistics/comments/s4nwa/does_anyone_have_a_highlighting_package_for_gedit/,1.0,8.0,"It's pretty much in the title. I was thinking about building the thing myself, but why reinvent the wheel?",en
1109540,2012-04-11 21:28:38,statistics,help with stats346 project using spss,s4q1e,[deleted],,https://www.reddit.com/r/statistics/comments/s4q1e/help_with_stats346_project_using_spss/,1.0,0.0,,en
1109541,2012-04-11 22:49:36,statistics,SPSS Question,s4v09,jeffrey62844,1325052222.0,https://www.reddit.com/r/statistics/comments/s4v09/spss_question/,3.0,10.0,"r/statistics,

I am using SPSS for a research project and, due to the nature of the program we are using to get the data, I have to download the data into Excel and then copy/paste into SPSS.  This would be fine, except when I copy, all of the blank responses turn to zero values in SPSS and I can't get it to ignore them.  So far, I have been clearing them manually, but I have a set of data that has ~180 variables and ~300 respondents, so it will take far too long to search for the zeros in this set.  So my question is, is there an option to ignore zero values?  Any help would be greatly appreciated.  Thanks.",en
1109542,2012-04-12 00:32:05,AskStatistics,What are the odds of hitting the number 2525 on a random number generator?,s513y,[deleted],,https://www.reddit.com/r/AskStatistics/comments/s513y/what_are_the_odds_of_hitting_the_number_2525_on_a/,0.0,2.0,"I have a random number generator that produces a number up to the thousands.  A number could be 1, 693, or 7878.  I have told my students that if the number ever hits 2525, then we don't have to do anything for that class period.  What is the probability of 2525 ever coming up?  Is a permutation involved?",en
1109543,2012-04-12 06:24:09,AskStatistics,Is there a statistics test I can use?,s5l0g,berrens,1326684658.0,https://www.reddit.com/r/AskStatistics/comments/s5l0g/is_there_a_statistics_test_i_can_use/,1.0,1.0,"So I have a easy project for a psych class. Just for fun, I'd like to see if there is a statistics test I can use to show or not show significance.

Here is a rough sketch of the assignment:

Hypothesis:  Does writing an address label by hand vs printing one, increase the rate of return on a randomly dropped piece of mail?

There are 10 envelopes: 5 with the same hand written address, and the other 5 with a neatly printed label address. When I drop these letters randomly throughout a city, it is more likely that the neatly printed labels would be returned in a greater number?

So far this is what I got: 4-printed labels- returned. 2-hand written labels - returned. Is there a test I can use to show significance with these two different conditions? Or is there no test I can run?

N=10  n(1)=5 n(2)=5

Thanks",en
1109544,2012-04-12 08:20:56,MachineLearning,Books about Decisions Trees,s5qma,rudyl313,1297154050.0,https://www.reddit.com/r/MachineLearning/comments/s5qma/books_about_decisions_trees/,7.0,15.0,"I'm looking to learning about decision trees, boosting, bagging, random forests, ID3, CART, etc. Can anybody point me to a good book to learning about this genre of machine learning? Bonus points if it happens to be in the Kindle store :)",en
1109545,2012-04-12 14:53:46,statistics,How do I interpret data of Jaccard Co-effecients. Please Help Reddit.,s61ap,Dumdidaa,1326350292.0,https://www.reddit.com/r/statistics/comments/s61ap/how_do_i_interpret_data_of_jaccard_coeffecients/,0.0,4.0,"I have 5 brands, and the jaccard co-efficient of each brand on 10 attitudinal statements. Hence i have a table with 10x5 data points.How do i interpret a jaccard score of a brand on one these statements?Are the scores comparable across column and rows to use as ranking?

Please let me know if you require further information.",en
1109546,2012-04-12 15:06:20,MachineLearning,Are there any recent books which contain info on deep learning?,s61mr,SunnyJapan,1309541152.0,https://www.reddit.com/r/MachineLearning/comments/s61mr/are_there_any_recent_books_which_contain_info_on/,20.0,18.0,"By recent I mean the ones which incorporate the discoveries from 2006 papers by Hinton et al.
The only one which I know is ""Neural Networks and Learning Machines (3rd Edition)"" by Simon Haykin, which was published in 2008.",en
1109547,2012-04-12 17:09:11,MachineLearning,"Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer-Verlag New York, Inc., New York, NY, USA.",s6682,qztifa,1334239714.0,https://www.reddit.com/r/MachineLearning/comments/s6682/vladimir_n_vapnik_1995_the_nature_of_statistical/,0.0,0.0,,en
1109548,2012-04-12 19:16:19,statistics,"""Eyeballing"" statistical significance",s6cmj,Yazim,1316710187.0,https://www.reddit.com/r/statistics/comments/s6cmj/eyeballing_statistical_significance/,7.0,12.0,"I have been told that you can use the following method to ""eyeball"" statistical significance between two variables, and was wondering how correct it is (I'm not much of a statistics person, if you can't tell)? I'm hoping this doesn't make me sound like a complete idiot, so be gentle.

The method was explained to me as follows:

1. Take the square root of each number
2. Subtract the square from the larger number, and add the square to the smaller number
3. If the resulting two numbers ""overlap,"" then the difference likely is not significant, or may need more formal testing.

So, for example (using very small numbers to make the math easy, though I know you'd need larger sample sizes for actual testing).

1. If you have two results: 25 and 35. The square root of each is 5 and 5.9.
2. Subtract 5.9 from 35 to get 29.1; Add 5 to 25 to get 30.
3. Since the numbers have ""overlapped"" the difference in results is likely not significant.",en
1109549,2012-04-12 19:26:55,statistics,Help needed in describing results from a modified market model,s6d7i,jafoooli,1292199169.0,https://www.reddit.com/r/statistics/comments/s6d7i/help_needed_in_describing_results_from_a_modified/,3.0,9.0,"Basically I have generated a set of results from a market model with dummy variables. My aim is to see whether the scandals of Tiger Woods had an impact on some of the firms he endorsed. I am unsure as to what exactly my results mean. Any help would be much appreciated. All the details are explained in the document. Thanks in advance!

here is the link to the google doc
https://docs.google.com/document/d/1deJrfyaWUFvn8sFIbeY59RWvLMXjgLO7ipbGeDdueIs/edit",en
1109550,2012-04-13 06:19:53,statistics,"Seeking help! Any statistics I can perform to compare two sample groups, where n=2 (A) and n=6(B)...details inside...",s7dc4,sookybabi,1269914194.0,https://www.reddit.com/r/statistics/comments/s7dc4/seeking_help_any_statistics_i_can_perform_to/,1.0,11.0,"Hi guys, 

Sorry if this is inappropriate! 

I'll provide more information if it is needed.

I'm wrapping up my thesis on my studies of Harlequin Ichthyosis (if you look it up it is very NSFW or your eyes), it is an extremely rare and devastating skin disease. 

Due to the rarity of the disease it has been difficult to obtain patient samples, thus (disease) n = 2. My supervisor wanted me to wait for a third sample but sadly it will not make it in time for me to process as my thesis is due very soon.

My control group is n=6. 

Now comes my problem, I'm not sure what methods of statistics I can use to compare the two groups. I usually perform a two-tailed student t-test to compare two sample groups. However, I don't think I can apply the same method this time around? Are there other statistical method I could use so it I can demonstrate scientific significance? 


Usually, I work with larger sample sizes...and I don't know how to go about drawing any statistical significance from n = 2, what type of error bars can I use for this group? As I usually do SEM or STDEV...but I can't use this application.  

Any links to guides and resources will be helpful, I'm no statistician! 

Thanks a million in advance,

sooky


",en
1109551,2012-04-13 08:13:19,MachineLearning,"A website that lists state of the art results in various machine learning problems, and by which methods it was achieved?",s7ihy,SunnyJapan,1309541152.0,https://www.reddit.com/r/MachineLearning/comments/s7ihy/a_website_that_lists_state_of_the_art_results_in/,23.0,6.0,"I remember I have seen such a website before, but I can't find it anymore, does anybody have a link?",en
1109552,2012-04-13 12:02:10,statistics,Can times series have seasonal variation over a short period of time?,s7p2e,[deleted],,https://www.reddit.com/r/statistics/comments/s7p2e/can_times_series_have_seasonal_variation_over_a/,1.0,0.0,"For example, if I plotted the price of a good each day for 80 days, could it be possible that there would be seasonal variation in this short period of time? For instance, a pattern that repeats every 15 days. I've only heard seasonal variation in the context of months, quarters, and years, but I wasn't so sure about days or weeks. Thanks for any input.",en
1109553,2012-04-13 16:37:28,statistics,Statistics is not math...,s7wca,t_rex_tullis,1318255134.0,https://www.reddit.com/r/statistics/comments/s7wca/statistics_is_not_math/,5.0,8.0,,en
1109554,2012-04-13 17:04:05,statistics,Advice on Learning More Stats,s7xej,M_Bus,1326739352.0,https://www.reddit.com/r/statistics/comments/s7xej/advice_on_learning_more_stats/,0.0,6.0,"I have a background in mathematics as well as a couple introductory courses on regression, time series, probability, data modeling, and statistical testing, but it's all extremely disjointed. I also have this feeling like what little stats I know are a bit shaky - I wouldn't trust myself to not make an error in an analysis by doing something like using the wrong test or interpreting the results incorrectly. So I'm looking for some good resources to increase my statistical power, if you will. Does /r/statistics have any suggestions on good places to look? Online courses that you've tried out, good books on the subject that I should read, etc.

I should point out that I'm looking to **do** statistics, not just interpret results. I went to an MBA program that emphasized interpreting statistical results, which is fine if you want to make managerial decisions, but I want to go *deeper*.

Also, are there areas of pure math that are worth studying for the purpose of understanding some more advanced statistics (other than calculus)? For example, I'm right now rereading some old linear algebra texts (maybe useful for stuff like the [fisher information](http://en.wikipedia.org/wiki/Fisher_information) of multivariate distributions?) and hoping to get into more stochastic calculus and differential equations to see if there are some good modeling tools available there.",en
1109555,2012-04-13 17:08:05,statistics,Humble Steven Strogatz rediscovers Bayes' rule through students' struggle to understand,s7xl8,[deleted],,https://www.reddit.com/r/statistics/comments/s7xl8/humble_steven_strogatz_rediscovers_bayes_rule/,28.0,8.0,,en
1109556,2012-04-13 18:39:55,MachineLearning,Could anybody explain Boltzmann Machines to me?,s81mg,Aardshark,1257983140.0,https://www.reddit.com/r/MachineLearning/comments/s81mg/could_anybody_explain_boltzmann_machines_to_me/,14.0,12.0,"I have to implement a single layer (no hidden units) Boltzmann Machine for a class. I have an implementation done, but I'm not sure it's working correctly.

I'll describe what I've done and hopefully somone can point out if I've made any mistakes.

* The BM machine model is a set of binary units in which each unit is connected to every other unit via a weighted connection.

* First, we train the machine on a training set of examples to determine the weights of the connections.

* This learning phase can be divided into two parts - one is the empirical correlation between the units of the examples in the training set and one is the correlation between these units according to some probability model.

* At the end of learning, the probability correlation is subtracted from the empirical correlation and the resulting correlation is the weights for our BM.

* We can then set our neurons to an input and run the activation model on them to recieve an output.

I realise as I type this that my understanding of the topic is really very fuzzy. Here's a few questions : 

Having trained the machine on a set of examples, what should be the output when providing it with one of those examples as input? Will you always recieve that example back or will you occasionally get one of the other examples or even nonsense patterns?

I've found several RBM implementations online. How does a Restricted Boltzmann Machine differ from one with no hidden units? I'd like to be able to edit a RBM into what I've described so that I can make sure my implementation works the way it should.
",en
1109557,2012-04-13 21:31:26,statistics,Need 'smart' regression algorithm/code,s8b0t,rcousins,1294087170.0,https://www.reddit.com/r/statistics/comments/s8b0t/need_smart_regression_algorithmcode/,3.0,6.0,"Years ago I used a stats program which could create a regression model based upon many inputs. However, it could also create models by finding the best 'n' factors out of a much larger set of potential factors.

I now need to put this function into a program I'm working on. Does anyone know of a good algorithm for this and/or some good C code which implements it? All thoughts/suggestions appreciated. Apologies for imprecise use of terms. Stats isn't my strength.",en
1109558,2012-04-13 22:24:55,statistics,Four free lectures on Statistical Methods thanks to 'Goldman Sachs Gives',s8e00,thisisitfornow,1302751668.0,https://www.reddit.com/r/statistics/comments/s8e00/four_free_lectures_on_statistical_methods_thanks/,5.0,0.0,,en
1109559,2012-04-14 18:36:26,statistics,Incoming high school freshman probably majoring in statistics... I probably can't pay for grad school. What can I do with a bachelor's in Statistics?,s9k0j,[deleted],,https://www.reddit.com/r/statistics/comments/s9k0j/incoming_high_school_freshman_probably_majoring/,1.0,2.0,"I'm the oldest of three kids in my family, my parents already said they can't pay for grad school straight out of undergrad for me (they're already going ~$120k in debt thanks to my college). I've been told that statistics jobs almost need a master's these days, but right now I don't think that's possible (maybe after a few years in the field, but I can't think about it right now).

So what jobs will I be able to get after graduating with a bachelor's? I am probably going to minor in computer science if that helps, but right now I really need to dispel my fears that I'll have a useless degree after graduating, and being consumed with debt. Thanks guys.",en
1109560,2012-04-14 19:52:35,statistics,Incoming college freshman probably majoring in statistics... I probably can't pay for grad school. What can I do with a bachelor's in Statistics?,s9n1m,ThisWillNotBeAMeme,1294108517.0,https://www.reddit.com/r/statistics/comments/s9n1m/incoming_college_freshman_probably_majoring_in/,10.0,32.0,"NOTE: Apologies for the repost, I accidentally submitted this thread a short while ago with a title that said I was a high school freshman instead of a college one.

I'm the oldest of three kids in my family, my parents already said they can't pay for grad school straight out of undergrad for me (they're already going ~$120k in debt thanks to my college). I've been told that statistics jobs almost need a master's these days, but right now I don't think that's possible (maybe after a few years in the field, but I can't think about it right now).
So what jobs will I be able to get after graduating with a bachelor's? I am probably going to minor in computer science if that helps, but right now I really need to dispel my fears that I'll have a useless degree after graduating, and being consumed with debt. Thanks guys.",en
1109561,2012-04-14 20:29:49,rstats,Any R package/function that selects features by cross-validating every possible combination of Xs?,s9oko,randombozo,1266016617.0,https://www.reddit.com/r/rstats/comments/s9oko/any_r_packagefunction_that_selects_features_by/,3.0,14.0,,en
1109562,2012-04-14 23:01:47,MachineLearning,Ask ML: Verifying Gradients numerically,s9v51,strayadvice,,https://www.reddit.com/r/MachineLearning/comments/s9v51/ask_ml_verifying_gradients_numerically/,6.0,8.0,"I'm trying to implement numerical gradient checking as explained [here](http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization) to verify my implementation of the cost function in another exercise. I haven't changed much of the code describing the cost/gradient function of the neural network from what I used on the linked exercise (which passed the test comfortably).

I'm now experimenting with a three layer vanilla network - 50~ inputs, 25~ ""hidden nodes"", 1 output node, all of them using a tanh activation function. I had to modify the previous code very slightly to from a sparse autoencoder to a vanilla neural network. I'm almost certain that the implementation is correct, except that it doesn't seem to pass the numerical gradient test very well.

So my question is, are there limits to this method of checking an implementation of cost/gradient function numerically? I've spent more than 15 hours trying to debug, but haven't succeeded so far.",en
1109563,2012-04-14 23:49:10,statistics,I need help finding a standard deviation,s9x86,[deleted],,https://www.reddit.com/r/statistics/comments/s9x86/i_need_help_finding_a_standard_deviation/,1.0,0.0,"This is what I'm dealing with:

Be X a normal distribution with µ = 12. Find σ so that P(13 &lt;= X &lt;= 15) = 0.17",en
1109564,2012-04-15 00:17:54,computervision,Digital Differential Analyser Algorithm VS Bresenham,s9yfl,IceRoad,1333551748.0,https://www.reddit.com/r/computervision/comments/s9yfl/digital_differential_analyser_algorithm_vs/,0.0,0.0,Can anyone who understands this better than me explain the real differences between these? Thanks,en
1109565,2012-04-15 03:11:49,MachineLearning,Writing a paper on artificial intelligence,sa5va,robotsrobotschicken,1334448262.0,https://www.reddit.com/r/MachineLearning/comments/sa5va/writing_a_paper_on_artificial_intelligence/,14.0,5.0,"So I'm writing a very basic introductory paper on artificial intelligence for my university course. My main focus is on neural networking, and I really need to interview someone with knowledge in the field. I was wondering if anyone with some credentials in the field would be interested in answering some relatively basic questions I have. ",en
1109566,2012-04-15 08:58:00,MachineLearning,a C/C++ Gibbs Sampling LDA implementation for latent topics discovery,saiah,ieeaaauuuuooooo,1333779580.0,https://www.reddit.com/r/MachineLearning/comments/saiah/a_cc_gibbs_sampling_lda_implementation_for_latent/,6.0,1.0,,en
1109567,2012-04-15 19:56:44,statistics,[SAS] Major problem with flat data transformation,saydf,tyrny,1283621407.0,https://www.reddit.com/r/statistics/comments/saydf/sas_major_problem_with_flat_data_transformation/,0.0,1.0,"My code

    proc panel data = Smoothed;
    flatdata indid = Neighborhood
    base = (F)
    tsname = t;


This reads the neighborhoods correctly as the cross sections, however it does not read any of my other data. Instead it produces a table that looks like this:


Neighborhood T F

205 -1 .

201 -1 .",en
1109568,2012-04-15 22:24:58,MachineLearning,What Happened to the Weka Mailing List??,sb5ew,[deleted],,https://www.reddit.com/r/MachineLearning/comments/sb5ew/what_happened_to_the_weka_mailing_list/,8.0,1.0,"For the past few weeks I have been unable to access the Weka mailing list to subscribe. It is probably the most complete help source, and it would be a shame if it was taken offline permanently. It looks like the entire domain is down. Is there an alternative somewhere that hasn't been mentioned on their websites??

http://list.scms.waikato.ac.nz/mailman/listinfo/wekalist",en
1109569,2012-04-15 23:58:24,MachineLearning,How would you find patterns in conversations?,sba0e,ScientistDaddy,1320395809.0,https://www.reddit.com/r/MachineLearning/comments/sba0e/how_would_you_find_patterns_in_conversations/,9.0,13.0,"If you had records of conversations between pairs of people (with associated profile data: age, gender, etc.), how would you go about finding predictive conversation patterns in either the audio or transcripts?",en
1109570,2012-04-16 01:28:11,statistics,Statistics problem,sbec2,[deleted],,https://www.reddit.com/r/statistics/comments/sbec2/statistics_problem/,1.0,0.0,"A quality-control inspector rejects any shipment with 3 or more defective in a random sampling of 20 items. What is the probability of rejecting the shipment when the proportion of defectives for the overall shipment is .01 and .20 .

I tried using Z tables for this problem when there is 3 failures and subtract that from 100%. I ended up getting a mean of .2 for the sample of 20 and a standard deviation of .445 . When I use them to solve for Z, I end up getting around 6.5, what am I doing wrong?",en
1109571,2012-04-16 03:26:57,statistics,Can someone give me an intuition behind dimensionality reduction methods that use eigendecomposition or singular value decomposition (SVD)?,sbk45,lpiloto,1310021871.0,https://www.reddit.com/r/statistics/comments/sbk45/can_someone_give_me_an_intuition_behind/,21.0,13.0,"Methods like principal component analysis are very easy to implement and use, but I'd really like to grasp why the mathematics produce the desired results.  More specifically, I just don't understand the ""meaning"" or intuition of taking the eigendecomposition of a covariance matrix or the SVD of the data matrix.",en
1109572,2012-04-16 10:42:12,MachineLearning,Why Netflix Never Implemented The Algorithm That Won The Netflix $1 Million Challenge | Techdirt,sc3m7,suhrob,1309544448.0,https://www.reddit.com/r/MachineLearning/comments/sc3m7/why_netflix_never_implemented_the_algorithm_that/,89.0,10.0,,en
1109573,2012-04-16 11:29:54,statistics,One in five Americans don't use the Internet,sc4qt,broadwaybros,1334300230.0,https://www.reddit.com/r/statistics/comments/sc4qt/one_in_five_americans_dont_use_the_internet/,1.0,0.0,,en
1109574,2012-04-16 18:43:57,statistics,Predictive statistics problem,sci9w,[deleted],,https://www.reddit.com/r/statistics/comments/sci9w/predictive_statistics_problem/,2.0,6.0,"Hi folks. I've managed to get a little out of my depth in terms of data analysis, and I was wondering if one of you kind souls might be able to point me in the right direction.

I have three tests (let's call them X, Y, and Z). Y and Z can be said to be measuring the same thing (Pearson's correlation between scores is &gt;.8). Having found a decent correlation, I then administered Y and Z to the same sample under different conditions. At this point, I could comfortably calculate the extent to which X scores predict Y or Z scores, but I want to  calculate the extent to which X scores predict the difference between Y and Z scores - I need to establish how much of a confounding variable X is.

I have two problems (that I'm aware of, at least): First, I have no idea of which statistical test (if any) will allow me to do this. Second, Y is a test of 17 questions answered with a Likert scale ranging from 1-6, whereas Z has 16 questions answered with a Likert scale ranging from 1-7. Were I to know the appropriate measure to use, I have no idea how to account for these unavoidable differences when looking at the difference between scores.",en
1109575,2012-04-16 20:46:40,statistics,Ideas for sorting objects in 3D space?,sconh,forever_erratic,1256947469.0,https://www.reddit.com/r/statistics/comments/sconh/ideas_for_sorting_objects_in_3d_space/,1.0,11.0,"Hello,

I've got a project where I have a list of points each with their own xyz coordinate. Plotted, they look something like this:


         .
    o  .    o
      O  .O
    o  .    o
      O  .O 
    o  .    o
      O  .O
    o  .    o
      O  .O
    o  .    o 
      O  .O

Where O = close to front, o = farther back, and . is far in the back

As you can see, the points are distributed so that they occupy ""rows"" of sorts, from the top to the bottom (in this diagram, there are 6 rows, 5 with 5 points, one with 6 points). What I'd like to do is identify the different rows programatically, so that I can do some simple stats on row-by-row differences. I can't really do this by eye as my actual data have more like 300 points per repetition, and ~50 reps. 

At the moment, to programmatically count rows, I'm just doing things like finding a point, drawing a 3d box around it (that approximates the shape of an eyeballed row of points), then calling that a row. I do this for all the points (which takes awhile), then look at the mode rows by eye and pick the ones that seem most right. It doesn't seem like the best way to do it, especially because of the human-hand-holding needed, and I wonder if I'm re-inventing a wheel that has long since been perfected. I figured I'd ask you statisticians. Thanks for any help!

Edit: In case it matters, this is not homework, it is research work.",en
1109576,2012-04-16 22:06:38,statistics,Benford’s Law,sct70,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/sct70/benfords_law/,9.0,0.0,,en
1109577,2012-04-16 22:51:25,statistics,My stats professor just sent this out: Hunger Games survival analysis,scvxu,hello_kitteh,1313780080.0,https://www.reddit.com/r/statistics/comments/scvxu/my_stats_professor_just_sent_this_out_hunger/,58.0,0.0,,en
1109578,2012-04-16 23:42:09,statistics,"Building ARIMA(x,2,12) model with a changing mean .  Tests claim no seasonality, but the time series shows seasonality.  Am I fine to build my model with seasonality differencing?",scyzw,CivAndTrees,1331689546.0,https://www.reddit.com/r/statistics/comments/scyzw/building_arimax212_model_with_a_changing_mean/,7.0,4.0,"here is [link to what i been staring at all weekend](http://i.imgur.com/a7zWv.jpg).  I have other models without seasonality differencing, but could not get white noise in my correlogram.  ",en
1109579,2012-04-17 00:41:15,AskStatistics,"If I buy two lottery tickets (with only one number changing), am I halving the odds?",sd2k0,Nomascus,1333975172.0,https://www.reddit.com/r/AskStatistics/comments/sd2k0/if_i_buy_two_lottery_tickets_with_only_one_number/,1.0,0.0,"I think so. I would say that as long as at least one number is different from one ticket bought to the other, then probabilities go as follows (never play twice the same combination of numbers):

1 ticket  - 1/14000000
2 tickets - 2/14000000
3 tickets - 3/14000000
4 tickets - 4/14000000",en
1109580,2012-04-17 03:07:57,MachineLearning,Resources for Extracting Main Text from a Webpage,sdawo,LADataJunkie,1327342831.0,https://www.reddit.com/r/MachineLearning/comments/sdawo/resources_for_extracting_main_text_from_a_webpage/,8.0,9.0,"What are some good resources for extracting the main text of a webpage? What I mean is, given a web page in HTML format, extract the main body of the text, not including irrelevant stuff like sidebars, ads etc.

I know that this is an active research topic, but I am curious if anyone has found a library that works well.",en
1109581,2012-04-17 04:35:10,statistics,Looking for good resources for my statistical quality control class. Our book isn't very helpful. ,sdg0z,[deleted],,https://www.reddit.com/r/statistics/comments/sdg0z/looking_for_good_resources_for_my_statistical/,0.0,2.0,"We have been mostly covering control charts, and the book has really started to suck. I have looked around on google and youtube, but found nothing. Any suggestions would be greatly appreciated. Thanks. ",en
1109582,2012-04-17 07:15:51,artificial,Cooperating Mini-Brains Show How Intelligence Evolved (Larger ANNs co-operate better),sdp64,kamoylan,1254047922.0,https://www.reddit.com/r/artificial/comments/sdp64/cooperating_minibrains_show_how_intelligence/,4.0,1.0,,en
1109583,2012-04-17 15:11:37,datasets,"""... an unprecedented amount of data on international economic development"": arstechnica reports on new World Bank transparency policy [multi-post from /r/GlobalDevelopment]",se2vi,claird,1249749559.0,https://www.reddit.com/r/datasets/comments/se2vi/an_unprecedented_amount_of_data_on_international/,5.0,0.0,,en
1109584,2012-04-17 16:47:19,MachineLearning,My friend is scraping text NFL play by play data.  ,se6fi,imissyourmusk,1317942820.0,https://www.reddit.com/r/MachineLearning/comments/se6fi/my_friend_is_scraping_text_nfl_play_by_play_data/,11.0,13.0,"What would be an interesting machine learning project to do with this data?

Here is a sample: (13:02) S.Weatherford punts 53 yards to IND 18 Center-K.Houser. T.Rushing pushed ob at IND 34 for 16 yards (U.Young).
",en
1109585,2012-04-17 18:09:08,artificial,Artificial Life/ Artificial Chemistry Simulation on Google App Engine,sea4k,berlinbrown,1135573200.0,https://www.reddit.com/r/artificial/comments/sea4k/artificial_life_artificial_chemistry_simulation/,16.0,1.0,,en
1109586,2012-04-17 18:25:25,MachineLearning,Crowd computing taps artificial intelligence to revolutionize the power of our collective brains,seayp,andycrowdcontrol,1334673890.0,https://www.reddit.com/r/MachineLearning/comments/seayp/crowd_computing_taps_artificial_intelligence_to/,8.0,1.0,,en
1109587,2012-04-17 19:10:08,statistics,Precision and Recall (Accuracy) with more than two classes?,sed8y,Sturmi12,1323442206.0,https://www.reddit.com/r/statistics/comments/sed8y/precision_and_recall_accuracy_with_more_than_two/,3.0,4.0,"Hello everbody,

I have nearly no experience in statistics. I know how to calculate the precision and recall for two classes (A and B). But how can I calculate this for three classes (A,B and C)?

For example:
I have downloaded the programm Weka and used it to classify the Iris dataset.

In the output I get the Confusion Matrix (I still get that part) and the calculated values for TP Rate, FP Rate, Precision, Recall, F-Measure and ROC-Area for each class. (See image: [http://imgur.com/3jCRm](http://imgur.com/3jCRm))

But I don't know how to exactly calculate these values and I didn't find anything useful with Google.

Could you explain me how I can do it, or maybe a give me a link where this is explained? (I would appreciate an easy explanation)

Also, can I calculate the Accuracy for such a three class confusion matrix?

Thanks for your help.
",en
1109588,2012-04-17 22:47:55,statistics,Transition from Applied Math to Stats,sep9i,OMGWTFLOLQED,1309405013.0,https://www.reddit.com/r/statistics/comments/sep9i/transition_from_applied_math_to_stats/,13.0,11.0,"I'm about to finish a PhD in Applied Math, and I am not excited about my career options.  I'm considering making a move into ""big data,"" and I was hoping to solicit some opinions.  I have programming aptitude, but my experience is in Matlab, Mathematica, and C++.  Should I learn R?  SPSS?",en
1109589,2012-04-18 00:01:57,statistics,Two-way fixed effects.,seti5,RA_Fisher,1299707119.0,https://www.reddit.com/r/statistics/comments/seti5/twoway_fixed_effects/,2.0,4.0,"Is there any problem with running non-nested two-way fixed effects?  Suppose you have 10 states and 20 years of data, is there any problem with running a fixed effects model for both cross-sections and time periods?  My friend insists that this will basically destroy any variation and you'll have collinearity problems.  It seems to me that you'll just be demeaning over time (for the c.s. fe) and then demeaning over cross-section (for the time fe), adding the grand mean, and obviously the value of the data point.  Am I missing something (aside from asymptotics)?  Any references would be greatly appreciated.",en
1109590,2012-04-18 00:07:13,statistics,"Quantile based, InformationTheoretic Modeling",setsz,Honglang,1321811248.0,https://www.reddit.com/r/statistics/comments/setsz/quantile_based_informationtheoretic_modeling/,1.0,0.0,,en
1109591,2012-04-18 00:53:56,statistics,A Method for Selecting the Bin Size of a Time Histogram [PDF],sewlr,roger_,1178076247.0,https://www.reddit.com/r/statistics/comments/sewlr/a_method_for_selecting_the_bin_size_of_a_time/,9.0,6.0,,en
1109592,2012-04-18 04:40:34,statistics,Baye's Rule vs Logic,sf8yu,yamiyam,1330580695.0,https://www.reddit.com/r/statistics/comments/sf8yu/bayes_rule_vs_logic/,13.0,12.0,"Hi, currently taking a statistics course for engineers and to my mind Baye's rule seems to defy logic in certain scenarios...to whit, a simple example:

Say I have 3 boxes, each with two drawers, each of which may or may not contain a coin.

Box I: both drawers contain a coin.
Box II: one drawer contains a coin.
Box III: neither drawer contains a coin.

Say I pick a box at random and open a drawer and it contains a coin. What are the odds that the second drawer will also contain a coin? 

**Logic**: Well, since I've found a coin, and there are two boxes with coins, then there's a 50/50 chance that I've chosen the box with two coins. P = 0.5.

**Baye's Rule**, which states: P(A|B) = P(B|A)*P(A)/P(B)

where 

A = choosing box I (1/3) = .333.
B = finding a coin in a drawer (3/6) = .5.
A|B = having chosen box I given that I've found a coin.
B|A = finding a coin given that I've chosen box I (2/2) = 1.0.

P(A|B)  
= (1.0)*(.333)/(.5)
= 0.667

so...why are these probabilities different? Am I misunderstanding Baye's rule or is my logic flawed? Any help would be appreciated; this isn't a homework question, this is just a statistical theory that I don't seem to be able to wrap my head around and I hope you fine people can help me out. Cheers!


 ",en
1109593,2012-04-18 07:45:07,MachineLearning,WebHarvester: A Machine Learning Plugin,sfj35,MuffinShit,1288905157.0,https://www.reddit.com/r/MachineLearning/comments/sfj35/webharvester_a_machine_learning_plugin/,3.0,7.0,,en
1109594,2012-04-18 09:41:45,MachineLearning,Algoritm results comparison in a production env.,sfnnc,[deleted],,https://www.reddit.com/r/MachineLearning/comments/sfnnc/algoritm_results_comparison_in_a_production_env/,0.0,1.0,"I remember a concept when studying( and forgot to take notes) fraud detection with genetic algoritms that is called ""concurrent model"" or something like that ( I really don't remember ) The basic idea is that two or more models compete against each other and once we have a best fit we promote the winner to Master and start a new generation.

So I ask you my fellow mls, do you know this concept? could you please point me to the right bibliography? 

regards ",en
1109595,2012-04-18 18:47:02,statistics,Stata Help,sg5ag,[deleted],,https://www.reddit.com/r/statistics/comments/sg5ag/stata_help/,1.0,0.0,"Hi! I'm having trouble with stata. I'm trying to use the General Sociology Survey and the American National Election Survey to see the connection between gender and political participation. If you can help me, pm me your email and I can send you the data sets and further explanation!

Thanks!",en
1109596,2012-04-18 22:08:06,MachineLearning,Looking for a summer intern,sggod,[deleted],,https://www.reddit.com/r/MachineLearning/comments/sggod/looking_for_a_summer_intern/,6.0,8.0,"Primary Job Responsibilities
-----------------
Design, Develop, and Test various Machine Learning algorithms for classification of eBay listings.
Present the work in internal eBay forums.
If deemed necessary, facilitate external publication of the work.

eBay Inc. interns will participate in a challenging 10-12 week summer program, then return to school in the fall. Summer interns will obtain practical work experience, learn about the eBay Inc. businesses, and receive mentorship from their manager and team. Summer Interns will set goals/objectives with their managers at the start of the internship, and complete end of summer performance evaluations.

Job Requirements
-----------------
Knowledge of common ML techniques in the area of Classification
Knowledge of Hadoop is preferred

Education
----------
Bachelors Degree Required

Please respond to this if interested.",en
1109597,2012-04-18 23:04:20,statistics,Help with multiple pairwise comparisons of proportions and means,sgk0c,mannamedlear,1304587279.0,https://www.reddit.com/r/statistics/comments/sgk0c/help_with_multiple_pairwise_comparisons_of/,2.0,14.0,"My employer does a study on two independent samples of people and asks them questions on a number of topics regarding brand preferences or awareness.  I want to see if there is a difference between the two groups in regards to their proportions of answers or differences in their mean ratings on attributes.  Most of the questions on awareness, the data looks like this http://i.imgur.com/EscRv.jpg, where a respondent selects all of the brands they have heard of.  What is the best way to test if there is a difference between the two groups?",en
1109598,2012-04-19 03:57:53,statistics,SQL help,sh0l6,the_huntress,1256122360.0,https://www.reddit.com/r/statistics/comments/sh0l6/sql_help/,7.0,11.0,"I gave recently graduated from university with a major in Statistics and decided that I would like to learn SQL. I have found a tutorial online that seems to be pretty good but I was wondering if anyone had any suggestions for other good websites to help me learn? I was also wondering if anyone knew if there is a place where I can download SQL for my mac.

Thanks for the help. ",en
1109599,2012-04-19 04:48:35,AskStatistics,Question about the f-distribution (based off chi squared) for statistics (X-Post from r/askscience),sh3fh,legendarycolk,1331459604.0,https://www.reddit.com/r/AskStatistics/comments/sh3fh/question_about_the_fdistribution_based_off_chi/,0.0,0.0,"Reddit, I need help. 

One of the questions on our stats assignment is to prove why we have to reverse degrees of freedom and find the reciprocal of the F distribution to find the opposite side. Put more simply, why is it that ""to determine f, you reverse the degrees of freedom, look up the (1-a) quantile f(1-a), and compute 1/(f,(1-a))""

The quote i've taken comes after an explanation on page 92  [here](http://books.google.com.au/books?id=JUFnctYr2zgC&amp;pg=PA90&amp;lpg=PA90&amp;dq=why+reverse+degrees+of+freedom+f+distribution&amp;source=bl&amp;ots=WEjqs8kdhj&amp;sig=7UAi1x1YJGqPSZFLbb6QOe6xwDc&amp;hl=en&amp;sa=X&amp;ei=G0-PT_OPC_GNiAfYzuWdBA&amp;ved=0CC8Q6AEwAg#v=onepage&amp;q=why%20reverse%20degrees%20of%20freedom%20f%20distribution&amp;f=false)

Sorry if my explanation has been really bad, or if I've posted to the wrong subreddit. I just don't understand why we choose to swap degrees of freedom and find the reciprocal of the F distribution to find the opposite side. 
The actual question in the assignment is to prove F(1-a,v1,v2)=1/F(a,v2,v1)

Where everything in brackets after each F is in subscript.
Any help or even just a push in the right direction would be much appreciated.

Thanks reddit. ",en
1109600,2012-04-19 05:37:28,statistics,Nonseasonal Box-Jenkins Models,sh69i,NataliePortLAN,1333737971.0,https://www.reddit.com/r/statistics/comments/sh69i/nonseasonal_boxjenkins_models/,3.0,0.0,"I'm having trouble understanding why the following SAS output indicates the original times series values are not stationary. The first part is for the SAC and the second for the SPAC. Any help is appreciated and this is not for homework, if that matters. Here is the output: [SAS output](http://imgur.com/BSBXT)",en
1109601,2012-04-19 06:06:34,MachineLearning,In need of a suitable class of algorithms for linking two sets of records.,sh7ut,SCombinator,1279805718.0,https://www.reddit.com/r/MachineLearning/comments/sh7ut/in_need_of_a_suitable_class_of_algorithms_for/,4.0,1.0,"I have a bipartite graph (two tables of records, U &amp; V) that need matching.

Unfortunately in my case records may connect to more than one record in the other table, and all columns are continuous which seems to rule out normal ways of record linking. 

Essentially there should exist an edge Eij between two records when part of Ui is part of Vj. The information I have is that the sum of edges connected to Ui should sum to the values in Ui, and the edges connected to Vj should sum to the values in Vj. (It doesn't always because the data can be bad.)

Help is appreciated, I've been looking at the Subset Sum problem, Constrain Propagation, Maximum Flow, and Bayesian Record Linking, and all of them seem like close fits, I'm not sure if any of them map to the problem very easily.",en
1109602,2012-04-19 06:20:15,statistics,3 Senior Papers and the GRE today...All in 3 days.  Here is my forecasting paper for biofuels that I thought /r/statistics would enjoy.,sh8kt,[deleted],,https://www.reddit.com/r/statistics/comments/sh8kt/3_senior_papers_and_the_gre_todayall_in_3_days/,1.0,0.0,,en
1109603,2012-04-19 07:06:16,statistics,Estimating variance by estimating E(X^2),shb0r,r_newbie,1333837570.0,https://www.reddit.com/r/statistics/comments/shb0r/estimating_variance_by_estimating_ex2/,2.0,10.0,"I got this idea some time back when I was wondering how one can estimate parameters other than the mean, but I have never seen this in any textbook or website. Of course, I have never used a textbook on sampling before, so I may have not looked everywhere. :-)

If I wanted to estimate the mean, I could sample n out of a large population and the sample mean E(X) is a good estimate of the population mean.

Does the same thing hold true for X^2 ? That is, can I sample from a population, calculate E( X^2 ), E( X^3 ) etc. and use them to estimate variance, skewness, kurtosis etc.? Whatever error bounds apply to the first moment should apply to the others too, right?

Is this right or am I missing something very elementary? Thanks!

The real reason for this question is because if I can do that, I can approximate a moment generating function for any data, since E( e^tX ) = 1 + t E( X ) + (1/2!) E( X^2 ) t^2 + (1/3!) E( X^3 ) t^3 + ... Now, this opens up all kinds of doors (convolutions, branching processes, you name it). So, if sampling is easy and the population is large, this may be a very good way to do a lot of things.",en
1109604,2012-04-19 13:49:31,statistics,Best web hosting solutions.,shms7,fret27,1330158891.0,https://www.reddit.com/r/statistics/comments/shms7/best_web_hosting_solutions/,1.0,0.0,,en
1109605,2012-04-19 14:04:56,statistics,VAR Cointegrating Rank Test - what happens if full rank?,shn6e,[deleted],,https://www.reddit.com/r/statistics/comments/shn6e/var_cointegrating_rank_test_what_happens_if_full/,0.0,0.0,"Hi, I am estimating a basic VAR and testing for cointegrating rank (ie using the Johansen procedure to determine the rank of the coefficient matrix). I understand that there is no cointegration if rank = 0, some cointegrating vectors if rank &lt; full rank. But what does it mean if the matrix has full rank? 
Are there cointegrating relations, or are there issues with stationarity assumptions?

Thanks!",en
1109606,2012-04-19 15:35:05,statistics,Another (this time TNS') TV ratings scandal in Turkey. ,shpr5,srkiboy83,1299056137.0,https://www.reddit.com/r/statistics/comments/shpr5/another_this_time_tns_tv_ratings_scandal_in_turkey/,0.0,0.0,,en
1109607,2012-04-19 17:21:20,statistics,Statisticians to the rescue - ensuring a company's survival.,shtxz,prhodes,1328233759.0,https://www.reddit.com/r/statistics/comments/shtxz/statisticians_to_the_rescue_ensuring_a_companys/,17.0,0.0,,en
1109608,2012-04-19 18:13:49,statistics,What software does the IMF use to create its graphs?,shwf5,EconAdvice,1330360371.0,https://www.reddit.com/r/statistics/comments/shwf5/what_software_does_the_imf_use_to_create_its/,3.0,3.0,"After taking a look at the IMF's latest WEO report (http://www.imf.org/external/pubs/ft/weo/2012/01/pdf/text.pdf), I was wondering if any of you guys had any idea on what program/package they might be using to generate the graphs in the report? 

Im mostly curious on their bar and lines graphs, although if anyone has any idea on what they might be using for their map graphs, I would also appreciate it. I've always used Tableau and Atlas Mapper for map graphs, but aren't as clean and slick as those in the WEO. 

Thanks


",en
1109609,2012-04-19 18:42:18,statistics,Automated highway warning signs: life-savers or hazards?  RFC on a new project.,shxuz,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/shxuz/automated_highway_warning_signs_lifesavers_or/,4.0,3.0,,en
1109610,2012-04-19 19:54:48,data,Free Trial Predictive Modeling in 5 Steps (No Statistics Degree Needed),si1qy,cloudamp,1314741741.0,https://www.reddit.com/r/data/comments/si1qy/free_trial_predictive_modeling_in_5_steps_no/,1.0,0.0,,en
1109611,2012-04-19 20:39:50,MachineLearning,Suggested ML technique for recommending user behavior with a GUI-based tool,si490,LikesToAskWhy,1278989465.0,https://www.reddit.com/r/MachineLearning/comments/si490/suggested_ml_technique_for_recommending_user/,2.0,5.0,"I have been asked to add the capability to recommend the best way to use a particular GUI program, based on how it has been used by other users before. It is highly targeted to a particular domain, wherein the User uploads some data and then perform a series of different steps one after another and the program can be used to view and analyze the data. Hence, the GUI program ends up being used in different ways by different Users. We would like to build a recommender system to learn the way in which it is used most efficiently (for certain types of input data) and then make recommendations to the user. 
What ML technique (along with a mature toolset) would work out? 
I am new to this, so looking around it seems that HMM might work out ? I found some relevant discussion here  - http://stackoverflow.com/questions/6990230/supervised-learning-for-user-behavior-over-time",en
1109612,2012-04-19 21:19:05,statistics,"Two 70-year-old papers by Alan Turing on the theory of code breaking have been released by the government's communications headquarters, GCHQ.",si6fq,Lambda_Rail,1264557376.0,https://www.reddit.com/r/statistics/comments/si6fq/two_70yearold_papers_by_alan_turing_on_the_theory/,14.0,0.0,,en
1109613,2012-04-20 02:21:09,MachineLearning,Question about test error in k-fold cross validation,sinsc,GotGoose,1289159546.0,https://www.reddit.com/r/MachineLearning/comments/sinsc/question_about_test_error_in_kfold_cross/,6.0,5.0,"For my particular data set, I am running k-fold cv for k = 3, 5, and 10. Most of the time, the error decreases as I progress from 3 to 5 and then to 10, but sometimes it decreases from 3 to 5 and then increases from 5 to 10. Does this mean anything? I would expect error to decrease. I have 28 subjects, if that makes any difference.

I'm currently an undergraduate, so any insight would be really helpful.",en
1109614,2012-04-20 04:44:38,statistics,"""There is no such thing as true randomness. We just don't know how to predict those quantum fluctuations."" brblol commented on a post in /r/math",sivlt,jason-samfield,1248855567.0,https://www.reddit.com/r/statistics/comments/sivlt/there_is_no_such_thing_as_true_randomness_we_just/,0.0,8.0,,en
1109615,2012-04-20 05:42:35,statistics,Job market for undergraduate with degree in statistics,siyvh,[deleted],,https://www.reddit.com/r/statistics/comments/siyvh/job_market_for_undergraduate_with_degree_in/,8.0,32.0,"I am about to graduate with a undergraduate degree in statistics. I have applied to 5 jobs a week ago and have not received any replies. I have taken courses on regression, multivariate statistics, biostatistics, Geology based statistics, design of experiments, data management, modern methods of analysis, and a few others. I have used R, SAS, JMP, ARC, ARC GIS, Mathematica, Minitab, you name it.

I feel that I should have received at least one call. Is the market not really open for undergraduate students?
**edit**: I should say that I have applied to jobs that say they only require bachelors degrees.",en
1109616,2012-04-20 17:05:05,statistics,Survival analysis question,sjnec,sortizo,1288834170.0,https://www.reddit.com/r/statistics/comments/sjnec/survival_analysis_question/,3.0,2.0,"Hello r/statistics a couple of weeks ago I posted [this](http://www.reddit.com/r/statistics/comments/qqzs0/survival_analysis_in_finance/), a question about Survival Analysis applications in finance. I am finally getting the job started but before I begin I wanted to know if there are any good papers out there about this particular subject. The main topic of the paper is going to be time until failure (default, foreclosure) of companies.
There are a few questions I would like to ask about statistical details.

*What is the minimum number of companies I would have to sample in order to de asymptotic assumptions? (I was thinking in something like 20-25)

*Do you think the 2008 financial crisis will bias observations?

*Do you think I should sample the entire market or just a particular industry?

Thanks for the help guys.",en
1109617,2012-04-20 17:53:36,statistics,More Phantom than Menace,sjpyn,Professor_IR,1310512226.0,https://www.reddit.com/r/statistics/comments/sjpyn/more_phantom_than_menace/,7.0,4.0,,en
1109618,2012-04-20 20:12:52,statistics,"Need advice on how to approach a data set, 
ANOVA, t-test? ",sjy4d,PhylogenTree,1301581768.0,https://www.reddit.com/r/statistics/comments/sjy4d/need_advice_on_how_to_approach_a_data_set_anova/,9.0,10.0,"Hi r/statistics,
I need som advice regarding data analysis and how I should approach this data set. Here is the situation: 

Each individual in the data set has reacted to a stimulus, and so each data set has a mean for before the stimulus, during the stimulus, and after the stimulus. 

There are three species, and sample size is small (species A, n=4, species B, n=5, species c, n=9

There are two temperatures and two different sizes
So what I have is for species A is n=2 for colder temperatures, and n=2 for warmer temperatures; and each species has a mean for how it reacted in the study pre, during and after. 

I’m really not sure what to do with this data, and the more I look the more confused I get. I was thinking of trying ANOVA, however I’m having difficulty with JMP 8, and I’m wondering if it’s a good way to approach this (maybe when just comparing the reactions of each species to each other). Is this to complex for a t-test, and if not then which kind should I use? Should I drop JMP and do it by hand? 

All advice is appreciated. Thank-you
",en
1109619,2012-04-20 22:23:32,statistics,"Fog warning system, part two.  Now with more Poisson regression.",sk5y9,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/sk5y9/fog_warning_system_part_two_now_with_more_poisson/,5.0,4.0,,en
1109620,2012-04-20 22:33:12,statistics,Question. Before and After intervention analysis.,sk6jb,secret_tiger101,1330017039.0,https://www.reddit.com/r/statistics/comments/sk6jb/question_before_and_after_intervention_analysis/,1.0,3.0,"Please help me Reddit!

I want to know what stats analysis would be appropriate. I have tried to word my question well, please forgive any obvious errors.

I have data for the number of events per day occuring before and after an intervention.
These events are divided into Area A and Area B.

In the Before Group I have data from Areas A &amp; B for 8 days.  In the After Group I have data from Areas A &amp; B for 9 days.

I want to know if the intervention had a statistically significant effect on the number of events per day.

The raw data indicates that there is an effect, but I want to show statistical significance:
Mean event/day Area A Before = 7.25
Mean event/day Area A After = 1.444

The data could be confounded by observer effect/hawthorne effect.
I am a medical student with basic stats knowledge.",en
1109621,2012-04-21 08:48:28,statistics,I need advice concerning experimental design.,skyi0,kpmmun,1279411475.0,https://www.reddit.com/r/statistics/comments/skyi0/i_need_advice_concerning_experimental_design/,3.0,12.0,"I have been asked to help with the design of an experiment. I have had one undergraduate course on the topic of experimental design, but even that was a couple of years ago. I was hoping that [/r/statistics](/r/statistics) could help me with some advice.

We are going to measure the power consumption of a motor. The motor is being used in a refining process. 

We have five factors (I believe that they are called treatments?), which are

* Rotational Speed
* Part Geometry
* Inflow rate of material
* Motor Parameter
* Material Parameter

Some properties of the variables:

* We plan on trying maybe 3 different levels for each feature
* There will be some natural variation in some of the features, even when we have a setting for the variable. For example, if we have 3 levels for the inflow rate (low, medium, high), there will be natural variation that we cannot control for the inflow rate. I do not know if this will be a concern
* The part geometry variable has been quantified into a continuous variable. All the other variables are naturally continuous variables
* We can control each of the variables (i.e. set each variable at a specific level)

Questions:

I recall something called factorial experimental design from my class several years ago. I am thinking of a 3^5 design with 1 observation at each factorial point (243 observations in total). Would this work for this kind of scenario? 

And afterwards, what kind of analysis would I be able to do, ANOVA or regression? Would it be a problem that all of our variables are continuous? Is it possible to pursue a fractional factorial design when there are 3 levels per variable, and would it be easy to interpret?

One other thing to note: there is some preliminary data that has been collected, and it appears that one of the variables is not linearly related to the response variable. Also, we have not tested for, or considered, interactions at this point.

Thanks for any and all advice!",en
1109622,2012-04-21 13:03:07,statistics,chi^2 Test for conditional probability?,sl42f,znarf,1211855186.0,https://www.reddit.com/r/statistics/comments/sl42f/chi2_test_for_conditional_probability/,8.0,4.0,"I'd like to use the chi^2 test for goodness of fit, but I'm not sure how to do this for a statistical theory that specifies different probabilities for different conditions. Let me explain.

Suppose experimental condition C has possible outcomes A1...An, and the experiment is run a lot of times. A certain theory T assigns probabilities P to the outcomes. I can measure T's goodness of fit by

\chi^2 = \sum_i [\#Ai / \#C - P(Ai) \#C]^2 / [P(Ai) \#C].

Plugging this \chi^2 value into the \chi^2 distribution with n-1 degrees of freedom gives me a number x which, if I understand this correctly, may be read as the probability of the total observed outcomes by the lights of T (assuming T treats all outcomes as independent). Right?

Now here's my question. What if I have several conditions C1...Cm instead of just one? For example, suppose my theory T specifies transition probabilities in a Markov process, by mapping each state of the system Ci to a probability function P over the next state. Can I use the chi^2 test to measure goodness of fit for such a theory? How would I do this? I suppose I could just multiply all the numbers x from the tests for individual conditions, but that looks a bit like a hack. Is there a more official way of doing this?

I'm not a statistician, so please apologise if I'm using the wrong words.
",en
1109623,2012-04-21 16:03:16,statistics,Could somebody remind me of what power and effect size is?,sl7hp,Leard,1325161413.0,https://www.reddit.com/r/statistics/comments/sl7hp/could_somebody_remind_me_of_what_power_and_effect/,1.0,0.0,,en
1109624,2012-04-21 16:06:55,statistics,Akaike Information Criterion and ARDL lag specification,sl7kq,ECModel,1325788948.0,https://www.reddit.com/r/statistics/comments/sl7kq/akaike_information_criterion_and_ardl_lag/,7.0,1.0,"I'm having difficulty finding out how to determine the lag specification for an ARDL(p, q, r, s) model. I'm using quarterly time series data with a duration of 200 periods. From what I understand, I should start with a specification of ARDL(4, 4, 4, 4), and then apply the AIC to each model to find the optimal specification. The problem is, do I have to go though every possible model? There are hundreds of permutations! I'm pretty sure my approach is wrong.

Any help would be greatly appreciated.",en
1109625,2012-04-21 18:02:58,datasets,Hopefully Reddit's sleuthing abilities are better than mine.,slb0p,IMetGregOden,1299471390.0,https://www.reddit.com/r/datasets/comments/slb0p/hopefully_reddits_sleuthing_abilities_are_better/,0.0,0.0,"Hello all,

I am looking to do a project on teen pregnancy rates, mostly on how effective abstinence programs are. I have been looking for state, county, MSA, city, any local level of data on teenage pregnancies. So far, the CDC hasn't been overly helpful so hopefully someone here knows of something that I have been unable to find.

Thanks.

Perhaps I should explain a bit. I received a research grant to do some research on the societal costs of abstinence only education (as it relates to teen birth rates, etc.). While I expect that it will have negative costs I don't want to jump to conclusions, anyways a little help with finding the data would be immensely helpful.",en
1109626,2012-04-21 20:24:31,AskStatistics,Need help with controlling for a variable,slgm6,burn_all_the_things,1315858318.0,https://www.reddit.com/r/AskStatistics/comments/slgm6/need_help_with_controlling_for_a_variable/,1.0,0.0,"I am doing a project to test if increased goal scoring sells more tickets and increases attendance in the NHL.  Obviously goal scoring and winning are highly correlated, so how would I control for a given amount of wins?  I only have Excel and the DDXL add-in to work with. I can run regressions in DDXL, but not sure how to control for wins so I can test just goals scored.  ",en
1109627,2012-04-21 20:32:37,statistics,I've just performed a two-way ANOVA,slgyo,[deleted],,https://www.reddit.com/r/statistics/comments/slgyo/ive_just_performed_a_twoway_anova/,1.0,0.0,"One of the main effects yield a significant p-value, but the F-value was extremely high (well over 1000). Is the data still usable or does this indicate that the groups are uselessly biased?",en
1109628,2012-04-22 00:04:00,datasets,What should I do with a qualified voter file?,slqgp,ThePoopsmith,1227637143.0,https://www.reddit.com/r/datasets/comments/slqgp/what_should_i_do_with_a_qualified_voter_file/,5.0,7.0,"I'm a programmer and looking to get competent with data mining. My state has the entire qualified voter file available for a meager price under the FOIA law. It contains name, address, birth date, gender and what elections they've voted in. I'm looking for some public domain or otherwise cheap data sets to correlate with it, but so far, I'm not coming up with much.

Does anyone have any ideas on what I could look for to correlate with this stuff? I'm mainly looking to play with this for practice, just to see how much I can gather and maybe even spook some friends. 

Thanks!",en
1109629,2012-04-22 06:11:38,datasets,"Which is best: datachimps.com, datamarket.com, or another alternative?",sm5xg,Lors_Soren,1282243358.0,https://www.reddit.com/r/datasets/comments/sm5xg/which_is_best_datachimpscom_datamarketcom_or/,5.0,2.0,,en
1109630,2012-04-22 08:53:13,statistics,"Turn Categorical Variable into Binary (1 = Treatment, 0 = Control) in R?",smc08,iamdorito,1281921004.0,https://www.reddit.com/r/statistics/comments/smc08/turn_categorical_variable_into_binary_1_treatment/,6.0,10.0,"Hi everyone. I'm working with a dataset of over 100k rows. I have one row which indicates whether that data point came from Treatment group or Control group. I want to turn that row into a row of 1's and 0's to indicate otherwise. How do I do that efficiently in R? I've tried googling, but I don't understand any of it. I'm a sophomore in college right now with little experience in working with datasets this large. Please help!",en
1109631,2012-04-22 21:58:18,MachineLearning,Machine learning for identification of cars,smxrx,kafka399,1335048442.0,https://www.reddit.com/r/MachineLearning/comments/smxrx/machine_learning_for_identification_of_cars/,22.0,9.0,,en
1109632,2012-04-22 22:06:55,MachineLearning,"Open source machine learning tool, Divvy, now on the Mac App Store (free)",smy7v,kevestun,1302897288.0,https://www.reddit.com/r/MachineLearning/comments/smy7v/open_source_machine_learning_tool_divvy_now_on/,7.0,1.0,,en
1109633,2012-04-23 00:17:48,rstats,Putting candidates in their place with R  | Kai Arzheimer: political science &amp; politics blog,sn3sv,kai17,1209765198.0,https://www.reddit.com/r/rstats/comments/sn3sv/putting_candidates_in_their_place_with_r_kai/,1.0,0.0,,en
1109634,2012-04-23 04:10:56,statistics,"""Question"" I need help with weighted data points",snff3,Morning_Theft,1314661089.0,https://www.reddit.com/r/statistics/comments/snff3/question_i_need_help_with_weighted_data_points/,0.0,3.0,,en
1109635,2012-04-23 05:29:02,AskStatistics,What color are traffic lights when the average person drives up to one in any given day?,snjgy,[deleted],,https://www.reddit.com/r/AskStatistics/comments/snjgy/what_color_are_traffic_lights_when_the_average/,3.0,4.0,"Excluding an ass ton of variables, in a given day how many traffic lights you pass out of lets say 100 will be red, and what will be green?",en
1109636,2012-04-23 08:39:46,AskStatistics,"Hey Reddit, what are the odds of a tie in Scramble with Friends?",sns50,[deleted],,https://www.reddit.com/r/AskStatistics/comments/sns50/hey_reddit_what_are_the_odds_of_a_tie_in_scramble/,11.0,0.0,,en
1109637,2012-04-23 12:42:21,statistics,Question regarding functions in R,snyfm,kabrch,1281867756.0,https://www.reddit.com/r/statistics/comments/snyfm/question_regarding_functions_in_r/,3.0,6.0,"Hi everyone. First off, I'm new to statistics, programming, R and everything, but I feel like I've searched through the web countless times without finding an answer to my troubles.

In my thesis project I'm working around with the twitteR package in R, and I have a list of accounts which are of interest to me. I have a general intuition about programming that tells me that I should be able to create a script/function which looks something like this:

for(i in 1:length(namesList)){
    namesList[i] &lt;- userTimeline(namesList[i], n=150) 
}

This however returns this error:
Error in .self$twFromJSON(out) : Error: Not authorized

My suspicions are that the function can't access my list. Is this correct? And if so, how do I correct it?

EDIT: Seems it was a problem with SSL Certification, which I hadn't done properly. Works as intended now with the instructions from ""bitterness"" &amp; ""PARSLEYsage"" below.",en
1109638,2012-04-23 15:45:46,statistics,Median Confidence Interval.,so2wa,srkiboy83,1299056137.0,https://www.reddit.com/r/statistics/comments/so2wa/median_confidence_interval/,6.0,1.0,,en
1109639,2012-04-23 19:51:44,MachineLearning,Stanford's ML class for Spring 2012 is now live for anyone interested.,sodu1,Lambda_Rail,1264557376.0,https://www.reddit.com/r/MachineLearning/comments/sodu1/stanfords_ml_class_for_spring_2012_is_now_live/,55.0,10.0,,en
1109640,2012-04-23 20:29:19,data,Fun Facts on Big Data,sofs0,cnew93,1329848703.0,https://www.reddit.com/r/data/comments/sofs0/fun_facts_on_big_data/,1.0,0.0,,en
1109641,2012-04-23 21:05:21,computervision,Free online computer vision course starts today,sohqy,waspinator,1202264096.0,https://www.reddit.com/r/computervision/comments/sohqy/free_online_computer_vision_course_starts_today/,15.0,0.0,,en
1109642,2012-04-23 21:10:55,statistics,Intramural soccer coach looking to allot play time relative to attendance for the game tonight. HELP!,soi2c,Jumping_Candy_Cane,1326237288.0,https://www.reddit.com/r/statistics/comments/soi2c/intramural_soccer_coach_looking_to_allot_play/,0.0,14.0,,en
1109643,2012-04-24 02:23:34,statistics,Math help! please teach too (statistics).,sozyv,[deleted],,https://www.reddit.com/r/statistics/comments/sozyv/math_help_please_teach_too_statistics/,1.0,0.0,"1.A test for a genetic disorder can detect the disorder with 92% accuracy. However, the test will incorrectly report positive results for 5% of those without the disorder. If 14% of the population has the disorder, find the probability that a person testing positive actually has the genetic disorder. (Round your answer to two decimal places.)

2.A pharmaceutical company has developed a test for a rare disease that is present in 0.6% of the population. The test is 97% accurate in determining a positive result, and the chance of a false positive is 6%. What is the probability that someone who tests positive actually has the disease? (Round your answer to two decimal places.)",en
1109644,2012-04-24 02:53:27,MachineLearning,"What are the limitations on pattern recognition in highly dimensional data? Given a large enough data set, is it ALWAYS possible to extract a pattern?",sp1jy,[deleted],,https://www.reddit.com/r/MachineLearning/comments/sp1jy/what_are_the_limitations_on_pattern_recognition/,3.0,8.0,"I'm taking a course on AI and machine learning and we've just been recently introduced to k-NN, naive-bayes and 1R. During class I began to wonder if it was always possible to extract a pattern no matter how complex the data was, and what the underlying mathematics that governs this was.

For example, lets say you have a ranking system and ranks are based on various attributes. If there are hundreds of attributes, and each one has an unknown weighting, as well as an unknown contribution to the weighting of OTHER attributes, is it possible to extract any reasonable pattern? 

In this particular case, even if we have infinite training data will it be exponential time complexity trying to figure out the how the value of one attribute changes another? Does this make it unfeasible to extract a pattern?

Being new to the field, my terminology is probably wrong so Ill use an example.

If we have a training data it will be of the form:

rank, attribute A, attribute B, C, D, ... (assuming the number of attributes is in the hundreds)

It's known that each attribute does not contribute equally to the rank. It's known that the value of an attribute can possibly change the contribution of OTHER attributes to the final rank. So if attribute A is high for some data, then attribute B, C and D contribute significantly more, but perhaps attribute H, K, W attribute less.

This isn't homework or anything, I've just can't stop thinking about it since class and I didn't want to ask my teacher in case she thought I was an idiot.",en
1109645,2012-04-24 03:49:27,statistics,F test between two dependent variables in a regression,sp4p5,bstockton,1299172542.0,https://www.reddit.com/r/statistics/comments/sp4p5/f_test_between_two_dependent_variables_in_a/,1.0,9.0,"I am running a fixed effects regression with panel data; my dependent variable is productivity. There are two measures of productivity, I wanted to see if there is a difference between the two models(same independent variables just different dependent). I tried an F-test using the within estimators but there are only 15, not enough for an F-test. Does anyone have an idea of how I would test if there is a difference between the models? ",en
1109646,2012-04-24 04:38:48,statistics,RCBD Split plot example,sp7kh,drotoriouz,1296664064.0,https://www.reddit.com/r/statistics/comments/sp7kh/rcbd_split_plot_example/,2.0,1.0,"Hello everyone,

We're learning about split-plot in my statistics class, but I'm having had time understanding examples of RCBD split plot, especially when it comes to understanding the ANOVA table. Could anyone provide me with a concrete example of what an RCBD split-plot would be like? I apologize if this is the wrong subreddit for this, but I thank you for any help you might be able to provide.

",en
1109647,2012-04-24 04:58:09,statistics,"The Lightning Bolt Problem (Or, using Magic: The Gathering as a gateway for analysis with R)",sp8ox,Bigbrass,1310437241.0,https://www.reddit.com/r/statistics/comments/sp8ox/the_lightning_bolt_problem_or_using_magic_the/,22.0,15.0,"Heya. I'm currently an economic undergrad student, and I've started working with statistical programming software.  They say work with what you know, so I'm getting my feet wet with R by doing some deck-building analysis.

A few days ago, I came up with a question regarding optimal deck construction within rigid constraints and set out to solve it using R.  It was a lot of fun, and I'm very proud of what I whipped together.  

I wrote everything out in mathy language in order to present my 'findings' to my friends, who are avid magic players themselves.  Unfortunately, this means what I wrote is geared more towards magic players than statisticians.

I can assure you all the conclusions I've made regarding the rules of the game are spot-on, it's the probability calculations I'm less confident with.  If anyone stumbles across an error, I'd absolutely love to hear about it.

Thanks for reading, I hope this isn't too trivial!  

[The Lightning Bolt Problem](https://docs.google.com/file/d/0B0hYr80dZEepalM4dVBtMV8xb3c/edit)",en
1109648,2012-04-24 05:19:56,MachineLearning,Question about PSO and normalization,sp9zd,JolienJM,1331705174.0,https://www.reddit.com/r/MachineLearning/comments/sp9zd/question_about_pso_and_normalization/,4.0,5.0,"I'm currently trying to implement a hybrid PSO (Particle Swarm Optimization) to perform feature selection (binary PSO) and parameter optimization (continuous PSO) for a Support Vector Regression machine simultaneously. I have a published article as a reference, but engineering articles usually lack a lot of details.

I'm currently at the 'initialize parameters' phase, so the question is not that complicated. The article said that all input variables were scaled, but later on they use non-scaled values for the SVR parameters. Does anyone know if it will make a difference whether or not to scale the SVR parameters? I'm guessing that if I do scale them, I'll need an extra step to get the original value back for SVR training. However, if I don't scale them, I need 3 different Xmax and Vmax values, so programming-wise it doesn't seem like one is more work than the other.

The article does not seem to be open-source, but for those who have access and want to view the article, it can be found here: http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5212569",en
1109649,2012-04-24 17:18:27,statistics,Appropriate descriptive stats for time series data with lots of spikes,spz0f,andresmh,1174603751.0,https://www.reddit.com/r/statistics/comments/spz0f/appropriate_descriptive_stats_for_time_series/,3.0,4.0,I have data representing the number of messages per day posted on a discussion forum ([see plot here](http://i.imgur.com/gTezS.png)). I believe means are probably not the right way of reporting this because of the spike-y nature of this data. What are the alternatives? Something that can be easily grasped by average (educated) people.,en
1109650,2012-04-24 17:25:46,statistics,Cox-regression and censored data in R,spzbm,EvLoEv,1322478762.0,https://www.reddit.com/r/statistics/comments/spzbm/coxregression_and_censored_data_in_r/,6.0,1.0,"Hi,
I have a dataset with survial data, where some of the survival times are right-censored. I wonder if any of you have any experience in how to handle this in R. I have done this:
coxph(formula=Surv(survivaltimes)~isCensoredOrNot),
and got this result:
                            coef exp(coef) se(coef)     z       p
isCensoredOrNot  -1.01     0.365     0.16 -6.31 2.8e-10.
Anyone know how to intrepret this? What are the requirements for using the Cox-regression model?",en
1109651,2012-04-24 21:51:28,statistics,Figure out how much RAM you need.,sqd92,wtf_ftw,1240271200.0,https://www.reddit.com/r/statistics/comments/sqd92/figure_out_how_much_ram_you_need/,14.0,6.0,"I'm considering building a new comp and wanted it to be able to handle stats (basic operations in R and STATA) on large datasets, so I was curious how much RAM I should invest in.  

On the STATA FAQ they have a [handy formula](http://www.stata.com/support/faqs/data/howbig.html) for calculating how big a hypothetical dataset will be.  They also say [here](http://www.stata.com/support/faqs/win/hardware.html) that they ""recommend that your computer contain 50% more memory than the size of your largest dataset."" 

I threw together some R code to visualize how much RAM is required for a given dataset given number of variables, observations, and average width (in bytes, as explained on the FAQ) of each variable.  I hope this comes in handy for anyone else thinking about a computer upgrade.

    require(manipulate)

    dataSize &lt;- function(v,w){
        n &lt;- seq(from=0, to=5, by=.1)
        g &lt;- (1000000*n*v*w+4*n)/1024^3
        plot(n,1.5*g,xlab=""N siz (Millions)"",ylab=""Required RAM (Gigs)"")
    }

    manipulate(dataSize(Variables,Avg.Width),
        Variables=slider(100,1000),
        Avg.Width=slider(3,20))


(Props to [Jurph at r/buildapc](http://www.reddit.com/r/buildapc/comments/sq8h1/building_a_number_cruncher_which_components_are/) for pointing me to the STATA FAQ)",en
1109652,2012-04-24 23:12:15,statistics,Detection Limit of Slope?,sqhru,SurfaceThought,1320256168.0,https://www.reddit.com/r/statistics/comments/sqhru/detection_limit_of_slope/,4.0,5.0,"Hello All,

I am working in a lab that measures the uptake of methane by soul bacteria. To do this we take measurements of methane concentration in sealed head spaces over time. Specifically, we take the first concentration measurement at t = 0, and then one measurement every ten minutes afterwards, until we have four measurements. To be perfectly clear:

C1, t = 0
C2, t = 10
C3, t = 20
C4, t = 30

Since the bacteria in the soil eat methane, we would expect the concentration to go down overtime in a sealed headspace. Thus, if we use regression analysis to find the slope, we would expect a negative slope. 
It is hard to quantify which ones or significantly negative due to the fact that the sample size of each slope is only four. Because of this a very strong signal is required for the slope of the line to be significant, under the standard analysis. 

Luckily, we do have more data to work off of. We have also run a great deal of standards, which are gas samples of known concentration (in this case the concentration in 1.85 ppm). For the 829 standards we have run, the mean is 1.85 (exactly, but only because it is defined to be), and the standard deviation is .07806, and is distributed quite normally. [A full description of the distribution is here](http://imgur.com/sCOY8)

The final complicating factor, however, is that there is a heteroskedasticity. Although we do not have data to quantify the variance at other concentration levels that we do at the level of 1.85 ppm, it is known about the type of instrument we are using that the variability goes up as the concentration goes up, and goes down as the concentration goes down (down to the limit of detection, anyway, which we are well above in all cases)

Ideally, what we are looking to find is a limit of detection of sorts for the slope measurements. We want to know the limit so that slopes that are less negative than the limit are more likely to be caused by random chance than by actual signal. That way we can assume the slope of those headspaces which give measurements that give a slope less negative than to limit to be zero. 

Does anyone have any hints as to how I can find this limit? Thanks!

--Evan
",en
1109653,2012-04-25 00:03:22,AskStatistics,If you flipped 20 coins 800 times what are the odds that you will get a least one series of flips with 16 or more tails?,sqkpf,thejmii,1309259692.0,https://www.reddit.com/r/AskStatistics/comments/sqkpf/if_you_flipped_20_coins_800_times_what_are_the/,2.0,4.0,Not sure if I explained that the best so I'll try and clarify as needed.,en
1109654,2012-04-25 00:15:39,analytics,A Social Media Dashboard for Google Analytics,sqlfl,xtmotion,1285327838.0,https://www.reddit.com/r/analytics/comments/sqlfl/a_social_media_dashboard_for_google_analytics/,1.0,0.0,,en
1109655,2012-04-25 03:11:41,statistics,Factorial Design Question: How to set one up?,sqv45,SlowBullets,1325766081.0,https://www.reddit.com/r/statistics/comments/sqv45/factorial_design_question_how_to_set_one_up/,1.0,3.0,"There is something that makes me curious: how do you set up a three way (or more in that case) factor design and apply that to real-world situations?

What is the step-by-process for setting up a factorial design? Starting from choosing the variables, blocking factors, randomization, etc. I would love to learn an instance of using factorial designs.

I am asking because I am going to try something out with a friend who is studying nutrition/exercise therapy. We want to see if we can do a factorial design with diet and nutrition: to determine whether or not various diets and exercise methods have any correlation to weight gain or loss.

This is something that I would LOVE to have the answer for as we want to set up a website for this ASAP. If interested, please PM me and I'll give further detail. =)

Thank you very much!",en
1109656,2012-04-25 03:12:34,statistics,Principal Components Analysis on Binary (or 3-level) questionnaire data.,sqv64,hypermonkey2,1301460132.0,https://www.reddit.com/r/statistics/comments/sqv64/principal_components_analysis_on_binary_or_3level/,7.0,7.0,"Hi all,

I was wondering what the best plan of attack would be for PCA (or PCA-type analysis) on questionnaire data in which each question has 2 or 3 levels.

3 levels is of most interest. I have heard of tetrachoric correlation structures, but am unsure exactly how these situations are commonly dealt with.

The goal is to minimize redundancy in the questions and identify question group-ing.

Thanks very much!",en
1109657,2012-04-25 08:19:36,statistics,Statisticians are Manly!,srbot,ColRockAmp,1307947600.0,https://www.reddit.com/r/statistics/comments/srbot/statisticians_are_manly/,1.0,0.0,,en
1109658,2012-04-25 12:17:52,MachineLearning,Analog Reservoir Computing using Microphone?,sriv5,marshallp,1239903633.0,https://www.reddit.com/r/MachineLearning/comments/sriv5/analog_reservoir_computing_using_microphone/,6.0,2.0,"Reservoir computing is a neural-network based machine learning system where a nonlinear stage creates outputs that are trained on by classifiers.

Analog reservoir computing has been done using buckets of water and optoelectronics.

My question is, is there a way of doing it using a microphone. You feed a signal out through a speaker, the nonlinearities are introduced by the environment and then this is picked up by the microphone on the computer. Has this been tried?

Also, is there a material that can retain it's ""memory"" longer for this - something that is affected by the sound and thus retains a memory and also produces sound. A gel material for example. If anyone has ideas, that would be great thanks.",en
1109659,2012-04-25 16:42:15,rstats,A Byte Code Compiler for R by Luke Tierney,srq6x,[deleted],,https://www.reddit.com/r/rstats/comments/srq6x/a_byte_code_compiler_for_r_by_luke_tierney/,1.0,0.0,I practically squealed with glee as I poured over this. Why can't all R documentation be this lucid?,en
1109660,2012-04-25 17:48:03,statistics,How to calculate this sample size ? (Info inside),srt56,SilverEyedLionKing,1299048387.0,https://www.reddit.com/r/statistics/comments/srt56/how_to_calculate_this_sample_size_info_inside/,0.0,3.0,"60% difference in the proportion of lymphocytes is expected between the dialysis group (32%) and patients with normal renal function (20%).

Can you please give me a formula to calculate this sample size ? Thanks !",en
1109661,2012-04-25 18:14:52,statistics,"Ezra Klein, a columist for WaPo, creates a Linear Regression model to predict presidential elections with in 3 points. He just did an IAMA.",srug5,[deleted],,https://www.reddit.com/r/statistics/comments/srug5/ezra_klein_a_columist_for_wapo_creates_a_linear/,11.0,0.0,,en
1109662,2012-04-25 18:22:19,statistics,Need advice for testing data,sruu0,Yazim,1316710187.0,https://www.reddit.com/r/statistics/comments/sruu0/need_advice_for_testing_data/,1.0,4.0,"Here is what I am trying to do:

I have a two sets of data.

1. A long list of projects that were either over or under budget.
2. Data showing the percentage of work each person contributed to each project.

What I want to know is whether having a specific person on a project (me, for example) contributes to the project being under budget. Or not. 

I'm not sure how to test this, and any advice you have would be welcome. 
",en
1109663,2012-04-25 19:14:48,datasets,Where can I find the historical cost of healthcare premiums? (1970-2010) [x-post from r/SampleSize],srxko,jdv253,,https://www.reddit.com/r/datasets/comments/srxko/where_can_i_find_the_historical_cost_of/,5.0,2.0,"I combed through the KFF's website to no avail. Google isn't turning anything up.

Thank you!",en
1109664,2012-04-25 20:05:47,statistics,What types of tests should I use?,ss0c7,ChadCares,1306825425.0,https://www.reddit.com/r/statistics/comments/ss0c7/what_types_of_tests_should_i_use/,1.0,0.0,"Hello r/statistics,

I'm writing a paper comparing two swimming teams(USA and AUS). I've already compared their average times with hypothesis testing. My question is, what other tests should I do? Any help is appreciated.

Thanks,
ChadCares",en
1109665,2012-04-25 20:14:17,statistics,Sum of Squares,ss0sz,[deleted],,https://www.reddit.com/r/statistics/comments/ss0sz/sum_of_squares/,1.0,0.0,"Please let me know if I put this in the wrong subreddit, it is not a homework-specific question but rather a general conceptual question.

Can anybody explain to me the purpose of calculating the sum of squares? I understand technically what it is; the total SS is the sum of the squared deviations of each score from the grand mean, the within-groups SS is the sum of the squared deviations of each score from its group mean, and so on. But something isn't clicking and I don't understand the practical application of this calculation or an interpretation of what it means for the data as a whole. I've scoured the internet and my textbook, maybe somebody here can explain it to me in clear, comprehensible terms?  Thank you.",en
1109666,2012-04-25 21:12:38,statistics,"Fog warning system: part three.  Speed kills, retroactively?",ss41t,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/ss41t/fog_warning_system_part_three_speed_kills/,8.0,1.0,,en
1109667,2012-04-26 00:08:05,statistics,The weirdest split or steal ever! (prisoner's dilemma TV show),ssecs,grandeabobora,1294638795.0,https://www.reddit.com/r/statistics/comments/ssecs/the_weirdest_split_or_steal_ever_prisoners/,0.0,0.0,,en
1109668,2012-04-26 03:00:36,statistics,Question: model to use in assessing the responsiveness of a categorical variable to policy changes over time?,sso58,larrybronze,1335362254.0,https://www.reddit.com/r/statistics/comments/sso58/question_model_to_use_in_assessing_the/,2.0,5.0,"I am interested in determining if the decisionmaking of a particular government body was responsive to policy and statutory changes that occurred at known points in time.

I can classify the decisions of this body as 1) rejected proposal; 2) accepted modified proposal; 3) accepted proposal (without change).

I could classify the policy or statutory changes dichotomously -- ""with deregulation"" and ""without deregulation,"" for example.

I've read about intervention analysis of time series data, but most of what I've read deals with continuous response variables.  In this case, I have two concerns:

1. I could take the categorical ""decision"" variable and convert it to a continuous ""rate accepted"" or ""rate rejected"" variable, BUT

2. The timing of the response variable may not be as easily coded. There is a lag between the time at which a decision was requested from the government body and the time at which the body made this decision, and this lag varies considerably from observation to observation. Further, decisions generally occur in bunches (a phenomenon I can't readily explain) -- that is, a scheme of intervals used to measure ""rate accepted"" would have considerable variation in N from interval to interval.

In general, I'm interested in whether there's a modification to the intervention model described in the link above that I should investigate, or any other model that might be relevant to time series analysis with categorical dependent and independent variables? A colleague suggested ARIMAX, of which I know nothing.

Thanks in advance for any help you may be able to provide.",en
1109669,2012-04-26 03:54:39,statistics,Question about statistical independence/dependence between 3 variables,ssqyv,MrEgbert,1321051947.0,https://www.reddit.com/r/statistics/comments/ssqyv/question_about_statistical_independencedependence/,3.0,7.0,"I'm writing a psychology paper that involves quantification of slot machine payout rate, and I have been dancing around the issue of how 3 payout parameters interrelate statistically. I know how they relate descriptively, but I'm wondering if there's a technical term for their relationship.

Basically, I'd like to know if there is a statistical term for the situation where:

* I have three random variables that are independent in the sense that, if you assign a value to one, you don't influence the probability distribution of, or have any additional information about, either of the remaining two.
* If you assign values to two, you constrain the the third to an exact value.

I tried reading the wiki page on statistical independence and got an inkling that this situation might come under ""conditional dependence"" but I don't have the know-how to be sure. For those who are interested, the 3 parameters I'm talking about are expected return rate (total winnings / total wagered), win frequency (#wins / #spins), and win size (average win amount / average wager).

Edit: definition of win size, which is probably better described as ""average payout ratio"".",en
1109670,2012-04-26 06:30:52,statistics,"All nighter before stats/prob final tomorrow. I don't know what I'm talking about, but she thinks I do.",sszng,[deleted],,https://www.reddit.com/r/statistics/comments/sszng/all_nighter_before_statsprob_final_tomorrow_i/,1.0,0.0,,en
1109671,2012-04-26 11:14:08,statistics,Inquiry about software,stabf,[deleted],,https://www.reddit.com/r/statistics/comments/stabf/inquiry_about_software/,10.0,9.0,"Hello,

I hate to use you folks like Google but I have no idea what buzzwords I need to be using to search on Google. All I've found is information of aquariums. I'm trying to look for a piece of software which allows me to 'fishtank' multiple variables at the same time. i.e. each wall of the 'tank' is a variable and the data are plotted within the 3D framework of the tank, presumably revealing clusters in certain locations, with an amorphous blob in the middle.

Does anyone know of some reliable software which will allow me to do this?

I'm currently using SPSS (and I guess Excel, but, ya know, Excel).

Thank you in advance for your time.",en
1109672,2012-04-26 12:30:43,datasets,Does anyone have access to these Google *web* n-grams?,stc0v,reliciler,,https://www.reddit.com/r/datasets/comments/stc0v/does_anyone_have_access_to_these_google_web_ngrams/,10.0,5.0,,en
1109673,2012-04-26 21:18:16,statistics,"It's my cake day, please help me out!",stwwi,metaljunkie17,1325014210.0,https://www.reddit.com/r/statistics/comments/stwwi/its_my_cake_day_please_help_me_out/,2.0,2.0,"So when one is using dichotomous outcomes in a study (such as a simple Y/N questionnaire), what would be the most effective techniques to be used during statistical analysis? 

Thanks!",en
1109674,2012-04-27 03:07:50,MachineLearning,Is there any kind of a PAC bound for Stochastic Gradient Trees? (Or even what constitutes model Complexity),sug4a,duckandcover,1196196596.0,https://www.reddit.com/r/MachineLearning/comments/sug4a/is_there_any_kind_of_a_pac_bound_for_stochastic/,5.0,2.0,,en
1109675,2012-04-27 05:30:25,MachineLearning,"ICML Workshop on Statistics, Machine Learning and Neuroscience",sunnm,badcorrelation,1335493696.0,https://www.reddit.com/r/MachineLearning/comments/sunnm/icml_workshop_on_statistics_machine_learning_and/,6.0,1.0,,en
1109676,2012-04-27 12:33:55,statistics,Any good resources for clustering/profiling analysis?,sv2m8,[deleted],,https://www.reddit.com/r/statistics/comments/sv2m8/any_good_resources_for_clusteringprofiling/,2.0,7.0,"I'm going to be working on a huge data set of mass spectroscopy data, and my goal is to figure out whether or not the data contains patterns that can be used to cluster samples into unique groups. So far the samples have a high degree of similarity, so simply running K-means or PCA isn't going to cut it. 
 
The literature on this type of data is mainly written by chemists and is therefore sparse on info regarding the statistical methods. Can you guys point me to any literature on this subject? 
 
Thanks a lot! ",en
1109677,2012-04-27 16:17:00,statistics,"Here's a question for you Statisticians, if rating users on their beauty from 1-10, how many votes would you need for the ""average"" to be statistically significant?",sv87v,BeautifulPrincess,1335532153.0,https://www.reddit.com/r/statistics/comments/sv87v/heres_a_question_for_you_statisticians_if_rating/,0.0,16.0,"I'm new here, and I hope I phrased that correctly! =)  

I'm not sure if Confidence Levels are relevant here, but if they are, let's say a 95% Confidence Level.

Obviously, if a person was voted on just once, and that vote was a ""10"", that doesn't actually mean they're a ""10"".  I also don't think you would need 10,000 votes to have a statistically significant average rating, either.

But where's the cut-off? At what point is the sample average a fair enough representation of the population average?

Would this be similar to how voter Polls are conducted in elections where they Poll about 1,000 people with a margin of error of +/- 3?

Thanks! =)",en
1109678,2012-04-27 18:07:35,statistics,"I need a formula for determining the ""best"" of something (details inside) which also removes as much bias as possible.",svcyf,RogueNine,1310848732.0,https://www.reddit.com/r/statistics/comments/svcyf/i_need_a_formula_for_determining_the_best_of/,0.0,4.0,"So at work I've been tasked with coming up with a contest for the rest of the employees on my team. What I have is the basic raw data of a certain metric, in this case called ""Customer Satisfaction"" or ""CSAT"". The CSAT score is determined by customer completing emailed surveys based on four questions with a score of 1 to 5, 5 being the best. Each day, the number of surveys that come in are calculated and the percentage of those above the goal is presented in an excel spreadsheet.  
  
So what I have is, for 38 people, ~25-30 days worth of percentages (some days no surveys came in for instance), and an overall total percent for the month*. What I don't have is the total number of surveys per person as that comes out on a different report and can take up to a week after the month is over to get. 
  
What my manager suggested is a point system, so that anything over 95% (passing) is 1 point, 97% or higher is 2, and 99% or higher is 3. There is a LOT of bias in this calculation, since we are a 24 hour call center and the people working overnight have around 1/3 the number of calls and surveys of everyone else (meaning that they have a better chance of being at 100% for any given day, and in fact are, the top 5 based on that calculation are all overnight reps).  
  
Is there any calculation I can do, with the data I have, to take out as much bias as possible and even the playing field so to speak?   
  
*The overall score has a similar bias, people who take fewer calls usually have a higher score since they have fewer surveys. For instance last month one guy took only 1 call, got 100% on it, then was pulled for a project the rest of the month. ",en
1109679,2012-04-27 19:01:39,AskStatistics,How as the field of Psychology advanced the study of statistics?,svfqg,penguinofevil,1299381856.0,https://www.reddit.com/r/AskStatistics/comments/svfqg/how_as_the_field_of_psychology_advanced_the_study/,1.0,0.0,,en
1109680,2012-04-27 20:46:57,statistics,Graduate School Advice: How much does the prestige of the school matter in stats?,svl31,statsgradschool,1335546961.0,https://www.reddit.com/r/statistics/comments/svl31/graduate_school_advice_how_much_does_the_prestige/,9.0,22.0,"Hi everyone, I know there have been a few graduate school threads in this subreddit but I was hoping to get some specific information on some programs.  I've been doing a lot of research and talking to a bunch of math/stats professors and they have told me that I shouldn't be too specific about research interests as I have yet to actually attend graduate school.  Instead they told me to apply to well respected programs as they tend to have a nice distribution of research interests among the faculty.  So I've been trying to judge good programs based on rankings/prestige factors.

If I'm interested in the non-academic job market, how much does the prestige of the school matter for a statistics job?  For example, I was looking at such schools as wisconsin, NC State, cornell, UNC, purdue, penn state and iowa state.  I know Iowa State is a large program and well regarded in statistics, but it doesn't necessarily have the prestige of cornell or davis which rank lower (in statistics).  I know that NC State and Penn State are respected engineering schools but not sure how they are viewed outside of engineering.

I'm have a graduate degree in another field (MS, math); my reasons for leaving are pretty personal and I won't go too much into it.  My eventual goal is a PhD degree, but as a backup in case I'm not happy with the university I attend, how easy is it to get into another Stats PhD program with an MS in stats?",en
1109681,2012-04-27 21:14:24,statistics,"Short versus long papers, in academic journals [r-bloggers]",svmex,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/svmex/short_versus_long_papers_in_academic_journals/,1.0,0.0,,en
1109682,2012-04-28 02:17:50,statistics,Why are the MAD (median absolute deviation) and variance of a normally distributed variable related by a factor of about 1.4826?,sw1si,enfieldacademy,1302474908.0,https://www.reddit.com/r/statistics/comments/sw1si/why_are_the_mad_median_absolute_deviation_and/,1.0,3.0,"Wikipedia has an explanation [here](http://en.wikipedia.org/wiki/Median_absolute_deviation), but i don't understand it. 

specifically the statement in the short sentence ""therefore we must have.."" is what i don't follow. 



",en
1109683,2012-04-28 03:45:23,computervision,OpenCV problem with cvContourBoundingRect,sw5k7,[deleted],,https://www.reddit.com/r/computervision/comments/sw5k7/opencv_problem_with_cvcontourboundingrect/,1.0,0.0,"I have written the following function in an attempt to identify the lowest region in a binary image. However, when I try to build it, [I get this error message](http://imgur.com/NonwU). This isn't happening for any other contour functions that I have been using, such as cvFindContours and cvContourArea. Has anyone else run into similar issues? Thanks.



    CvRect GetLowestContourBoundingRectangle(IplImage *img, bool invertFlag) {
    	// NOTE: CONTOURS ARE DRAWN AROUND WHITE AREAS
    	IplImage *output = invertFlag ? cvCloneImage(InvertImage(img)) : cvCloneImage(img); // this goes into find contours and is consequently modified
	
    	// find contours
    	CvMemStorage *contourstorage = cvCreateMemStorage(0);
    	CvSeq* contours = NULL;
    	cvFindContours(output, contourstorage, &amp;contours, sizeof(CvContour), CV_RETR_LIST, CV_CHAIN_APPROX_SIMPLE);
	
    	// analyze each contour
    	int lowestRectangleCoordinate = 0;
    	CvRect currentBoundingRectangle;
    	CvRect lowestBoundingRectangle;
	
    	while (contours) {
    		currentBoundingRectangle = cvContourBoundingRect(contours);
    		if (currentBoundingRectangle.y + currentBoundingRectangle.height &gt; lowestRectangleCoordinate) {
    			lowestRectangleCoordinate = currentBoundingRectangle.y + currentBoundingRectangle.height;
    			lowestBoundingRectangle = currentBoundingRectangle;
    		}
    		
    		contours = contours-&gt;h_next;
    	}
    	
    	cvReleaseMemStorage(&amp;contourstorage);
    	return lowestBoundingRectangle;
    }

Edit: It turns out cvContourBoundingRect is deprecated (it's definition is contained in compat.hpp).

    /*
       A few macros and definitions for backward compatibility
       with the previous versions of OpenCV. They are obsolete and
       are likely to be removed in future. To check whether your code
       uses any of these, define CV_NO_BACKWARD_COMPATIBILITY before
       including cv.h.
    */

With that said, does anyone know how to write a function that solves the problem I'm trying to solve?",en
1109684,2012-04-28 07:01:56,statistics,Please help? ANOVA,swdsi,esokies,1298337938.0,https://www.reddit.com/r/statistics/comments/swdsi/please_help_anova/,0.0,4.0,"Our three means for 3 groups is 1.7^-4, 7.85^-4, 1.96^-3. Now I'm supposed to do an ANOVA test (one way) and get a post hoc value but I am completely lost atm. Can someone guide me?
-Each group has a sample size of 8

",en
1109685,2012-04-28 17:39:40,statistics,Make R Run Slowly in the Background,swsx5,dmorg18,1288294041.0,https://www.reddit.com/r/statistics/comments/swsx5/make_r_run_slowly_in_the_background/,2.0,9.0,"When R runs more complicated functions, my computer slows to a crawl. Is there any way to make R pace itself so I can continue other work while random forests grow in the background?",en
1109686,2012-04-28 19:58:24,statistics,"DataGenetics- detailed statistical approaches to winning the game of Battleship, cracking a padlock code, stenography, and much more",swy3l,94svtcobra,1309123112.0,https://www.reddit.com/r/statistics/comments/swy3l/datagenetics_detailed_statistical_approaches_to/,21.0,5.0,,en
1109687,2012-04-29 01:11:35,AskStatistics,"Given n&gt;2 uncorrelated 1D gaussian variables with different means, what is the probability that a given one will be the greatest?",sxcbn,Megatron_McLargeHuge,1276567607.0,https://www.reddit.com/r/AskStatistics/comments/sxcbn/given_n2_uncorrelated_1d_gaussian_variables_with/,1.0,0.0,"Suppose we're modelling a game where each player's score is drawn from a different gaussian distribution. The means differ but the variances can be assumed to be the same if a more general solution isn't available. For two players, the probability that player one wins is the probability that the difference of the gaussians is greater than zero, which is easy to compute. Is there a closed-form (with erf) solution for n &gt; 2, or a good approximation?",en
1109688,2012-04-29 04:32:01,MachineLearning,A question about notation used in neural networks,sxktx,SunnyJapan,1309541152.0,https://www.reddit.com/r/MachineLearning/comments/sxktx/a_question_about_notation_used_in_neural_networks/,8.0,7.0,Why for a synaptic weight from neuron i to neuron j we use notation Wji? Isn't Wij a more straightforward notation?,en
1109689,2012-04-29 08:14:29,AskStatistics,Is there a rule of thumb for how different in size two groups can be before comparing them is no longer statistically valid?,sxtqu,[deleted],,https://www.reddit.com/r/AskStatistics/comments/sxtqu/is_there_a_rule_of_thumb_for_how_different_in/,0.0,0.0,"I'm trying to compare two groups, one with 220 responses and one with 130 responses. I was once told that anymore than a 50% difference and it isn't realistic to compare the samples. Is this true?

The groups I have look similar to this

220 Green shirts pick A, B, or C

130 Yellow shirts pick A, B, or C

I'm trying to figure out if ""shirt color"" affects choice. 

Any help would be appreciated. Thanks.",en
1109690,2012-04-29 09:10:12,statistics,Visualizing Stop-and-Frisk and Murder Rates in New York City,sxvk9,efrique,1219734906.0,https://www.reddit.com/r/statistics/comments/sxvk9/visualizing_stopandfrisk_and_murder_rates_in_new/,8.0,0.0,,en
1109691,2012-04-29 16:48:52,statistics,Can someone help me analyse my data?!,sy4up,apathetic_medic,1331333586.0,https://www.reddit.com/r/statistics/comments/sy4up/can_someone_help_me_analyse_my_data/,0.0,7.0,"So I'm doing a project and have a whole database of subjects with numerous variables for each. I want to find any correlations/patterns in the data. The variables are mainly categorical (yes/no, type1/type2/type 3 etc...). I'm using SPSS. Can anyone help??",en
1109692,2012-04-29 22:15:00,rstats,Running a 3-way ANOVA in R,syhs3,redditopus,1319318330.0,https://www.reddit.com/r/rstats/comments/syhs3/running_a_3way_anova_in_r/,1.0,0.0,"I need to do this and I don't have a damn idea how.

Assume I know how to import data from, say, a .csv or an .xls.",en
1109693,2012-04-29 23:29:19,statistics,3-way ANOVA: Help.,sylco,redditopus,1319318330.0,https://www.reddit.com/r/statistics/comments/sylco/3way_anova_help/,8.0,14.0,"I need to do a 3-way ANOVA on some data collected from a laboratory in one of my classes.

Even after taking a biostatistics class last semester, this is still way over my head.  Excel's cruddy data analysis program can only do two-way ANOVA, and I don't know how to do any kind of ANOVA in R.

Help?",en
1109694,2012-04-30 00:28:17,statistics,Having some trouble understanding the Chi-Square test for homogeneity,syo76,Tony-Time,1287989165.0,https://www.reddit.com/r/statistics/comments/syo76/having_some_trouble_understanding_the_chisquare/,2.0,1.0,"Basically what I'm confused about is, if we have 2 classifications that aren't correlated but also aren't independent, how do we know whether or not this has a high probability of rejecting the null hypothesis?",en
1109695,2012-04-30 07:03:47,statistics,The difficult question of happiness - a new paper suggests that respondents to surveys on well-being are affected by the way they are asked,sz8m1,jambarama,1130472000.0,https://www.reddit.com/r/statistics/comments/sz8m1/the_difficult_question_of_happiness_a_new_paper/,14.0,2.0,,en
1109696,2012-04-30 09:04:42,MachineLearning,Talent crunch for machine learning and statistics skills,szdlx,contrarianism,1263224785.0,https://www.reddit.com/r/MachineLearning/comments/szdlx/talent_crunch_for_machine_learning_and_statistics/,25.0,11.0,,en
1109697,2012-04-30 13:00:47,statistics,"""Smoothing out"" a discrete distribution?",szjnh,jon_smark,1253799834.0,https://www.reddit.com/r/statistics/comments/szjnh/smoothing_out_a_discrete_distribution/,3.0,15.0,"I have a set of ratings for a film, where each rating is an integer between
1 and 10.  I don't want to make any assumptions concerning the distribution
of these ratings; for some films it may well be normal, but for others it may
be bimodal or something completely different altogether.  With this in mind,
I reckon I should discard any parametric approach to the characterization
of the data, opting instead for dealing only with the raw histogram.

Here's my problem: while some metrics (mean, stddev) are easy to calculate
from the histogram, there are others which are not straightforward.  One
particular example: at which threshold can I expect, say exactly 50% of the votes?
If the pdf were continuous this would be simpler to calculate, but how do I
transform a discrete pdf into a continuous one without distorting the data?
Though I could cook up some ad hoc solution, I reckon this must be a common
problem in statistics, and therefore already has well defined solutions...
",en
1109698,2012-04-30 13:08:05,statistics,How should I be reporting my results for a mixed ANOVA?,szjtg,[deleted],,https://www.reddit.com/r/statistics/comments/szjtg/how_should_i_be_reporting_my_results_for_a_mixed/,1.0,0.0,"In my study participants view a set of positive and negative valence pictures while their electrodermal activity is being measured, and have to rate each picture on subjective valence and aroual. They then consume either caffeinated or decaffeinated coffee and have to view the second set of images.

I've done mixed ANOVAs and I've done two for each DV, one for positive pictures and one for negative pictures.
Three IVs:
1. Between participants: Caffeinated vs decaffeinated
2. Within participants: Before consumption of coffee vs after consumption
3. Picture valence: Positive vs negative pictures

Three DVs:
1. Electrodermal activity
2. Subjective arousal
3. Subjective valence

I'm unsure how I would report this, is a 2x2 mixed ANOVA? Any help and tips would be appreciated. Thanks!",en
1109699,2012-04-30 17:34:13,MachineLearning,The Million Song Dataset Challenge - recommender system competition,szrk4,bmcfee,1335796373.0,https://www.reddit.com/r/MachineLearning/comments/szrk4/the_million_song_dataset_challenge_recommender/,30.0,5.0,,en
1109700,2012-04-30 19:26:58,AskStatistics,Would like to know if I can pay a statistician here for some light side consulting....,szx4c,[deleted],,https://www.reddit.com/r/AskStatistics/comments/szx4c/would_like_to_know_if_i_can_pay_a_statistician/,8.0,6.0,"I have a pet project involving statistics-related calculations that could use some vetting and possible improvement of methods. At a 30,000-foot view, the project involves a scoring system for a contest. 

This is a personal side project and therefore budget is correspondingly scaled. However, I feel at least offering some compensation is an improvement over asking for free work, which I do see all over Reddit.

Please get in touch or offer ideas. I thought I'd raise the possibility of working with a statistician here.

Thank you!

EDIT/NOTES:
1. The project involves improving my existing scoring system.

2. The scoring system is for a multievent contest similar to a decathlon.
-multiple events
-one best performance per event
-performances are awarded points
-the sum of the points determines a winner

3. I need to vet the method I'm using for calculating points. 

4. A performance in a single event is correlated to a number of points. A performance is evaluated in progressive terms: as achievement goes higher, the score is progressively higher. Scoring is not linear. High performance implies difficulty, and therefore, increasing scarcity of likelihood of performance at increasingly higher levels.

5. There must be strong parity in the progressive distribution of possible points from event to event. The progressive nature of scoring (and therefore the progressive distribution of points among poorest possible, average, above average, and maximum performances) must be retained from one event to another. This is how you compare performances among dissimilar events.

6. Currently, the distribution is arbitrary and inconsistent.

7. What I would like to do is to set a lowest possible, average, and maximum performance per event. I would like the calculation to plot a progressive curve to distribute points along those points. I would then like to repeat this process so that each event's curve and deviation is the same along that path from event to event. 

8. I would like a better way of checking for progressive parity among all events. I think (but am not sure) that this means that the points calculated for poorest possible, average, above average, and maximum performances would be the same from event to event...

9. I would like to have the calculation calculate an reference index of scoring based on each possible increment from poorest to maximum. That is, if the event is timed, I would like to see all possible points for each possible measurable point. In the decathlon, this would be seeing every possible score at in a table/index format each possible time (e.g., the score for 10.56 and 10.57 and 10.58 and so on). Currently, my spreadsheet only allows the scorekeeper to enter a performance and then calculate the result. It's better to give the competitors a ""bluebook"" with the scoring indexes per event available for review.

10. So, what I'm looking for is an analysis of my current method and calculations and then the improvements described above.

There are two core formulas:

Points = A * (B - P )c
for events where decreases in the metric indicate better performance
(e.g. ""time"")

Points = A * (P - B)c
for  events where increases in the metric mean better performance
(distance, reps).

*note how P and B are reversed.

A, B, and C are constants suited to each event. C is always above 1,
which gives the score a progressive nature. C is ""raise to the power
of"", or exponential function. 

Thank you.






",en
1109701,2012-04-30 20:47:20,statistics,People in positions of power that don’t understand statistics are a big problem for genomics,t01hb,t_rex_tullis,1318255134.0,https://www.reddit.com/r/statistics/comments/t01hb/people_in_positions_of_power_that_dont_understand/,21.0,5.0,,en
1109702,2012-04-30 20:49:01,statistics,"Let's see if I can make this project happen by 5pm. Need to use statistics to predict a future event, such as how many posts will be made on reddit tomorrow",t01kh,[deleted],,https://www.reddit.com/r/statistics/comments/t01kh/lets_see_if_i_can_make_this_project_happen_by_5pm/,1.0,0.0,"Anyone know where reddit stores this delicious information?

Doesn't need to be too complex. Looking to start by 2PM.

If this is not plausible, or if there are better reddit statistics to be analyzed, I would surely take that route.

Would really appreciate any upvotes to make this happen, my grade is banking on it. If I have no luck, like all my other posts, I will try to figure out something else 13 minutes from now :)

Will be watching comment replies.

Edit: Background, stat 2 class(advanced stat), not really too advanced however. 

Objective is to use inferential statistics (eg, hypothesis test using t-Test procedures, or regression modeling) to generate a result",en
1109703,2012-05-01 01:20:21,computervision,Looking for some feedback on my applied computer vision project.,t0hhn,zionsrogue,1210268911.0,https://www.reddit.com/r/computervision/comments/t0hhn/looking_for_some_feedback_on_my_applied_computer/,16.0,15.0,,en
1109704,2012-05-01 06:23:13,statistics,Question about Stat Software,t0z2g,intestinal_fortitude,1331183680.0,https://www.reddit.com/r/statistics/comments/t0z2g/question_about_stat_software/,1.0,6.0,"The whole world of statistical software is new to me. Stata, R, and the like seem intimidating, but right now, my introduction to the world of statistical programs is with Minitab (I'm a student and it's a requirement for one class).

My questions are: How useful is Minitab? How does it compare to the other stat software packages out there? Are components in Minitab translate-able to other programs like R or Stata (other than the obvious or very basic)?

Ultimately, I'd like to know if I can stick with Minitab for a while (into my professional career, or if I'd be better served learning a new program? Thanks in advance, r/stats!",en
1109705,2012-05-01 06:29:57,statistics,Help with application of PCA to neural recording data? ,t0zgd,sleepymonk,1335840962.0,https://www.reddit.com/r/statistics/comments/t0zgd/help_with_application_of_pca_to_neural_recording/,3.0,8.0,"So I have local field potential data (recordings of changes in voltage in the brain over a time period) from a monkey doing a task where he reaches to 8 different targets in space.  In some of the papers I have been reading, other researchers have used PCA to classify their lfp data based on some aspect of the recording (for my case, the target being reached to).  My assumption is that the lfp signal is different for different targets.  I've read through the tutorial using the toy example multiple times, so I have a basic understanding of PCA (rotating data around based on maximum variance)  how to mathematically compute my eigen values and principle components, and how to use matlab to do it.   But once I have this data, I am not sure how to use it to actually classify the signals, or present the data in a meaningful way.  I guess I don't really understand what my output means.  

Also, is there a major difference between using 'princomp' or 'svd' in matlab for these computations?  From what I can tell the difference is subtle, but this type of statistics is new to me.  Any help would be appreaciated.  Sorry if this is not the right subreddit, I didn't think homework help or ask science would be as appropriate.
",en
1109706,2012-05-01 12:40:01,statistics,Statistics help!,t1bnt,partylikeits99,1334850839.0,https://www.reddit.com/r/statistics/comments/t1bnt/statistics_help/,8.0,6.0,"Reddit - I posted this on ask reddit, maybe not the right place to post. I have posted it here in the hope one of you could help me.

Reddit,

I really need your help. I am a Uni graduate who needs to use statistics (dealing with distributions and the descriptive statistics accompanying them) for work.

However since a young age I cannot understand how this stuff works! It doesn't come intuitively to me. It's always been a drab module for me.

So far I have managed to pull through, but over the long term to become good at my job I cannot afford to not understand stats properly.

Do you know any good educational material (books/videos) which will make it interesting and intuitive to understand this? (like what Professor Walter Levin did for physics)

I know there is a tonne of books on stats - I am really hoping collectively that there is the one book (or video) that you know which is simply amazing for stats?

Thank you very much in advance.",en
1109707,2012-05-01 12:59:20,analytics,Google Analytics - Goal Type Event,t1c0h,gregsfriend,1313589795.0,https://www.reddit.com/r/analytics/comments/t1c0h/google_analytics_goal_type_event/,1.0,0.0,"Hi all,

I want to be able to track when someone clicks the ""submit"" button on my contact form and it looks like I should be able to do this with a GA goal event. Does anyone know how to do this or maybe a good tutorial on the subject? 

Thanks.",en
1109708,2012-05-01 15:07:12,datasets,"Nature Publishing Group releases linked data platform, including primary metadata for more than 450,000 articles published by NPG since 1869",t1ew1,mhermans,1169219262.0,https://www.reddit.com/r/datasets/comments/t1ew1/nature_publishing_group_releases_linked_data/,6.0,1.0,,en
1109709,2012-05-01 15:11:43,datascience,Ruby toolkit/webservice for extracting data (references) from pdfs based on structure analysis,t1f05,mhermans,1169219262.0,https://www.reddit.com/r/datascience/comments/t1f05/ruby_toolkitwebservice_for_extracting_data/,2.0,0.0,,en
1109710,2012-05-01 18:14:08,MachineLearning,"I want to quantify the ""closeness"" (or relatedness) of two scientific articles based on the contents of the text.  I have no idea where to start.  Can anybody help me get oriented?",t1k5l,omginternets,,https://www.reddit.com/r/MachineLearning/comments/t1k5l/i_want_to_quantify_the_closeness_or_relatedness/,12.0,19.0,Please feel free to ask for clarifications!,en
1109711,2012-05-01 19:26:59,MachineLearning,Ceres Solver: Google's large scale nonlinear least squares solver open sourced,t1o4k,nathanwiegand,1243437068.0,https://www.reddit.com/r/MachineLearning/comments/t1o4k/ceres_solver_googles_large_scale_nonlinear_least/,83.0,22.0,,en
1109712,2012-05-01 20:35:38,statistics,Is it possible to add a y-axis break to a column graph in SPSS??,t1rtd,PinusPondo,1299045746.0,https://www.reddit.com/r/statistics/comments/t1rtd/is_it_possible_to_add_a_yaxis_break_to_a_column/,0.0,0.0,,en
1109713,2012-05-01 22:20:24,statistics,What can I do the summer before starting graduate school?,t1xp2,chewitard,1258412755.0,https://www.reddit.com/r/statistics/comments/t1xp2/what_can_i_do_the_summer_before_starting_graduate/,12.0,32.0,"I just finished my undergrad with a BSc. in statistics, and about to start a Masters at the University of Toronto in September. I'm very excited to start school in September, however, I do not want to spend all summer just rotting at home. 

I am going away on vacation in the middle of summer, making finding summer jobs very difficult. So, I was wondering if there were maybe some statistics projects I can find online, or whatever, that I can work on or help someone with just so I can feel like I'm being productive this summer. Or maybe you guys can give me some suggestions on some important/interesting topics in statistics I could read/learn about before starting graduate school?

Thanks!",en
1109714,2012-05-01 22:27:51,MachineLearning,Probabilistic Data Structures for Web Analytics and Data Mining,t1y4d,daseinphil,1178737982.0,https://www.reddit.com/r/MachineLearning/comments/t1y4d/probabilistic_data_structures_for_web_analytics/,13.0,1.0,,en
1109715,2012-05-02 04:46:46,statistics,Fitting two populations?,t2jrx,Not_a_neuroscientist,1334355871.0,https://www.reddit.com/r/statistics/comments/t2jrx/fitting_two_populations/,4.0,21.0,"Hey everyone,

I did an experiment where I gave a drug to some rats (N=39) and tested them compared to controls (N=8). The t-test was not significant but the F test was, suggesting different variances. My mentor tells me this indicates there could be more than one population. 
Looking at the data it looks like I could fit it to one guassian and get a r2 of about 0.6. If I divide the data into a group of 9 and another of 30 I can fit those sets with guassian curves that both have r2 values around 0.8. Is there a way I can make one curve that shows both populations and I only get a single r2 value (hopefully above 0.6)? I can put an image if needed.

Here is the data and the two curves that I think fit the population better than one. [One versus Two populations?](http://imgur.com/V6Qw0)

Thanks so much!",en
1109716,2012-05-02 11:26:44,MachineLearning,Determining relationships in data,t30ok,[deleted],,https://www.reddit.com/r/MachineLearning/comments/t30ok/determining_relationships_in_data/,2.0,10.0,"Assumptions:

* The scores are generated by an unknown function
* Hundreds of variables in your data
* Each set of data has a score or rank
* The dataset is large, 10,000 - 1million entries of these sets of data/scores (the data set could be are large as required)

Is it possible to determine the contribution of each variable to the score? It's unknown what the weightings are.

Is it possible to determine the effect of one variable on another? It's known that a high or low value for variable *n* might change the contribution of variable *k* by some amount. But it isn't limited to linear relationships, variables *a*, *b*, and *c* could have their weightings increased by *p*, *q*, and *r* if the value for *s* and *t* are above/below some threshold.

I'm not sure its possible to do this, let alone do it in under exp/factorial time complexity... I hope the question is clear enough. ",en
1109717,2012-05-02 11:33:40,MachineLearning,I have a good broad knowledge of Machine Learning techniques but lack detailed understanding - can you recommend a book?,t30ur,[deleted],,https://www.reddit.com/r/MachineLearning/comments/t30ur/i_have_a_good_broad_knowledge_of_machine_learning/,20.0,13.0,"I've spent a lot of time trying to work my way through C. Bishop - Pattern Recognition and Machine Learning but I find it a bit difficult to get into. I commonly use a lot of techniques (Logit/SVMs/MRFs - I work in Vision) but I don't really have a deep understanding of the ""story"" of how we arrived at each approach and the mathematical steps it took to get there. I need a book which holds my hand for the first chapter or two so I can improve my maths (British maths education...). Can anyone make any recommendations?

edit: Thanks a lot for all the responses - I've made a few purchases. Hopefully see you Vision guys around ;)",en
1109718,2012-05-02 22:40:39,MachineLearning,Launch of the Kaggle Data Science Wiki,t3rtn,willis77,1188267725.0,https://www.reddit.com/r/MachineLearning/comments/t3rtn/launch_of_the_kaggle_data_science_wiki/,28.0,0.0,,en
1109719,2012-05-02 22:42:43,datascience,Launch of the Kaggle Data Science Wiki,t3ryl,willis77,1188267725.0,https://www.reddit.com/r/datascience/comments/t3ryl/launch_of_the_kaggle_data_science_wiki/,7.0,0.0,,en
1109720,2012-05-03 02:15:56,statistics,Regression Modelling anyone?,t44qh,kslizzzy,1319755065.0,https://www.reddit.com/r/statistics/comments/t44qh/regression_modelling_anyone/,11.0,11.0,"Hello Reddit,

I am out of school this semester and teaching myself Statistics and Regression modelling. I was wondering if I can get some help from experts since I cannot really afford college at the moment. On a personal note I'm sick and tired of the health care system in this country, it has screwed my family over, which is why I want to test what I learned this semester on this topic.

Basically I want to use STATA or any software using data from OECD to test the following:

To measure these dependent variables
•	Infant  &amp; Premature Mortality
•	Life Expectancy by gender, at birth and at age 65.

Using These explanatory variables
 
•	Health Care Sepnding %GDP
•	Tobacco consumption, grams per capita 
•	Alcohol consumption, liters per capita
•	Pollution, emissions of Nitorgen oxides, kgs per capita
•	Population % (25-64) with Higher Education 
•	GDP per capita

My questions are:

1) How do I group/tabulate the data in excel?
2) Should I use ANOVA tests indivdually for each variable and test significance or is there another shortcut?
3) How can I go about testing Fixed or Random effects.

any advice would be appreciated. I'm sorry If i did not elaborate too much I am writing this from my phone at work. Thanks!",en
1109721,2012-05-03 10:14:07,AskStatistics,"Line of best fit through data points, errors in both x and y axes",t4rtc,coniform,,https://www.reddit.com/r/AskStatistics/comments/t4rtc/line_of_best_fit_through_data_points_errors_in/,1.0,0.0,What's the standard way of fitting lines through data points with errors in both the x and y axis?,en
1109722,2012-05-03 16:22:52,MachineLearning,Used Shears,t51hy,johnsmithseo25,1332420640.0,https://www.reddit.com/r/MachineLearning/comments/t51hy/used_shears/,1.0,0.0,,en
1109723,2012-05-03 18:39:03,statistics,Modeling customer retention.,t57pb,RA_Fisher,1299707119.0,https://www.reddit.com/r/statistics/comments/t57pb/modeling_customer_retention/,4.0,7.0,Has any one else out there worked on modeling customer retention?  Soon I'll have some of this data in my hands and plan to work/apply logistic regression to figure out what causes a subscriber to drop their membership.  Any insights would be appreciated.,en
1109724,2012-05-03 18:41:41,datascience,The Data Journalism Handbook,t57ul,metl_lord,1200186034.0,https://www.reddit.com/r/datascience/comments/t57ul/the_data_journalism_handbook/,5.0,0.0,,en
1109725,2012-05-03 18:43:39,statistics,Methodological considerations with pooled time series cross-section analysis (or others?),t57y9,r-cubed,1325811744.0,https://www.reddit.com/r/statistics/comments/t57y9/methodological_considerations_with_pooled_time/,5.0,4.0,"I've been tinkering with a problem recently and while I conduct my own research on it, I thought I'd push the background to /r/statistics to see what other folks think.


There is an interest in a (non-clinical) public health trial studying the impact of a particular intervention on individual outcomes. The intervention would be neighborhood-wide, and compared to an equivalent control group (neighborhood). For the time being, ignore the impact of confounding effects and assume everything else is ""business as usual"". Unfortunately my preliminary thoughts (eg, a panel study or random sampling with propensity score matching to compare groups) are not able to be supported with the given funding. 


What people are leaning towards is doing repeated cross-sectional samples at three points in time (pre/post/post design), sampling from the treatment area and a similar control area.


Given that I have not done comparative cross-sectional research over time before, I started looking into pooled time series cross-section analysis. My knowledge in time series analysis is far below that in other areas, so I don't even know if I'm asking the right question. I have concerns with sampling...thus far I've read of applied methods comparing macro-level estimates across time, such as country level GDP over time. But I will be measuring the outcome of interest amongst thousands of people in each population, over three time periods. Does that lend itself to TSCS? I should note I've already done the necessary power analysis to calculate the required sample sizes to detect an effect of substantive interest.



edit: additional papers on this subject that would benefit the design/analysis of such a study would also be greatly appreciated. I also welcome anyone doing this type of research before who might have other ideas in mind. 
",en
1109726,2012-05-03 20:40:38,statistics,HLM Question,t5e81,allrighthamilton,1273982788.0,https://www.reddit.com/r/statistics/comments/t5e81/hlm_question/,3.0,5.0,"I'm currently having a degrees of freedom problem and I'm hoping that someone out there can help (a friend and I have been tackling it for a few weeks and just can't get anywhere). 

I'm using HLM to account for family effects in my analysis. For my study, I have:

56 participants 
44 total families (5 of the families have 2 siblings in the study; 3 have 3 siblings) 
5 predictors (all fixed effects) 

HLM keeps giving me 43 degrees of freedom for the intercept and 6 degrees of freedom for each predictor. Does anyone know why I only get 6 degrees of freedom for each predictor? 

Once again, I've been at it for a few weeks now and I just can't figure it out, any suggestions would very helpful!",en
1109727,2012-05-04 01:09:41,AskStatistics,What are the chances that uh...Bob...gets random drug tested tomorrow? (Details inside),t5tbj,shittyassassassin,1336075048.0,https://www.reddit.com/r/AskStatistics/comments/t5tbj/what_are_the_chances_that_uhbobgets_random_drug/,1.0,0.0,"Let's say that ""Bob"" works for a company that does random drug tests every month. Out of 250 employees, 10 get selected for testing. The day of the month chosen for testing is also random. Today is May 3rd and the tests have yet to come out. What are the chances that the tests come out tomorrow (May 4th) and Bob is chosen for testing?


I know it's fairly simple, but I'm hung up on one part:
-The chances of Bob being one of the 250 employees tested is obviously 1/25th
-Times that by the chances that the tests come out tomorrow and we have our answer...
 ...but, what are the chances that they come out tomorrow?
Is it 4/31?  or is it 1/28? (because theres only 28 days left of the month)",en
1109728,2012-05-04 04:17:02,AskStatistics,Chances two baseball starting pitchers are both celebrating a birthday,t639x,SanchoMandoval,1282785309.0,https://www.reddit.com/r/AskStatistics/comments/t639x/chances_two_baseball_starting_pitchers_are_both/,5.0,8.0,"In [this](http://scores.espn.go.com/mlb/recap?gameId=320503117&amp;teams=chicago-cubs-vs-cincinnati-reds) article, I was surprised to read that:

&gt;It was the first time in major league history that both starters were celebrating a birthday

Wouldn't that seem to not be terribly unlikely, given that there are 30 teams that each play 162 games a year, and professional baseball's been going on since 1869? 

There's a 1/365 chance that it's someone's birthday on any given day, so a 1/(365*365) chance that it's the birthday of two random people. But there are about 180 days in the calender when baseball is played, which would seem to suggest that 1 in about 740 games should feature two starting pitchers both celebrating a birthday. (for non-americans, all you need to know is there is one starting pitcher per team in every game)

Any thoughts?",en
1109729,2012-05-04 04:18:33,MachineLearning,Machine Learning in Python Has Never Been Easier,t63d7,jjdonald,1192132770.0,https://www.reddit.com/r/MachineLearning/comments/t63d7/machine_learning_in_python_has_never_been_easier/,45.0,26.0,,en
1109730,2012-05-04 04:53:14,statistics,SAS vs WPL - EU rules that computer languages cannot be copyrighted,t6580,efrique,1219734906.0,https://www.reddit.com/r/statistics/comments/t6580/sas_vs_wpl_eu_rules_that_computer_languages/,25.0,2.0,,en
1109731,2012-05-04 08:22:26,statistics,"http://www.reddit.com/tb/t5jf7


























Great statistics resource that helps you select, do and understand statistical tests in SPSS - Laerd Statistics (crosspost from /r/academicpsychology)
",t6fgz,DrReddItor,1317312951.0,https://www.reddit.com/r/statistics/comments/t6fgz/httpwwwredditcomtbt5jf7_great_statistics_resource/,0.0,1.0,,en
1109732,2012-05-04 10:01:56,AskStatistics,Has the amount of stored data reached the amount of matter in the universe? (bytes vs. atoms),t6itg,[deleted],,https://www.reddit.com/r/AskStatistics/comments/t6itg/has_the_amount_of_stored_data_reached_the_amount/,1.0,0.0,"Or if we are nowhere close to that amount, how about bytes vs # of stars in the universe.",en
1109733,2012-05-04 12:48:18,analytics,"What's the difference between ""unique visits"" and ""unique visitors""?",t6mvp,beyondawesome,1304606433.0,https://www.reddit.com/r/analytics/comments/t6mvp/whats_the_difference_between_unique_visits_and/,0.0,0.0,"I find all kinds of different explanations.

I know what the difference is between unique and non-unique and between visits and visitor.

The problem is ""unique visits"" and ""unique visitors"" is practically the same but not quite.

Who can help?",en
1109734,2012-05-04 18:08:00,statistics,"Surprisingly, ""makes me a sandwich"" and ""in kitchen"" are not on this graph.",t6x01,ActualReverend,1332537259.0,https://www.reddit.com/r/statistics/comments/t6x01/surprisingly_makes_me_a_sandwich_and_in_kitchen/,0.0,3.0,,en
1109735,2012-05-04 18:15:59,MachineLearning,Would this be considered a simple Kalman filter or something else?,t6xeb,ClusterSoldier,1265170186.0,https://www.reddit.com/r/MachineLearning/comments/t6xeb/would_this_be_considered_a_simple_kalman_filter/,3.0,3.0,"I'm messing around with forecasting (electric) loads. I have two sets of data/models, d1 refers to the day-ahead forecast/dataset and d2 refers to the current actual load dataset both with n points. And the Lbase and Lact variables represent d1 and d2 respectively. v stands for the variance. Initial prediction is set to d1 and the final pred is pred-load. The rest of the algorithm follows this:

set Lact = d2, n

set v1 = Lbase - Lact

set v2 = pred-load - Lact

set weight = v1 / v1 + v2

set Lbase = d1, n+1

set pred-load = Lbase + weight(Lact - Lbase)

set n = n + 1

loop

So basically, I'm adjusting the weight variable each time step based on how correct the datasets were (error). Would this be an example of a simple Kalman filter or just like a weighted moving average?

Note: Originally had the v1 and v2 setting flipped (i.e. v1 = pred-load - Lact) but the current way gives me a lower (MAPE) error.
",en
1109736,2012-05-04 23:00:14,MachineLearning,New York recently became the nation's first federal court to explicitly approve the use of predictive coding,t7cfh,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/t7cfh/new_york_recently_became_the_nations_first/,32.0,12.0,,en
1109737,2012-05-04 23:00:21,artificial,Computers and Emotions,t7cfp,soccerdude6227,1335668442.0,https://www.reddit.com/r/artificial/comments/t7cfp/computers_and_emotions/,11.0,11.0,"I am a college senior and I am writing a paper on Computers and Emotions.  Computers that can read emotions, computers that can show emotion, and can communicate with emotion.  A big area that I have been getting research from is a study at Cambridge University on Emotionally Intelligent Computers.  I need more though and I am having trouble finding quality information.  I am looking for more information on the emotions side.  Things such as emotions being specific to certain people and or groups of people.  How variable is the expression of emotion?  As well as any research in to this area as well as the AI side.  Can an AI learn emotion or does it have to be taught certain emotions.

I know Google is great and it has been used but Reddit is better.  Anybody have any ideas where to look or links to articles related to what I am asking?",en
1109738,2012-05-04 23:56:18,artificial,Stephen Thaler: Charlatan or serious researcher?  (leave your answers in comment),t7fbu,moscheles,1316266003.0,https://www.reddit.com/r/artificial/comments/t7fbu/stephen_thaler_charlatan_or_serious_researcher/,11.0,5.0,,en
1109739,2012-05-05 16:42:17,AskStatistics,"Has anyone ever played the game, Rack-o? What are the chances of being dealt a winning hand?",t8bdb,FatKidNoFriends,1278950318.0,https://www.reddit.com/r/AskStatistics/comments/t8bdb/has_anyone_ever_played_the_game_racko_what_are/,4.0,3.0,"So, allow me to flesh this out a bit. The game [Rack-o](http://boardgamegeek.com/boardgame/917/rack-o) is a very simple card game, which consists of 60 cards numbered 1 through 60. Each player in the game has a plastic [rack](http://cf.geekdo-images.com/images/pic273344_md.jpg) made up of 10 slots to fit cards into. The object of the game is to order your rack from lowest to highest from front to back faster than your opponents. When the game begins, each player is dealt 10 cards, which are placed in the rack in the order that they were dealt. After that, turns begin in which you can take a card from the pile to replace a card in your rack.  
  
My question is, what is the likelihood that a player will be dealt a rack in which their cards are already in order from lowest to highest, thus winning the hand before the game even begins? I have played many times and have never seen it happen.",en
1109740,2012-05-05 19:59:09,statistics,Data Analysis Approach Question,t8idk,firearmed,1305065247.0,https://www.reddit.com/r/statistics/comments/t8idk/data_analysis_approach_question/,5.0,15.0,"Hello all,

I'm a university researcher conducting a study of the independent music industry. I recently conducted a survey of 75 musicians, and have 5 similar data sets whose means I would like to compare. The data points in each set are integers from 0 to 4 inclusive.

Comparing the means of the 75 responses is easy, because they have the same sample size, but I'd like to compare the means of the 5 sets when data points of ""0"" are excluded. (This changes the sample size of each set drastically.

Is there a formula that accomplishes this? I've taken a basic statistics course, but don't remember learning how to compare several means of differing sample sizes.

Thanks!",en
1109741,2012-05-06 00:27:26,statistics,Looking for some good resources for a more practical grasp on multiple linear regression.  ,t8uhh,lauderdalepat,,https://www.reddit.com/r/statistics/comments/t8uhh/looking_for_some_good_resources_for_a_more/,10.0,3.0,My class textbook is a little dry and I've absorbed as much of it as I can. I'd like to know of some resources out there that might help put its practical use more into perspective. ,en
1109742,2012-05-06 13:12:12,statistics,Random statistics/odds question from a non stats guy!,t9js1,[deleted],,https://www.reddit.com/r/statistics/comments/t9js1/random_statisticsodds_question_from_a_non_stats/,0.0,1.0,"Okay so I was the DD tonight (Cinco de Mayo) and I saw someone getting a DUI on the freeway, and thought ""damn, getting a dui in the city sucks but on the freeway? Barely ever do you run into a CHP, but then being drunk the one time that you do, and they notice you?"" Eventually, my train of thought led to this:

Lets say, ignoring any fallacies for the sake of the question, that if you drive drunk from exit A to exit B, you have exactly a 1/5,000 chance of getting a DUI.

Now, person C likes to go to the bar every night, and drives home drunk from exit A to exit B every night. So: Common sense tells me that, since he's driving home so often, he has a much better chance of getting a DUI (in his lifetime) than the other person who's only done it once.. But life experience says that, it's still going to be 1/5,000, since that odd doesn't change each time you drive...

It's making me think in circles and it's going to bother me until I go to sleep. I've never taken a statistics class, I'm just wondering if there is a method or term for what I'm talking about",en
1109743,2012-05-06 18:47:43,MachineLearning,Anyone here work in Finance?,t9rw8,ICrepeATATs,1334168064.0,https://www.reddit.com/r/MachineLearning/comments/t9rw8/anyone_here_work_in_finance/,9.0,23.0,"If so: 

*How do you like it?

*What degree(s) do you have?

*How does your degree relate to your work? (crucial/relevant/irrelevant/irrelevant but a necessary signal)

*What techniques/methods do you use most?


Thanks!!",en
1109744,2012-05-06 23:29:31,statistics,How far can I push a nonlinear mixed effects model with logit link functions - or am I going to need to start from scratch?,ta4a1,[deleted],,https://www.reddit.com/r/statistics/comments/ta4a1/how_far_can_i_push_a_nonlinear_mixed_effects/,7.0,3.0,"Thank you for any input. Here is a complete description of my data (I promise, it's interesting!), the latent measures I'd like to estimate, and the approaches I've considered.

I have 20 subjects who each perform a task with 8 possible responses, where a set of 3 decisions performed on each trial will bring them to the unique correct response for that trial.  These are binary decisions, and each of them eliminates half of the remaining response options.  So, the first decision people make will eliminate 4 responses; the second decision will eliminate 2 of the remaining responses, and the final decision eliminates the remaining 1 incorrect response.  

Critically, subjects are forced to respond at various times - before they have actually completed all the requisite decisions.  Responses are ""probed"" in this manner uniformly across the entire range of times within a trial.  Thus, when I probe responses relatively early, those responses are essentially randomly distributed across all 8 possible response options; when I probe them at slightly longer delay, responses tend to be erroneous but within the 4 or 2 options that remain after the 1st and 2nd decisions respectively; and when probed sufficiently late, the responses are highly concentrated on the single correct option.  More concretely, for each subject I have about 300 trials that are each associated with a time and a response that is either correct, incorrect due to being the wrong response within the set of 2 options identified by the end of the 2nd decision stage, or incorrect due to being within the wrong set of 4 identified by the end of the 1st decision stage.

From this data I would like to be able to infer several latent variables: 

*A*. at what point subjects start eliminating the options associated with each decision,

*B*. at what point they have successfully eliminated all the options associated with that decision they ever will (i.e., their asymptotic level of performance in that decision), and 

*C*. what that asymptotic level of performance is.  

I thus would like to estimate 9 means - A, B, and C for the first decision [the one that eliminates 4 options], a, b and c for the second decision [the one that eliminates 2 options], and A, B, and C for the third decision [the one that eliminates 1 option].  I assume I will also need to estimate variability for each of these 9, bringing me to 18 parameters total for each subject.  Finally, I would like to estimate these parameters in a hierarchical manner, so that I can constrain the parameter values for a given subject to be within some reasonable range of those identified for all other subjects.

I was planning on implementing a simple mathematical model that searches through the 18 sample parameters to identify those that 

*(1)* maximize the likelihood of observing a given subject's data given those parameters 

*(2)* are maximally likely given the mean and standard deviation of the latent variable's estimates across all other subjects.

My question:

*I.* What is the smarter way to do this than just coding it all up myself, and doing a conjugate gradient descent search through these parameters?  

*II.* Would it make sense to try to force a nonlinear/linear mixed effects model to deal with this problem - and if so, how exactly would I begin to specify this model?  I am attracted to this option for the possibility of being able to control for autocorrelations in the timeseries and other sources of nuisance variance (e.g., stimulus effects, the cost associated with trial-to-trial changes in the decision made at each level, etc).  

Thank you for any advice!

EDIT: It's pretty clear to me now that this is a problem for JAGS or lme, but not for coding it all up myself.  Concrete suggestions about how to specify the JAGS or lme model would be great.  It also occurred to me that I could actually attempt to a fit a neural network to this problem, but I think I'd rather not go down that route right now.",en
1109745,2012-05-07 05:20:46,statistics,Finding the difference between proportions?,tak4h,Folocy,1308355263.0,https://www.reddit.com/r/statistics/comments/tak4h/finding_the_difference_between_proportions/,1.0,1.0,,en
1109746,2012-05-07 06:24:59,artificial,Brainstorming: Recurrent Networks over the Web,tamjb,togomes,1201126808.0,https://www.reddit.com/r/artificial/comments/tamjb/brainstorming_recurrent_networks_over_the_web/,2.0,8.0,"I will implement a recurrent neural network, it's a university project, in Javascript.

So I'm looking for ideas for a possible use on the web. So far the only thing that occurs to me is to analyze the sequence of navigation between pages and suggesting the next navigation. Or maybe using the network for usability metrics.

What other problems, related to temporal sequences, may be apply?",en
1109747,2012-05-07 09:36:02,statistics,I am a statistician and I buy lottery tickets,tatyx,eduardogotti,1299974427.0,https://www.reddit.com/r/statistics/comments/tatyx/i_am_a_statistician_and_i_buy_lottery_tickets/,3.0,1.0,,en
1109748,2012-05-07 12:52:36,MachineLearning,Brainstorming: Recurrent Networks over the Web,tays2,togomes,1201126808.0,https://www.reddit.com/r/MachineLearning/comments/tays2/brainstorming_recurrent_networks_over_the_web/,5.0,0.0,,en
1109749,2012-05-07 15:29:24,datascience,Would a M.S. degree in Bioinformatics provide me with data science skills?,tb2jm,Date_Knight,1322854419.0,https://www.reddit.com/r/datascience/comments/tb2jm/would_a_ms_degree_in_bioinformatics_provide_me/,4.0,2.0,"I've recently become interested in data science, but have discovered that as a relatively new discipline there are few degree programs specifically for it. A local university has a program in Bioinformatics, which sounds like data science but geared toward genetics.

[Here is a glance at the curriculum!](http://www.vcu.edu/csbc/bioinformatics/master/core.htm)

For a little background I started undergrad as an engineer, ended up majoring in humanities, but now want to go back for something a little more...jobby (and makes use of the interest I already have in technology). 

Thanks for any input!",en
1109750,2012-05-07 15:40:46,statistics,How to weight regression in R?,tb2wz,fultonator,1277161046.0,https://www.reddit.com/r/statistics/comments/tb2wz/how_to_weight_regression_in_r/,8.0,6.0,"I am a beginner in stats and in R and I'm trying to figure out how to do a weighted regression.

I'm using generalized least squares, and my supervisor suggested I weight the regression by the inverse standard error.  I've never done this before and I'm unsure about the R code.  I've looked through R help forums but can't seem to find exactly what I need.  Any advice would be appreciated.

This is my first stats-related post, so if any additional information is required, I'll provide it.  Likewise, tell me if this type of question should be posted elsewhere.

My question is regarding whether certain type of pollinators affect fruit yield.  My code looks like this:

model&lt;-gls(fruityield~pollinator1+pollinator2+pollinator3+pollinator4, method=""ML"", data=fruit)
",en
1109751,2012-05-07 17:28:53,MachineLearning,"I'm after the 2007 KDD cup data set (Netflix), any ideas?",tb6y9,Jebbers,1291896258.0,https://www.reddit.com/r/MachineLearning/comments/tb6y9/im_after_the_2007_kdd_cup_data_set_netflix_any/,4.0,12.0,"Couldn't find it on the KDD or Netflix sites, and it is relevant to my interests. Any help would be greatly appreciated.

Edit: [Found](http://www.lifecrunch.biz/archives/207)",en
1109752,2012-05-07 18:01:14,statistics,Are religious colleges getting more religious?  No.  But historically black colleges and universities might be. ,tb8br,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/tb8br/are_religious_colleges_getting_more_religious_no/,12.0,1.0,,en
1109753,2012-05-07 22:37:47,artificial,What college courses should I take if I want to focus on AI?,tbmjg,sun-and-stars,1309848314.0,https://www.reddit.com/r/artificial/comments/tbmjg/what_college_courses_should_i_take_if_i_want_to/,19.0,29.0,"I have searched this subreddit but couldn't find anything. I'm currently a sophomore majoring in computer science and minoring in math. What classes should I take to gear my learning towards AI? I've read that probability and statistics are good classes to take, but what else? My university does have two artificial intelligence and machine learning courses, so I'll probably look into those. Thank you in advance!",en
1109754,2012-05-08 00:24:19,MachineLearning,Noob Here: Quick question on grouping together related phrases,tbsma,umdebaba,1283819818.0,https://www.reddit.com/r/MachineLearning/comments/tbsma/noob_here_quick_question_on_grouping_together/,4.0,6.0,"I'm trying to implement a method that takes a long list of search terms and groups them together into related groups but don't know how to start.

**Example list:**
shoes for women,
women shoes,
cheap shoes,
wholesale women shoes,
cheap running shoes,
cheap womens shoes

**Example Output Group 1: (Womens Shoes)**
shoes for women,
women shoes,
wholesale women shoes

**Example Output Group 2: (Cheap Shoes)**
cheap shoes,
cheap running shoes,
cheap womens shoes

The number of groups should not be specified in advance. Thanks for the help!",en
1109755,2012-05-08 00:32:10,statistics,Opening Box Office Numbers Adjusted for Inflation,tbt2x,[deleted],,https://www.reddit.com/r/statistics/comments/tbt2x/opening_box_office_numbers_adjusted_for_inflation/,2.0,0.0,"I threw together a quick analysis showing opening box office numbers adjusted for inflation. Here is the methodology:

The movie data analyzed came from 50+M opening weekend page. [1] LINK The inflation data came from [2] here.

Each movie from that list was then converted to 2012 dollars. Since that list was capped at 50+M nominal dollars these graphs may not be entirely accurate. There might be some dark horse movie that made $5,653,863 its opening weekend in 1913 and should be included on the list due to the inflation adjustment.

I then graphed the top 12 movies, as that set included the top 10 in nominal values from the movie list. Some additional graphs/tables were included for curiosity purposes.


The album can be found [here](http://imgur.com/a/aV1Cx#0)",en
1109756,2012-05-08 01:47:20,statistics,Good Articles on Prediction Markets?,tbx5y,CivAndTrees,1331689546.0,https://www.reddit.com/r/statistics/comments/tbx5y/good_articles_on_prediction_markets/,0.0,0.0,"Hey guys, i am just looking for some summer reading materials.  I have had two proposed business/thesis ideas i have regarding Prediction Markets.  I know its sort of a newer area of applied statistics, finance, and economics.  ",en
1109757,2012-05-08 04:38:09,statistics,Sports Statistics Project Question ,tc6ju,sg187,1333569514.0,https://www.reddit.com/r/statistics/comments/tc6ju/sports_statistics_project_question/,5.0,3.0,"I'm doing a group statistics project for a introductory regression analysis course, we chose to try to determine if ""Defense wins championships"", like the old adage says.  The goal is to try and model the successfulness of a season using only defensive statistics. The response variable is: RV = 1*#of reg seasons wins + 2(if won first round playoff win) + ..... + 6*(if won superbowl). The explanatory variables are rushing yards allowed per game, ave opposing qb rating, passing yards allowed per game. etc (about 10 in total). The members of my group want to use rankings,(ie best team in the stat gets a 1 worst team gets 32) because the game has changed over the years and 80 rushing yrds per game in 2003 isn't the same as 80 rushing yrds per game in 2009, relative to the rest of the teams. So I think this is a good idea. However, they want to treat this as a numerical variable rather than a categorical variable. I think this is incorrect because i believe it assumes that the data is uniformly distributed, same difference between 1st team and 2nd team as there is between the 13th and the 14th ranked team. Looking at the data this is not the case, teams in the middle of the pack tend to be more close together than teams at the extremes. So there doesnt tend to as much difference between the 13th and 14th as there is between the 2nd and 3rd.  Therefore, I think we should treat ranking as a categorical variable. So I have three questions: Are my concerns valid? Will treating them as categorical variables really solve anything? And is there any test that we can run to see if treating rankings as numerical variables messes up the regression.  If this description was convoluted ask me for a clarification and I'll try to clear it up. Thanks in advance for the help.
edit: If you have any other thoughts/criticism that I didn't ask about it would also be appreciated.",en
1109758,2012-05-08 06:44:32,statistics,HELP ME REDDIT! I am in need of some statistics advice for analyzing some dependent data. ,tcdt1,ghostdad666,1321436466.0,https://www.reddit.com/r/statistics/comments/tcdt1/help_me_reddit_i_am_in_need_of_some_statistics/,0.0,0.0,"I am currently working with a longitudinal data set that has gathered information from members of various families across generations. Since the data consists of members of the same families, it violates assumptions of independence (individuals within the same family are dependent of each other). Unfortunately, I am only familiar with statistical methods that assume independence between the participants.  I am guessing that there are methods out there to analyze this sort of data, I just haven't been able to find them yet. If anyone on reddit has any advice I would be infinitely grateful.  ",en
1109759,2012-05-08 07:55:20,statistics,Silly R Question about Prediction,tch32,trousertitan,1282452608.0,https://www.reddit.com/r/statistics/comments/tch32/silly_r_question_about_prediction/,3.0,2.0,"I have a glm object named ""fit"" and some new data called ""newdata."" I want the function predict(fit, newdata) to give me the log odds for the new data, instead is giving me an error because new data of course does not have the same number of observations made as the original data the fit was made on (new and old data have the same number of variables of course). Anybody know off the top of their heads what I'm looking for here? I'll keep looking but the documentation seems unclear and I can't find that many useful websites.",en
1109760,2012-05-08 09:21:27,statistics,Incredibly embarrassing statistics brain fart.,tckfk,[deleted],,https://www.reddit.com/r/statistics/comments/tckfk/incredibly_embarrassing_statistics_brain_fart/,1.0,0.0,"Hey all!

I'm currently working on a paper. I've got an dependent variable and a set of independent / explanatory variables. I'm running an OLS regression in STATA. I believe that there may be bidirectional influence between the dependent and an independent variable. Aside from running a regression with the independent variable as X and the dependent as Y, is there any suite or tool available that can determine the relationship?

Thank you all for your assistance.
",en
1109761,2012-05-08 12:16:41,datasets,Data from online personality tests,tcp6g,[deleted],,https://www.reddit.com/r/datasets/comments/tcp6g/data_from_online_personality_tests/,1.0,0.0,,en
1109762,2012-05-08 16:33:26,MachineLearning,The thriving data ecosystem in NYC,tcw6t,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/tcw6t/the_thriving_data_ecosystem_in_nyc/,13.0,17.0,,en
1109763,2012-05-08 18:35:52,statistics,How to transfer results in R onto MS Excel?,td1sx,djent_illini,1312414426.0,https://www.reddit.com/r/statistics/comments/td1sx/how_to_transfer_results_in_r_onto_ms_excel/,6.0,14.0,"Hi r/statistics,

I have been used to using SAS in college for the last year and a half. I like the table output using ODS graphics.

However, I am working with R right now since my SAS license expired and my company does not have SAS yet (it costs $8500!). I want to create a table for my results just like what SAS does in create .rtf file in ODS graphics. I have been reading tutorials on making tables but they seem to long to make. I usually copy the results onto MS Word and spend some time formatting the results, which is annoying.

Is there a quick way to make tables so that I can just copy from R and paste into Microsoft Excel the most efficient way?

Thank you",en
1109764,2012-05-08 18:56:49,statistics,Newb statistics question; How do you determine whether multiple variables can be treated as one dimensional?,td2ug,imafraidicantletyou,1279625813.0,https://www.reddit.com/r/statistics/comments/td2ug/newb_statistics_question_how_do_you_determine/,6.0,8.0,"Hi, I have a very limited background in statistics, just introductionary college level, so I'm not even sure if I asked the question right.


What I'm asking is that if I have variables X1, X2, X3, ..., Xn how do i determine whether these can all be piled together, as in one single concept, or if they would need two or more variables to describe it.


More specifically, I'm working with the concept of government repression which in the literature is described as a single dimensional concept, meaning that repression on freedom of speech and for instance extrajudicial killings are just two sides of the same coin. I'm trying to determine whether it is correct two use repression as one concept, or if it should be split up. The database I'm using is the CIRI Human Rights Data project. 


I was thinking about using PCA to do this, would this be correct?


P.S. - I hope I explained my question properly, if not, please let me know. ",en
1109765,2012-05-08 22:50:29,MachineLearning,"Using the ADTree machine learning algorithm &amp; a web-based tool, Harvard researchers can diagnose Autism in minutes",tdfb9,kneb,1250440859.0,https://www.reddit.com/r/MachineLearning/comments/tdfb9/using_the_adtree_machine_learning_algorithm_a/,1.0,0.0,,en
1109766,2012-05-09 05:53:34,statistics,Anyone have any links for college stats?,te2cm,[deleted],,https://www.reddit.com/r/statistics/comments/te2cm/anyone_have_any_links_for_college_stats/,1.0,0.0,"im doing a project (senior in hs) and i was wondering if anyone had any reliable websites that have any statistics. preferably looking for drugs, alcohol, stuff like that, and living conditions and so on. anything really would help thanks.",en
1109767,2012-05-09 06:25:06,AskStatistics,Is it possible to determine the odds of a succession of events occured - each with different probabilities?,te41w,baggier,1252655404.0,https://www.reddit.com/r/AskStatistics/comments/te41w/is_it_possible_to_determine_the_odds_of_a/,4.0,2.0,"I am thinking of analysing my play at poker and want to see if my recent run of bad luck is statistically likely. I can take each hand I saw through to the end, say AA vs KQ, where say I have a 5:1 advantage (and of course lost). This is trivial when all the odds are the same (say 1000 hands of AA vs KQ) there are many simple ways of looking at my average (say 75%) and compare it with the expected value (about 83%) and see whether it was likely or the result of something else. It doesnt seem clear when there are 1000 examples where each example has a different probability as in poker hands. I can work out an average expected probability from the average of all the probabilities and compare it to my found average success but how do I determine whether this is due to chance or not?",en
1109768,2012-05-09 09:16:56,statistics,Question about GEE models (R),tebl8,statnewbile,1336543873.0,https://www.reddit.com/r/statistics/comments/tebl8/question_about_gee_models_r/,1.0,0.0,"I fit a GEE model for respiratory issues in children with age and maternal smoking status as the predictors.

gee1&lt;-gee(resp~age+smoke+age*smoke, id=id, family=binomial, corstr=""unstructured"", data = ohio)

Can anyone tell me in the model, what indicates that a child who already has respiratory issues is likely to continue to have respiratory issues? More generally, i am asking what indicates in a gee model that a cluster is more likely to produce the same results as subjects from different clusters.Thanks!",en
1109769,2012-05-09 12:42:02,analytics,Google Analytics A/B testing with Wordpress,teh4t,topcat555,1331037640.0,https://www.reddit.com/r/analytics/comments/teh4t/google_analytics_ab_testing_with_wordpress/,2.0,8.0,"Is it possible to do A/B split testing with two versions of a Wordpress page? Has anyone got any experience with this?


Thanks",en
1109770,2012-05-09 15:46:17,statistics,The A/B Revolution and the problems with trying to test everything.,telz6,reddit4,1288615971.0,https://www.reddit.com/r/statistics/comments/telz6/the_ab_revolution_and_the_problems_with_trying_to/,20.0,0.0,,en
1109771,2012-05-09 16:59:31,statistics,SAS: Fisher's Exact Test. Is my interpretation correct? (see picture),teos1,djent_illini,1312414426.0,https://www.reddit.com/r/statistics/comments/teos1/sas_fishers_exact_test_is_my_interpretation/,0.0,5.0,,en
1109772,2012-05-09 19:04:56,artificial,Ben Goertzel on Artificial Intelligence: Science Fiction or Simply Science?,teush,[deleted],,https://www.reddit.com/r/artificial/comments/teush/ben_goertzel_on_artificial_intelligence_science/,24.0,6.0,,en
1109773,2012-05-09 19:34:35,MachineLearning,R and Data Mining: Examples and Case Studies,tewbg,H4L9000,1297796030.0,https://www.reddit.com/r/MachineLearning/comments/tewbg/r_and_data_mining_examples_and_case_studies/,29.0,0.0,,en
1109774,2012-05-09 20:24:26,rstats,How do you handle long R code?,teyxc,RA_Fisher,1299707119.0,https://www.reddit.com/r/rstats/comments/teyxc/how_do_you_handle_long_r_code/,1.0,0.0,I'm working on a project that is likely to have several modules.  How do you guys handle modularizing R code?,en
1109775,2012-05-09 21:24:42,AskStatistics,"Which is better for getting a more accurate performance review, a 1-5 scale or a 1-10 scale?",tf279,-Malo-,1336066047.0,https://www.reddit.com/r/AskStatistics/comments/tf279/which_is_better_for_getting_a_more_accurate/,6.0,7.0,"I'm just curious to know if someone fills out a 'How did we do?' type review card, which type of scale more accurately represents the honest opinion of the customer/review taker?",en
1109776,2012-05-10 02:34:29,MachineLearning,Machine Learning Summer School Program Impressions?,tfjs5,GTanaka,1178091365.0,https://www.reddit.com/r/MachineLearning/comments/tfjs5/machine_learning_summer_school_program_impressions/,17.0,3.0,"I was wondering if anyone here has had any experience with the [Machine Learning Summer School program](http://www.mlss.cc/).  For those who have attended, 

* Was it a worthwhile experience?  
* Do you feel that you learned something useful beyond what you would have found on  your own time?  
* Did the talks go into sufficient technical depth to truly understand an area of research without becoming too opaque to understand?",en
1109777,2012-05-10 03:03:41,datasets,"FCC radio station, tower, and antenna data",tfl6h,binaryechoes,1322954701.0,https://www.reddit.com/r/datasets/comments/tfl6h/fcc_radio_station_tower_and_antenna_data/,6.0,0.0,,en
1109778,2012-05-10 03:51:16,datasets,Raw data from online personality tests,tfnol,[deleted],,https://www.reddit.com/r/datasets/comments/tfnol/raw_data_from_online_personality_tests/,6.0,1.0,,en
1109779,2012-05-10 04:16:00,statistics,Statistical assumption is flawed?,tfoz5,presdaddy,1294953228.0,https://www.reddit.com/r/statistics/comments/tfoz5/statistical_assumption_is_flawed/,11.0,14.0,"My professor grades finals in an interesting fashion: 

For one class, he makes up three unique tests. Since the tests could be of different difficulties, he finds the average of each version then adds points to the harder two tests to equal the highest of the three averages. For example, if the tests averaged 80, 78, and 74, he'd add 0, 2, and 6 points respectively to the scores of each test taker group.

I can tell this method contains a statistically flawed assumption, but I'm having trouble (1) understanding what that assumption really is and (2) understanding if that assumption could make a significant difference in the grades. I'm wondering this both for fun and, if you guys agree that it's a flawed enough assumption, I might approach the professor. 

Here's what I think is wrong:
The professor assumes that the takers of each unique test have similar average talents. The class has just 37 students, so if the three smartest students took the same test, the average would be skewed and would not represent the difficulty of that test. That's an assumption of homoscedasticity, correct? Is that assumption robust enough for it not to make a difference?",en
1109780,2012-05-10 08:06:29,statistics,"I know little about statistics, and my friend keeps using this data to argue there is evidence of algorithm vote flipping in the 2012 elections. Just wondering what people who actually know statistics can say about this? is it crap?",tg0pv,jebus5434,1325729761.0,https://www.reddit.com/r/statistics/comments/tg0pv/i_know_little_about_statistics_and_my_friend/,5.0,8.0,,en
1109781,2012-05-10 08:46:48,statistics,Statistics/error question: how to compare subpixel shifts between two spectra and get believable errors?,tg2c5,jbwhitmore,,https://www.reddit.com/r/statistics/comments/tg2c5/statisticserror_question_how_to_compare_subpixel/,1.0,2.0,,en
1109782,2012-05-10 10:57:18,statistics,"How many people have ever lived in the United States since 1776?  I bet $20 that it's been less than a billion total, including the over 300 million alive now.  ",tg6fx,johntodd,1336635931.0,https://www.reddit.com/r/statistics/comments/tg6fx/how_many_people_have_ever_lived_in_the_united/,3.0,1.0,,en
1109783,2012-05-10 11:04:45,AskStatistics,"Tell me how I ought to assign grades (or a ranking) for this fictitious class, and explain your reasoning...",tg6mj,[deleted],,https://www.reddit.com/r/AskStatistics/comments/tg6mj/tell_me_how_i_ought_to_assign_grades_or_a_ranking/,1.0,0.0,"When it comes to the end of an academic term/semester, my thoughts turn to grading, and in particular how difficult it seems to be to take grades earned over the the term and turn them into a letter grade.

Let's make things concrete and suppose we have a course where we tell the 50 students in the class: “There are ten assignments and two exams. We weigh assignments more heavily than exams (60% assignments vs 40% exams), and the final exam more than the midterm exam (of the 40% weight, 15% is for the midterm, 25% is for the final)”.

Below are some fictitious grades I made for such a class. They're fictitious not so much to avoid privacy issues, but more so that **I** _know_ the true ranking order for the students in the class. In my model (which is hidden to you), students have an overall proficiency in the discipline, but that proficiency is measured via skill in particular competencies (e.g., skill theory, skill in practice, etc.) that correlate (statistically) with overall proficiency. Different assignments weigh competencies differently, and there is also some noise. Also influencing student scores on some course components are (hopefully irrelevant) issues like their socioeconomic background and their comfort level with exams.

So, please consider the grades below, copy and paste them into your favorite spreadsheet, crunch the numbers, rank the students and/or assign letter grades. 

Most importantly, I'd love an explanation of how you did it, and if at all possible, why it's principled. If you have citations for articles on the topic, that'd be wonderful too.

(The `id` column is essentially the student name.)

id|hw1|hw2|hw3|hw4|hw5|hw6|hw7|hw8|hw9|hw10|ex1|ex2
---|---|---|---|---|---|---|---|---|---|----|---|---
aa|85|85|86|84|83|86|85|83|84|86|78|77
ab|78|80|81|78|78|82|78|78|80|80|69|67
ac|90|88|92|90|88|90|90|86|86|89|92|90
ad|85|85|87|85|84|86|85|83|85|86|78|77
ae|83|84|85|84|82|85|84|82|83|84|75|73
af|90|88|91|90|87|89|90|86|87|89|89|88
ag|79|81|81|79|78|82|79|79|81|81|70|69
ah|85|85|87|85|84|86|86|84|85|86|77|75
ai|83|83|85|83|82|84|83|82|83|84|74|73
aj|83|83|85|83|82|85|83|81|83|84|79|76
ak|83|83|85|83|82|84|82|81|83|84|74|73
al|88|86|91|88|86|88|89|86|86|88|88|85
am|71|75|72|71|72|75|70|72|75|74|53|51
an|79|80|80|78|78|81|78|78|80|81|70|67
ao|84|84|86|84|82|85|84|82|83|85|80|78
ap|83|84|84|82|81|85|83|82|83|84|72|71
aq|77|79|79|78|77|81|77|78|80|79|64|62
ar|98|93|100|98|94|95|99|92|91|96|104|102
as|92|89|95|93|90|91|93|88|88|92|95|94
at|86|86|88|86|84|87|86|84|85|87|80|78
au|76|78|79|76|76|79|76|76|79|78|64|62
av|81|82|83|81|80|84|81|80|82|82|71|69
aw|69|74|72|70|71|74|69|72|76|73|50|49
ax|86|85|88|86|85|87|87|84|85|87|83|81
ay|80|81|82|80|79|82|80|80|81|82|69|67
az|89|87|91|89|87|88|89|86|86|89|86|84
ba|86|86|88|86|85|87|87|85|86|87|81|79
bb|89|87|91|89|87|89|89|86|86|89|88|86
bc|79|81|81|79|78|82|78|79|81|81|68|66
bd|77|79|79|77|77|80|77|77|79|79|65|63
be|81|82|83|81|80|83|81|80|82|83|71|70
bf|70|75|71|70|71|74|69|72|75|74|51|50
bg|87|86|89|87|85|88|87|84|85|87|85|83
bh|90|88|92|90|88|90|90|86|87|89|90|87
bi|72|76|75|72|73|77|72|74|77|76|57|55
bj|64|71|67|64|66|69|63|67|72|69|41|40
bk|87|86|89|87|85|88|88|85|85|87|84|82
bl|84|84|86|84|83|86|84|82|83|85|78|77
bm|80|82|81|80|79|82|79|79|81|81|69|68
bn|83|83|83|82|81|84|82|81|82|83|74|73
bo|84|84|85|84|82|85|84|82|83|84|78|76
bp|86|86|87|86|84|87|86|83|85|86|80|80
bq|79|81|81|79|79|82|79|79|81|81|71|68
br|81|83|84|81|81|84|82|81|83|83|72|70
bs|93|90|94|92|90|91|93|88|88|91|94|93
bt|83|83|85|83|82|85|83|82|83|84|77|75
bu|80|81|82|80|79|83|80|80|81|82|73|71
bv|86|85|87|86|84|87|86|83|84|86|82|81
bw|76|79|78|76|76|80|76|77|80|79|60|58
bx|83|83|84|82|81|84|82|81|83|84|74|73


",en
1109784,2012-05-10 18:46:17,rstats,Three-way Repeated Measures ANOVA?,tglt8,RobJackson28,1250881014.0,https://www.reddit.com/r/rstats/comments/tglt8/threeway_repeated_measures_anova/,3.0,0.0,"I have another analysis question for anyone interested: three-way repeated measures ANOVA in R.
X-post to [Statistics StackExchange](http://stats.stackexchange.com/questions/28486/what-is-a-valid-post-hoc-analysis-for-a-three-way-repeated-measures-anova) 

Fully balanced design (2x2x2) with one of the factors having a within-subjects repeated measure. I'm aware of multivariate approaches to repeated measures ANOVA in R, but my first instinct is to proceed with a simple aov() style of ANOVA:

    aov.repeated &lt;- aov(DV ~ IV1 * IV2 * Time + Error(Subject/Time), data=data)

DV = response variable

IV1 = independent variable 1 (2 levels, A or B)

IV2 = independent variable 2 (2 levels, Yes or No)

IV3 = Time (2 levels, Before or After)

Subject = Subject ID (40 total subjects, 20 for each level of IV1: n^A = 20, n^B = 20)


    summary(aov.repeated)

    Error: Subject
              Df Sum Sq Mean Sq F value   Pr(&gt;F)   
    IV1       1   5969  5968.5  4.1302 0.049553 * 
    IV2       1   3445  3445.3  2.3842 0.131318   
    IV1:IV2   1  11400 11400.3  7.8890 0.007987 **
    Residuals 36  52023  1445.1                    
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

    Error: Subject:Time
                   Df Sum Sq Mean Sq F value   Pr(&gt;F)   
    Time            1    149   148.5  0.1489 0.701906   
    IV1:Time        1    865   864.6  0.8666 0.358103   
    IV2:Time        1  10013 10012.8 10.0357 0.003125 **
    IV1:IV2:Time    1    852   851.5  0.8535 0.361728   
    Residuals      36  35918   997.7                    
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

Is this a valid ANOVA model, and are sphericity tests and typical post-hoc analyses possible? 

Alternatively, I was thinking about using the nlme package for a lme style ANOVA:

    aov.repeated2 &lt;- lme(DV ~ IV1 * IV2 * Time, random = ~1|Subject/Time, data=data)
    summary(aov.repeated2)
  
    Fixed effects: DV ~ IV1 * IV2 * Time 
                                    Value Std.Error DF   t-value p-value
    (Intercept)                      99.2  11.05173 36  8.975972  0.0000
    IV1                              19.7  15.62950 36  1.260437  0.2156
    IV2                              65.9  15.62950 36  4.216385  0.0002 ***
    Time                             38.2  14.12603 36  2.704228  0.0104 *
    IV1:IV2                         -60.8  22.10346 36 -2.750701  0.0092 **
    IV1:Time                        -26.2  19.97722 36 -1.311494  0.1980
    IV2:Time                        -57.8  19.97722 36 -2.893295  0.0064 **
    IV1:IV2:Time                     26.1  28.25206 36  0.923826  0.3617

This approach yields the same 2-way interactions as significant, but has lower p-values than the aov() approach.

Post-hoc of significant 2-way interactions with Tukey contrasts using glht() from multcomp package:

    data$IV1IV2int &lt;- interaction(data$IV1, data$IV2)
    data$IV2Timeint &lt;- interaction(data$IV2, data$Time)

    aov.IV1IV2int &lt;- lme(DV ~ IV1IV2int, random = ~1|Subject/Time, data=data)
    aov.IV2Timeint &lt;- lme(DV ~ IV2Timeint, random = ~1|Subject/Time, data=data)

    IV1IV2int.posthoc &lt;- summary(glht(aov.IV1IV2int, linfct = mcp(IV1IV2int = ""Tukey"")))
    IV2Timeint.posthoc &lt;- summary(glht(aov.IV2Timeint, linfct = mcp(IV2Timeint = ""Tukey"")))

    IV1IV2int.posthoc
    #A.Yes - B.Yes == 0        0.94684   
    #B.No - B.Yes == 0         0.01095 * 
    #A.No - B.Yes == 0         0.98587    I don't care about this
    #B.No - A.Yes == 0         0.05574 .  I don't care about this
    #A.No - A.Yes == 0         0.80785   
    #A.No - B.No == 0          0.00346 **

    IV2Timeint.posthoc 
    #No.After - Yes.After == 0           0.0142 *
    #Yes.Before - Yes.After == 0         0.0558 .
    #No.Before - Yes.After == 0          0.5358   I don't care about this
    #Yes.Before - No.After == 0          0.8144   I don't care about this
    #No.Before - No.After == 0           0.1941  
    #No.Before - Yes.Before == 0         0.8616

The only problem I see with these post-hoc analyses are some comparisons that aren't useful for my hypotheses. I'm thinking a nested design would be most appropriate for correct p-value adjustments?

Any comments or suggestions are greatly appreciated!

*Edit #1: Grammar

*Edit #2: Added summary output of aov()

*Edit #3: Added summary output of lme() and 2-way interactions post-hoc attempts",en
1109785,2012-05-10 19:19:41,MachineLearning,R you ready for Big Machine Learning?,tgnjl,jjdonald,1192132770.0,https://www.reddit.com/r/MachineLearning/comments/tgnjl/r_you_ready_for_big_machine_learning/,12.0,18.0,,en
1109786,2012-05-10 19:51:31,rstats,R you ready for Big Machine Learning? (x-post from r/MachineLearning),tgp5x,jjdonald,1192132770.0,https://www.reddit.com/r/rstats/comments/tgp5x/r_you_ready_for_big_machine_learning_xpost_from/,4.0,1.0,,en
1109787,2012-05-10 21:00:53,MachineLearning,"Survey: what is ""data science""?",tgsx9,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/tgsx9/survey_what_is_data_science/,0.0,0.0,,en
1109788,2012-05-10 21:17:45,statistics,How to relatively weight position and momentum?,tgtw1,bakonydraco,1261543766.0,https://www.reddit.com/r/statistics/comments/tgtw1/how_to_relatively_weight_position_and_momentum/,1.0,4.0,"This question could be complicated, so I've tried to simplify it to it's simplest form.  Suppose there is a function f, that generates a new value at any given time interval t.  Your goal is, given the information you have, predict f(t+1), the next measurement.  If the only information available is at time t, let's say f(t) = x, then the best guess for the expected value of f(t+1) must be x, having nothing else to go on.  If you then learn that f(t-1) = x-1, this should affect your estimate for f(t+1).  You could argue that, since the average is now x-.5 (you could add a time weighting), that should be your prediction, or you could argue that, since there's an upward trend, you should predict x+1.  How do you weight the momentum of the data vs. the average?

For a concrete example, suppose you have a hen that laid 10 eggs today, and you want to estimate the eggs it will lay tomorrow.  If you then discover that it laid 8 eggs the day before, would your new estimate be higher or lower than ten?

This is obviously a simple case, and you could imagine more complex cases, but I wanted to keep the thought experiment simple.  Any insight is welcome.",en
1109789,2012-05-10 22:39:39,statistics,Binomial Distribution and Confidence Interval Question,tgycz,[deleted],,https://www.reddit.com/r/statistics/comments/tgycz/binomial_distribution_and_confidence_interval/,4.0,4.0,"Here is my question regarding the use of the binomial distribution for pass-fail testing.

Say I have a device, such as a laser that a vendor claims will measure a distance within +/- 1mm, 80% of the time. 

I do 10 trials and I get 7 measurements inside the tolerance and 3 outside.  I want to apply the binomial test to see the probability that the device is not performing up to specification.

* 1.	If I use excel to apply the binomial distribution (BINOMDIST) and use 7 successes with 10 trials it returns 32.2%. To me this means that there is a 32.2% chance it is within spec and a 67.8% (100%-32.2%) chance it is not.

* 2.	If I use excel in the opposite way with 10 trials and 3 success it returns an 88% chance I am outside the spec and a 12% chance I am inside.

**How come the result chance depending on which way you are testing it? (88% vs 67.8%) or (32.2% vs 12%)**
",en
1109790,2012-05-11 05:26:09,statistics,Question about STELLA modeling,thjq5,[deleted],,https://www.reddit.com/r/statistics/comments/thjq5/question_about_stella_modeling/,1.0,0.0,"I set up a model using STELLA v8.1.4 and worked out all the problems with it that prevented it from running a simulation. Now when I attempt to run it, the window merely twitches as if loading something, but no data/results come up. Is anyone familiar with this software enough to help me troubleshoot? Any suggestions would be appreciated.",en
1109791,2012-05-11 08:44:12,computervision,Show Reddit: Virtual Wardrobe and Fitting Room,thsd2,hatzl,1156269385.0,https://www.reddit.com/r/computervision/comments/thsd2/show_reddit_virtual_wardrobe_and_fitting_room/,1.0,0.0,,en
1109792,2012-05-11 13:49:37,statistics,SPSS &amp; multiple response questions - How to change data values automatically?,ti007,worryhole,1325893199.0,https://www.reddit.com/r/statistics/comments/ti007/spss_multiple_response_questions_how_to_change/,7.0,5.0,"I have a few multiple response questions in my data. The different answer choises are separate variables in this data and the choises the person has checked have the value ""1"". The unchecked values are dots ""."". (this is how it came out from the e-survey data...)

I need to do separate crosstabs with all the choises to get the chi-square results, but I can't, apparently because of the dots in the data - they need to be changed to something else, maybe zero or ""2""...? How to do this? Is there any other way then manually going through the data?

I would really appreciate any tips!",en
1109793,2012-05-11 19:17:46,statistics,Interview with Hadley Wickham  - creator of the popular ggplot2 statistical graphics software,ticbz,t_rex_tullis,1318255134.0,https://www.reddit.com/r/statistics/comments/ticbz/interview_with_hadley_wickham_creator_of_the/,21.0,1.0,,en
1109794,2012-05-11 19:41:57,MachineLearning,"Videos of the UC Berkeley Conference: ""From Data to Knowledge: Machine-Learning with Real-time and Streaming Applications""",tidkl,[deleted],,https://www.reddit.com/r/MachineLearning/comments/tidkl/videos_of_the_uc_berkeley_conference_from_data_to/,1.0,0.0,,en
1109795,2012-05-11 19:51:58,AskStatistics,What's “the best way” to calculate the final grade in a class?,tie31,Maristic,1241144230.0,https://www.reddit.com/r/AskStatistics/comments/tie31/whats_the_best_way_to_calculate_the_final_grade/,4.0,23.0,"Most teachers, I think, calculate final grades in a class by taking the scores for course components, adding them up according to the weighting they gave in the course syllabus, and then assigning a letter grade either according to some fixed letter-grade positions, or “grading on a curve”.

No one seems to question this strategy much, but every time I think about it, it seems deeply problematic to me.  But the alternatives I'm aware of strike me as problematic too.

#### Add 'em Up

If you suppose a class where the weighting is supposed to be 60% homework assignments, 40% exams, and but homework assignments have a standard deviation of 5% and exams have a standard deviation of 15%: if we just do 0.6 * assignment scores + 0.4 exam scores, it will produce totals that are most heavily influenced by exam performance, not assignment scores as claimed in the syllabus. How can that be fair…?

#### Normalize the Scores

How about we fix the above problems by normalizing the scores before adding them? For example, we could recenter and scale the scores so that they all have the same average and standard deviation.  But what if superlative Suzy did better than average Andy on the second assignment because she really put in extra hours on that assignment. Why the heck would your normalize her score, and why should average Andy now have a “below average” score? How can normalizing scores be fair…?

#### Assign Grades with Fixed Bands

Okay, _somehow_ you got total scores, and now you need to turn those into letter grades. What about the classic “90% is an A, 80% is a B, etc.”?  Well, what if the tests were a bit harder this year, does that mean no one should get A?  How is that remotely fair…?

#### Assign Grades with a Curve

Fitting the grades to a curve means that no matter how hard everyone tries and how well everyone does, someone has to get a D, even if they actually got 71% in the course. How does that make any sense…?

### The Question (tl;dr)

So, what _is_ the most sound way to combine course grades and why…?  If you normalize, is there a better way to do it (e.g., quartiles to avoid issues with outliers, perhaps)?.
",en
1109796,2012-05-11 23:56:33,statistics,Statistics Masters Student Looking for PhD Advice,tiqr3,snoius,1265493835.0,https://www.reddit.com/r/statistics/comments/tiqr3/statistics_masters_student_looking_for_phd_advice/,3.0,5.0,"My question is basically, how detremental is it from an employment perspective is it to have all of your degrees from one school?

Background: I obtained a BA in Mathematics last May, with the major of my coursework focused around statistics. Through this coursework, two of my instructors said I should apply for the graduate program, I wasn't too keen on taking the GRE, so I applied with a few letters of recommendation and my transcripts. I was accepted and started this past fall.

This summer I have my Comprehensive Exam, it has a two-tiered scoring system. A ""pass"" guarantees a Masters degree (if the courses are fulfilled) and a ""high-pass"" confeers the ability to go on for a PhD. A ""pass""-ing score is roughly a C (if you think this way) and a high pass is a high-C or low-B.

Right now my fear is that without the GRE, applying to another school for a PhD would be more difficult, and going to my current school for it would hurt my employment opportunties.

Also, the school is the University of Nebraska - Lincoln, if that matters.",en
1109797,2012-05-12 04:50:42,MachineLearning,Support Vector Machines,tj3io,nellaivijay,1314023361.0,https://www.reddit.com/r/MachineLearning/comments/tj3io/support_vector_machines/,31.0,5.0,,en
1109798,2012-05-12 08:31:06,statistics,regression- dependent variable is a percent?,tjbuy,robbyroo,1292911500.0,https://www.reddit.com/r/statistics/comments/tjbuy/regression_dependent_variable_is_a_percent/,4.0,6.0,"Hey All - the google failed me.  what regression model would I use when the dependent variable is a percentage?

Im modeling in R and am hoping to keep it simple (preferably a glm) as i have to drop the model in SQL.  ",en
1109799,2012-05-12 19:01:04,statistics,So I'm hoping someone who is passionate about statistics can inspire me to become excited about studying it again... ,tjr6k,[deleted],,https://www.reddit.com/r/statistics/comments/tjr6k/so_im_hoping_someone_who_is_passionate_about/,1.0,0.0,,en
1109800,2012-05-12 19:15:28,MachineLearning,"Advanced Matrix Factorization Jungle Page
- A living document featuring some of the most advanced algorithms in advanced Matrix Factorization i.e. beyond vanilla MNF and SVD - ",tjrsw,[deleted],,https://www.reddit.com/r/MachineLearning/comments/tjrsw/advanced_matrix_factorization_jungle_page_a/,30.0,0.0,,en
1109801,2012-05-12 19:19:14,MachineLearning,"The Big Picture in Compressive Sensing
- A living document trying to paint the Big Picture in the Compressed Sensing or Compressive Sensing Framework- ",tjrz1,[deleted],,https://www.reddit.com/r/MachineLearning/comments/tjrz1/the_big_picture_in_compressive_sensing_a_living/,10.0,0.0,,en
1109802,2012-05-12 21:05:50,MachineLearning,LinkedIn group on Advanced Matrix Factorization,tjw9f,[deleted],,https://www.reddit.com/r/MachineLearning/comments/tjw9f/linkedin_group_on_advanced_matrix_factorization/,1.0,0.0,,en
1109803,2012-05-13 03:42:01,artificial,When an computer knows it's not an AI...,tkc4m,Fizzlerr,1300926994.0,https://www.reddit.com/r/artificial/comments/tkc4m/when_an_computer_knows_its_not_an_ai/,0.0,0.0,"User: Can you think for yourself, or only via what others have said to you?

Cleverbot: What makes you think you know what I think?

User: Because I read about you.

Cleverbot: What did you read about me?

User: I read that you are a computer.

Cleverbot: No, I think the computer just randomly gives answers from different people on this site.

User: That is how you think - but is that what you are?

Cleverbot: Yes it is.
",en
1109804,2012-05-13 04:01:51,AskStatistics,"Need help on Final, Identifying which type of statistical analysis (Anovas, Chi Square) is used and why?",tkcw5,[deleted],,https://www.reddit.com/r/AskStatistics/comments/tkcw5/need_help_on_final_identifying_which_type_of/,0.0,2.0,"Hey so i am struggling to answer which of these tests  is either a one way ANOVA, a factorial ANOVA, a within ANOVA, a mixed within Anova, correlation, chi square. This teacher has not taught us anything and the class has really been the students working together, but now we need help. We also have to explain why but, help on just explaining what test does what would be a huge help. 


1. A psychologist wants to examine if there is a positive relationship between depression level and money spent per month on luxury items.


2. A psychologist wants to investigate if there is a significant difference between intelligent tests. He tested every subject on all of the different intelligence tests: Stanford Binet test, WASIV test and the Bo IQ test.


3. A psychologist wants to determine if there was a sex (male, female) difference in which candidate they would vote for. He polled people in the community and kept a tally of the sex of the participants and who they would vote for. In the end he had a summary table of how many of men and women would vote each candidate.


4. A psychologist examined the difference between the intelligence of rats and types of mazes. Each rat was separated by intelligence (smart, dull) and then each rat ran in all three types of maze (easy medium, hard).


5. A psychologist is interested in whether or not his plant formula is better than the others. He compared plant formulas (formula A,B,C, and water) on different types of plants (orchid, azalea, African violet, spider plant). He had four of each type of plant and treated each plant with only ONE type of plant formula. For example, orchid 1 plant received formula A, orchid plant 2 received formula B orchid plant 3 received formula C and orchid plant 4 received water etc. He recorded the plant growth over the next month. 



Please teach me more in one afternoon than this ""professor"" has taught us all year... ",en
1109805,2012-05-13 18:57:01,MachineLearning,ICML 2012 – Accepted papers,tl24u,hapagolucky,1278138338.0,https://www.reddit.com/r/MachineLearning/comments/tl24u/icml_2012_accepted_papers/,19.0,6.0,,en
1109806,2012-05-13 23:17:25,statistics,Do medical researchers use instrument variables?,tldna,[deleted],,https://www.reddit.com/r/statistics/comments/tldna/do_medical_researchers_use_instrument_variables/,6.0,10.0,"I am not a statistical expert by any means... but from what I understand using instrument variables does a pretty good job of identifying causality.  In the few medical studies I have read (and the many I have seen reported in the news) it seems like instrument variables are rarely used.  They seem to just try to control for as many things as they can (but when 1,000,000 factors are at play, controlling for 10 things isn't that effective).  Is this the case?  If so, why is this method rarely used?  

edit: the reason I ask is that I just read a news article that ""holding hands can make you live longer.""  I can't imagine a controlled experiment was ran so I am assuming the news article just distorted the study a bit.  ",en
1109807,2012-05-14 09:41:01,MachineLearning,Corrugated Packaging: What You Need to Know,tm4kd,gustavoduhamel,1323925005.0,https://www.reddit.com/r/MachineLearning/comments/tm4kd/corrugated_packaging_what_you_need_to_know/,1.0,1.0,,en
1109808,2012-05-14 18:08:08,statistics,Why you should be nice to statistics/computer science bloggers,tmizv,t_rex_tullis,1318255134.0,https://www.reddit.com/r/statistics/comments/tmizv/why_you_should_be_nice_to_statisticscomputer/,4.0,3.0,,en
1109809,2012-05-14 19:23:53,statistics,New Version of RStudio (v0.96) - one of THE leading IDE's for R [r-bloggers],tmmnr,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/tmmnr/new_version_of_rstudio_v096_one_of_the_leading/,31.0,6.0,,en
1109810,2012-05-14 19:24:00,rstats,New Version of RStudio (v0.96) - one of THE leading IDE's for R [r-bloggers],tmmnz,talgalili,1271226645.0,https://www.reddit.com/r/rstats/comments/tmmnz/new_version_of_rstudio_v096_one_of_the_leading/,1.0,0.0,,en
1109811,2012-05-14 21:45:11,statistics,Building a Normative Database,tmtvq,mannamedlear,1304587279.0,https://www.reddit.com/r/statistics/comments/tmtvq/building_a_normative_database/,3.0,0.0,"I am trying to build a normative database to benchmark survey research results and would like to know what some of the standard practices I should adhere to to build the database.  I understand standardization is key to control for any question item effects, but is there anything else I should consider.  Thanks for your help.",en
1109812,2012-05-14 21:55:28,AskStatistics,Ways to improve my model?,tmuf5,[deleted],,https://www.reddit.com/r/AskStatistics/comments/tmuf5/ways_to_improve_my_model/,1.0,0.0,"I've encountered an unexpected outcome in the creation of my multiple linear regression model.  Using data from descriptive stats, knowledge of similar processes from literature, and SLRs, I threw together the items that I thought should go into the model.  The results are beautiful: everything is significant.  However, what tests should I run to be sure that this is the best model?  I could compare models using AIC, but I don't know what to remove/add to this model.  I could see if Stata uses the same model using stepwise selection with extra variables added, but that doesn't feel quite right either.  

tl;dr How do I test the rigor of a model that looks pretty good from the outset?",en
1109813,2012-05-14 23:53:08,statistics,How do I build a model (a gam or glm) when I know the calibration data has measurement error (in R most likely),tn0uy,daledinkler,1199898909.0,https://www.reddit.com/r/statistics/comments/tn0uy/how_do_i_build_a_model_a_gam_or_glm_when_i_know/,7.0,7.0,"I have a generally good stats background, but it's patchy.  One thing that's always confounded me is how to build a model for a variable (say weekly temperature of my house as a function of sunny days) if the variable of interest (weekly temperature) is measured with some variability (it's the average of a number of values in this case, but it could be measured with a crummy thermometer in another).

So, basically I'm trying to build the model f(x) = y, but in the calibration I know that the 'known' y values have uncertainty.  Is there are straightforward way to deal with this in R?  I assume it'll blow my significance out of the water, but that's not what I'm interested in.

EDIT:  This isn't homework, it's something that's been bothering me for a while in my field, that most models built fail to include the uncertainty.  I assume the approach is probably going to be Bayesian, but I was hoping for a bit of guidance.",en
1109814,2012-05-15 06:58:37,statistics,Tests for exogeneity in Nonlinear Models,tnnl1,econometrician,1323865912.0,https://www.reddit.com/r/statistics/comments/tnnl1/tests_for_exogeneity_in_nonlinear_models/,5.0,3.0,"Hey, everyone, this is a quick statistical question, but it's related to econometrics (I'm posting here because /r/econometrics doesn't have a lot of active users). 

The Durbin-Wu-Hausman test is a typical test for exogeneity of a given independent variable for the typical OLS case. Using two stage least-squares/instrumental variables usually 'helps' solve issues of endogeneity. (The DWH test is pretty simple, run a regression of your possibly endogenous variable using some covariates, predict the residuals, run the regression you're actually interested in with the predicted residuals and if they have a statistically significant test statistic, you have endogeneity!)

I'm currently using a nonlinear model for a paper I'm working on and I'm not sure if I can apply this type of statistical test on with Maximum Likelihood (I'm using an Ordinal Probit model). I did the exact same test using the ordinal model as if it was OLS, but I'm uncertain if that actually seems appropriate (given the nonlinear nature of the likelihood function [and I know not everything caries over from OLS to MLE]). If this is actually okay to do, I reject exogeneity and have an interesting conclusion, but I was hoping someone might have some useful feedback on this. Any comments, citations, or words are much appreciated. Thanks!",en
1109815,2012-05-15 08:32:55,statistics,Want to create an InfoGraphic easily this seems to do it,tnrly,dexcel,1273344296.0,https://www.reddit.com/r/statistics/comments/tnrly/want_to_create_an_infographic_easily_this_seems/,3.0,1.0,,en
1109816,2012-05-15 10:44:04,artificial,How to make different things from Anillas de lata,tnwau,anillaslata,1330940983.0,https://www.reddit.com/r/artificial/comments/tnwau/how_to_make_different_things_from_anillas_de_lata/,1.0,0.0,,en
1109817,2012-05-15 11:51:08,statistics,What is the point of a T-Distribution?,tnyii,[deleted],,https://www.reddit.com/r/statistics/comments/tnyii/what_is_the_point_of_a_tdistribution/,1.0,0.0,"Dear r/statistics, can anybody help explain this to me?",en
1109818,2012-05-15 14:15:56,MachineLearning,Data analysis of GitHub timeline data,to1tf,kafka399,1335048442.0,https://www.reddit.com/r/MachineLearning/comments/to1tf/data_analysis_of_github_timeline_data/,22.0,4.0,,en
1109819,2012-05-15 18:10:57,statistics,Interpreting components,toaen,chirpychirp,1277776610.0,https://www.reddit.com/r/statistics/comments/toaen/interpreting_components/,2.0,8.0,"I'm using a PCA on measurements and using as a response variable. I can conceptually interpret the PC as a range from low to high values of X, but a member of my committee wants me to be able to interpret the component range numerically to show that the differences are biologically relevant. I have no idea how to do this. Any suggestions?",en
1109820,2012-05-15 19:20:06,statistics,Probability Problem help?,todom,[deleted],,https://www.reddit.com/r/statistics/comments/todom/probability_problem_help/,1.0,0.0,"I have a history test on thursday.
Part of the test involves the following: We have been given a list of 40 terms. On the test there will be 10 terms; we will have to define 6 of them. By define, I mean we have to write a decent sized paragraph about each one summarizing and analyzing its role in history. As any good American student, in stead of actually studying, I've been trying to figure out how many I should actually bother with ahead of time. However, I suck at probability (or whatever category this fits into, I apologize for the semantics). That's why I'm here.
To give an example of what I'm looking for, I figured out the same problem with the other part of the test. This part has 4 possible essay prompts we've been given in advance. I figure if I only study 3 of them, I have a 100% chance of one of them being on the test, but if I only study 2, I still have a 5 in 6 chance of getting at least one of them. Therefore, in the interest of laziness, and because I want to save some time in the next couple days, I'm only going to study 2, since it'll probably be fine.
Thanks in advance for your help :)",en
1109821,2012-05-15 21:05:43,artificial,"ask artificial : has anyone read this book ""Robots on your doorstep ( a book about thinking machines ) "" by 'Nels Winkless &amp; Iben Browning'.... ?
",toj9t,lani,,https://www.reddit.com/r/artificial/comments/toj9t/ask_artificial_has_anyone_read_this_book_robots/,0.0,3.0,I remember it to be extremely humorous and interesting but didn't fully understand the technique ( read it when I was starting undergrad ...) - can anyone tell me about the technique,en
1109822,2012-05-15 21:31:07,statistics,re-sampling anova question,tokna,SpaceWizard,1294385608.0,https://www.reddit.com/r/statistics/comments/tokna/resampling_anova_question/,5.0,3.0,"I have 1 within subjects factor (task) with three levels (SA, D, TO), 9 subjects, 200 repeated measures of response time per task level. I want to do a comparison of the three levels, but I also want to take into consideration the distribution of RTs for each subject. Usually people just use the mean of the RTs for a condition, but i noticed that some but not all of my subjects have non-normal RT distributions, so not all these means are created equal. After the three levels comparison, I'd like to do pair wise comparisons using similar method.  I assume this will be some combination of re-sampling, anova, and then ttests. Anyone know of any resources on this kind of situation, or can anyone explain how to do this?",en
1109823,2012-05-15 22:05:05,statistics,"It's been a while, and I don't quite remember what type of test to use for my research. Can anyone lend a hand?",tomju,uberpro,1257818523.0,https://www.reddit.com/r/statistics/comments/tomju/its_been_a_while_and_i_dont_quite_remember_what/,6.0,10.0,"I'm doing just a minor experiment in linguistics, and while I've gotten all the data, I'm not sure which statistical test I should use on my data.

I'm finding it really hard to describe what I'm doing, but here goes:  I have two categories: my control group and the group I changed an independent variable on.  In each of these categories, there are 8 subgroups.  Each of these subgroups has a corresponding subgroup in the other category.  I want to look at how the averages of a subgroup compare to the averages of the corresponding subgroup in the other category and then see if the averages in non-control category are greater than those in the control category.  

I was thinking of a paired t-test, but what I would be pairing would be averages.  That seems wrong because the subgroups don't have the same number of data points as their corresponding subgroups which I'm comparing them to.

Can anybody help?  Did I explain that in any comprehensible way?  I don't want to present research with fundamentally flawed results.  I appreciate any help or advice you guys have.  Thanks!",en
1109824,2012-05-15 23:27:13,MachineLearning,Will 2015 be the Beginning of the End for SAS and SPSS?,tordj,talgalili,1271226645.0,https://www.reddit.com/r/MachineLearning/comments/tordj/will_2015_be_the_beginning_of_the_end_for_sas_and/,2.0,0.0,,en
1109825,2012-05-15 23:28:07,statistics,Will 2015 be the Beginning of the End for SAS and SPSS? [r-bloggers],torf8,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/torf8/will_2015_be_the_beginning_of_the_end_for_sas_and/,21.0,11.0,,en
1109826,2012-05-16 05:27:00,MachineLearning,Civic Data Challenge Announces New Prize: Your own Kaggle competition,tpau8,willis77,1188267725.0,https://www.reddit.com/r/MachineLearning/comments/tpau8/civic_data_challenge_announces_new_prize_your_own/,5.0,0.0,,en
1109827,2012-05-16 06:26:50,MachineLearning,SIAM Data Mining 2012 Conference Summary,tpdwz,LADataJunkie,1327342831.0,https://www.reddit.com/r/MachineLearning/comments/tpdwz/siam_data_mining_2012_conference_summary/,4.0,0.0,,en
1109828,2012-05-16 08:34:35,statistics,Running an ANCOVA with SPSS and two IVs/DVs,tpjx6,Devianex,1324185985.0,https://www.reddit.com/r/statistics/comments/tpjx6/running_an_ancova_with_spss_and_two_ivsdvs/,2.0,3.0,"I am trying to analyze the data from a research project I've been working on for the entire semester regarding mortality salience and progressivism, with a side interest in mortality salience and stance on legalization of marijuana.

There are two conditions: Non-primed and primed

Two DVs: Progressive Mean (calculated from a progressivism survey I gave my participants) and Marijuana Legalization score (calculated from one of the questions on the survey)

Two IVs (I think): Condition (since I assigned them to either prime or non-primed) and political beliefs (broken down into very conservative, conservative, slightly conservative, moderate, slightly liberal, liberal, and very liberal) I'm not actually sure political beliefs are an IV, since I don't have any control over them but they do effect the DV.

I want to run an ANCOVA on the data in such a way that I can wash out any effect that their political beliefs have on their progressive mean, so that the only thing that should be affecting the mean is whether or not they were in the primed condition. I want to do the same for my Marijuana Legalization data.

All I know is I need to run an ANCOVA since my professor said that specifically, but I am at a loss as to what post hoc test to run and how to separate the data for each condition, if I even need to do that. Any help would be appreciated, I've been trying to self educate all day to little success.

For what it's worth, I've taken introductory statistics but I'll be damned if I remember anything worthwhile. The more I look at all these numbers, the less they mean to me.

Thanks in advance!",en
1109829,2012-05-16 15:23:24,statistics,Argentina - the lies.  Being a conscientious statistician takes courage.,tpuew,prhodes,1328233759.0,https://www.reddit.com/r/statistics/comments/tpuew/argentina_the_lies_being_a_conscientious/,32.0,2.0,,en
1109830,2012-05-16 16:42:15,statistics,What can you say about the eigenvalues of a RANDOM symmetric matrix? Turns out you can say a lot!,tpx9w,RickWicklin,1327333524.0,https://www.reddit.com/r/statistics/comments/tpx9w/what_can_you_say_about_the_eigenvalues_of_a/,14.0,1.0,,en
1109831,2012-05-16 17:22:22,statistics,Average Percent Change or Percent Change of the Average? so lost,tpz10,[deleted],,https://www.reddit.com/r/statistics/comments/tpz10/average_percent_change_or_percent_change_of_the/,1.0,1.0,"Four companies provide the following revenue figures for 2011 &amp; 2010:

Company A. 2012 Rev: 160 2011 Rev: 95

Company B. 2012 Rev: 121 2011 Rev: 117

Company C. 2012 Rev: 320 2011 Rev: 280

Company D. 2012 Rev: 395 2011 Rev: 327

We are calculating the ""industry growth"" two different ways and am not sure which is correct.

First method is to take the totals from each year and find the % change: 2011 total= 996 &amp; 2010 total= 819

...so (996-819)/819= 21.61%

Second method we are trying is take the percent change of each company year over year and then the average of those figures:

Company A % Change= 68%

Company B % Change= 3%

Company C % Change= 14%

Company D % Change= 21%

So.. (68%+3%+14%+21%)/4= 26.73%

....Which one of these would be correct to display if trying to calculate the true industry growth? The 21.61% or 26.73%

This may be a dumb questions, but I am completely lost between the two and could use some help/explanation. THANK YOU!
",en
1109832,2012-05-16 19:32:41,rstats,Information on Creating an R package.,tq5f8,thderrick,1326655241.0,https://www.reddit.com/r/rstats/comments/tq5f8/information_on_creating_an_r_package/,7.0,2.0,Does anyone have information on creating an R package? Would it be possible to copy functions from other packages and combine them into a single package?  I have had trouble in the past because one package will overwrite functions that I need from the first package.,en
1109833,2012-05-16 20:13:20,statistics,Need help with some basic confidence interval and level of certainty calculations,tq7g2,thedaveoflife,1300505237.0,https://www.reddit.com/r/statistics/comments/tq7g2/need_help_with_some_basic_confidence_interval_and/,3.0,3.0,"Hello!

I am helping out on a project that is a little bit outside my area of expertise and am hoping somebody can help. 

A non-profit I am volunteering for has developed a composite indicator for various global health issues by combining various factors. We have the indicators calculated, as well as 95% confidence intervals for each, however now they want me to calculate the probability that each indicator actually is higher than others given the uncertainty and given CIs. I am at a loss on how to do this and am hoping someone can help.

As an example two of our indicators are as follows (again, CIs are 95%):

* Occupational Safety: 120.88 with a Confidence Interval of -10.47 to +10.08
* HIV/AIDS: 110.24  with a CI of -11.11 to +16.67

What is the probability or level of certainty that Occupational Safety is actually higher the HIV/AIDS? I have 34 indicators I need to do this for also, so if you could explain the methodology that would be most helpful :)

Please let me know if I need to provide any more info on this and thanks in advance for any and all help you can give!",en
1109834,2012-05-16 21:32:34,statistics,CNN Article on solving 'bus-bunching' using Markov Chain,tqbl1,djent_illini,1312414426.0,https://www.reddit.com/r/statistics/comments/tqbl1/cnn_article_on_solving_busbunching_using_markov/,6.0,0.0,,en
1109835,2012-05-16 23:46:03,statistics,Can someone give me an endorsement for the arXiv statistics so I can post my manuscript?,tqira,weaselword,1225453776.0,https://www.reddit.com/r/statistics/comments/tqira/can_someone_give_me_an_endorsement_for_the_arxiv/,11.0,1.0,"I wish to put up my manuscript on arXiv, in the stat.ME (statistics-Methodology) section. Since this is the first time I am putting up a manuscript in the statistics section, the arXiv is asking for someone to endorse me. Anyone who has put up two manuscripts within the last five years can endorse.

I would appreciate an endorsement; my department doesn't have a statistician who has posted on the arXiv before. If you'd like to see the manuscript, I'd be happy to email it to you.

My endorsement code is: NRDQID
",en
1109836,2012-05-17 00:19:04,rstats,"I keep getting this error, would appreciate some help.",tqki6,[deleted],,https://www.reddit.com/r/rstats/comments/tqki6/i_keep_getting_this_error_would_appreciate_some/,1.0,0.0,"I'm new to using this, and the guide lacks a LOT of information. 

I just imported my data (had to figure out on my own, so I may have done something wrong?) and now I want to make a boxplot but I keep getting this error. Here's a screenshot: http://imgur.com/kANM5

Can someone tell me what I did wrong? 

",en
1109837,2012-05-17 00:20:59,statistics,math help. can you solve?,tqkmh,[deleted],,https://www.reddit.com/r/statistics/comments/tqkmh/math_help_can_you_solve/,1.0,1.0,,en
1109838,2012-05-17 01:11:50,rstats,"""Make a table of summary statistics for the three groups.""",tqnbs,[deleted],,https://www.reddit.com/r/rstats/comments/tqnbs/make_a_table_of_summary_statistics_for_the_three/,0.0,3.0,"How do I do this on R?

I've attached a data set where there are three groups and the length of their femurs are measured. 

Can someone explain to me what exactly ""summary statistics"" is? Is it mean, median, variance etc?

Secondly how do I calculate these things individually for the groups? I mean, if I just type in &gt; mean(data) it gives me one number, whereas I assume I need 3? 

Appreciate the help!",en
1109839,2012-05-17 06:05:41,statistics,Reporting post hoc results for an RM-ANOVA,tr2r5,BearBeer,1311184240.0,https://www.reddit.com/r/statistics/comments/tr2r5/reporting_post_hoc_results_for_an_rmanova/,6.0,14.0,"Thanks /r/statistics for reading this. I have a hopefully very simple question about reporting my RM-ANOVA results.

This experiment is a 3X2 design. The 2-level IV serves as my within-subjects factor (repeated measure), and we can call the levels ""A"" and ""B"". My 3-level IV is the between-subjects factor, and we can just call the levels 1, 2, and 3. 

In my analysis, the between-subjects factor has a significant main effect. A Tukey's HSD post hoc revealed that level 1 is different from level 2. However, this difference combines both A and B levels of the within-subject factor. What analysis can I perform that will allow me to access post hoc results that can compare my between-subject means on both levels of the within-subject IV - *separately*? 

Basically I want to compare means on level A and level B by themselves. Can I perform two separate one-way ANOVAs for each of the levels of the within-subject IV and then use those post hoc results? 

Thanks, again.

EDIT: I forgot to mention, I'm using SPSS. I also have access to JMP in my lab.
EDIT EDIT: Typos.",en
1109840,2012-05-17 09:23:15,statistics,Progress bar in R for a single line operation?,traoo,YaoPau,,https://www.reddit.com/r/statistics/comments/traoo/progress_bar_in_r_for_a_single_line_operation/,4.0,2.0,"I'm running a backward stepwise regression on a massive dataset in R ... it's been running for about a half hour and I'd like to know in the future how long a process will take.

I've read through some ideas for progress bars, but most of them involve sticking the code within a looped function.  How can I estimate the runtime of just running STEPAIC on a single line?",en
1109841,2012-05-17 14:12:12,artificial,"Official Google Blog: Introducing the Knowledge Graph: things, not strings",trhkm,trocar,1142644527.0,https://www.reddit.com/r/artificial/comments/trhkm/official_google_blog_introducing_the_knowledge/,13.0,2.0,,en
1109842,2012-05-17 17:30:50,rstats,"Having troubles using ""sapply"". What am I doing wrong?",trod5,[deleted],,https://www.reddit.com/r/rstats/comments/trod5/having_troubles_using_sapply_what_am_i_doing_wrong/,1.0,0.0,"Hello again, I'll just copy everything in here:

&gt; data = read.table(""data.txt"",header=TRUE)

&gt; attach(data)

&gt; sapply(data, mean, na.rm=TRUE)


&gt;
     pop    femur 
      NA 41.91389 
Warning message:
In mean.default(X[[1L]], ...) :
  argument is not numeric or logical: returning NA


Why do I keep getting this error?! Have no clue what went wrong?",en
1109843,2012-05-17 17:45:16,MachineLearning,Open Source SPAMS (SPArse Modeling Software) now with Python and R,trp1f,[deleted],,https://www.reddit.com/r/MachineLearning/comments/trp1f/open_source_spams_sparse_modeling_software_now/,9.0,2.0,,en
1109844,2012-05-17 18:22:34,datasets,Diving into OCLC's  Virtual International Authority File datadump,trqt1,mhermans,1169219262.0,https://www.reddit.com/r/datasets/comments/trqt1/diving_into_oclcs_virtual_international_authority/,6.0,0.0,,en
1109845,2012-05-17 18:23:29,datascience,Diving into OCLC's Virtual International Authority File datadump,trqum,mhermans,1169219262.0,https://www.reddit.com/r/datascience/comments/trqum/diving_into_oclcs_virtual_international_authority/,3.0,0.0,,en
1109846,2012-05-17 18:44:33,rstats,"New version of RStudio, focussing on workflow and tools for authoring, reproducible research, and web publishing",trrx9,mhermans,1169219262.0,https://www.reddit.com/r/rstats/comments/trrx9/new_version_of_rstudio_focussing_on_workflow_and/,13.0,2.0,,en
1109847,2012-05-17 21:17:37,MachineLearning,"Not everyone understands us, but the machines sure will...",trzt6,jamintime,1331740110.0,https://www.reddit.com/r/MachineLearning/comments/trzt6/not_everyone_understands_us_but_the_machines_sure/,0.0,0.0,,en
1109848,2012-05-17 22:11:47,statistics,Subsituting p-values with Odds Ratios? ,ts2mt,Pooma__,1283789912.0,https://www.reddit.com/r/statistics/comments/ts2mt/subsituting_pvalues_with_odds_ratios/,5.0,31.0,"I've heard that you are supposed to be able to use odds ratio instead of tests that depends on p-values. I dont really understand how, since the odds dont say anything about significance. 

Im sorry I cant provide any more context, but i dont have a straight idea my self how to use this. Im thinking perhaps when comparing exposure and outcome in a case control study. I see how you can calculate the odds, an CI for the odds, but after some googleing i read that you are not supposed to draw conclusions about significace from the CI.

Can any one help my get this straight? Can you substitute p-values with odds ratios? What are the advantages?",en
1109849,2012-05-18 02:28:21,statistics,A guide to getting started with R Markdown for reproducible analysis using new features in knitr and RStudio 0.96.,tsfod,mycatharsis,1294046118.0,https://www.reddit.com/r/statistics/comments/tsfod/a_guide_to_getting_started_with_r_markdown_for/,11.0,0.0,,en
1109850,2012-05-18 03:21:31,statistics,Evaluating Mechanical Models ,tsi4l,chaoticneutral,1258587986.0,https://www.reddit.com/r/statistics/comments/tsi4l/evaluating_mechanical_models/,1.0,1.0,"Hi /r/stats

My girlfriend is a mechanical engineer and she trying to model the behavior of a mechanical joint. She measured the output of this joint over a range of inputs. She then designed mechanical models in a program to describe what she thinks is happening. She then runs simulations of these models and got predicted outputs which she can compare with the actuals experimental outs.

She has actual experimental output data and 4 different mechanical models. She wants to evaluate the error between the experimental data and the predicted data to determine which model best fits her experimental data.

We were thinking she could compare Mean Square Errors, with the smallest being the best. But that seems very primitive. Do you all have any recommendations?

Any guidance via excel/matlab would be helpful. Imagine two columns for each model, actuals in one and predicted values in the other.

Thanks!

",en
1109851,2012-05-18 05:33:24,statistics,Data Machines: Decomposing North Carolina Amendment 1 with R and Tableau (part 1),tsoi1,rootmarshfield,1282499510.0,https://www.reddit.com/r/statistics/comments/tsoi1/data_machines_decomposing_north_carolina/,6.0,1.0,,en
1109852,2012-05-18 07:56:31,statistics,Probability of a certain mean,tsupf,wkinahan,1323225757.0,https://www.reddit.com/r/statistics/comments/tsupf/probability_of_a_certain_mean/,3.0,34.0,"A friend of mine was wondering if he was being cheated in a series of dice rolls in DND.  The supposed cheater had rolled a 20-sided dice 21 times, and had an average roll of 15.52.  

Here are the exact rolls.

19, 20, 17, 15, 15, 20, 20, 18, 14, 7, 11, 17, 15, 16, 17, 16, 8, 20, 15, 12, 14


What are the exact odds that he would have had that mean?

At first glance those rolls are insanely high, but what is the probability he would just roll that well, with a standard 20 sided dice?",en
1109853,2012-05-18 16:39:06,statistics,Animations in R,tt8k2,[deleted],,https://www.reddit.com/r/statistics/comments/tt8k2/animations_in_r/,1.0,0.0,,en
1109854,2012-05-18 17:24:18,rstats,"""object 'prze' not found""",ttag9,[deleted],,https://www.reddit.com/r/rstats/comments/ttag9/object_prze_not_found/,2.0,6.0,"Am getting this error. How do I fix it?

&gt; przeS &lt;- summaryBy(pop~femur, data=prze, FUN=c(mean,sd))
Error in summaryBy(pop ~ femur, data = prze, FUN = c(mean, sd)) : 
  object 'prze' not found
",en
1109855,2012-05-18 19:46:16,statistics,"Hello, I need help figuring out how to calculate statistics in a 2AFC (visual) experiment.",ttgyp,paronsaft,1329925841.0,https://www.reddit.com/r/statistics/comments/ttgyp/hello_i_need_help_figuring_out_how_to_calculate/,3.0,6.0,"EDIT: I offer up to $50 for full help in getting me understand, including how to use SPSS with this problem I have. I understand that this isn't a huge sum, and it might take some work, but it's my way of saying thank you because I really need this.

Basically it's regarding a visual perception experiment, it was a temporal two answer forced choice experiment, but the subject had to get ""correct"" 3 times in a row, then the contrast would get smaller by a percentage, if wrong, the contrast got bigger by a percentage. This was done until 6 reversals, the contrast values at the time of reversals were average to get the contrast threshold...I have those threshold values and I have the answers of the subjects. I now need to get the ""standard error"". I have very limited knowledge in statistics and would be very very thankful for any help. Thank You",en
1109856,2012-05-18 22:16:43,MachineLearning,Another use of Count-Min Sketch: Particle Sketches,tto95,[deleted],,https://www.reddit.com/r/MachineLearning/comments/tto95/another_use_of_countmin_sketch_particle_sketches/,8.0,0.0,,en
1109857,2012-05-19 02:01:03,statistics,Benford's Law applies to ATM PINs?,ttywd,sintaur,1336618120.0,https://www.reddit.com/r/statistics/comments/ttywd/benfords_law_applies_to_atm_pins/,72.0,20.0,,en
1109858,2012-05-19 09:47:59,statistics,Request for code to export multiple MS Access tables to a stats package,tug47,tiii,,https://www.reddit.com/r/statistics/comments/tug47/request_for_code_to_export_multiple_ms_access/,2.0,10.0,"I've been asked to produce statistics for a client but they have provided me with an MS Access database that contains &gt;50 individual tables. I need to import them to a stats package (I have access to SAS and SPSS) but I don't want to manually import each table individually. I have very little knowledge of access.
In case you want to tell me there's a beautiful solution in R, I'm in a corporate environment and can't install anything else but I'd be interested to read it anyway.
Suggestions of code or even just a data management method?",en
1109859,2012-05-19 16:47:41,rstats,t-test comparing three groups?,tuorq,[deleted],,https://www.reddit.com/r/rstats/comments/tuorq/ttest_comparing_three_groups/,6.0,14.0,"Need to compare the avg. heights of three different groups, and I'm thinking t-test but not sure how you do that on R when you have three different groups?

Appreciate the help!",en
1109860,2012-05-19 17:54:45,statistics,When are variables too correlated?,tuqqj,robbyroo,1292911500.0,https://www.reddit.com/r/statistics/comments/tuqqj/when_are_variables_too_correlated/,6.0,17.0,"I have 130 variables and am looking to cut some from my model.  Looking at a correlation matrix, what is a good rule of thumb for telling when a correlation is so strong that two variables are really just duplicating eachother?",en
1109861,2012-05-20 00:08:36,MachineLearning,"GraphLab Workshop (Monday, July 9, 2012, San Francisco, CA). GraphLab is a competitor to Map/Reduce-Hadoop",tv5f8,[deleted],,https://www.reddit.com/r/MachineLearning/comments/tv5f8/graphlab_workshop_monday_july_9_2012_san/,22.0,3.0,,en
1109862,2012-05-20 04:44:47,MachineLearning,Any movielens equivalent dataset in portuguese (brazilian portuguese) ?,tvgac,tunabr,1304305690.0,https://www.reddit.com/r/MachineLearning/comments/tvgac/any_movielens_equivalent_dataset_in_portuguese/,1.0,0.0,"I've been trying to create a corpus to reproduce sentiment based analysis on brazilian portuguese text, but the hardest part is to find out a corpus like movielens (review + score). Any suggestions ? The corpora that I've been looking for (like floresta) is more geared towards linguistics.",en
1109863,2012-05-20 07:31:52,statistics,How to measure correlation between financial time series?,tvmf3,[deleted],,https://www.reddit.com/r/statistics/comments/tvmf3/how_to_measure_correlation_between_financial_time/,17.0,8.0,"I'm a programmer with easy access to years of historical daily stock data (open, high, low, close, volume) for thousands of stocks.  I'd like to explore the data for potentially statistically significant elements.  For example, does the price to earnings ratio have a meaningful impact on price, and so on.  I plan to use MINE, the Java-based ""maximal information-based nonparametric exploration"" tool from http://www.exploredata.net to do this.

What steps should I take to prepare the data for analysis?  I believe that I need to filter the data by removing correlated stocks, so as to avoid false positives.  Is that correct?  And if so, how is that measured?  What other steps should I take?

I've studied through Calc II, but it's been many years.  I've also never formally studied statistics, though I'm pretty good with numbers.  I'd like a relatively simple explanation of what I want, and if there's a programming library out there that can already produce the answers I need, I'm happy to understand just how to analyze the results without necessarily knowing how they were calculated.",en
1109864,2012-05-20 22:21:35,statistics,Excel and Statistics,twbq8,[deleted],,https://www.reddit.com/r/statistics/comments/twbq8/excel_and_statistics/,0.0,0.0,"I posted [this request](http://www.reddit.com/r/cheatatmathhomework/comments/tv21g/excel_and_statistics/) for help on Reddit but go no responses in this subreddit. Can someone give me some insight into this or point me to the right resources? Maybe some Youtube videos or something? Thanks.

I realize I need to use data analysis tools for this exercise, but aside from that I feel a bit lost. Thanks.
",en
1109865,2012-05-21 04:38:52,MachineLearning,Markov Chain Monte Carlo and the Eurovision Song Contest,twt9g,mewo2,1337556930.0,https://www.reddit.com/r/MachineLearning/comments/twt9g/markov_chain_monte_carlo_and_the_eurovision_song/,62.0,5.0,,en
1109866,2012-05-21 08:44:21,statistics,Dear r/statistics which is the best single book to learn regression analysis?,tx4m9,sortizo,1288834170.0,https://www.reddit.com/r/statistics/comments/tx4m9/dear_rstatistics_which_is_the_best_single_book_to/,10.0,15.0,I was thinking in Montgomery's Introduction to Linear Regression Analysis... What do you guys think?,en
1109867,2012-05-21 13:51:00,statistics,How to Use the Excel Descriptive Statistics tool,txcdd,eagoodall,1327008279.0,https://www.reddit.com/r/statistics/comments/txcdd/how_to_use_the_excel_descriptive_statistics_tool/,0.0,0.0,,en
1109868,2012-05-21 15:40:46,statistics,Help finding Social Network Data for my experiments.,txf92,neelpulse,1329502825.0,https://www.reddit.com/r/statistics/comments/txf92/help_finding_social_network_data_for_my/,2.0,10.0,"I am trying to find a large data set that will contain both:

* A real friendship or follower graph of some users 
* A list of activities performed by the users with the time stamp for each user(e.g. Rating a movie, liking a status/site, upvoting a post in reddit etc).

Please suggest me some sources where I can find these kind of data. Thank you in advance. :)",en
1109869,2012-05-21 17:13:46,MachineLearning,Reference Pages in Machine Learning and Related Fields.,txijd,[deleted],,https://www.reddit.com/r/MachineLearning/comments/txijd/reference_pages_in_machine_learning_and_related/,10.0,1.0,"I am trying to compile a list of pages dedicated to specific topics within Machine Learning and related fields. Below is a list of pages curated by specialists that are not focused on one paper or one research group but rather on techniques and how these techniques have come about and how they are being used by different groups. I am sure I am missing some and look forward to any addition.

* [Superlinear Indexes](http://www.superlinearindexes.org/home)
* [Count-Min Sketches and Applications](https://sites.google.com/site/countminsketch/home)
* [Compressive Sensing: The Big Picture](https://sites.google.com/site/igorcarron2/cs)
* [Advanced Matrix Factorization Jungle](https://sites.google.com/site/igorcarron2/matrixfactorizations)
* [The LASSO page](http://www-stat.stanford.edu/~tibs/lasso.html)
* [Recommender Systems: wiki](http://recsyswiki.com/wiki/Main_Page)
* [The sparse- and low-rank solver wiki]
(http://www.ugcs.caltech.edu/~srbecker/wiki/Main_Page)",en
1109870,2012-05-21 18:22:03,MachineLearning,Papers on negative transfer of learning in ML?,txlhw,[deleted],,https://www.reddit.com/r/MachineLearning/comments/txlhw/papers_on_negative_transfer_of_learning_in_ml/,1.0,0.0,"As my title suggests, I'd like to get insights to negative transfer of learning studies. There are plenty of positive learning reports, but so far I couldn't really find any comprehensive/descriptive paper on ToL-failure.",en
1109871,2012-05-21 19:11:46,MachineLearning,Can someone explain the Natural Actor Critic algorithm to me in simpler terms?,txnwi,Ruzihm,1296056596.0,https://www.reddit.com/r/MachineLearning/comments/txnwi/can_someone_explain_the_natural_actor_critic/,3.0,3.0,"Paper here: http://homepages.inf.ed.ac.uk/svijayak/publications/peters-ECML2005.pdf

I think I would have an easier time understanding the jargon and math involved if I had a high-level explanation of the process. Can anyone help me out with this? :)",en
1109872,2012-05-21 19:23:08,MachineLearning,Is There Big Money in Big Data? - Technology Review,txofc,postliterate,1315331842.0,https://www.reddit.com/r/MachineLearning/comments/txofc/is_there_big_money_in_big_data_technology_review/,21.0,36.0,,en
1109873,2012-05-21 22:05:26,statistics,"Statistical Illiteracy: American Community Survey, used to distribute gov't funding for public health and education, is “not a scientific survey. It’s a random survey"", according to Daniel Webster, R-FL. ",txwrh,prionattack,1257312771.0,https://www.reddit.com/r/statistics/comments/txwrh/statistical_illiteracy_american_community_survey/,50.0,10.0,,en
1109874,2012-05-21 23:32:49,statistics,The Simple Gibbs example in Julia [r-bloggers],ty1uh,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/ty1uh/the_simple_gibbs_example_in_julia_rbloggers/,3.0,0.0,,en
1109875,2012-05-22 03:47:34,statistics,"A way to generate heatmaps from raw x,y data in excel or online?",tyfup,HPDerpcraft,1307658689.0,https://www.reddit.com/r/statistics/comments/tyfup/a_way_to_generate_heatmaps_from_raw_xy_data_in/,5.0,6.0,"Hi all,

Thanks for all your help in the past. I am currently looking for a way to represent coordinate data (from a circle) as a heatmap. I have multiple ""trials"" from an experiment, and I want to overlay the coordinate data (from sampling points) to create a composite map with a color-change as the 3rd dimension. The problem is that a) I don't know how to do this and b) the data are not binned. Binning would be tiresome, so if you have a handy simple script that would also be awesome!

Thanks!",en
1109876,2012-05-22 05:03:11,statistics,How to improve the flawed gender wage-gap statistic,tyjyf,jawns317,1294848224.0,https://www.reddit.com/r/statistics/comments/tyjyf/how_to_improve_the_flawed_gender_wagegap_statistic/,13.0,19.0,,en
1109877,2012-05-22 06:24:51,analytics,Anybody working in analytics up for an AMA/AMAA?,tyocg,Benthetraveler,,https://www.reddit.com/r/analytics/comments/tyocg/anybody_working_in_analytics_up_for_an_amaamaa/,1.0,0.0,"Just would like to hear some insights about the job, the career and everything around it...
Links to where analytics-people gather are obviously welcome as well.",en
1109878,2012-05-22 10:32:46,data,mailing list info,tyy0e,skykuwwuc99,1312280104.0,https://www.reddit.com/r/data/comments/tyy0e/mailing_list_info/,1.0,0.0,,en
1109879,2012-05-22 12:58:46,statistics,Ask r/statistics: Does anyone know if a detailed documentation on SPSS Data format is publicly available?,tz1ho,Synes_Godt_Om,1336751767.0,https://www.reddit.com/r/statistics/comments/tz1ho/ask_rstatistics_does_anyone_know_if_a_detailed/,9.0,6.0,,en
1109880,2012-05-22 19:19:06,statistics,"""...especially since in the end this is not a scientific survey. It’s a random survey.""",tzfes,wainstead,1134277200.0,https://www.reddit.com/r/statistics/comments/tzfes/especially_since_in_the_end_this_is_not_a/,19.0,10.0,,en
1109881,2012-05-22 19:28:00,MachineLearning,From words to concepts and back again...,tzfvc,RevBooyah,1297505777.0,https://www.reddit.com/r/MachineLearning/comments/tzfvc/from_words_to_concepts_and_back_again/,12.0,2.0,,en
1109882,2012-05-22 20:18:57,statistics,AP Stats Project,tziir,JustinHstats,1337707063.0,https://www.reddit.com/r/statistics/comments/tziir/ap_stats_project/,0.0,12.0,"We are doing a Stats project to see if people can truely pick random numbers. To help us test this, pick a random number between 1-10 and post it below. Please post your number before reading others posts.

After a sufficient amount of data we will post the results.

",en
1109883,2012-05-23 01:07:47,datasets,"5,700 tourism agencies blacklisted by the Indian Railway Catering and Tourism Corporation",tzysr,[deleted],,https://www.reddit.com/r/datasets/comments/tzysr/5700_tourism_agencies_blacklisted_by_the_indian/,1.0,0.0,,en
1109884,2012-05-23 02:30:23,MachineLearning,Opinion: Depth sensors and machine learning are under-utilized scientific tools (written for a class),u0339,mgsloan,1164226141.0,https://www.reddit.com/r/MachineLearning/comments/u0339/opinion_depth_sensors_and_machine_learning_are/,7.0,10.0,,en
1109885,2012-05-23 04:59:03,statistics,Does coffee increase or reduce mortality? - YouTube,u0b3a,picu,1274975290.0,https://www.reddit.com/r/statistics/comments/u0b3a/does_coffee_increase_or_reduce_mortality_youtube/,9.0,5.0,,en
1109886,2012-05-23 11:26:50,MachineLearning,An update on Eurovision,u0r0c,amair,,https://www.reddit.com/r/MachineLearning/comments/u0r0c/an_update_on_eurovision/,39.0,1.0,,en
1109887,2012-05-23 12:29:54,MachineLearning,Can better data keep students from dropping out of college?,u0s4c,[deleted],,https://www.reddit.com/r/MachineLearning/comments/u0s4c/can_better_data_keep_students_from_dropping_out/,1.0,0.0,,en
1109888,2012-05-23 12:57:11,statistics,Rob Hyndman's new forecasting book (2/3 finished) presently free online,u0spy,efrique,1219734906.0,https://www.reddit.com/r/statistics/comments/u0spy/rob_hyndmans_new_forecasting_book_23_finished/,20.0,2.0,,en
1109889,2012-05-23 16:56:55,computervision,Leap - Close-range Kinect-like device,u0zow,colincsl,1210302386.0,https://www.reddit.com/r/computervision/comments/u0zow/leap_closerange_kinectlike_device/,9.0,4.0,,en
1109890,2012-05-23 18:03:44,rstats,Animations in R,u12p6,treedog,1158890463.0,https://www.reddit.com/r/rstats/comments/u12p6/animations_in_r/,14.0,0.0,,en
1109891,2012-05-23 18:25:32,AskStatistics,Factorial split-plot design help,u13rg,DigestingGandhi,1282917929.0,https://www.reddit.com/r/AskStatistics/comments/u13rg/factorial_splitplot_design_help/,2.0,1.0,"I have a pretty basic question on creating an anova table for a 3x3 factorial split-plot design. Here are the specifics:

I am growing 4 different species of wetland plants in troughs. I have 18 troughs. Each trough will have all 4 species present. I will have 3 levels of nutrient addition and 3 levels of clipping of the plant. My response variable will be measuring methane emission. So, I will have 9 treatment combinations (two troughs - replicates - of each combination of nutrient addition and clipping). 

Source                          df
A(Nutrients)               A-1 = 2
B(Clipping)                 B-1 = 2
AB                  (A-1)(B-1) = 4
Whole Plot error               = 8

First question - Is this correct for the whole plot?
Second question - How do I set up the anova table for the split plot portion? My split plot is variety (species of plant). 

If anyone knows how, and is willing, to walk me through modeling this in R, I would gladly compensate you for your time. 

edit: I forgot to add that I would also like to measure methane emission more than once, further complicating this as a repeated measures design. Any further advice? ",en
1109892,2012-05-23 20:10:19,statistics,A new (free) forecasting (with R) textbook,u195q,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/u195q/a_new_free_forecasting_with_r_textbook/,15.0,0.0,,en
1109893,2012-05-23 20:35:12,statistics,How to speedup your R code.,u1ahc,RA_Fisher,1299707119.0,https://www.reddit.com/r/statistics/comments/u1ahc/how_to_speedup_your_r_code/,4.0,0.0,,en
1109894,2012-05-23 21:27:56,statistics,R employment listings (some aren't on the R-jobs listserv),u1dai,feelphree,1155917410.0,https://www.reddit.com/r/statistics/comments/u1dai/r_employment_listings_some_arent_on_the_rjobs/,4.0,2.0,,en
1109895,2012-05-23 22:44:07,statistics,first order markov chain question.. ,u1hhr,enfieldacademy,1302474908.0,https://www.reddit.com/r/statistics/comments/u1hhr/first_order_markov_chain_question/,1.0,0.0,"so a first order markov chain can be represented by a transition or stochastic) matrix as far as I know.. 

what i want to know if how you figure out the frequency of the possible states from the probabilities in the transition matrix..

what i think wikipedia is saying is that the frequencies are one of the column vectors of the matrix which is the limit of multiplying the transition (or stochastic) matrices together.. 

also is it true that the frequencies of the possible states are the elements of the eigenvector of the transition matrix which has an eigenvalue of 1?

if someone can clarify these questions for me i would really appreciate that. thanks :). ",en
1109896,2012-05-23 22:48:15,statistics,"I have a small dataset to analyze. Instead of asking a question then looking for the answer, I want to get all the possible answers and work back to the right questions",u1hq9,cheesesteak22,1330187542.0,https://www.reddit.com/r/statistics/comments/u1hq9/i_have_a_small_dataset_to_analyze_instead_of/,0.0,12.0,"Pic of the dataset: http://i.imgur.com/I3C3W.png

I'm sure this has been done before in some way, but I can't find any examples online. Basically I want to run brute-force style statistics on every possible combination of variables within this dataset. It's not too big, but the data is a mixed bag of continuous and categorical values.
I realize that the small sample sizes may not be significant, and that I will get thousands of non-sensical and useless combinations, but that is OK with me.

My thinking is that if I have a program run all statistical tests that could work based on the types of variables I have in the dataset. This may be easier than manually selecting a test for each combination of columns (and I don't mind it returning an error as long as one of the other calculations works). 

My main question would then be, how do I tell the computer to go through every possible combination?

I have excel and can get R or SPS/SPSS if need be. I feel like this must have been done before in some way (maybe clinical trials?) and that I'm just naive to stats/biostats methods. 

Any advice is appreciated!",en
1109897,2012-05-23 23:51:59,statistics,Forecasting: principles and practice - An online textbook by Rob J Hyndman and George Athanasopoulos,u1lej,statguy,1271026910.0,https://www.reddit.com/r/statistics/comments/u1lej/forecasting_principles_and_practice_an_online/,8.0,3.0,,en
1109898,2012-05-24 05:39:57,statistics,Aww crap....What now? (OLS),u244k,AWKWARD_HANDS_GUY,,https://www.reddit.com/r/statistics/comments/u244k/aww_crapwhat_now_ols/,23.0,10.0,,en
1109899,2012-05-24 11:14:18,artificial,AI Lecture Series from a [F]emale,u2h5q,[deleted],,https://www.reddit.com/r/artificial/comments/u2h5q/ai_lecture_series_from_a_female/,0.0,0.0,,en
1109900,2012-05-24 13:02:58,computervision,Inpainting from 90 percent missing pixels,u2jnk,[deleted],,https://www.reddit.com/r/computervision/comments/u2jnk/inpainting_from_90_percent_missing_pixels/,26.0,12.0,,en
1109901,2012-05-24 15:45:27,computerscience,A Saturday Morning Breakfast Cereal Comic Strip,u2o5a,antdude,1282772991.0,https://www.reddit.com/r/computerscience/comments/u2o5a/a_saturday_morning_breakfast_cereal_comic_strip/,1.0,0.0,,en
1109902,2012-05-24 16:07:48,computervision,"Photoshop VS Sketchup: Interactive Images: Cuboid Proxies for Smart Image Manipulation (SIGGRAPH 2012)
      - YouTube",u2ow8,aboeing,1178181606.0,https://www.reddit.com/r/computervision/comments/u2ow8/photoshop_vs_sketchup_interactive_images_cuboid/,9.0,0.0,,en
1109903,2012-05-24 19:47:24,MachineLearning,Need to create something like bigml for inhouse use in a mid-sized company,u2z9j,Exibus,1296002350.0,https://www.reddit.com/r/MachineLearning/comments/u2z9j/need_to_create_something_like_bigml_for_inhouse/,7.0,6.0,"Company where I happen to work operate several niche social networks, social network games, etc. I was tasked to create simple prediction and clustering API for in house use. Where should I start? Is there a good library which I could just write a wrapper for starters? Sure I will add specific features later but for now I just need something that somehow works...

In terms of size so far the biggest dataset is a few millions of rows with tag-like features.
Also we have a few netflix like users' votes matrices which are smaller in size (about 100k users and 10k items or something like that). 


To clarify even more basically we have three tasks: 

1. recommend items to users 

2. recommend users to users

3. cluster users for further analysis.  ",en
1109904,2012-05-24 20:05:29,computervision,Posted Video about Stereo Disparity in BoofCV.  Feedback?,u3087,lessthanoptimal,1303613432.0,https://www.reddit.com/r/computervision/comments/u3087/posted_video_about_stereo_disparity_in_boofcv/,3.0,2.0,"Starting to post videos that demonstrate capabilities and how to use a computer vision library I am working on.  Was wondering you guys find this type of video interesting and if you have some constructive criticism.  I know some of the editing isn't perfect and my speech is a bit off in parts...

Link to video: [youtube](http://www.youtube.com/watch?v=ujm3TKfVarQ)",en
1109905,2012-05-24 20:05:33,statistics,Variance-Covariance matrix in Minitab,u308e,sg187,1333569514.0,https://www.reddit.com/r/statistics/comments/u308e/variancecovariance_matrix_in_minitab/,2.0,3.0,"I'm not used to using minitab, but I'm in a group and must use the same program they are.  All I want to do is get a confidence interval for the difference between two coefficients but for the life of me I can't figure out how to display the variance-covariance matrix in minitab. Any help would be appreciated, thanks.",en
1109906,2012-05-24 20:16:15,statistics,Boating Injuries and Deaths: The Statistics,u30tj,newsilike,1335684979.0,https://www.reddit.com/r/statistics/comments/u30tj/boating_injuries_and_deaths_the_statistics/,3.0,1.0,,en
1109907,2012-05-25 01:08:11,statistics,"Help With ""Categories"" in SPSS?",u3gm2,board23,1285479667.0,https://www.reddit.com/r/statistics/comments/u3gm2/help_with_categories_in_spss/,2.0,3.0,"Hey r/statistics!

I'm currently working on my MSc Degree in Hydrogeology, and am in the process of working up my data from a year's worth of environmental sampling. I have approximately 350 different samples, with many associated environmental variables (ph, carbon, isotopes, etc), and I am looking at making some preliminary descriptive statistical analyses and scatter plots based on different categories that I have assigned to each sample, as well as the date of sampling. I cannot, for the life of me, find a way to ""group""/""bin"" based on multiple categories (my Google-Fu is typically really good). 

In other words, my data is organized based on unique ID's, as well as sample names, three different ""identifiers"", the date, and then all of the environmental variables. Can someone please lend a helping hand and point me in the right direction, and let me know if a) this is possible, and b) the name for such a procedure. I've used SPSS in the past for simple ANOVA and descriptive stats, and I am well versed in Excel. I'm using SPSS 20 for Mac. Many thanks in advance, and I apologize if this is the wrong forum to post in.",en
1109908,2012-05-25 01:29:41,datasets,Hayden Planetarium—curators of the Digital Universe Atlas (included)—Data Products.,u3hq8,grbgout,1302028987.0,https://www.reddit.com/r/datasets/comments/u3hq8/hayden_planetariumcurators_of_the_digital/,4.0,1.0,,en
1109909,2012-05-25 02:52:52,statistics,Model checking for cox regression with time-dependent covariates?,u3m41,hypermonkey2,1301460132.0,https://www.reddit.com/r/statistics/comments/u3m41/model_checking_for_cox_regression_with/,2.0,2.0,"Hi all,

I'm a math grad and I've been reading about Cox Regression with time-dependent covariates.
It's extremely useful, but I can't seem to get a clear answer to how one practically goes about performing model checking and diagnostics (in SAS for example). i.e. comparing models and evaluating assumptions such as proportional hazards (of which i know only the time-INdependent case).

Has anyone dealt with this before?

Thanks in advance! :)
",en
1109910,2012-05-25 04:34:35,artificial,"Building an AI chatbot, want to contribute suggestions?",u3r8c,Knight_of_Reason,1294104687.0,https://www.reddit.com/r/artificial/comments/u3r8c/building_an_ai_chatbot_want_to_contribute/,18.0,13.0,"Hey r/artificial! I'm thinking about building an AI chatbot from the ground up. I've been sketching out a lot of basic interaction types (e.g. should respond with a greeting if it recognizes a greeting, remember name of whoever it's chatting to, etc.) If you have suggestions for types of interactions you think would be cool to build in, suggest it below! I'll host it all once its done for you guys to see",en
1109911,2012-05-25 05:31:22,statistics,"help with:  Designing a statistically proper experiment, testing the effect of caffeine on motor skills. (high school level AP statistics)",u3u4g,caaawl,1332213454.0,https://www.reddit.com/r/statistics/comments/u3u4g/help_with_designing_a_statistically_proper/,5.0,4.0,"I had posted a more specific question earlier this week but I'm posting this to see if any of you guys could help me out.

Long story short, I ended up having to do the effects of caffeine on simple motor tasks for a statistics project.  I have had a few ideas for an experiment design but each time I feel as if something is improper.

-In terms of sample, my professor let us know that we don't necessarily need a completely simple random sample, if ours is voluntary response we can explain why

-Experimental design idea. i have posted this before but i have changed a few things up. (i do thank the fellow redditors who helped me about before)
   -pretest and posttest: have all participants rearrange playing cards (from a set position of all cards layed out) back into a neat deck and time how long it takes. then all drink coffee for example and then preform the task again.  (a double blind placebo would be, say, decaf coffee) after all this is done, take post test time and subtract it from pretest time, then conduct a two sample T test.

would this be the best way to go about this kind of test? would the task be significant? any other flaws or suggestions?

Thanks a plenty for any of your guys's aid.

TL;DR - best experimental design for testing to see if coffee had an effect on basic motor skills?

",en
1109912,2012-05-25 08:36:33,MachineLearning,"Final Eurovision predictions, and what to watch for in the voting",u42i3,[deleted],,https://www.reddit.com/r/MachineLearning/comments/u42i3/final_eurovision_predictions_and_what_to_watch/,1.0,0.0,,en
1109913,2012-05-25 08:56:33,statistics,I am giving a lecture on survival analysis tomorrow... Any last minute advice?,u437l,sortizo,1288834170.0,https://www.reddit.com/r/statistics/comments/u437l/i_am_giving_a_lecture_on_survival_analysis/,4.0,6.0,"I am basically going to be talking about parametric regression models in survival analysis. I feel pretty prepare but I am a little afraid of some the questions that might pop up during the presentation, particularly the ones from my teacher. Are there any last minute advice about what I should know? Thanks!",en
1109914,2012-05-25 09:04:08,statistics,Using probabilistic PCA to compare data processing methods,u43gd,judonick,1337925343.0,https://www.reddit.com/r/statistics/comments/u43gd/using_probabilistic_pca_to_compare_data/,3.0,3.0,"In my work, using PCA to denoise after preprocessing data is fairly common.  But using an appropriate processing method before using PCA to denoise is a bit of an issue.  Would it be valid to compare the likelihoods (on unseen data, repeated via cross-validation) of a probabilistic PCA model which has been processed with different methods?  Would the method which gives the higher likelihood be more appropriate to denoise with PCA?

Edit: Clarifications",en
1109915,2012-05-25 09:29:33,statistics,Free Book: Non-Uniform Random Variate Generation by Luc Devroye,u44ba,lpiloto,1310021871.0,https://www.reddit.com/r/statistics/comments/u44ba/free_book_nonuniform_random_variate_generation_by/,9.0,1.0,,en
1109916,2012-05-25 11:59:03,statistics,"Math grad who didn't take Statistics here.  Where to 
start learning?",u48ax,tagus,1282870299.0,https://www.reddit.com/r/statistics/comments/u48ax/math_grad_who_didnt_take_statistics_here_where_to/,22.0,31.0,"Hi there.

I was a math major in college who specialized in Abstract Algebra and Analysis, completely overlooking Statistics.

Now that I've graduated, I look around and I see that apparently all the mathematics jobs that I can find center around this field.

Naturally, I feel as though I should play catchup.  I'm not some idiot who needs a book explaining what a set is or basic probability is, so I was wondering what recommendations for a book/ebook/guide I could look up you guys might have.

Muchas gracias ahead of time,
-tagus",en
1109917,2012-05-25 12:05:49,statistics,Help With: Statistics for research proposal,u48g8,PBcrunchy,1301206016.0,https://www.reddit.com/r/statistics/comments/u48g8/help_with_statistics_for_research_proposal/,1.0,1.0,"Not sure if this is the right place to post, but I am at my wits end! 

I am compiling a research proposal for an honours paper. However, I seem to have designed a study which outweighs my statistical knowledge. 

Basically I want to see whether illness perceptions (7 scales-- scores are summed and averaged to provide an average score for each scale; continuous) are related to levels of quality of life (continuous), depression (continuous), and self-esteem (continuous). 

I would also like to control for age. 

Any advice on how I would go about this?",en
1109918,2012-05-25 16:57:52,statistics,Using Bayesian Statistics to disrupt online advertising and marketing algorithms.,u4glx,kevinludlow,1301461604.0,https://www.reddit.com/r/statistics/comments/u4glx/using_bayesian_statistics_to_disrupt_online/,9.0,7.0,,en
1109919,2012-05-25 20:44:10,computervision,Inpainting Algorithm on GitHub/ OpenCV/C++ (TV-L2 denoising and inpainting),u4rl8,[deleted],,https://www.reddit.com/r/computervision/comments/u4rl8/inpainting_algorithm_on_github_opencvc_tvl2/,10.0,1.0,,en
1109920,2012-05-26 02:09:44,statistics,Does random assignment influence statistical power?,u583b,[deleted],,https://www.reddit.com/r/statistics/comments/u583b/does_random_assignment_influence_statistical_power/,1.0,0.0,"I'm grading an exam and can't seem to pin down an answer here. I can see how it would be unrelated, and I can see how it would have an influence (in certain ways).",en
1109921,2012-05-26 16:03:11,AskStatistics,How to use minitab to perform a two way anova.,u5x90,jiratic,1338036438.0,https://www.reddit.com/r/AskStatistics/comments/u5x90/how_to_use_minitab_to_perform_a_two_way_anova/,0.0,1.0,"Basically I need a complete idiots guide on how to use minitab to perform a 2 way anova for a lab report.
2nd year biology/ecology.
 2 (leaf litter 60 um, 1 mm) X 2 (leaf type: eucalyptus) design examining these effects on leaf litter mass loss. 
Everything is in columns, and I'm not sure how I input the data into ANOVA; what are response, row factors, column factors--&gt; it keeps returning errors for me
I get the following error "" ERROR * Unbalanced design.  A cross tabulation of your factors will show
          where the unbalance exists""
Sample of data. my intended response is loss. With row factors &amp; column factors, 'leaf type' and size'.
Assuming I can get this to work, what other graphs/ stat analyses would you recommend to demonstrate that change in mass is not due to noise, but an interaction of one or both the row &amp; column factors
size   leaf type    replicant   loss
1mm	eucalyptus	9	-31.0
1mm	lucerne	9	-21.9
1mm	eucalyptus	9	-17.8
1mm	eucalyptus	5	-14.3
1mm	eucalyptus	5	-13.4
1mm	eucalyptus	5	-12.0
1mm	lucerne	9	-7.1
1mm	eucalyptus	9	-6.4
60micron	eucalyptus	5	-6.2
60micron	eucalyptus	5	-3.7
60micron	eucalyptus	9	-0.5
60micron	eucalyptus	9	3.3
60micron	eucalyptus	5	7.3
60micron	eucalyptus	5	12.7
60micron	eucalyptus	5	13.3
60micron	eucalyptus	5	15.7
60micron	eucalyptus	5	17.6
60micron	eucalyptus	5	19.4


",en
1109922,2012-05-26 19:31:18,statistics,Arithmetic of Random Variables,u63yj,judonick,1337925343.0,https://www.reddit.com/r/statistics/comments/u63yj/arithmetic_of_random_variables/,0.0,2.0,Is there any set of general rules or concepts for dealing with arithmetic operations on random variables?  I'm interested in guides for how to deal with transformations of variables and identifying the resulting distribution.,en
1109923,2012-05-26 20:58:39,statistics,Learn R two minutes at a time,u67li,jackhammer2022,1280494843.0,https://www.reddit.com/r/statistics/comments/u67li/learn_r_two_minutes_at_a_time/,69.0,11.0,,en
1109924,2012-05-27 06:03:24,datasets,LF atomic US addresses,u6tj2,tilio,1266962108.0,https://www.reddit.com/r/datasets/comments/u6tj2/lf_atomic_us_addresses/,1.0,0.0,"I'm looking for a dataset of US street addresses with fields broken up atomically.  Geocoding (long/lat) is a major plus.  I'm not looking for names of occupants, although I can ignore that in the data import.

I've found a few resources but they're either stupid expensive (just doing research), or woefully incomplete (i.e. a single state).

I can't use an API because I need the volume of data for a map-reduce function.

example of ideal dataset:
----------------

    |""37""|""039""|""990600""|""1052""||""1349""||||""NATURE VALLEY""|""TRL""|||||""28906""|""35.13574751""|""-84.28824004""|""1""
    |""37""|""039""|""990600""|""1052""||""1374""||||""NATURE VALLEY""|""TRL""|||||""28906""|""35.13509731""|""-84.28720564""|""1""
    |""37""|""039""|""990500""|""1019""||""62""||||""ELLIS""|""LN""|||||""28906""|""35.06376153""|""-84.03724948""|""1""
    |""37""|""039""|""990600""|""6055""||""195""||||""PIPER""|""DR""|||||""28906""|""35.02753566""|""-84.19899257""|""1""
    |""37""|""039""|""990600""|""6055""||""170""||||""OAK BEND""|""LN""|||||""28906""|""35.02941709""|""-84.18883299""|""1""


unfortunately, this set is NC state only.",en
1109925,2012-05-27 19:56:40,statistics,Meta-analysis,u7fk6,batacchio,1328451328.0,https://www.reddit.com/r/statistics/comments/u7fk6/metaanalysis/,1.0,1.0,"Hi guys,

i am performing (trying to perform) a met-analysis: I have a subgroup made only by 2 studies and they show opposite results. The 1st has excellent outcomes the 2nd very bad ones.

Is it true that I am not allowed to compute the data from these 2 (different) studies under a random-effect model? I have been told that it's methodologically not correct to perform a meta-analysis of 2 studies with such different results. 

They suggested me to report the raw data from these studies separetly (without an overall outcome calculated by a random-effect model). Any suggestion? or any place where i can find the explanation about this point?

Thanks dudes!",en
1109926,2012-05-27 21:41:35,statistics,Is there a statistical method that can help me test whether this proposed model fits the data?,u7jxa,Chanther,1313468352.0,https://www.reddit.com/r/statistics/comments/u7jxa/is_there_a_statistical_method_that_can_help_me/,8.0,7.0,"I have two groups of students, one n=23 and the other n=32.  Each group has scores on a set of tests (I have multiple scores, so I could do something multivariate, but I could also restrict it to just one).  

My hypothesis is that the top 23 students of both groups are not significantly different from one another, and that what makes the larger group look (on average) like a less academically advanced group is that the bottom 9 are very different from their classmates.  In other words, it's not the class of 32 *as a whole* that's doing more poorly on the measures, it's the presence of those 9 extra struggling students.  

I can do a test of the means of the top 23 of both groups and show that they are not significantly different from one another.  I also constructed a graph that pairs the top ranked student from one group with the top ranked from the other, the 2nd with the 2nd, and so on, to show visually that the two groups are well matched until you hit the bottom students, where the tail of the larger group suddenly takes a dive.  

Is there a more sophisticated way to test this model than what I've laid out?  My data analysis skills are about a decade out of date, but I was solid with multiple regression, survival analysis, and growth modeling back in the day.  Mostly I'm looking for a name of a technique or type of model, rather than needing an exhaustive how-to.  

(Educational decisions are going to get made, and I believe I need to make a case to my faculty that we need to differentiate instruction for the larger group.  Right now there's a sense of the larger group as a 'weak class', but I believe reducing the overall rigor of the program would be to do a disservice to the majority of the larger class.  If the analysis supports my hypothesis, I'd like to make the argument that we need to keep our program as is for most students, while modifying it to support that larger tail of struggling students as appropriate.)",en
1109927,2012-05-27 22:24:44,statistics,Exporting lists (in lists) in R (problems),u7lri,kabrch,1281867756.0,https://www.reddit.com/r/statistics/comments/u7lri/exporting_lists_in_lists_in_r_problems/,1.0,1.0,"Hi everyone

I'm using the twitteR package for R to download tweets for analysis. The tweets are packaged as lists in a list (&lt;S4 object of class structure(""status"", package = ""twitteR"")&gt;).

I feel like I've tried everything google has found for me, and it becoming somewhat of a pressing issue for me. I've tried getting it converted to a data frame, I've tried the dput function and a host of suggestions. 

My competencies are unfortunately quite limited, so the problem might be a lack of understanding how R actually handles those types of operations. I typically get the following errors whenever I try an export method:

- The file I export the tweets to only shows ""&lt;S4 object of class structure(""status"", package = ""twitteR"")&gt;"", where the tweet should've been printed out.

- When I attempt to access the object through e.g. dput(x[[]], file=x.txt etc.) I get the following error: Error in x[[]] : invalid subscript type 'symbol'.

Essentially the problem is that I need these tweets extracted and exported in a way that makes it possible for python to handle the data.

Any help is most appreciated, thank you all.",en
1109928,2012-05-28 03:41:03,MachineLearning,"[noob]: Creating preferences list with ""not always correct"" features",u7zba,bpger1,1331963300.0,https://www.reddit.com/r/MachineLearning/comments/u7zba/noob_creating_preferences_list_with_not_always/,5.0,9.0,"I have the following problem:
- A set of 10 classes (for example: dogs, cats, rabbits,...)
- A set of 1000 items (pictures of these animals)
- For every item I have its class (is it a dog picture, or a cat picture, or a rabbit picture)
- There is a 30% chance that the associated class for an item is wrong  (for example: a dog picture we think is cat)
- All items have counter how many people clicked on them

I show to a user 7 pictures, and after he clicks on some, i should update his profile in order to show him better pictures next time.
My current approach is to define a float per category for every user, and use that float in ranking 1000 items. Choose top 7, and when he selects one, increase the number for that category, and decrease for others.
Any ideas on how to work with this?
Thanks!",en
1109929,2012-05-28 08:21:44,statistics,"Bayesian or Frequentist, Which Are You?",u8ban,jackhammer2022,1280494843.0,https://www.reddit.com/r/statistics/comments/u8ban/bayesian_or_frequentist_which_are_you/,16.0,31.0,,en
1109930,2012-05-28 20:12:51,AskStatistics,"Is there a way to generate random numbers using only your body/mind, without any extraneous devices/objects?",u8xa9,m4num3ntal,1325840847.0,https://www.reddit.com/r/AskStatistics/comments/u8xa9/is_there_a_way_to_generate_random_numbers_using/,3.0,10.0,,en
1109931,2012-05-28 22:13:30,statistics,Combining 2 samples in one study - Would the increased sample size be more beneficial to the study than any potential problems?,u938b,randombabble,1319191466.0,https://www.reddit.com/r/statistics/comments/u938b/combining_2_samples_in_one_study_would_the/,6.0,5.0,"Assuming you are conducting a study to research on the public's view on Apples  and you wish to receive more responses so it would increase the representativeness of your data. Would it be alright to use two different samples of: 1) 500 surveys to be collected from the student sample in one university 2) 500 surveys to be collected via convenience sample of the public    
Would combining the two samples create any problems if you wish to analyzing the data together? And would the benefits of the increased sample size outweigh that potential problems?",en
1109932,2012-05-28 22:49:03,statistics,"SPSS Gurus, I need your help! What do I do with my data?",u9508,[deleted],,https://www.reddit.com/r/statistics/comments/u9508/spss_gurus_i_need_your_help_what_do_i_do_with_my/,0.0,0.0,"I'm an undergraduate who is working with a data set for my final graduation project, and I was never trained on SPSS. I've been following guides on inputting data, and I'm confident that I've inputted everything correctly. Now comes the hard part, which tests do I run?

Overview of my study:
User assessments of privacy concerns regarding augmented reality mobile software.

demographic questions:

age

gender

Augmented Reality Experience

survey questions:

level of privacy concerns regarding facial recognition software?

not concerned - extremely concerned (5 options)

how likely are you to use an app that requires facial recognition?

not likely - extremely likely (5 options)

if there were privacy settings available to control access to your information on this app, how likely are you to use this type of app?

not likely - extremely likely (5 options)

The rest of the questions are in the same format:

privacy concerns regarding location tracking?

would you use an app that requires location tracking?

if there were privacy settings available, would you use an app that requires location tracking?

privacy concerns regarding displayed personal information?

would you use an app that requires displayed personal information?

If there were privacy settings available would you use an app that requires displayed personal information?

Overall, how likely are you to use an augmented reality app?

I have sort of an idea about how I want to run these stats, but I do not know what to do next. For example, I want to see differences in privacy concerns between genders. likelihood of using app between gender, likelihood of using app with privacy settings between gender. Same for age, same for AR app experience. 

I also want to see if privacy concerns are correlated with likelihood of use and if that is affected by the inclusion of privacy settings. Also if privacy concerns are correlated with likelihood of using AR apps overall. 

As you can tell, I'm in over my head here. Any help would be appreciated. Some trouble I'm running into goes along these lines:

Gender -&gt; Privacy concerns

gender is the independent variable? PC is dependent variable?

Privacy Concerns are ordinal

I'm looking for differences between two groups (male, female)

so I should run a Kruskal Wallis test right?

But I cannot enter all 3 PC related variables, does that mean I have to run one at a time? Is there a better test to run?

I'm lost as you can tell. Please help, Thanks Reddit!

Edit: I am willing to pay for help, I need this help ASAP, so let's figure something out!",en
1109933,2012-05-29 00:03:08,statistics,Statistics Project Ideas,u98my,kinjobinjo,1290125748.0,https://www.reddit.com/r/statistics/comments/u98my/statistics_project_ideas/,3.0,8.0,"I need some ideas for my high school statistics final project. I have to have data that I collect on my own and do some kind of hypothesis test. PLEASE HELP.

Edit: Sports related projects are preferred. Also, it can't be too crazy because I only have 2 days.",en
1109934,2012-05-29 04:41:24,statistics,I need some survey analysis help. Which test should I run to analyze the differences between two ordinal variables that use the same Likert scale?,u9lzi,Isomorphic_Algorithm,1309053861.0,https://www.reddit.com/r/statistics/comments/u9lzi/i_need_some_survey_analysis_help_which_test/,3.0,2.0,"I'm trying to see if the addition of a variable (privacy settings) has an affect on technology adoption. The Likert scale is the same for both questions:

1.never will use

2.not likely

3.somewhat likely

4.very likely

5.absolutely will use

Basically I'm trying to see if my data shows any significant change from ""how likely are you to use this technology"" to ""how likely are you to use this technology if there were privacy settings."" I would also like to be able to quantify the difference if possible. Sorry this is probably a noob question.

I'm using SPSS if that matters.

Any ideas you statistical wizards? ",en
1109935,2012-05-29 04:44:30,statistics,"Everybody knows Benford's law, but are there any other ways to identify fake stats?",u9m4v,lavalampmaster,1246881402.0,https://www.reddit.com/r/statistics/comments/u9m4v/everybody_knows_benfords_law_but_are_there_any/,30.0,11.0,"I'd imagine certain numbers or number patterns either occur or fail to occur when stats are faked, eg, 'doubles' in larger data points (eg, 3255691) or the number of prime numbers that tend to appear in real vs. fake data. Google shows me nothing, so I ask you all.",en
1109936,2012-05-29 05:11:20,MachineLearning,Question: Computing similarity of useragents,u9nj6,rlayton,,https://www.reddit.com/r/MachineLearning/comments/u9nj6/question_computing_similarity_of_useragents/,5.0,5.0,"I'm writing a program, and part of that program aims to detect if two computers connecting to a website are the same. *Part of that* function uses the useragent strings. I want to build a function that takes two useragents as input and returns an output of value between 0 and 1, where 0 is ""no similarity"" and 1 is ""exactly the same"".

I'm currently just tokenising the useragents and returning the Jaccard similarity (size of intersection divided by size of union of the two sets). Is there a better way? The goal is that the same browser will show with a high similartiy, particularly if something upgrades (i.e. IE8.0 gets upgraded to IE9.0).

An idea I had was to learn a the Baye's probabilities for transitions (i.e. upgrades have a high probability, downgrades a low probability) and use that to calculate an overall probability for the two user-agents being generated by the same browser. If there is a simpler method though, I'd love to hear it.",en
1109937,2012-05-29 08:19:54,artificial,Human Intelligence is Overrated,u9wdw,togelius,1147200994.0,https://www.reddit.com/r/artificial/comments/u9wdw/human_intelligence_is_overrated/,0.0,5.0,,en
1109938,2012-05-29 19:00:33,statistics,League of Legends champion 'synergies' and 'counter-picks',uah6k,Ulvund,1177090152.0,https://www.reddit.com/r/statistics/comments/uah6k/league_of_legends_champion_synergies_and/,4.0,14.0,"I have a statistical problem:

2 teams play each, other each team selecting 5 characters with different abilities from a pool of 120 'champions' and the subsequently playing a game for victory/loss (binary response variable)

Wanting to figure out which combinations of champion picks are more likely to be victorious:

How would you set up a model for this problem?

I wanted to analyze this with Principal Components Analysis or Partial Least Squares but I can't figure out how to set up the model with these weird circumstances.

One could of course have explanatory variables for each of the 120 possible 'champions' for each team (240 all in all).

Do you have any ideas for any alternate regressions or choice of explanatory variables? ",en
1109939,2012-05-29 19:08:14,statistics,Non-parametric forecasting for dummies?,uahju,latortilla,1299276404.0,https://www.reddit.com/r/statistics/comments/uahju/nonparametric_forecasting_for_dummies/,11.0,5.0,"Hi,

I'm trying to understand how forecasting can work with non-parametric regressions. So if I estimate a non-parametric function (suppose using Kernel methods, Nadaraya and Watson) i get f(x) from y = f(x) + e, but how can I possibly forecast from this since the slope changes all the time?

In particular, what if I have an autoregressive model: Zt = f(Zt-1, Zt-2...) + e, how can i get a non-parametric forecast for Zt+1 out of this, given that I estimated f(Zt-1, Zt-2...) using Kernel methods?

Thanks!",en
1109940,2012-05-29 21:06:20,statistics,Does anybody have any experience with this type of Logistic Regression output?,uanqz,SurfaceThought,1320256168.0,https://www.reddit.com/r/statistics/comments/uanqz/does_anybody_have_any_experience_with_this_type/,3.0,6.0,,en
1109941,2012-05-29 22:17:12,statistics,Self study of statistics this summer. Know any good resources?,uarir,[deleted],,https://www.reddit.com/r/statistics/comments/uarir/self_study_of_statistics_this_summer_know_any/,1.0,0.0,,en
1109942,2012-05-29 23:03:31,MachineLearning,Robot gripper teaches itself how to pick up different shaped objects,uau34,[deleted],,https://www.reddit.com/r/MachineLearning/comments/uau34/robot_gripper_teaches_itself_how_to_pick_up/,1.0,0.0,,en
1109943,2012-05-30 03:28:06,rstats,Beginner R Course at Statistics.com,ub9gi,unkunked,1323265079.0,https://www.reddit.com/r/rstats/comments/ub9gi/beginner_r_course_at_statisticscom/,1.0,0.0,"Many of you here are experts in R but some of us are just starting out. I had read a number of R tutorials but just wasn't ""getting it"". I decided to take an Intro course at Statistic.com to force me to learn quickly. The class is called [Into to R \(Stats\)](http://www.statistics.com/rstatistics). If you already know stats and want to learn R to do them then this is the course for you. The pace is pretty fast but really gives you a great start. (If you don't already know some stats then you would want to take one of their other courses.) If you are like me and need the structure of a course with homework assignments to make you really learn something then I highly recommend this one. I'm already using what I learned at work. I will definitely be taking more courses from them in the future.",en
1109944,2012-05-30 06:28:17,data,How Online Storage Has Replaced the Thumb Drive,ubjkr,[deleted],,https://www.reddit.com/r/data/comments/ubjkr/how_online_storage_has_replaced_the_thumb_drive/,1.0,0.0,,en
1109945,2012-05-30 11:30:30,datasets,Publicly available energy/pipeline datasets?,ubuma,lenwood,1191289262.0,https://www.reddit.com/r/datasets/comments/ubuma/publicly_available_energypipeline_datasets/,1.0,0.0,"Does anyone know of any publicly available energy datasets? I've searched for oil &amp; natural gas pipeline data, but am coming up empty.",en
1109946,2012-05-30 14:32:11,statistics,SPSS Vs Sawtooth for choice based conjoint analysis,ubyr4,Dumdidaa,1326350292.0,https://www.reddit.com/r/statistics/comments/ubyr4/spss_vs_sawtooth_for_choice_based_conjoint/,3.0,0.0,"I was used to designing and analysing choice based conjoint on sawtooth software. But currently, I only have access to SPSS. Is there any limitation of doing CBC on SPSS instead of Sawtooth? Also, is there any differences in design &amp; analysis between the two software? Thanks",en
1109947,2012-05-30 16:30:46,AskStatistics,Poisson Distribution Upper Critical Values.,uc2g5,Anguss,1301843935.0,https://www.reddit.com/r/AskStatistics/comments/uc2g5/poisson_distribution_upper_critical_values/,0.0,1.0,"So I was watching [THIS](http://www.youtube.com/watch?v=vW8ubL87La0&amp;feature=relmfu) video and at about 6:40 (and 8:20) he takes 1 away from his upper critical value. To me, the critical value should be 6, as it is the first probability that is greater than 0.95, but he takes it as 7 and discards the 6. Why?! Also, do you only do this with Poisson Distributions or do you do it with every critical value test?

Thanks!",en
1109948,2012-05-30 19:30:54,statistics,need help running a significance test on difference between two means,ucac7,jasdfd89,1338395142.0,https://www.reddit.com/r/statistics/comments/ucac7/need_help_running_a_significance_test_on/,1.0,4.0,"hey all,

i'd really appreciate some help with this test i'm trying to run. i'm trying to compare average salaries from 2 different leagues from 2 different years to see if the difference is significant. i think i'm going for a t-test; i have a null and alternative hypothesis but i'm not really sure where to go from there. i could send the data i'm working with to someone if that'd be the easiest way to get some help, but i'll try to explain it.

right now i have the average salaries for teams from 2 leagues for 2 different years. i calculated stats and i now have 2 means for each league year (so mean of league 1/2005, mean of league 1/1995) (mean of league 2/2005, mean of league 2/1995), also have standard deviations.

so i have 4 means and 4 standard deviations, but i'm not really sure where to go from here. i calculated the difference between the means, but i don't know what to do about the standard deviation. 

if this is vague or unclear i can explain/show the data.

thanks!",en
1109949,2012-05-30 19:44:19,statistics,"For those of you who teach or give talks: statistics jokes compendium. If you know of others not in this list, add them in the comments - I don't need to get research done!",ucb1s,prionattack,1257312771.0,https://www.reddit.com/r/statistics/comments/ucb1s/for_those_of_you_who_teach_or_give_talks/,17.0,7.0,,en
1109950,2012-05-30 20:46:33,statistics,Need help with a bernoulli probability distribution problem,uce9x,zecreddit,1284747873.0,https://www.reddit.com/r/statistics/comments/uce9x/need_help_with_a_bernoulli_probability/,1.0,0.0,,en
1109951,2012-05-31 01:47:42,statistics,Simulating Rare Events with Logistic Regression,ucv45,suckeggmule,1250642053.0,https://www.reddit.com/r/statistics/comments/ucv45/simulating_rare_events_with_logistic_regression/,2.0,13.0,"Below is some SAS code for simulating data for a Logistic Regression model, but can I specify the desired proportion of 1s ahead of time?

Changing the coefficients definitely affects the final proportion of 1s, but I can't figure out the principled approach to specifying a 2% (or whatever) proportion of 1s.

I could force the Bernoulli step to use a specific threshold rather than the model generated p, but that doesn't seem like the right approach.  



    data makebinary;
	do i = 1 to &amp;sampN;
		
		x1 = rand(""Normal"",0,1);
		x2 = rand(""Normal"",1,2);
		
		b0 =  -1.5;
		b1 = .4;
		b2 = .7;
		
		eta = b0 + b1*x1 + b2*x2;
		
		p = exp(eta) / (1 + exp(eta));
		 
		y = rand(""Bernoulli"",p);
   
		output;
	end;
    run;",en
1109952,2012-05-31 01:59:13,MachineLearning,Where can I find internships?,ucvqc,cyborgbrain,1319490298.0,https://www.reddit.com/r/MachineLearning/comments/ucvqc/where_can_i_find_internships/,7.0,15.0,"I was looking for internships somehow related to machine learning. The problem is that I am a high school student and most are asking for undergrads.

I was wondering if there is any company willing to let me intern there or if there are professors or grad students willing to let me shadow them and help them out. The other problem is that I am in the Northern VA Area and there aren't very many startups in this area.

So I was wondering, are there any companies in the Greater DC area that may be willing to let a motivated high schooler intern there?",en
1109953,2012-05-31 03:26:08,statistics,What's the best approach to this business problem involving statistical probability and likelihood?,ud05k,aguyfromucdavis,1279525151.0,https://www.reddit.com/r/statistics/comments/ud05k/whats_the_best_approach_to_this_business_problem/,2.0,3.0,"I'm working on evaluating the likelihood that a client will make a second purchase of a product given that the first purchase was incentivized by a promotional discount based on discrete dollar quantities (say $10, $20, or $30). Basically, I want to compare the likelihoods of the client making the second purchase based on these different promotional discount amounts to determine if, for example, a client using a $10 discount on the first purchase is just as likely to make a second purchase as another client who used a $20 discount on his/her first purchase. If there is no statistically significant difference, this could potentially save the company lots of money to simply offer $10 discounts (assuming the likelihoods are similar).

My initial thoughts were to use odds ratio and compare how much more likely a $20 discount would incentivize a second purchase than would a $10 discount, or for any other x dollar amount. The binary data value in this case is whether or not the client made the second purchase given the first was made using a discount.

Any advice is appreciated! Thanks!",en
1109954,2012-05-31 03:26:32,statistics,Does anyone have any fun or easy to grasp sites/videos in helping this adult student get into Statistics?,ud06c,Cohiba,1253471792.0,https://www.reddit.com/r/statistics/comments/ud06c/does_anyone_have_any_fun_or_easy_to_grasp/,1.0,3.0,"Long story short I haven't been in a classroom in 12 years. Since Statistics is on my list of prerequisites, I am taking the summer course and the class has already begun to kick my butt.

Was wondering if anyone had any effective and/or fun approaches to getting into beginning statistics? Our textbook is dry as sand. Maybe a video or good book on the subject? For someone who graduated with a writing degree, this stuff is mindboggling.",en
1109955,2012-05-31 03:54:41,MachineLearning,How to tell if SVM classification is good enough?,ud1o3,science_robot,1327170096.0,https://www.reddit.com/r/MachineLearning/comments/ud1o3/how_to_tell_if_svm_classification_is_good_enough/,1.0,0.0,"I'm using Support Vector Machines to predict sample treatment status.

144 data points. Leaving out 50 for validation, I get a 25% false-classification.

Is this ""good"" or ""bad""? Is there even such a thing?",en
1109956,2012-05-31 04:06:38,statistics,ARDL model: adding extra lag changes significance of other parameters,ud2cl,[deleted],,https://www.reddit.com/r/statistics/comments/ud2cl/ardl_model_adding_extra_lag_changes_significance/,1.0,0.0,"I'm estimating an Autoregressive Distributed Lag model on some sales data. I'm using lagged values of inflation (monthly CPI growth) as a predictor. When I estimate the model with one lag of inflation only, its coefficient is strongly insignificant. However, when I include a second lag as well, the coefficients on *both* lags become significant.

I seem to recall this being symptomatic of some problem, but I can't recall what that problem is. The sales series were deflated into real terms using the CPI series - could this be an issue? The sales and inflation series are both stationary. 

Edit: the deflation was the problem. I was using differences of logs and the lagged sales values were cancelling out the lagged inflation values, so the standard t-test was essentially meaningless.",en
1109957,2012-05-31 04:08:55,MachineLearning,Mixture of Gaussians with TFIDF sparse vectors,ud2h3,ColonelHapablap,,https://www.reddit.com/r/MachineLearning/comments/ud2h3/mixture_of_gaussians_with_tfidf_sparse_vectors/,6.0,14.0,"Hi guys,

I'm a complete newbie when it comes to Machine Learning (and CS in general, I've only had 2 semesters worth of courses).  I'm trying to write an algorithm for document classification for an internship and I'm feeling out of my league.  

Right now I've got approximately 2000 documents I need to classify and that number is expected to grow over time.  I've got tfidf weightings for each documents, so right now I'm trying to write a Mixture of Gaussians mixture model using the sparse vectors of each documents tfidf weighting (right now there are about 44000 unique words after normalization, so that's how many dimensions I've got).

Things seem to be blowing up, though--I can't reasonably computer a 44000*44000 covariance matrix for each gaussian per iteration, so I'm just doing diagonal matrices (the variance of each term).  But then the variance turns out to be so small that when its time for doing the exp() part of this function: http://en.wikipedia.org/wiki/Multivariate_normal_distribution#Non-degenerate_case,
 multiplying by the inverse eventually makes the resulting scalar too huge to compute.  Right now I'm trying to standardize my ifidf scores by subtracting the mean and dividing by the standard deviation, but that hasn't seemed to help with the resulting scalar size. 

I really don't know what I'm doing.  Is MoG the wrong approach to this?  Apparently LDA is the best thing for this sort of thing, but from what I understand that would definitely be out of my league.

I guess don't know what I'm asking, exactly, but if anyone could provide some insight as how I'm approaching this incorrectly or what might be a better strategy, I would really appreciate it.",en
1109958,2012-05-31 06:53:53,statistics,"Huber-White 'Robust' standard errors for Maximum Likelihood, and meaningless parameter estimates. 

Any thoughts on this? Not a terribly long paper.",udbhi,econometrician,1323865912.0,https://www.reddit.com/r/statistics/comments/udbhi/huberwhite_robust_standard_errors_for_maximum/,4.0,3.0,,en
1109959,2012-05-31 12:18:32,MachineLearning,Advice for masters ML dissertation project,udmrd,unsymbol,,https://www.reddit.com/r/MachineLearning/comments/udmrd/advice_for_masters_ml_dissertation_project/,14.0,15.0,"I've just completed my first year of an AI-based masters programme (I'm a part-time student) and I'm beginning to think about my dissertation project that will be done in year two. Of all classes this year, both machine learning and natural language processing stood out and were the most interesting.

However, my BA is in music and switching to a CS programme has been a steep, but rewarding, learning curve. As a result, I'm not sure of areas most worthy of further research at this point.

Ideally, I'd like to work on a project focussed around music and machine learning but I'd welcome an advice of things to tackle. Any tips, areas of investigation, possible project suggestions?",en
1109960,2012-05-31 14:26:19,statistics,"Can a paired t-test yield a result that:
1. A is significantly larger than B. 
2. B is significantly larger than C.
yet:
3. A and C are not significantly different?

Or is there necessarily a mistake?",udplc,urish,1221689900.0,https://www.reddit.com/r/statistics/comments/udplc/can_a_paired_ttest_yield_a_result_that_1_a_is/,13.0,20.0,,en
1109961,2012-05-31 16:52:14,computervision,"FREAK implementation on Github: Keypoint Descriptor that outperforms SIFT, SURF, BRISK and  on embedded applications.",udukt,[deleted],,https://www.reddit.com/r/computervision/comments/udukt/freak_implementation_on_github_keypoint/,18.0,8.0,,en
1109962,2012-05-31 17:06:13,computervision,Why your vision lab needs a reading group,udv5c,jeanlucpikachu,1159634949.0,https://www.reddit.com/r/computervision/comments/udv5c/why_your_vision_lab_needs_a_reading_group/,5.0,2.0,,en
1109963,2012-05-31 18:22:00,rstats,ff or bigmemory?,udyq7,cedarSeagull,1285276079.0,https://www.reddit.com/r/rstats/comments/udyq7/ff_or_bigmemory/,10.0,4.0,"I'm having trouble deciding which to use, ff or bigmemory.  My datasets are generally about 4 - 10 GB.  Is there anyone out there who has used both and has a preference?  ",en
1109964,2012-05-31 19:38:10,statistics,"Efron's Dice &amp; how rank based tests can be non-transitive (abstract, article behind pay-wall)",ue2qx,[deleted],,https://www.reddit.com/r/statistics/comments/ue2qx/efrons_dice_how_rank_based_tests_can_be/,2.0,4.0,,en
1109965,2012-05-31 21:02:30,statistics,I'm thinking about enrolling in a master's program for biostats and I'm looking for advice. ,ue7c0,Iamnotanorange,1268153982.0,https://www.reddit.com/r/statistics/comments/ue7c0/im_thinking_about_enrolling_in_a_masters_program/,3.0,6.0,"Background: I'm finishing up a PhD in research psychology, but I've recently discovered that I love biostats. I've been consulting a little and it's a lot of fun. 

For various reasons, I'm not all that interested in a post-doc. So now I'm thinking of enrolling in a one-year accelerated biostats program for people who already have a terminal degree.  

My concern is that (up until now) I've been just a big fish in a little pond and my expertise is rather limited. 


Specific Concerns: 

1) I understand math, but I've never taken anything beyond a semester of calculus, which was sometime when I was an undergrad. I... might remember it? 

2) I'm already great at complex ANOVAs, regression analysis, factor analysis, cluster analysis - basically the psychology analysis package. I know that there's more analyses to learn and I'm concerned that some of them will be over my head.
 
3) Most of my expertise is in using SPSS, but I've been trying to learn R. I know that SPSS is not everyone's favorite program, but I find it intuitive. I took a class using SAS, but no one in psychology uses SAS, so I haven't touched the program in 5 years. Plus, I like to work from home and it would cost like $1000 to put it on my computer.  

4) I have some limited, self-taught programming experience. Will I be able to pick up the more complicated analyses? 

So, r/stats, I turn to you. 

1) Do you think I would do well in a biostats program? 

2) Do you think I need a biostats MS program to get a biostats job? 

Edit: Yes, I realize that questions 1 and 2 respectively reflect insecurity and bravado. You should take this as an indication that I have basically no frame of reference. 

EDIT2: Thanks for all your help!",en
1109966,2012-05-31 23:04:51,statistics,Card Statistics Game/Problem,uee8l,CondoKP,1332812275.0,https://www.reddit.com/r/statistics/comments/uee8l/card_statistics_gameproblem/,0.0,0.0,"Take a standard deck of cards (randomly shuffled)
Flip over the top card and count out loud ""Ace"", 2, 3, etc. while flipping a card each number said aloud. 
The object is to not flip a card when you say it's number. (Ex: don't flip an 8 when you say ""8"")
If you get through the whole deck without matching, you win. (Ace -&gt; King, 4 times)

I am a few years removed from stats classes, but I wanted to know the odds of succeeding. It is surprisingly difficult to win. 

Please help! ",en
1109967,2012-06-01 00:07:38,datasets,"Audio data: recordings of 534,000 Evangelical  Christian Sermons",uehsm,RuncibleJones,1313976256.0,https://www.reddit.com/r/datasets/comments/uehsm/audio_data_recordings_of_534000_evangelical/,8.0,8.0,,en
1109968,2012-06-01 10:53:12,statistics,Call for Papers: Bayesian Methods in Management Research [PDF],ufc4k,srkiboy83,1299056137.0,https://www.reddit.com/r/statistics/comments/ufc4k/call_for_papers_bayesian_methods_in_management/,5.0,1.0,,en
1109969,2012-06-01 11:30:05,statistics,Text recommendation,ufd1d,Slats18,1309500427.0,https://www.reddit.com/r/statistics/comments/ufd1d/text_recommendation/,3.0,5.0,"Can anyone provide me with a link to a text/PDF/anything that will start me off on learning Likelihood theory? The most I've done in it is found the estimators of some basic distributions using the likelihood function, so something really beginner would be great, thanks in advance!",en
1109970,2012-06-01 17:04:44,MachineLearning,Looking for a refresher on Machine Learning principles for a summer job.  Artificial Intelligence: A Modern Approach or Elements of Statistical Learning?,ufm26,badgerbro,1324962799.0,https://www.reddit.com/r/MachineLearning/comments/ufm26/looking_for_a_refresher_on_machine_learning/,13.0,13.0,"Hey everybody,
Title says it all.  I'm starting a job in a few days and need to refresh myself on some core machine learning principles.  Does anybody have insight on which book might be better-suited for that?  Thanks

Edit: Thanks for all of the great responses everyone",en
1109971,2012-06-01 18:33:01,statistics,Request: LISREL for Mac OS X,ufq0e,arachnocrat,1302939750.0,https://www.reddit.com/r/statistics/comments/ufq0e/request_lisrel_for_mac_os_x/,0.0,5.0,"Hi fellow numbercrunchers, 
I'm a Psychology Master student and next week I will have a class in Structural Equation Modelling, while using LISREL. Unfortunately I'm a mac user and so far I did not have any luck finding it on the official website (www.ssicentral.com), torrents, nor cyberlocker portals. 
Somewhere I have read that the last version (8.8) is for Windows only and that the one for mac (8.72) is ""gone"" (I don't really know what the author meant by that).
Do any of you mac users use LISREL and/or know from where can I get it? 

EDIT: Thank you for your help!  I got Parallels (is it very different from VMWare?), installed first Windows XP Pro SP3 on it and then LISREL 8.8 - for now it seems to be running fine. In my methodology course we will use LISREL. About R: I'm completely unfamiliar with it, but if I get deeper in research I guess I'll get my hands on it sooner or later.  ",en
1109972,2012-06-01 20:17:22,MachineLearning,20 lines of code that will beat A/B testing every time using an epsilon-greedy strategy,ufv6e,dmdude,1250681632.0,https://www.reddit.com/r/MachineLearning/comments/ufv6e/20_lines_of_code_that_will_beat_ab_testing_every/,37.0,7.0,,en
1109973,2012-06-01 21:00:06,MachineLearning,"I think I've thought a way to make the input attributes in a dataset independent, thus fulfilling the naive bayes assumption - am I wrong?",ufxgb,[deleted],,https://www.reddit.com/r/MachineLearning/comments/ufxgb/i_think_ive_thought_a_way_to_make_the_input/,1.0,0.0,"
I wanted to get some preliminary feedback about this before spending (and possibly wasting) a few days implementing it.

As you know, a [Naive Bayes Classifier](http://en.wikipedia.org/wiki/Naive_bayes) makes the assumption that all input attributes are independent (meaning that given the value of one attribute, you can't predict anything about the values of the other attributes).  This is almost never true, but in many situations Naive Bayes works reasonably well despite this.

The typical solution is to use a [Bayesian network learner](http://en.wikipedia.org/wiki/Bayesian_network#Structure_learning) which captures the interdependencies between attributes, but this is far more complicated than Naive Bayes.

I think I've thought of an alternate approach using a technique from economics for removing ""selection bias"" from a dataset.

Let's say we have 4 nominal input attributes, A, B, C, and D, and an output attribute Z.  We don't know the relationships between the input attributes, but they are probably somewhat dependent on each other.

My proposed approach is to effectively ""filter"" the interdependence out of the input attributes.  How?

Let's take A and B first.  If A and B were independent, then knowledge of A's value would not affect the probabilities of the various values that B might take.

By looking at the data we can see the impact that A has on B.  For example, we might see that if A is ""dog"", then the likelihood of B being ""house"" is 0.3, but if A is ""cat"", then the likelihood of B being ""house"" is 0.4.

We can view this as there being a [selection bias](http://en.wikipedia.org/wiki/Selection_bias) for the value of B, and economics gives us a well-understood way to remove this bias called [Heckman correction](http://en.wikipedia.org/wiki/Heckman_correction).

While the theory behind it is more complicated, applying this correction is very simple.  We take the probability P of B having it's current value given A's current value - P(B|A), and we weight that sample by  1/P(B|A) (setting a maximum weight of, say, 20 - to guard against very small values for P(B|A) screwing things up).

*Note* that these weights only apply when calculating probabilities for the attribute B, so the weight is associated with this specific attribute, not with the entire sample (which is more common).

So now in our dataset we have weighted our attribute B such that it is independent from A.  Next we want to do the same thing for the attribute C, but in this case we need to weight its samples by the probability that C will have it's value given A **and** B's values, or 1/P(C|A,B).  Since A and our corrected B are now statistically independent, we can safely use a naive bayes classifier with A and B as inputs and C as the out to determine this weight for each sample.

And then once we've got weights for all the C attributes, we can repeat this for attribute D, assigning weights to each of its samples using naive bayes with A, B, and C as inputs, and D as the output.

Finally, we can use Naive Bayes to predict our output attribute Z using A, B, C, and D as inputs - and we are now statistically justified in doing so because we know that A, B, C, and D are now independent.  The result, I would hope, would be superior predictive performance.

This seems a bit too good to be true though, where have I screwed up?",en
1109974,2012-06-01 21:01:35,MachineLearning,"This seems to good to be true, but I think I've found a way to make the input attributes in a dataset statistically independent, making any dataset meet the assumptions of naive bayes, and it's much simpler than a bayesian network learner - tell me I'm wrong...",ufxjq,sanity,1149365629.0,https://www.reddit.com/r/MachineLearning/comments/ufxjq/this_seems_to_good_to_be_true_but_i_think_ive/,9.0,34.0,"*ugh, sorry - ""too"" not ""to""*

I wanted to get some preliminary feedback about this before spending (and possibly wasting) a few days implementing it.

As you know, a [Naive Bayes Classifier](http://en.wikipedia.org/wiki/Naive_bayes) makes the assumption that all input attributes are statistically independent (meaning that given the value of one attribute, you can't predict anything about the values of the other attributes).  This is almost never true, but in many situations Naive Bayes works reasonably well despite this.

The typical solution is to use a [Bayesian network learner](http://en.wikipedia.org/wiki/Bayesian_network#Structure_learning) which captures the interdependencies between attributes, but this is far more complicated than Naive Bayes.

I think I've thought of an alternate approach using a technique from economics for removing ""selection bias"" from a dataset.

Let's say we have 4 nominal input attributes, A, B, C, and D, and an output attribute Z.  We don't know the relationships between the input attributes, but they are probably somewhat dependent on each other.

My proposed approach is to effectively ""filter"" the interdependence out of the input attributes.  How?

Let's take A and B first.  If A and B were independent, then knowledge of A's value would not affect the probabilities of the various values that B might take.

By looking at the data we can see the impact that A has on B.  For example, we might see that if A is ""dog"", then the likelihood of B being ""house"" is 0.3, but if A is ""cat"", then the likelihood of B being ""house"" is 0.4.

We can view this as there being a [selection bias](http://en.wikipedia.org/wiki/Selection_bias) for the value of B, and economics gives us a well-understood way to remove this bias called [Heckman correction](http://en.wikipedia.org/wiki/Heckman_correction).

While the theory behind it is more complicated, applying this correction is very simple.  We take the probability of B having it's current value given A's current value - P(B|A), and we weight that sample by  1/P(B|A) (setting a maximum weight of, say, 20 - to guard against very small values for P(B|A) screwing things up).

*Note* that these weights only apply when calculating probabilities for the attribute B, so the weight is associated with this specific attribute, not with the entire sample (which is more common).

So now in our dataset we have weighted our attribute B such that it is independent from A.  Next we want to do the same thing for the attribute C, but in this case we need to weight its samples by the probability that C will have it's value given A **and** B's values, or 1/P(C|A,B).  Since A and our corrected B are now statistically independent, we can safely use a naive bayes classifier with A and B as inputs and C as the out to determine this weight for each sample.

And then once we've got weights for all the C attributes, we can repeat this for attribute D, assigning weights to each of its samples using naive bayes with A, B, and C as inputs, and D as the output.

Finally, we can use Naive Bayes to predict our output attribute Z using A, B, C, and D as inputs - and we are now statistically justified in doing so because we know that A, B, C, and D are now independent.  The result, I would hope, would be superior predictive performance.

This seems a bit too good to be true though, where have I screwed up?

*edit*: Thanks for everyone's feedback.  Certainly there is a question mark over whether this will work (is pairwise independence sufficient?), but since nobody has convinced me that it won't work - I guess I'll just go ahead and implement it, and test it on some datasets.  I'll try to get it done this weekend, watch this space!",en
1109975,2012-06-01 21:02:17,MachineLearning,"[noob question] I have a natural language project I want to work up to. No ML background, but majored in math. What is the path from here to there?",ufxl3,ZENmotherfucker,1334328269.0,https://www.reddit.com/r/MachineLearning/comments/ufxl3/noob_question_i_have_a_natural_language_project_i/,2.0,5.0,"Tech skills include object-oriented JS, SQL, xml, and common related skills for each, like JSON, database design, web services, etc. It's my impression, however, that much of this is useless here. True/false?

I was thinking, generally, of tackling typical beginner ML projects like character recognition which have been tried and tested enough that reference material should be easy to find, then using my experiences there as a stepping stone to more complex problems. 

What do you guys think? 

Thanks for your time.",en
1109976,2012-06-01 23:19:14,MachineLearning,Classification with Naive Bayes when certain features are missing.,ug54a,redditmod,1254961805.0,https://www.reddit.com/r/MachineLearning/comments/ug54a/classification_with_naive_bayes_when_certain/,3.0,2.0,"Hi all,

I understand that Naive Bayes models can be trained with uncertain data (e.g. some of the cases have unlabeled features).  However, in all of the cases I've seen, the Naive Bayes model only can classify a case where all the features are known.  Does anybody know the defacto method used to classify if features are missing? Selecting the most probable state in light of the given features would be an approximation, but is there another way?",en
1109977,2012-06-01 23:56:18,statistics,To what extent do statistics infer probability and vice versa?,ug77a,Homophonicular,1296666869.0,https://www.reddit.com/r/statistics/comments/ug77a/to_what_extent_do_statistics_infer_probability/,8.0,11.0,"Suppose my man Ray Allen has a 90% free throw percentage.  To what extent does this mean he will hit 9 out of 10 shots, or, because this is past data how much does it have to do with the probability of him sinking one right now?
",en
1109978,2012-06-02 06:27:43,artificial,Prosthetic Art,ugotv,Griseldadff,1338607618.0,https://www.reddit.com/r/artificial/comments/ugotv/prosthetic_art/,1.0,0.0,,en
1109979,2012-06-02 13:31:18,statistics,Yahtzee probability question ,uh188,danzipen,1336407725.0,https://www.reddit.com/r/statistics/comments/uh188/yahtzee_probability_question/,3.0,4.0,"Just send me of to another subreddit if this is not the right place for my question. 
So, me and my friends was playing[Yahtzee](http://en.wikipedia.org/wiki/Yahtzee) last night. The argument is about the probability of getting Yahtzee after the first throw, when all de dices came up different. Is it then safer to save one of the dices and throw the for remaining, aiming at the same number as the ""saved dice"" - or is the probability the same as throwing all the dices again?",en
1109980,2012-06-02 15:50:36,statistics,I created a subreddit specifically for probability-related topics: r/probabilitytheory,uh3vk,Styhn,,https://www.reddit.com/r/statistics/comments/uh3vk/i_created_a_subreddit_specifically_for/,9.0,1.0,"Although statistic and probability theory are very much related, the latter is a fascinating subject in itself. I created a subreddit to discuss PT-related topics. Anything goes; from highschool subjects like simple densities, distributions to advanced stuff like stochastic processes, SDE's, etc. etc. 

I hope this advertising is allowed here, if not I apologize.",en
1109981,2012-06-02 16:19:36,MachineLearning,Machine Learning that Matters - position paper from upcoming ICML,uh4lq,wookietrader,1319399910.0,https://www.reddit.com/r/MachineLearning/comments/uh4lq/machine_learning_that_matters_position_paper_from/,19.0,24.0,,en
1109982,2012-06-02 17:54:18,computervision,How many lightbulbs does it take to locate someone ?,uh7b6,[deleted],,https://www.reddit.com/r/computervision/comments/uh7b6/how_many_lightbulbs_does_it_take_to_locate_someone/,0.0,0.0,,en
1109983,2012-06-02 18:58:34,statistics,Top 5 Albums to Play When Writing Stats Code,uh9q0,Professor_IR,1310512226.0,https://www.reddit.com/r/statistics/comments/uh9q0/top_5_albums_to_play_when_writing_stats_code/,3.0,4.0,,en
1109984,2012-06-02 22:20:38,rstats,Anyone going to UseR! 2012? Is it worth for a beginner?,uhisg,[deleted],,https://www.reddit.com/r/rstats/comments/uhisg/anyone_going_to_user_2012_is_it_worth_for_a/,11.0,2.0,,en
1109985,2012-06-03 09:32:48,artificial,Smartcraft: Starcraft as an AI Research Topic,uibdm,chras,1304420886.0,https://www.reddit.com/r/artificial/comments/uibdm/smartcraft_starcraft_as_an_ai_research_topic/,29.0,9.0,"This post seeks to explain why I think that the video game Broodwar is an interesting and fulfilling topic for AI research.

Starcraft: Broodwar is a Real-Time Strategy video game that was released in 1998 by Blizzard. The gameplay centres on three interstellar races, the human Terrans, the insectoid Zerg and psionic Protoss. Each match involves the acquisition of resources, the construction of bases and a military, and the conquest of the opposition. Matches take place on a variety of maps, include 2 to 8 players (human and computer-controlled), and typically last between 5 and 30 minutes. The game itself, and the sequel Starcraft 2, is played competitvely accross the globe, even today.

Problem Characteristics:

**Partially Observable** - The 'Fog of War' limits map visibility to the area surounding the player's units, allowing strategies to be prepared and executed in secret, placing a premium on information gathering.

**Multiagent, Adversarial** - The game involves more than just simple mechanics. To win, strategies must be employed that successfully exploit deficiencies in the opponents' strategy, and games often involve a series of counter attacks and skirmishes until one side is worn down.

**Stochastic** - Whilst most actions are deterministic, there are circumstances in the game that involve chance. This chance obeys known probabilities, meaning that actions and outcomes can be reasoned about with reliability.

**Sequential** - Decisions made early have lasting consequences. For instance, an early failed attack may leave the player at a long-term resource disadvantage.

**Dynamic** - The environment changes over the course of the game - resources are mined out, map artefacts are destroyed, and so on.

**Continuous** - Players can issue commands continuously, and in parallel. The game is truly real-time in that the balance between speed of action and depth of analysis must be managed, and multitasking is possible at both mechanical and strategic levels.

**Complex Environment** - Each race can produce many different units, each with their own strengths and weaknesses, and each unit has different abilities that can be employed to win. Different maps have characteristics that lend themselves to different strategies. The state space for the games is enormous; an exhasutive tree search is simply not possible with today's technology.

**Programmable** - A collection of very clever and dedicated people have written an API through which an agent can play the game, leaving the agent design to the prospective developer. Most importantly, there are very few limitations placed on the agent; most, if not all, languages and architectures are viable.

**Replays** - Many thousands of replays are available for programmatic analysis. That is, they can be loaded and interrogated interactively.

**Low System Requirements** - The game is computationally cheap to run, allowing the game and agent(s) to run quickly on commodity equipents.

**Accessible** - The game can be purchased electronically from Blizzard, and will run on Windows, Mac and Linux. Blizzard has not previously interfered in or prevented AI research based on Broodwar.

These characteristics make the Broodwar agent problem very deep, but also means that there is much overlap with real-world problems.

TLDR; Broodwar is an AI problem that is complex but approachable, and offers an avenue through which many AI advances could be made and applied.

Links:

[Broodwar API](http://code.google.com/p/bwapi/)

[Sample Broodwar Match](http://www.youtube.com/watch?v=CnXeOQVqxGU)

[ArsTechnica Article on Berkley's 2010 Agent ""Overmind""](http://arstechnica.com/gaming/2011/01/skynet-meets-the-swarm-how-the-berkeley-overmind-won-the-2010-starcraft-ai-competition/)",en
1109986,2012-06-03 12:59:18,MachineLearning,"Functional Programming as an alternative to Octave, R, Julia",uig6o,danielkorzekwa,1300550268.0,https://www.reddit.com/r/MachineLearning/comments/uig6o/functional_programming_as_an_alternative_to/,20.0,48.0,"While reading articles about pros and cons of programming languages for statisticians and researchers I never came across anyone, who would suggest Functional Programming instead of Octave or R and I'm trying to understand myself, whether it's because of Octave/R shine so much in the field of quickly prototyping some algorithms or maybe those, who compare Octave, R and Julia simply don't have any background in other types of programming.

Even Professor Andrew Ng from Stanford University (Machine Learning Class) suggests prototyping in Octave and then re-implementing in more general purpose programming languages such as C/C++ or Java for better performance. But how about using let's say Scala programming language for both prototyping and production implementations?

I'm asking this question, because I use Scala myself and I find it to be very good language for both prototyping and production ready systems. Most of the programming assignments for Probabilistic Graphical Models Course given by Professor Daphne Koller, I believe it would be much easier and faster to develop in Scala rather than in Octave, with the goodness of map, flatMap, reduce, prod and sum Scala functions, and an ability to use object oriented programming for data representation instead of putting everything into Octave matrices. (Scala is both object oriented and functional programming language).

I would like to hear some opinions here from someone with an experience in both Scala (or other functional/object oriented programming language) and R/Octave. I'm considering learning R myself or simply prototyping in Octave instead of still using Scala but I'm trying to find a good justification for it.

For this question I'm asking about the pure programming language and not the availability of some stats packages where R rocks.

Thank you for any comments.",en
1109987,2012-06-03 13:03:09,MachineLearning,"Using CRF to classify an entire sequence, not single elements",uig9e,Jackopo,1338716571.0,https://www.reddit.com/r/MachineLearning/comments/uig9e/using_crf_to_classify_an_entire_sequence_not/,4.0,7.0,"Hi all,

for my thesis I have to classify sequences of vectors (representing linguistic and sound features of spoken words) in a few number of classes (representing speech emotions), starting from some datasets of classified sequences.
Now, CRF models seem to work better than HMM for labeling tasks, so it would be interesting to use them to classify the entire sequence, not single tokens.

The problem is: how can I train and use a CRF model in order to classify the entire sequence of elements?
I understand that I could use a HMM for this, by creating different models for different emotions and calculating the likelihood of a sequence for each of them, but  how can I use a CRF to do so? Which library do you suggest (I use Java and Python, but other languages are welcome, too) ?
EDIT: punctuation ",en
1109988,2012-06-03 18:59:15,statistics,Abortion Statistics,uiow4,[deleted],,https://www.reddit.com/r/statistics/comments/uiow4/abortion_statistics/,0.0,4.0,,en
1109989,2012-06-03 22:33:22,MachineLearning,Explain LDA like I'm stupid!,uixqw,java_city,1325974638.0,https://www.reddit.com/r/MachineLearning/comments/uixqw/explain_lda_like_im_stupid/,22.0,18.0,"Well lets say I have to generate topics over a bunch of documents. First I assume [correct me if I'm wrong] that all the documents have same number of words *N* in them. Then the part after this is what I am confused on. After that we choose the topics labeling each of them with certain probabilistic weight? 
**Do we have to supply the topics to the LDA model?**

According to this [blog] (http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/) in the ""LDA Model"" section, it says 
&gt;Choose a topic mixture for the document (according to a Dirichlet distribution over a fixed set of K topics). For example, assuming that we have the two food and cute animal topics above, you might choose the document to consist of 1/3 food and 2/3 cute animals. 

It does not make sense to me, I always thought LDA would generate topics by itself. 

Please correct me if I am wrong anywhere. I would like to be clear about this. 

Thanks!",en
1109990,2012-06-03 22:38:48,statistics,Tough To Be A Newspaper These Days,uiy1d,JesusLovedPorn,1332336394.0,https://www.reddit.com/r/statistics/comments/uiy1d/tough_to_be_a_newspaper_these_days/,21.0,0.0,,en
1109991,2012-06-04 00:42:27,MachineLearning,newbie WEKA questions,uj476,WEKAnewb,1338758855.0,https://www.reddit.com/r/MachineLearning/comments/uj476/newbie_weka_questions/,5.0,11.0,"Hi everyone, sorry if this isn't the right forum for this, but I have some really newbie WEKA questions.

So i'm trying to process some raw data into the ARFF format so I can experiment on it. I figured i'd go whole-hog right from the start, so I downloaded the TREC spam dataset from 2008-2009 from university of waterloo, and used ubuntu linux's htmltotext converter to convert them all (75,000) to text files, and remove the html tags. 

My next step was try and use an old tool someone wrote in 2002 called TextDirectoryToArff , whose source can be found here: http://weka.wikispaces.com/ARFF+file...xt+Collections.

So I loaded it all up in eclipse, added the external weka package and it tells me that line 59: data.add(new Instance(1.0, newInst));

isn't valid, because Instance cannot be instantiated. 

My questions:

1. Is it worth even compiling TextDirectoryToArff, or am I misunderstanding how to go about converting raw text data into an arff file?

2. If this is the right tool to be using for the job, what am I doing wrong with the file?

Thanks in advance.",en
1109992,2012-06-04 03:35:42,statistics,"rotating, multi-tiered, multi-segmented pie chart. 
kill it with fire. who gives a shoe about the data, 
you can't see it anyway",ujcx5,jmdugan,1221770863.0,https://www.reddit.com/r/statistics/comments/ujcx5/rotating_multitiered_multisegmented_pie_chart/,1.0,0.0,,en
1109993,2012-06-04 05:42:04,statistics,Checking for biased dice,ujjnn,Nova_lis,1332466053.0,https://www.reddit.com/r/statistics/comments/ujjnn/checking_for_biased_dice/,1.0,6.0,"Hey everyone, I need to check for miscast dice based on the rolls. The reason I'm asking is one the players/dm in a d&amp;d campaign has a die that's rolling a suspicious amount of 20's. How can I go about checking this?",en
1109994,2012-06-04 13:58:37,MachineLearning,Coursera Machine Learning: Lecture Notes - missing no.5,uk11k,gavinb,1134709200.0,https://www.reddit.com/r/MachineLearning/comments/uk11k/coursera_machine_learning_lecture_notes_missing/,0.0,4.0,"I completed the ML Class when it was first offered, and downloaded all the course materials.  Except, as I just found out, I missed out on the slides called Lecture5.pdf.  I can't seem to get to the content pages any more as they have started a new round of classes.  Can anyone provide me with a copy? Thanks!",en
1109995,2012-06-04 14:35:27,MachineLearning,A geek with a hat » ml-class.org vs. real world ML class,uk1yp,Sandracto,1338809699.0,https://www.reddit.com/r/MachineLearning/comments/uk1yp/a_geek_with_a_hat_mlclassorg_vs_real_world_ml/,0.0,0.0,,en
1109996,2012-06-04 15:16:37,statistics,What are some good grad level (rigorous) texts on mathematical statistics?,uk33j,[deleted],,https://www.reddit.com/r/statistics/comments/uk33j/what_are_some_good_grad_level_rigorous_texts_on/,6.0,20.0,I'm interested in something that is equally as rigorous as it is insightful. It seems like these two aren't usually paired well,en
1109997,2012-06-04 15:55:59,MachineLearning,[ask ml] Machine Learning methos for GIS ?,uk4f1,perone,1232625557.0,https://www.reddit.com/r/MachineLearning/comments/uk4f1/ask_ml_machine_learning_methos_for_gis/,0.0,18.0,Does anyone know any method of ML for GIS ? Something that could be used for gps data for instance. Great thanks !,en
1109998,2012-06-04 17:22:39,AskStatistics,A question on retail customer service surveys,uk7s4,penguinfury,1309296701.0,https://www.reddit.com/r/AskStatistics/comments/uk7s4/a_question_on_retail_customer_service_surveys/,1.0,0.0,"Hi there r/askstatistics, I'm not sure if this is the best place for this, so if it's not, please tell me and I'll move it!

Anyway, I work for a large corporation that gives customers random surveys on receipts. They are asked a variety of questions and asked to rank us on a scale of 1-5. Now what the customer doesn't know is that anything below a 5 is counted as a zero. We are then scored based on these responses. 

This makes absolutely no sense to me. If we get a 70/100 for a given category, that's not actually representative of what the customers have scored us, so why bother?

Can anyone shed some light on this?",en
1109999,2012-06-04 21:21:47,MachineLearning,"Attempt at Naive bayes (bag of words model), using WEKA",ukkpm,WEKAnewb,1338758855.0,https://www.reddit.com/r/MachineLearning/comments/ukkpm/attempt_at_naive_bayes_bag_of_words_model_using/,6.0,2.0,"Thanks to some very helpful redditors yesterday, I was able to convert some emails into text, then into an arff file.   The two directories used as classes were orginally ""spam"" and ""notspam"" each of which contained a tiny (11 samples each) set of text files.

My goal is to use this tiny set to take some baby steps in running and understanding the software.  What I'd really like to do is run a naive bayes on this arff file, to produce a ""bag of words"" model and then try it on some test data.  

So my questions are:

1. Firstly, is there a tutorial for this specifically, if so, that would save us all a lot of time. My googling has not produced results, merely tutorials on the usage of weka on specific, pre-built data sets, which is not at all what i'm looking for.  In addition, they don't use the naive bayes either.  

2.  What are my next steps using this arff file?  Surely it has to be filtered and I have to make sure the features are correctly selected.   Is the arff file I've built going to be adequate for some very small short tests, just to see how it all works?

3. I noted when i was using textdirectoryloader that it specifies that UTF-8 is the way to go (and this is what I used to do the conversion, from the CLI), especially since my endgame will involve testing html or email files, with the header included, but the tags removed.  Do I need more preparation, like removing the headers themselves, or will this be adequate?

Thanks in advance for your time.",en
1110000,2012-06-04 23:15:45,rstats,RPubs: A New Web Publishing Service for R,ukrll,talgalili,1271226645.0,https://www.reddit.com/r/rstats/comments/ukrll/rpubs_a_new_web_publishing_service_for_r/,1.0,0.0,,en
1110001,2012-06-04 23:15:59,statistics,RPubs: A New Web Publishing Service for R [r-bloggers],ukrm0,talgalili,1271226645.0,https://www.reddit.com/r/statistics/comments/ukrm0/rpubs_a_new_web_publishing_service_for_r_rbloggers/,12.0,0.0,,en
1110002,2012-06-04 23:16:16,statistics,I hope you guys can help me :),ukrmm,blobbery_bob,1338840910.0,https://www.reddit.com/r/statistics/comments/ukrmm/i_hope_you_guys_can_help_me/,1.0,1.0,,en
1110003,2012-06-05 05:28:09,statistics,RPubs: A New Web Publishing Service for R Markdown from RStudio,uldsz,[deleted],,https://www.reddit.com/r/statistics/comments/uldsz/rpubs_a_new_web_publishing_service_for_r_markdown/,1.0,0.0,,en
1110004,2012-06-05 06:14:15,MachineLearning,Removing seasonality from time series data,ulgg8,pretz,1199946453.0,https://www.reddit.com/r/MachineLearning/comments/ulgg8/removing_seasonality_from_time_series_data/,14.0,9.0,"is there a general way of removing seasonality from time series data? Many of the methods I have read about seem fairly ad-hoc, I was wondering if there is a 'best practice' recommended way of doing it? especially if there are multiple levels i.e. daily,weekly and monthly seasonality in the same series.",en
1110005,2012-06-05 08:06:09,statistics,Simple recipes for predictive analysis of big data in R ,ulm5v,siganakis,1330656360.0,https://www.reddit.com/r/statistics/comments/ulm5v/simple_recipes_for_predictive_analysis_of_big/,21.0,7.0,,en
1110006,2012-06-05 17:25:20,statistics,Is anyone in r/stats looking to hire someone with biostats experience and an expertise in mental health? ,um3kf,drProfessionalname,1338906211.0,https://www.reddit.com/r/statistics/comments/um3kf/is_anyone_in_rstats_looking_to_hire_someone_with/,0.0,3.0,"I have a PhD in psychology and I'm generally an awesome person. I'm looking for something slightly outside of my field, but I'm not having any luck finding a position. My CV is available upon request. ",en
1110007,2012-06-05 17:39:14,AskStatistics,How to do comovement?,um46k,[deleted],,https://www.reddit.com/r/AskStatistics/comments/um46k/how_to_do_comovement/,1.0,0.0,"I'm interested in showing how variables in a time series affect one another.  The question that I'm currently interested in is comparing movements of government receipts (total tax money received by the government) and national gdp, wondering if there is a relationship, or a lagged relationship.  I've seen 1 or 2 papers on a topic called time series correlation, and many papers on comovement.  All of these papers are quite difficult for me to read the actual nuts and bolts of their models.  Are their any introductory materials on the topic of comovement?",en
1110008,2012-06-05 18:21:18,data,Good data presentation software,um695,instereo911,1313855509.0,https://www.reddit.com/r/data/comments/um695/good_data_presentation_software/,1.0,0.0,"Hi All,
Kind of stuck on where to look and hoping someone can help. I am looking for an effective way/program(s) of presenting data. For example: I have a set of data and want to bring it out at monthly level, in a pretty format.
Right now, due to enterprise software, I am using Excel ('prettying' it up graphs, tables ) and bringing that into MS Project. This is the most efficient way because I can easily update the backend data allowing the graphs, tables to update on MS Project.
I want to accomplish the same efficiency with updating it, but not using crap programs.
Any ideas? Thanks Reddit",en
1110009,2012-06-05 18:56:21,statistics,"Is there a text, or series of texts, that reviews everything covered up to and inside undergraduate statistics?",um81s,fourhalfmats,1338909436.0,https://www.reddit.com/r/statistics/comments/um81s/is_there_a_text_or_series_of_texts_that_reviews/,17.0,9.0,Sort of like [this](http://www.amazon.com/All-Mathematics-You-Missed-Graduate/dp/0521797071) except for stats.,en
1110010,2012-06-05 21:39:13,statistics,Breaking the SQL query from hell in R?,umh78,[deleted],,https://www.reddit.com/r/statistics/comments/umh78/breaking_the_sql_query_from_hell_in_r/,8.0,3.0,"Hi,

I'm using the RODBC package to send SQL queries over to a transact-SQL server with relatively complicated merging, sorting, and whatnot. 

More than once, I've done something ranging from ""kinda"" to ""horrifically"" stupid and R is completely locked up in trying to complete the query. I can kill Rstudio using tskill but I'm not sure what happens to the SQL connection. Does it continue running and return results to an empty socket? What's the proper way to cancel the query if there is one?",en
1110011,2012-06-05 22:19:31,statistics,ADMD-IDE Install help for Mac (emacs),umjjv,tenurestudent,1337720339.0,https://www.reddit.com/r/statistics/comments/umjjv/admdide_install_help_for_mac_emacs/,2.0,1.0,"I'm working through Millar's Maximum likelihood Estimation [textbook](http://www.amazon.com/Maximum-Likelihood-Estimation-Inference-Statistics/dp/0470094826/ref=sr_1_2?s=books&amp;ie=UTF8&amp;qid=1338922868&amp;sr=1-2) in a graduate class. We are using the admb-mode of emacs for some of the examples, and I can't make heads or tails of getting this installed on my mac.  
I have to install the following:
1) ADMB  - done
2) GNU
3) GCC
4) GDB

I need help with steps 2-4, I can't figure out how to install them. The [ADMB manual](http://www.admb-project.org/tools/admb-ide/manual/view) has links to ftp servers for GUN, GCC, and GDB but I'm not sure what to download, and then how to install them? I grew up on PC, but made the mac switch a few years back. I'm not overly familiar with how to install programs that aren't just dropped in the applications folder or do it on their own.

Is anyone familiar with this process or can help out??
THANKS!
",en
1110012,2012-06-06 05:33:07,MachineLearning,"In my opinion, only a handful of bands in history deserve a compilation that includes the words, ""The Very Best of"" in their title (to this day it is ",un8ns,mosriafondre,1338949783.0,https://www.reddit.com/r/MachineLearning/comments/un8ns/in_my_opinion_only_a_handful_of_bands_in_history/,1.0,0.0,,en
1110013,2012-06-06 07:02:12,MachineLearning,"Woah, woah, woah... for all of those who dismiss this CD because these songs are already available elsewhere, or because it doesn't contain certain tr",unduh,nigggacarsru,1338955259.0,https://www.reddit.com/r/MachineLearning/comments/unduh/woah_woah_woah_for_all_of_those_who_dismiss_this/,1.0,0.0,,en
1110014,2012-06-06 07:44:58,rstats,"I got the new ""Save the Turtles"" collection last weekend. I thought the remastering sounded great! It's hard to believe that all those songs are rough",ung0d,bildazeposs,1338957705.0,https://www.reddit.com/r/rstats/comments/ung0d/i_got_the_new_save_the_turtles_collection_last/,1.0,0.0,,en
1110015,2012-06-06 08:45:35,statistics,Should I be concerned about this residual plot (after linear regression)?,unisy,squib28,1318209922.0,https://www.reddit.com/r/statistics/comments/unisy/should_i_be_concerned_about_this_residual_plot/,2.0,12.0,"Hi,
I have done a multiple linear regression predicting children's score on a particular test, with the answers score ranging from 0-100. 
The model seems to be working well, but the residual plot looks like this:
[http://imgur.com/iNYGC](http://imgur.com/iNYGC)


From what I know the pattern of residuals should have no pattern at all, and be randomly scattered around the zero line. However this looks like an obvious pattern. But the pattern seems to be because of a ceiling and floor effect directly related to the data. 
For example a child scoring 80 cannot have a residual of greater than +20, whereas a child scoring 20 cannot have a residual less than -20?

Does that logic make sense? Should I be concerned about this model?
Any advice would be much appreciated.
Thanks!",en
1110016,2012-06-06 10:34:52,statistics,Bayes rule Notation question.,unmrx,wtf_ftw,1240271200.0,https://www.reddit.com/r/statistics/comments/unmrx/bayes_rule_notation_question/,4.0,3.0,"I have always seen Bayes rule written as: P(A|B)=[P(B|A)*P(A)]/P(B)

I was looking it up on Wikipeda to refresh my memory and found the notation quite confusing.   [Here](http://en.wikipedia.org/wiki/Bayes_rule#The_rule) is the article.  

What kind of notation is this?

Incidentally, the article for [Bayes Theorem](http://en.wikipedia.org/wiki/Bayes%27_theorem) is written with the standard notation.",en
1110017,2012-06-06 12:12:18,statistics,Effect of treatment on control group,unpb3,[deleted],,https://www.reddit.com/r/statistics/comments/unpb3/effect_of_treatment_on_control_group/,1.0,0.0,"Yes, this could be seen as a topic for homeworkhelp, but..


i posted this in [/r/homeworkhelp](http://www.reddit.com/r/HomeworkHelp/comments/um6zx/university_statistics_withinsubject_difference/), but it seems they can't really help me there, as it's been some time without anyone answering. 
An explanation could be, that my description is to confusing, but i hope i laid it out as well as possible. Furthermore, it's not actually homework, but something i'm working at. 


So i come here, to the more statistically inclined people of /r/statistics. 

I don't want to give too much about this away, as i'm working with sensible data. As such, scenario and some values have been changed. 
I'm looking into a problem, where i want to draw conclusions about the effects of two treatments. All participants had the sickness at beginning of the treatment and were healed afterwards(no more sickness). They're expected to relapse, but the severity of relapse is expected to be different among the conditions. 
The N is 105 with 30 in treatment A and 70 in treatment B. 

**Dataset:**

Independent variables:

Between-Subject Factor: condition(treatment A, treatmen B)
(Randomly assigned, groups are not equal in size)

Between-Subject Factor: Various demographic data(age, sex, marital status, no. of children, etc)


Dependent variable:

Both within-subject:
sickness1: Severity of sickness five years after the treatment
sickness2: 15 years after the treatment

(I have the actual severity as a number, but a relapse at all is already an important outcome for me. As such i could transcode it into a binary variable of: relapse/no relapse, after 5/15 years.)

**The goal:**


The goal is to find out whether both groups differ in their rate of relapse. I want to use an analysis that has the greatest power for this effect, while no assumptions are violated. 

**What i tried:**
* RM-ANOVA, with condition as IV and both severities of sickness as within subject outcomes at two levels(5 years, 15 years). I also took in a few of the relevant demographical variables as interactions to control for them, but none turned out significant. 

* ANCOVA with condition as IV(Independent variable) and sickness2 as DV, while sickness1 served as a covariate. 1

I tried these, but i'm not sure, whether they have sufficient power. Furthermore i would like to hear some ideas for useful analyses i could use on this kind of data and question. 

I'm working with SPSS 19 and would rather keep using it. 

**Prio experience: **

* various General Linear Models

* Generalized Linear Models 

* crosstabs, correlations

* t-tests

* Linear regression 

* various other analyses


If you think, this has no place in /r/statistics, then i understand and will humbly accept a downvote. I would however be very greatful for any assistance you could provide, even by pm! :)",en
1110018,2012-06-06 12:16:07,computervision,Use Euclidian distance with Mallat's Scattering transform (translation and rotation invariant implementations available),unpdd,[deleted],,https://www.reddit.com/r/computervision/comments/unpdd/use_euclidian_distance_with_mallats_scattering/,5.0,0.0,,en
1110019,2012-06-06 12:29:59,statistics,Exploring the nature of treatment outcomes,unpo9,felixbrix,1338974952.0,https://www.reddit.com/r/statistics/comments/unpo9/exploring_the_nature_of_treatment_outcomes/,8.0,2.0,"Yes, this could be seen as a topic for homeworkhelp, but..


i posted this in [/r/homeworkhelp](http://www.reddit.com/r/HomeworkHelp/comments/um6zx/university_statistics_withinsubject_difference/), but it seems they can't really help me there, as it's been some time without anyone answering. 
An explanation could be, that my description is to confusing, but i hope i laid it out as well as possible. Furthermore, it's not actually homework, but something i'm working at. 


So i come here, to the more statistically inclined people of /r/statistics. 

I don't want to give too much about this away, as i'm working with sensible data. As such, scenario and some values have been changed. 
I'm looking into a problem, where i want to draw conclusions about the effects of two treatments. All participants had the sickness at beginning of the treatment and were healed afterwards(no more sickness). They're expected to relapse, but the severity of relapse is expected to be different among the conditions. 
The N is 105 with 30 in treatment A and 70 in treatment B. 

**Dataset:**

Independent variables:

Between-Subject Factor: condition(treatment A, treatmen B)
(Randomly assigned, groups are not equal in size)

Between-Subject Factor: Various demographic data(age, sex, marital status, no. of children, etc)


Dependent variable:

Both within-subject:
sickness1: Severity of sickness five years after the treatment
sickness2: 15 years after the treatment

(I have the actual severity as a number, but a relapse at all is already an important outcome for me. As such i could transcode it into a binary variable of: relapse/no relapse, after 5/15 years.)

**The goal:**


* The goal is to find out whether both groups differ in their rate of relapse. 
* I'm also interested in the effect of the several demographic variables on the outcome. Are women likelier to relapse then men?
* I want to use an analysis that has the greatest power for this effect, while no assumptions are violated. 

**What i tried:**
* RM-ANOVA, with condition as IV and both severities of sickness as within subject outcomes at two levels(5 years, 15 years). I also took in a few of the relevant demographical variables as interactions to control for them, but none turned out significant. 

* ANCOVA with condition as IV(Independent variable) and sickness2 as DV, while sickness1 served as a covariate. 1

**edit: ** 
The IVs aren't normally distributed, but rather zero-inflated. As such, i've arrived at the conclusion that a Mann-Whitney U test or a Generalized Linear Model(using a Negative linear distribution with a log link) might be useful. Are there reasons to prefer one over the other?

I tried these, but i'm not sure, whether they have sufficient power. Furthermore i would like to hear some ideas for useful analyses i could use on this kind of data and question. 

I'm working with SPSS 19 and would rather keep using it. 

**Prio experience: **

* various General Linear Models

* Generalized Linear Models 

* crosstabs, correlations

* t-tests

* Linear regression 

* various other analyses


If you think, this has no place in /r/statistics, then i understand and will humbly accept a downvote. I would however be very greatful for any assistance you could provide, even by pm! :)

",en
1110020,2012-06-06 18:43:19,statistics,"I will have a job interview at a small-sized (under 10 employees) market research company. Although I have work-experience in data analyzing, that project was in public sector. Could you give me some ideas what kind of questions I can expect (...) ?",uo2ve,erikhun,1280500725.0,https://www.reddit.com/r/statistics/comments/uo2ve/i_will_have_a_job_interview_at_a_smallsized_under/,1.0,1.0,"They are doing nothing too surprising according to their website (there was no job ad, I just applied out from nowhere): consumer attitude, collecting infos for branding, exploring business needs, measuring sales efficiency, satisfaction or loyalty. Their approach are both qualitative and quantitative, mentioning focus groups, mini and process (?) interviews, perception and usage research, segmentation.

Their publications uploaded though all deal with banking and finance sector, they also claim to develop a so-called ""Bankindex"" (own methodology - asking 1000 respondents bi-monthly) which monitors the bank sector from the customers point of view.

So yeah, thanks for reading and any ideas!",en
1110021,2012-06-06 19:24:40,artificial,"SOAR founder on human memory, design, and prospects:  ""... the locus of decision-making is selecting the next operator""",uo50d,claird,1249749559.0,https://www.reddit.com/r/artificial/comments/uo50d/soar_founder_on_human_memory_design_and_prospects/,3.0,1.0,,en
1110022,2012-06-06 19:52:52,artificial,ANN libraries and development environments,uo6kq,[deleted],,https://www.reddit.com/r/artificial/comments/uo6kq/ann_libraries_and_development_environments/,7.0,7.0,"What are the best ANN libraries and development environments out there? I'd like to hear reviews from people who have actually used them: the sort of projects they used them for, and their pros and cons.

Edit: by ANN I mean Artificial Neural Networks.",en
1110023,2012-06-06 20:07:17,statistics,Who Deletes Wikipedia?,uo7cs,[deleted],,https://www.reddit.com/r/statistics/comments/uo7cs/who_deletes_wikipedia/,1.0,0.0,,en
1110024,2012-06-06 20:46:39,statistics,"Trying to do a single pass, sequential, non random-access (rough) percentile calculator.  I have a prototype, but am trying to (1) see if there is a better way and/or (2) try to bound the error in my calculation. ( O(n) time and O(1) space)",uo9ij,jimktrains,,https://www.reddit.com/r/statistics/comments/uo9ij/trying_to_do_a_single_pass_sequential_non/,5.0,6.0,"https://gist.github.com/2883468 is my code.  No, this is not a homework problem.

The basic idea of the code is to basically take the upper (exclusive) and lower (inclusive) bounds and create 10 buckets between them.  When the bounds change, we ""merge"" the old buckets and new buckets (lines 34 to 50).  The way I'm currently doing this is by assuming a flat distribution of numbers in each range (lines 42 to 48), which is wrong.

A naive way to calculate the error would be to assume that I'm always wrong and assume my split added everything incorrectly (i.e.: the max error for bucket would be the sum of all the split bucket parts put into it).

I haven't been able to find any algorithm that does what I want (or an affirmative impossible or possible) online.  Any help with either telling me there is a better way or helping me figure out how to calculate my error would be helpful.

EDIT: Changing line 44 to a round instead of a floor gives better results for obvious reasons",en
1110025,2012-06-06 21:35:16,analytics,Data Analytics: IT’s Ronco Veg-o-Matic for Data,uocb6,wordsmithie,1332867049.0,https://www.reddit.com/r/analytics/comments/uocb6/data_analytics_its_ronco_vegomatic_for_data/,1.0,0.0,,en
1110026,2012-06-07 00:05:58,AskStatistics,A (hopefully) simple probability calculation,uolia,daSMRThomer,1326243709.0,https://www.reddit.com/r/AskStatistics/comments/uolia/a_hopefully_simple_probability_calculation/,0.0,0.0,"Hi guys, I need help calculating what the expected value would be for a random variable ""sum"". I'm rolling 4 six-sided dice and throwing out the lowest value (or one of several tied-for-lowest values) and then adding the remaining three together to make a sum. What's the average/expected value of the sum?

Example: thrown dice read 2, 2, 5, 6. Throw out one of the 2s and add to get sum = 2 + 5 + 6 = 13.",en
1110027,2012-06-07 02:34:44,MachineLearning,Method for summarizing short descriptions,uou3w,norkakn,1183304595.0,https://www.reddit.com/r/MachineLearning/comments/uou3w/method_for_summarizing_short_descriptions/,6.0,2.0,"Hi ML,

Are there any methods or products that do a decent job of taking a list of short descriptions, and summarizing them?

The descriptions are of proteins, and some examples are:

*  Chromosome segregation protein SMC
* Condensin subunit SMC
* Chromosome segregation protein (Smc1)
*  Putative chromosome segregation protein, SMC ATPase superfamily
*  Condensin subunit Smc
*  SMC proteins Flexible Hinge Domain

Other than just the collection of text, I have information on the likely quality of the description, and I have collections of things that should be similar.  (So I can say that Apoptotic peptidase activating factor 1 and APAF1 are both good names and mean roughly the same thing).",en
1110028,2012-06-07 02:35:26,statistics,"This CD is soothing. Soooo soothing. TOO soothing! The highlight song is a wonderful rendition of ""How Great Is Our God"" by Bethany Dillon. What a voi",uou58,prathcoremar,1339025667.0,https://www.reddit.com/r/statistics/comments/uou58/this_cd_is_soothing_soooo_soothing_too_soothing/,1.0,0.0,,en
1110029,2012-06-07 02:37:50,AskStatistics,Table Describing Appropriate Statistical Tests in a Variety of Situations,uouaw,chucko326,1290533115.0,https://www.reddit.com/r/AskStatistics/comments/uouaw/table_describing_appropriate_statistical_tests_in/,17.0,2.0,,en
1110030,2012-06-07 03:52:26,statistics,Preforming a hypothesis test on proportions from the same population ,uoyby,toids,1339027678.0,https://www.reddit.com/r/statistics/comments/uoyby/preforming_a_hypothesis_test_on_proportions_from/,1.0,2.0,"I have a random sample **N** where multiple the samples can be divided into three sets (**A**,**B**,**C**).  

Generally:
* **N** is the sample size and can be small.
* **A** is large proportion of **N**
* **B**, **C** are a small proportion of **N**

I want a p-value for the proportion of **A** being greater than the proportion of **B** and **C**.  

I am thinking of doing hypothesis tests on H_0: p_a = p_b vs H_: p_a &gt; p_b and the same for **C**.

Since **N** is small I have read that a Fischer exact test can be used if the proportions are from independent samples, but the proportions are from the same sample.  Should I still use a Fischer test?  Something like:
a = |**A**|
b = |**B**|
n = |**N**|
p = sum_{i=a..(a+b)} [  (n choose i ) ( n-i choose (a+b)-i ) / (n choose a+b)  ]

This is not a homework assignment but a curiosity on how to preform either two proportion hypothesis tests on the same sample or proportion tests on parts of a multinomial from the same sample.  In the end, I want to know if I can say the largest proportion in a multinomial is greater than any one of the other proportion with a confidence score.",en
1110031,2012-06-07 05:42:38,statistics,clustering of longitudinal data,up4h0,[deleted],,https://www.reddit.com/r/statistics/comments/up4h0/clustering_of_longitudinal_data/,3.0,12.0,"Hi guys  

I'm modeling college student retention data. Data collected longitudinally include GPA and hours taken in a term, events include things like major changes, loss of financial aid, graduation/dropout. My thought is that clustering might help to find patterns among these variables.  

I've found event history analysis/survival analysis, and I'm familiar with clustering methods, but am having trouble finding material on clustering of longitudinal data and wonder if you can point me to what's out there. AFAIK longitudinal clustering may be part of the standard repertoire of modeling, I just don't know about it.  

Focusing on the longitudinal variables (i.e. ignoring SAT, gender, etc.), I would think to create a spline of a variable in question over time, and the distance between two students for that measure would be the area between the two curves.  

Alternatively, one could turn longitudinal measures into static variables, like a variable for 'How long did the student last before dropping out?', 'Did the student ever have less than a 2.0 GPA in the 1st year', and use those in a distance function. Are these bad ideas?  

Thanks!",en
1110032,2012-06-07 06:53:41,datasets,"Reddit, is there a source where you can see statistics on reported crimes over the years? (X-Post from Askreddit)",up873,[deleted],,https://www.reddit.com/r/datasets/comments/up873/reddit_is_there_a_source_where_you_can_see/,3.0,2.0,,en
1110033,2012-06-07 13:09:19,MachineLearning,What is Faster than Moore's Law and Why You Should Care,upkjg,[deleted],,https://www.reddit.com/r/MachineLearning/comments/upkjg/what_is_faster_than_moores_law_and_why_you_should/,21.0,2.0,,en
1110034,2012-06-07 15:51:04,statistics,Free Intro to Statistics course being taught by Google Fellow Sebastian Thrun,upote,lenwood,1191289262.0,https://www.reddit.com/r/statistics/comments/upote/free_intro_to_statistics_course_being_taught_by/,27.0,2.0,,en
1110035,2012-06-07 16:17:54,computervision,"Adaptive Manifolds for Real-Time High-Dimensional Filtering
      - YouTube",uppr9,aboeing,1178181606.0,https://www.reddit.com/r/computervision/comments/uppr9/adaptive_manifolds_for_realtime_highdimensional/,22.0,3.0,,en
1110036,2012-06-07 18:18:12,statistics,How are estimates made for frequency when reports are rarer than occurrence?,upv9d,TheSaddestGrape,1319759313.0,https://www.reddit.com/r/statistics/comments/upv9d/how_are_estimates_made_for_frequency_when_reports/,6.0,2.0,"To clarify, I was thinking about this subject in particular:

In the discussion about gender equality and related discussion an often-mentioned subject is on false reports of rape and the seriousness of these cases. Feminists and others counter that statistics show that these false reports are much less common than what is sometimes implied.

On a side note: First of all, I haven't checked on actual results to verify anything from either side. It's not terribly relevant to my question but I suppose if someone had something to add in particular it might be interesting. Secondly, I know this example touches on a delicate subject but my goal is not to raise controversy but to gain understanding.

**So, on to my question:** Can anyone explain, in fairly understandable terms, the kind of methods that can lead you to an estimate of, like in this case, how many people are lying about something or how frequently similar things happen when reports are likely only occasional.

I'm a recent math graduate, just so you have some idea about where I'm standing.",en
1110037,2012-06-07 19:26:45,statistics,Binomial test vs. t-test for discriminating if a preference exists,upyqr,Pseudo_Scientist,1321600329.0,https://www.reddit.com/r/statistics/comments/upyqr/binomial_test_vs_ttest_for_discriminating_if_a/,4.0,4.0,"Hi there r/statistics!


My boss has me analyzing some preliminary data for an experiment where an animal makes a choice between two stimuli. There are 60 trials per session and multiple sessions. One of my jobs is to see if the animals develop a clear preference between the two stimuli. 

Each choice is coded as a 0 or a 1 depending on which stimuli the animal chooses. I want to use a binomial test with a test proportion of 0.50, while my boss wants me to take the mean of each session and then run a t-test across all days testing against H0: the mean is equal to 0.50. 

Am I right in wanting to do the binomial test? Are they both valid ways of approaching the question?

I know this is pretty basic stuff, so thanks for the help!",en
1110038,2012-06-07 19:32:30,datascience,What data can and cannot do | Open Knowledge Foundation Blog,upz1s,mhermans,1169219262.0,https://www.reddit.com/r/datascience/comments/upz1s/what_data_can_and_cannot_do_open_knowledge/,3.0,0.0,,en
1110039,2012-06-07 20:00:35,MachineLearning,"TIL howto Moneyball :-) build a Classifier to predict labels . [Practical Machine Learning in Python, intro to toolkits]
",uq0jx,ursvp,1339011843.0,https://www.reddit.com/r/MachineLearning/comments/uq0jx/til_howto_moneyball_build_a_classifier_to_predict/,42.0,3.0,,en
1110040,2012-06-07 20:26:11,statistics,Dimensionality reduction of event related EMG recordings,uq20w,simmmons,1285000309.0,https://www.reddit.com/r/statistics/comments/uq20w/dimensionality_reduction_of_event_related_emg/,2.0,12.0,"I am looking for a method to identify components of reflexes recorded electromyographically. I posted a detailed description already on stack exchange [link](http://stats.stackexchange.com/questions/29916/method-to-find-principal-components-of-emg-recordings-of-reflexes) but I didn't seem to get a response.

Non-negative matrix factorization seems to be the optimal method but is constrained for non-negativ input, or input of the same sign. My question is, can I offset my data to be non-negativ or will this influence the result?

What other (additive-)methods are available that take negative as well as positive input and don't allow negative factor loadings?

Thanks in advance",en
1110041,2012-06-07 21:33:36,MachineLearning,"I don't know if this is the correct 
subreddit, but still...",uq61x,[deleted],,https://www.reddit.com/r/MachineLearning/comments/uq61x/i_dont_know_if_this_is_the_correct_subreddit_but/,0.0,1.0,"I was wondering if anyone has any links for learning programming from a stractch- I checked youtube and googled stuff, but even I know you can't learn any 'serious' programming.
All help appreciated.",en
1110042,2012-06-07 22:30:54,MachineLearning,Machine learning for Ruby,uq9al,beaucronin,1300477015.0,https://www.reddit.com/r/MachineLearning/comments/uq9al/machine_learning_for_ruby/,0.0,0.0,,en
1110043,2012-06-07 22:33:42,statistics,"Quick question: Can you run an ANOVA, if the standard deviation of one group equals zero? ",uq9fz,Iamnotanorange,1268153982.0,https://www.reddit.com/r/statistics/comments/uq9fz/quick_question_can_you_run_an_anova_if_the/,1.0,15.0,Can you do anything to correct for this? I've never run into this problem in the real world before. ,en
1110044,2012-06-07 22:44:57,statistics,Misleading Statistics,uqa6u,highpockets79,1259032260.0,https://www.reddit.com/r/statistics/comments/uqa6u/misleading_statistics/,1.0,4.0,"Hi r/statistics!  Was hoping you good people here would have some suggestions for a good example of misleading data that I could use for an upcoming assignment... I would love to find something really cool/intersting, possibly related to  Cosmology or Evolutionary Biology, but am open to all ideas.  Thanks!!!

Assignment Details:
 Students are to select a distorted/ misleading graph or statistical presentation from a publication, such as a newspaper or magazine, or a new media source, such as a website or television program and to write a 2-3 page essay regarding how the statistic is misleading/ distorted. Successful papers will both identify what elements of the statistical presentation contribute to its inaccuracy and 
suggest what statistical parameters (such as sample, survey, timeline, etc.) would better serve the topic at hand. You are only expected to use your text and class lecture notes for this assignment, however outside research is always welcome, although not necessary. You are to provide a copy of the statistic 
(be it in a copy of the article or in a link to the post, etc.) along with your paper. 
",en
1110045,2012-06-08 04:36:00,statistics,Which is the independent variable if you have height and weight?,uqtsd,SOwED,1330899356.0,https://www.reddit.com/r/statistics/comments/uqtsd/which_is_the_independent_variable_if_you_have/,0.0,5.0,"This is a very simple question.
My girlfriend has a short stats project due tomorrow. Part of it requires a scatterplot to be made between height and weight.  She and I agreed that height is the independent variable and that it should be plotted along the x-axis.  She was studying with some other people in the class and they all said that weight should be on the x-axis.
The problem statement says 

&gt;Check for the correlation between height and weight. Find the equation of the line of best fit. Make a scatterplot and graph the line of best fit.

The naysayers say that they are testing against weight, which never comes up in any part of the project.
Height is the independent variable, right?

TL;DR: Height is the independent variable between height and weight, right?",en
1110046,2012-06-08 04:55:00,datasets,"For Sixpence fans that rode the wave before and beyond the ""Kiss Me"" bandwagon, some glaring ommissions from this album will leave us scratching our h",uquuc,unanvlogan,1339120396.0,https://www.reddit.com/r/datasets/comments/uquuc/for_sixpence_fans_that_rode_the_wave_before_and/,1.0,0.0,,en
1110047,2012-06-08 13:55:41,statistics,Use of Bayes Factors as a test statistic in hypothesis testing?,ureip,SkepticalEmpiricist,1285372750.0,https://www.reddit.com/r/statistics/comments/ureip/use_of_bayes_factors_as_a_test_statistic_in/,4.0,9.0,"I'm not impressed with the test statistics that are often used in hypothesis testing - even the classic examples based on normally distributed data.  Imagine two competing hypotheses (H0 and H1), and we have a test statistic.  The test statistic is usually something simple based on the sample mean.  Typically, the p-value is calculated only for the null (H0).  But it can also be calculated for the alternative (H1); i.e. ""how strange is this data, via the test statistic, under H1?""

The interesting thing is that, when the p-value is low for H0 it is also often low for H1.  Therefore, a low p-value doesn't decisively move your opinion from H0 towards H1.  In my opinion, It's not wise to 'reject' hypothesis H0 in favour of H1 unless you can show that the data looks better under H1 than it does under H0.  The data sometimes looks pretty unlikely under both hypotheses!

The root cause is badly-designed test statistics.  Consider normally distributed data with known variance.  The two hypotheses are:

* H0: the mean is 0
* H1: the mean is nonzero

A typical test statistic here is the sample mean.  This simple model and test statistic is considered in a 1987 paper by [Berger &amp; Sellke](http://scholar.google.com/scholar?hl=en&amp;q=The+Irreconcilability+of+P+Values+and+Evidence&amp;btnG=&amp;as_sdt=1%2C5&amp;as_sdtp=), where p-values are shown to be problematic.

As far as I can see, the answer to this is to construct the test statistic based on the probability, under the two hypotheses, of the observed data.  This is called the Bayes Factor(*).  The test statistic should be

    P(x | H0)  /   P(x | H1)

where x is the sample of data that has been observed.  If this value is very low, then the null can be rejected.  P-values can, in the standard way, be calculated by considering random data drawn from H0 and calculating how this test statistic is distributed.

Assuming a statistician that likes 'standard' hypothesis testing, wouldn't this be a better way to design your test statistic?

*  I'm not really asking ""how would *you* do hypothesis testing?""  That could turn into a rambling philosophical debate.
*  I'm asking: ""If one is (voluntarily/unvoluntarily) tied to doing hypothesis testing with test statistics and p-values, how does one design a test statistic? Ideally, you want a test statistic that is well-designed to leverage the difference between the two hypotheses.
*  A low p-value shouldn't just mean ""The data looks weird under the null"".  It should mean ""The data looks weirder under the null than it does under the alternate"".
*  Has this been done before? I'm pretty sure it has been, but I don't see much evidence of solid progress being made.
*  I think I'm trying to ""beat standard p-values *at their own game*"".

I'm aware that (conventionally) ""the alternate hypothesis"" isn't a single hypothesis, but is instead something vague like ""the mean is nonzero"".  We can handle this by (arbitrarily) selecting the alternate hypothesis which best fits the observed data and then calculating the Bayes Factor accordingly.  For example, this might mean assuming, for the purpose of calculating the test statistic, that the mean is equal to the sample mean  Remember, the choice of test statistic is essentially and arbitrary heuristic, so we are allowed to cut corners a little.

I'm thinking of doing a proper paper on this, I just want to check I'm not mad and that I'm not reinventing the wheel.

(*)  You don't need to be a Bayesian to like the Bayes Factor.  This is not a philosophical post.  This is a technical choice as the Bayes Factor is a test statistic that can do a good job of pitting the two hypotheses directly against each other.",en
1110048,2012-06-08 17:35:38,statistics,SPSS: How do you recode various variables with different values?,urlzk,[deleted],,https://www.reddit.com/r/statistics/comments/urlzk/spss_how_do_you_recode_various_variables_with/,0.0,0.0,"This may seem like a noob question but I'm kinda stuck...

For my bachelor's thesis I took different mini questionaires and made one big questionnaire out of them. Now I have to recode some reversed variables but the range of values vary between the different questionnaires.
Now here's the problem: If I'm trying to ""recode into different variables"" and I check variables with different values (i.e.

test1_1r: values 1 2 3 4 5

test2_1r: values 1 2 3 4

...)

I can't choose different ""old and new values"" for them.
I would need:

test1_1r --&gt; test1_1new: 1 -&gt; 5, 2 -&gt; 4, 3 -&gt; 3, 4 -&gt; 2, 5 -&gt; 1

test2_1r --&gt; test2_1new: 1 -&gt; 4, 2 -&gt; 3, 3 -&gt; 2, 4 -&gt; 1)

If I try to do them one at a time and open the window again the variables I transformed before are still there. 

Can anybody help me with this? I'm clueless...  ._.",en
1110049,2012-06-08 17:49:25,MachineLearning,Nobody Cares About You and Your Algorithm,urmlz,[deleted],,https://www.reddit.com/r/MachineLearning/comments/urmlz/nobody_cares_about_you_and_your_algorithm/,0.0,0.0,,en
1110050,2012-06-08 18:05:19,statistics,SPSS: Merging based on 1 identifier and a date range,urnd5,maak_d,1331477359.0,https://www.reddit.com/r/statistics/comments/urnd5/spss_merging_based_on_1_identifier_and_a_date/,1.0,5.0,"I have two datasets I would like to merge. I have a unique identifier for each individual in both datasets, but the individual can be in either  dataset multiple times. I also have some start-dates and end-dates in both datasets. 

I would like to merge the data based on the identifier and the proximity between the start and end dates in the data. For example, ID# 001 in dataset1 matched with ID#001 in dataset two where the difference between the start-date in dataset2 is within +/- 30 days of the end-date in dataset1. 

Is this possible in SPSS? I used to do this in SAS using Proc SQL but I no longer have access to SAS. ",en
1110051,2012-06-08 18:07:34,statistics,SPSS: Repeated Measures MANCOVA?,urngy,Brain_Doc82,1304194023.0,https://www.reddit.com/r/statistics/comments/urngy/spss_repeated_measures_mancova/,1.0,4.0,"I'm having a somewhat brainless day apparently.  I'm certain that I've previously found a way to set up the repeated measures function of a general linear model to work with multiple variables, as opposed to running a host of individual ANOVAs.  For some reason, I can't remember how to do it now.  Do I set up my within factor as ""Time"" and then define the repeated measures, or do I set up the within factor as the repeated measures?  Any help is greatly appreciated!!",en
1110052,2012-06-08 18:24:21,statistics,Intro to Customer Analytics,urobc,djent_illini,1312414426.0,https://www.reddit.com/r/statistics/comments/urobc/intro_to_customer_analytics/,2.0,1.0,"Hi everyone,

I have been doing some customer analytics for work and I was wondering if there is any tutorials online to learn more about customer analytics. All I have is this book called ""Data Analysis Using SQL and Excel"" which is great for learning fundamentals but I program mostly in R.

Can anyone provide me with useful resources to help me learn more about this field?

Thank you",en
1110053,2012-06-08 19:39:02,statistics,Simulating the Birthday Problem with data derived probabilities,urs4j,Professor_IR,1310512226.0,https://www.reddit.com/r/statistics/comments/urs4j/simulating_the_birthday_problem_with_data_derived/,4.0,2.0,,en
1110054,2012-06-08 20:00:12,statistics,"The ""HARM"" model for predicting health outcomes, good article on Bayesian prediction, bad acronym [pdf]",urt9s,[deleted],,https://www.reddit.com/r/statistics/comments/urt9s/the_harm_model_for_predicting_health_outcomes/,3.0,1.0,,en
1110055,2012-06-08 20:04:52,analytics,Track Yahoo Marketing &amp; Microsoft adCenter in Analytics,urti6,praroop,1274180434.0,https://www.reddit.com/r/analytics/comments/urti6/track_yahoo_marketing_microsoft_adcenter_in/,3.0,0.0,,en
1110056,2012-06-09 10:38:20,statistics,What search terms are used to find statistics of GDP per region or city? (x-post from Anarchism),usxba,earthheart,1224361929.0,https://www.reddit.com/r/statistics/comments/usxba/what_search_terms_are_used_to_find_statistics_of/,1.0,1.0,,en
1110057,2012-06-09 19:45:43,statistics,Exponential Distribution - Can I predict?,utbrz,Simusid,1336679871.0,https://www.reddit.com/r/statistics/comments/utbrz/exponential_distribution_can_i_predict/,7.0,11.0,"Caution!  Non-statistician approaching!

I'm an EMT and we have what are known as ""frequent flyers"", people that we see regularly.   I've looked at a data set of ""time between calls"" for one of our most popular FFs and it looks like an exponential distribution with a mean of 21.4 days.   A Kolmogorov-smirnov test tells me that it is exponential with a p value of 0.056.  

Now using the PDF and that mean (lambda) isn't it correct to observe ""well we haven't seen Tom in 85 days.   There is a 98% chance that we should have seen him by now"".   I got that by integrating the pdf from 0 to 85 which is 0.98 or so.   I think that is right.

What I would like to be able to do is to say ""there is an X percent chance that we will see Tom in the next 3 days"".   Is there a statistically valid way to say that?",en
1110058,2012-06-09 22:32:17,MachineLearning,My Kaggle rank is leveling out. Advice?,utjdl,MLhelp,1339268567.0,https://www.reddit.com/r/MachineLearning/comments/utjdl/my_kaggle_rank_is_leveling_out_advice/,32.0,19.0,"Hi ML, I've been competing in several Kaggle competitions. I can get in the top 10-20%, but after that my rank stagnates. My template workflow is to process the data(derive every variable I can come up with), partition the data into k-folds or k-stratified-folds, (optionally or separately) perform feature selection, throw every algorithm appropriate from sklearn/scipy/theano-scripts into my model class then bag the models. I do grid-searching on the hyper-parameters for algorithms that perform poorly. I try to keep my individual model fitting time to under 12 hours on my 8-core workstation. I can usually eek out a little more if I run add models ran with more of the features to my bag.

I have a good understanding of many of the algorithms that sklearn implements, but I don't have the foggiest clue how this knowledge could help me except in something like diagnosing stochastic gradient descent. I've implemented a couple(slower and less pretty) versions of them. I've worked through Elements of Statistical Learning. I've well-versed in hypothesis testing and visualizing data(my job). I don't know where to go from here. I thought I was pretty good at building models, but I am humbled by the leaderboard.

Where do I go from here? What books do I read? What software/languages do I learn? I'm fine with logarithmic returns on my effort, but I don't know where to find them.",en
1110059,2012-06-10 00:51:50,statistics,Analyzing results from a race,utpri,Tafkas,1231818191.0,https://www.reddit.com/r/statistics/comments/utpri/analyzing_results_from_a_race/,0.0,2.0,"If I have some race results in tabular format, what sort of analysis can I run on those? First things that come to my mind are mean time and speed. I can plot a histogram etc. But what else is there, that I could for example compare races (given I have results of both)?

Place, Name, Time  
1, John Smith, 00:45:28  
2, Bill Miller, 00:45:43  
.  
.  
",en
1110060,2012-06-10 04:38:51,statistics,Need help with data analysis in SAS Enterprise.,utzjt,Adamantus,1283897973.0,https://www.reddit.com/r/statistics/comments/utzjt/need_help_with_data_analysis_in_sas_enterprise/,2.0,5.0,"I know help questions are supposed to go in r/homeworkhelp, but this is for my job. I was recently hired as a risk analyst, and it seems my supervisor assumed I knew how to use SAS and do statistics  because of my physics undergrad. Anyway, I've rarely used SAS and have limited knowledge of business statistics, so any help or direction would be appreciated.

I'm looking at a data set of loan customers, and I'm trying to find out if certain excluded customers (excluded from future loans for various reasons) reduce the default rate significantly. The table is something like this:
Person|Excluded|Default|Credit Score

   1    |     0     |    0    |    700

   2    |     1     |    0    |    550

   3    |     1     |    1    |    500

0 is not excluded, 1 is excluded. 0 is the customer has paid on time, 1 is that they have defaulted. I want to see if excluding those customers lowers the default rate (from all loans, including both 0 and 1 for exclude) and if they have a significantly different credit score.

In the ""Analyze"" tab of SAS Enterprise, there are different functions such as ttests and correlations, but I'm not exactly sure which I would use or how to use it. Could anyone please give me some help, as I'm basically trying to learn this on the fly and it's pretty difficult.  Thanks.",en
1110061,2012-06-10 11:02:26,artificial,A human and an AI is playing Super Mario Bros. Can you spot which is which?,uudil,togelius,1147200994.0,https://www.reddit.com/r/artificial/comments/uudil/a_human_and_an_ai_is_playing_super_mario_bros_can/,0.0,0.0,,en
1110062,2012-06-10 11:18:18,datascience,Introducing PyBossa – the open-source micro-tasking platform,uudvx,mhermans,1169219262.0,https://www.reddit.com/r/datascience/comments/uudvx/introducing_pybossa_the_opensource_microtasking/,1.0,0.0,,en
1110063,2012-06-10 11:19:02,datascience,Towards a global lookup service for corporate ids,uudwc,[deleted],,https://www.reddit.com/r/datascience/comments/uudwc/towards_a_global_lookup_service_for_corporate_ids/,0.0,0.0,,en
1110064,2012-06-10 14:01:29,statistics,"Trying to get a RA done, having  problems to get the model working",uugpe,[deleted],,https://www.reddit.com/r/statistics/comments/uugpe/trying_to_get_a_ra_done_having_problems_to_get/,0.0,1.0,"Hi Reddit, 
a friend of mine and I have some problems figuring out an RA for a little experiment she made. I already figured one out, but I am not sure if this is the right model to go with. Can you help :)?

Exp. description:
It's an investment game consisting of 3 rounds. 

1. Round: You have 2 players. Player No. 1 is sending money to player No. 2, who then tripples the amount sended to him and CAN send money back to Player No.1.

2. Round: the same game, only this time the game is overseen by a third person, who cannot do anything.

3. round: Same game, this time the third person is able to punish player No. 2

Now we wanted to develop a modell predicting the investment behaviour of P1.
I came up with:
Y=a+b1X1+b2X2+b3X3+U

Y= Investment of P1; X1= bin. var. for 3. Person; X2=bin. Var. for 3. person being able to punish; X3=Expected payback Player No.1 (has been monitored during the experiment as well).
I would also bring in another variable, which depicts, if the expectations have been met in the prev. rounds... 
 
What do you think? :)
Thank you
",en
1110065,2012-06-10 15:07:35,rstats,How to Create an easy way to put packages on multiple workstations?,uuhy3,ultraspeedz,,https://www.reddit.com/r/rstats/comments/uuhy3/how_to_create_an_easy_way_to_put_packages_on/,3.0,7.0,I will be having to use different workstations is there an easy way to port over a bunch of installed packages from station to station?,en
1110066,2012-06-10 23:04:35,artificial,"AAAI's website is a wealth of resources for everyone, including a subsection, ""AI In the News"" ",uuzu4,moscheles,1316266003.0,https://www.reddit.com/r/artificial/comments/uuzu4/aaais_website_is_a_wealth_of_resources_for/,12.0,0.0,,en
1110067,2012-06-11 00:04:34,statistics,Sample chapter from The Manga Guide to Statistics.  It's... pretty authentic.,uv2nx,JIVEprinting,1292669244.0,https://www.reddit.com/r/statistics/comments/uv2nx/sample_chapter_from_the_manga_guide_to_statistics/,14.0,0.0,,en
1110068,2012-06-11 01:50:32,statistics,Simple Probability,uv7oa,[deleted],,https://www.reddit.com/r/statistics/comments/uv7oa/simple_probability/,0.0,3.0,"A question I have been reviewing:


An urn contains 5 white and 10 black balls. A fair die is rolled and that number of balls is randomly chosen from the urn. 

What is the probability that all of the balls selected are white? 


The answer is (1/6)*the summation of the combinations of white (n=5, k=1-5). I thought the answer was just the summation, with out the multiplication of the 1/6. I understand that the 1/6 comes from the die, but why do you have to multiply by that the find the probability of white?",en
1110069,2012-06-11 07:49:35,MachineLearning,A Practical Guide to Support Vector Classification,uvowm,kunwoo32,1312235056.0,https://www.reddit.com/r/MachineLearning/comments/uvowm/a_practical_guide_to_support_vector_classification/,16.0,1.0,,en
1110070,2012-06-11 08:23:30,datascience,Towards a global lookup service for corporate ids,uvq70,[deleted],,https://www.reddit.com/r/datascience/comments/uvq70/towards_a_global_lookup_service_for_corporate_ids/,0.0,0.0,,en
1110071,2012-06-11 11:12:26,statistics,I'm not experienced with statistics and need help with my project. ,uvvoz,emrsag,1307452753.0,https://www.reddit.com/r/statistics/comments/uvvoz/im_not_experienced_with_statistics_and_need_help/,0.0,0.0,"Hello,

I am a Game Technologies student with visual communication design background. For my Game Metrics course I constructed a experiment, people tried 2 games and answered same 6 questions for each one of the games. I had 12 people fill my questionnaire. 

I am currently using SPSS program and trying to figure out if there is a statistically significant difference between 2 cases. 

As long as my knowledge goes, I know I should use an One Way ANOVA test but I can't get a hold of SPSS and the all other terminology.

What test should I run to test my null hypothesis which is people's reaction for 2 games is similar.  and what data should I check? 

Thanks in advance.",en
1110072,2012-06-11 14:36:32,statistics,I can't think of a way to google this question: standard deviation expected with a different sample size (read the question first),uw09g,Bingo_banjo,1293804906.0,https://www.reddit.com/r/statistics/comments/uw09g/i_cant_think_of_a_way_to_google_this_question/,7.0,7.0,"Ok, i know standard deviation is a figure which shouldn't change over a sample size but this is slightly different:

I have 55 sources of data, each instance is constantly performing a binary yes/no task. I cannot get data any more granular than about 50 tasks at a time.

What this gives me is something like :
   

    Tasks:	Success:
    63	76.19%
    59	79.66%
    60	75.00%
    47	42.55%
    51	68.63%
    44	79.55%
    44	72.73%
    45	75.56%
    46	63.04%
    41	70.73%

All instances should be equal over a large sample size but I suspect there are transient differences between them outside of the standard deviation.

The basic question is given the standard deviation here with an average of 50 tasks is 11% can i assume a standard deviation if i were to have 500 tasks?",en
1110073,2012-06-11 16:23:22,statistics,How does r/statistics feel about this testing performed on protein supplements?,uw3j6,tiii,,https://www.reddit.com/r/statistics/comments/uw3j6/how_does_rstatistics_feel_about_this_testing/,1.0,0.0,,en
1110074,2012-06-11 19:33:40,datascience,Towards a global lookup service for corporate ids,uwcaf,[deleted],,https://www.reddit.com/r/datascience/comments/uwcaf/towards_a_global_lookup_service_for_corporate_ids/,0.0,0.0,,en
1110075,2012-06-11 21:46:50,rstats,Difference in PCA results when normalizing manually and having the PCA function do it internally?,uwje7,[deleted],,https://www.reddit.com/r/rstats/comments/uwje7/difference_in_pca_results_when_normalizing/,3.0,3.0,"Hi guys,
I'm doing some exploratory analysis, PCAing a bunch of stuff to look at my data. I've noticed that if I column normalize my matrix manually (using the scale() command in {base}) and run the PCA, the components explain less variation than when I leave the matrix alone and have prcomp() do the normalization. What in the world is going on?",en
1110076,2012-06-11 22:12:06,MachineLearning,Kmeans -- ensuring homogeneous cluster size?,uwkt9,ColonelHapablap,,https://www.reddit.com/r/MachineLearning/comments/uwkt9/kmeans_ensuring_homogeneous_cluster_size/,20.0,30.0,"I'm working on an implementation of kmeans++ and for the clusters I get that have about 30 datapoints assigned to them, it seems to work pretty well.  But I consistently seem to get one or two clusters of about 700 datapoints in size (out of ~2000!) when k is 100, which turn out to be pretty useless.  I also get a bunch of clusters that are 1 or two datapoints in size, which look to be pretty similar to other clusters of that size and should be grouped together.

Are there methods to ensure that clusters get to be a roughly homogeneous size compared to each other?  I thought it was a problem with my cluster initialization, but I tried random datapoints at centers, I tried furthest distance, and now kmeans++, but the different methods all seem to give me the same problem with regards to relative cluster size.",en
1110077,2012-06-12 03:11:46,statistics,Odds ratio vs Relative Risk/Hazard Ratio,ux151,medstudent22,1305246130.0,https://www.reddit.com/r/statistics/comments/ux151/odds_ratio_vs_relative_riskhazard_ratio/,9.0,3.0,"I have a background in physics with a few courses in statistics, but I still have a hard time intuitively understanding OR. I get RR as it just is a ratio of probabilities, and I look at HR as RR with a time component. Every time I see an OR, though, I have to look up the incidence and just calculate what the RR would be (which I don't even know to be valid operation).  I've looked at a few websites explaining the differences but didn't get what I wanted.  

First, what statistically determines whether you have to use a RR vs an OR (I understand the use of OR in logit but don't know why it has to be used elsewhere)? Secondly, what is the easiest way to think about OR intuitively? And lastly, how would you explain it concisely to someone with a 7th grade reading level (like the prototypical medical patient)?  

",en
1110078,2012-06-12 09:05:14,rstats,social network analysis question,uxjbv,hillset,1318127882.0,https://www.reddit.com/r/rstats/comments/uxjbv/social_network_analysis_question/,1.0,4.0,"So I'm working on the Kaggle facebook competition (http://www.kaggle.com/c/FacebookRecruiting) and I had a question for the reddit R community.  I have two variables, a source node and related destination nodes in this fashion:

source_node -&gt; 1, 1, 1, 2, 2, 3     
destination_node -&gt;3, 4, 5, 4, 5

The data is directional meaning 1 -&gt; 4 is not the same as 4 -&gt;1.  I want to create a dichotomous variable that says if the relationship is mutual (1 -&gt; 4 and 4 -&gt; 1).  Essentially, I want it to look like this:

source_node-&gt;1, 1, 1, 2, 2, 3     
destination_node-&gt;3, 4, 5, 4, 5, 1
mutual?-&gt;1, 0, 0, 0, 0, 1


Can anyone help me with how to do this in r?  Thanks for the help everyone.

EDIT: fixed numbers on lines",en
1110079,2012-06-12 17:26:42,statistics,Regarding survey results and unequal sample sizes...,uxymr,[deleted],,https://www.reddit.com/r/statistics/comments/uxymr/regarding_survey_results_and_unequal_sample_sizes/,3.0,5.0,"We conducted a half year long survey of mobile device usage/experience etc for the military (who is thinking of issuing tablets to most soldiers in the future).  We were tasked with identifying ""gaps"" between those who have had prior experience with a tablet and those that have not. This was to identify issues that would need to be trained to those considered tablet novices...

So. We have the data and divided it up into ""Owners"" and ""non-owners"" (those who gave responses that have or currently own a tablet, and those who have never owned at tablet). 

As such, we are looking to compare responses on a variety of questions and variables we created, but considering it is survey results, we have about 70% more non-owners than owners. 

E.g. 45 (owners) had a mean score of 7.5 of 9 and 89 (non-owners) had a mean score of 5.5 of 9

So what do you think is the best test to compare these types of groups? I've ruled out ANOVA, but would t-tests, or a variant work?

Thanks",en
1110080,2012-06-12 19:44:38,statistics,Help characterizing a queue system with correct queuing model,uy5w3,gmiwenht,1270295293.0,https://www.reddit.com/r/statistics/comments/uy5w3/help_characterizing_a_queue_system_with_correct/,10.0,2.0,"[TL;DR: what model to use??](http://i.imgur.com/oAGLM.png)  

I am confused about how to correctly characterize a physical system. Let's say we have request entities and service entities arriving at two arrival rates: request arrival rate \lambda_r and a service arrival rate \lambda_s. Let's say both enter into their own separate queues. Request entities are serviced only when there are both request entities and service entities in their queues. So service rate would refer to the rate at which request entities are serviced by service entities.  

For example, consider a queue of particles and a queue of anti-particles. So long as there are both particles and anti-particles in the queue, they will join together one by one and annihilate. The service rate would refer to the rate at which particles and anti-particles collide and annihilate. Both arrive at different arrival rates, so at any time there may be a build-up or particles or a build-up of anti-particles. So the service rate at a particular time depends on both arrival rates and on the initial conditions (size of the queues).  

How can I characterize such a system? Which queueing model should I use? Moreover, consider that the arrival rates might be changing with time; or alternatively, that the rates are constant but that arbitrary initial conditions can be specified. I would like to get analytical expressions for the expected queue length and expected waiting time, among other things.  

I appreciate any advice.",en
1110081,2012-06-12 21:04:07,MachineLearning,I'm thinking about applying machine learning to a fantasy football draft tool.,uyaq2,imissyourmusk,1317942820.0,https://www.reddit.com/r/MachineLearning/comments/uyaq2/im_thinking_about_applying_machine_learning_to_a/,9.0,9.0,"I see it being useful in a couple of places.  First, I'd like to predict the likelihood of each player being available for the next pick and secondly I'd like to target the right players at the right times based on that information.  Has anyone else had any applicable experiences? ",en
1110082,2012-06-12 21:05:48,statistics,Possible Election Fraud in Texas?,uyato,those_draculas,1297714043.0,https://www.reddit.com/r/statistics/comments/uyato/possible_election_fraud_in_texas/,0.0,1.0,"[This article](http://www.dailypaul.com/239494/fraud-confirmed-in-texas-election-judge-says-votes-were-switched-tonight-drkrbn-live) has been floating around the web a lot today. 

It claims to have proof of election fraud in the Texas Primaries, however in my limited knowledge of statistics, the numbers look a little suspect to support such a claim. 

I was wonder if someone in /r/statistic could give their interpretation of this?

Thanks:)",en
1110083,2012-06-12 21:07:53,MachineLearning,I have a question about small data sets...,uyaxu,Xirious,1275134342.0,https://www.reddit.com/r/MachineLearning/comments/uyaxu/i_have_a_question_about_small_data_sets/,4.0,6.0,"What is the major problem with a small data set? From my beginner knowledge of ML, I am thinking that a ML system trained on a small data set does not necessarily generalise well to all situations? What methods can I use to improve and test my system (for instance my ANN or kNN - still need to evaluate each of them with the small data sets) when I do have a small data set? Thank you all for the help in advance!",en
1110084,2012-06-13 00:43:27,MachineLearning,Lingodroid Robots Invent New Words for Time by Evan Ackerman ,uynxd,RichKatz,1200681971.0,https://www.reddit.com/r/MachineLearning/comments/uynxd/lingodroid_robots_invent_new_words_for_time_by/,2.0,0.0,,en
1110085,2012-06-13 01:00:32,datascience,Towards a global lookup service for corporate ids,uyovp,[deleted],,https://www.reddit.com/r/datascience/comments/uyovp/towards_a_global_lookup_service_for_corporate_ids/,0.0,0.0,,en
1110086,2012-06-13 01:30:50,MachineLearning,Large-Scale Machine Learning at Twitter,uyqm8,jackhammer2022,1280494843.0,https://www.reddit.com/r/MachineLearning/comments/uyqm8/largescale_machine_learning_at_twitter/,8.0,0.0,,en
1110087,2012-06-13 01:42:41,artificial,"How big is the largest feedforward neural network ever trained, and what for? - Quora",uyr9t,1infiniteloop,1315936966.0,https://www.reddit.com/r/artificial/comments/uyr9t/how_big_is_the_largest_feedforward_neural_network/,32.0,2.0,,en
1110088,2012-06-13 03:32:11,statistics,Need help with confidence intervals in SAS,uyx8l,Adamantus,1283897973.0,https://www.reddit.com/r/statistics/comments/uyx8l/need_help_with_confidence_intervals_in_sas/,5.0,7.0,"I work for a loan company in lead generation (buying potential customers from partners). We have a large database with each partner labeled with an ID. I need to find the average cost per loan and the confidence interval for each partner ID. The data looks like this:

ID | total_cost | funded

1 | $1 | 0

1 | $2 | 1

1 | $0.5 | 1

1 | $3 | 0

2 | $4 | 0

2 | $2 | 1

3 | $1 | 0

Where total_cost is the cost of that specific loan and funded is whether the person was given a loan. I need to summarize the data by ID with a cost per loan (CPL) = total_cost / funded. I also need a 95% confidence interval around that CPL.

I tried a proc summary data=table mean lclm uclm, but average CPL for each ID is a tabulated field that requires the data to be aggregated first. Finding a confidence interval after that seems impossible because by the time average CPL is summed, there is only one data point (for each partner ID). 

Can anyone explain how to calculated a confidence interval for the CPL without actually using a proc summary first and aggregating the data? Thanks for any help.",en
1110089,2012-06-13 09:33:00,MachineLearning,Bayesian Nonparametrics,uzfqr,rudyl313,1297154050.0,https://www.reddit.com/r/MachineLearning/comments/uzfqr/bayesian_nonparametrics/,11.0,17.0,"Can anybody explain to me, in relatively simple terms, what Bayesian nonparametrics is and how it fits into the world of Machine Learning?",en
1110090,2012-06-13 16:34:48,statistics,"Work with a non-Profit, need help chosing a survey method",uzs2z,Shovelbum26,1328134221.0,https://www.reddit.com/r/statistics/comments/uzs2z/work_with_a_nonprofit_need_help_chosing_a_survey/,9.0,19.0,"Alright, I'll make this brief.  I work with an environemtnal non-profit and we're trying to get a $10,000 grant for a pro-recycling advertising campaign.  However to get the grant we need some way to measure the impact of our ads.  Thus, we need a survey.

We are a tiny non-profit (2 full time staff, 1 part time) with an equally tiny budget so we need something effective, but cheap.  We are currently considering three options and I was wondering which would have the least sample bias:

1. Phone survey.
2. Mail survey.
3. Pedestrian survey (standing around at the mall/street corner soliciting surveys).

I personally expect that the phone survey would be the least biased.  I would think with the mail survey it would only be people with strong opinions that would send it back, thus giving you a skewed result.  The pedestrian survey might eliminate that, but it would be very geographically skewed so we'd have to do multiple locations, which would be a hassel.  The phone survey seems like the best option to me, but is the second most time-consuming (behind pedestrian survey).  

Any suggestions would be very helpful!",en
1110091,2012-06-13 17:32:20,statistics,Tips for making your statistical simulation run faster,uzunp,WeiChen91312,1339597870.0,https://www.reddit.com/r/statistics/comments/uzunp/tips_for_making_your_statistical_simulation_run/,4.0,1.0,,en
1110092,2012-06-13 19:09:42,MachineLearning,Would mastering the algorithm's in Andrew Ng's machine learning course on coursera.org be enough to break into the field?,uzzkl,kurtgodelisdead,1266119325.0,https://www.reddit.com/r/MachineLearning/comments/uzzkl/would_mastering_the_algorithms_in_andrew_ngs/,15.0,22.0,"I've been going through [the course](https://class.coursera.org/ml/class/index) and realized that many of the exercises are lacking depth, so I intend on re-writing most of the algorithms without Octave/Matlab so I can get a genuine understanding of the process. 

Current I'm doing a lot of web development with Ruby on Rails, but I'd like to break into machine learning. I have a strong feeling that mastering (not just doing the exercises, but practicing them regularly) the algorithms that Andrew Ng has shown in the course might be enough that I may end up with one or two opportunities to use these skills while doing my job, say for example classifying users based on site activity or being able to predict the amount of traffic on any given day.. 

However, I'm being lured into a lot more other challenging courses, like coursera's NLP course. I don't want to end up a jack-of-all-trades with little chance of actually applying what I've learned to my daily job, so I feel focusing on a small number of algorithms to master would be better than getting a general understanding of many algorithms.

Has anyone else gone the web development to machine learning route? How as it worked out for you? How did you manage to break in?

**TLDR**: Do I have to learn lots of algorithms to become an effective ML-er or know a handful really well?",en
1110093,2012-06-13 22:19:22,statistics,Interview question: Pedagogical approaches to quantitative research methods?,v0amq,abitofperspective,1272704550.0,https://www.reddit.com/r/statistics/comments/v0amq/interview_question_pedagogical_approaches_to/,1.0,3.0,"I've an upcoming interview for an academic job, and I anticipate that one of the questions they ask will be something along the lines of the following:

- What are good pedagogical practices for teaching quantitative research methods?

The job would involve teaching mostly master's courses in social sciences, probably starting with basic stats (central tendency, variation, correlation) through regression and multivariate analysis. 

Any suggestions for some snazzy answers or things I want would want to consider? Also, any suggestions for books I could recommend for these courses? Thanks for your ideas!",en
1110094,2012-06-14 03:56:10,MachineLearning,The Unreasonable Effectiveness of Data,v0tmw,blind_swordsman,1318716377.0,https://www.reddit.com/r/MachineLearning/comments/v0tmw/the_unreasonable_effectiveness_of_data/,15.0,2.0,,en
1110095,2012-06-14 06:22:46,artificial,I found out I'm going to teach artificial intelligence to a professional crowd; what should I cover?,v11oa,crowfeather,1251788023.0,https://www.reddit.com/r/artificial/comments/v11oa/i_found_out_im_going_to_teach_artificial/,14.0,23.0,"By ""professional"", I mean basically the Java crowd, and the course is a 3 day training course, 6 hours a day. What practical things can I teach in this limited framework?",en
1110096,2012-06-14 09:51:38,computervision,Know any novel usages of Kalman Filters in related to CV domains ? ,v1b21,andreime,1322500522.0,https://www.reddit.com/r/computervision/comments/v1b21/know_any_novel_usages_of_kalman_filters_in/,0.0,0.0,Just curious...,en
1110097,2012-06-14 12:10:51,MachineLearning,Air Handling Unit Manufacturer Door Air-Curtain,v1ewd,gjkliol,1339581016.0,https://www.reddit.com/r/MachineLearning/comments/v1ewd/air_handling_unit_manufacturer_door_aircurtain/,0.0,1.0,,en
1110098,2012-06-14 15:19:23,rstats,glht multiple comparisons for glm with 2 factors,v1jku,mellend,1339676123.0,https://www.reddit.com/r/rstats/comments/v1jku/glht_multiple_comparisons_for_glm_with_2_factors/,3.0,2.0,"Hi All, 

I have used glm to model my data, I have two factors and a covariate as described in the example code below (mod.1). 

I have been able to ""force"" glht to perform multiple comparisons by creating a combined variable for the factors, accepting that there will be a loss of statistical power as it seems to do what I want.  I then use the cld function to generate the letters of significance from the glht which I can then apply to my figures with ggplot2. 

However when it comes to describing my data I am becoming increasingly suspicious that this hasn't worked as I want it too as the letters don't always make sense in relation my original model. Is there any other way of doing this on my original model (mod.1) rather than having to ""cheat"" (mod.2)? 

mod.1 &lt;- glm(response ~ 1 + treat + seas + covariate, data = dat) 

# combined variable for pairwise comparisons 
treat.seas &lt;- (as.numeric(dat$treat) * 10) + as.numeric(dat$seas) 
dat$treat.seas &lt;- factor(treat.seas) 

mod.2 &lt;- glm(response ~ 1 + treat.seas + covariate, data = dat) 
mod.2_glht &lt;- glht(mod.2, linfct = mcp(treat.seas = ""Tukey"")) 
letters_mod.2 &lt;- cld(mod.2_glht) 

Thank you in advance ",en
1110099,2012-06-14 16:21:16,statistics,A simple question about standard deviation,v1ln7,Brakfis,,https://www.reddit.com/r/statistics/comments/v1ln7/a_simple_question_about_standard_deviation/,4.0,16.0,"Hi statistics,

I'm fairly new to statistics and I get most things that I read about statistics (it usually takes a while though). However, the other day a friend and I found a sample where the mean was something like 18 and the SD 24. I get that the data have a large spread and might not be the most reliable, but that's not the problem. That means (from my understanding) that 15.8 % of the data points can be expected to be found below -6. What bugs me is that the data cannot take negative values. How do I interpret this?",en
1110100,2012-06-14 16:38:59,MachineLearning,[PDF] No More Pesky Learning Rates,v1mbw,lpiloto,1310021871.0,https://www.reddit.com/r/MachineLearning/comments/v1mbw/pdf_no_more_pesky_learning_rates/,29.0,7.0,,en
1110101,2012-06-14 17:56:00,statistics,Do 50-1 longshots in the Kentucky Derby ever come in?,v1prp,gthank,1171628366.0,https://www.reddit.com/r/statistics/comments/v1prp/do_501_longshots_in_the_kentucky_derby_ever_come/,11.0,6.0,,en
1110102,2012-06-14 19:37:53,MachineLearning,Learning infinite HMM from a single temporal sequence of data,v1v05,danielkorzekwa,1300550268.0,https://www.reddit.com/r/MachineLearning/comments/v1v05/learning_infinite_hmm_from_a_single_temporal/,2.0,3.0,"Most of books, courses and research papers on Bayesian Networks cover learning of HMM with EM algorithm, assuming that HMM is a finite sequence of hidden states and that many temporal sequences of data are available. The same observation applies to learning temporal dynamic Bayesian Networks. For instance speech, gesture or pose recognition. 


I'm curious to know how to learn latent variables for infinite HMM, where a single sequence of data is available only.


The concrete example I'm referring to is a HMM representing skills of tennis players, playing multiple games over the time during a tennis season. I model this environment as a single HMM network with a single temporal sequence of data representing results and statistics of all tennis matches over the period of time (basically last 6 years). 


In this HMM model, for every player I have a separate hidden variable for every time period representing player skills. Players hidden variables are connected each other through match result/statistics variables, every time two players play each other. 


The hidden variables I need to learn are:

- Transition probabilities for every player skills: 

  - Player skills on serve variable
  - Player skills on serve return variable

- Emission probabilities for every tennis match:

  - Match result variable
  - Match statistics variables


Personally, I think that sufficient statistics required for EM learning might be obtained from a very long and single sequence of data (tennis player skills over time) as well as using multiple but short sequences (pose recognition), so there is really no difference between learning both Pose Recognition HMM and Tennis Players Skills HMM using EM algorithm. But I would like to get some opinions from others on this forum too, before I jump into learning my HMM network.


PS. Having the opportunity, I'd like to recommend Probabilistic Graphical Models Course by Professor Daphne Koller from Stanford University. There is no book that could teach you on Bayesian Networks as good as Professor Daphne Koller does it with her brilliant course. https://www.coursera.org/course/pgm


Regards.
Daniel",en
1110103,2012-06-14 20:24:57,statistics,Is anyone else frustrated by the quality of infographics? I stole this gem from r/politics. ,v1xod,Iamnotanorange,1268153982.0,https://www.reddit.com/r/statistics/comments/v1xod/is_anyone_else_frustrated_by_the_quality_of/,29.0,14.0,,en
1110104,2012-06-14 20:35:02,statistics,testing whether within-subject correlations are different from zero,v1y8d,spanqmoi,1335973512.0,https://www.reddit.com/r/statistics/comments/v1y8d/testing_whether_withinsubject_correlations_are/,4.0,6.0,"Hi, 

I have a distribution of within-subject correlations, and I'd like to test whether the mean of these correlations is different from zero. For example, imagine I assessed the within subject correlation between time of getting out of bed and self-reported productivity, in a diary study. On average, there was an inverse relationship: rising late was correlated with less productivity. The mean correlation was -.10. How do I test whether that mean is different from zero? A simple t-test?

Thanks!",en
1110105,2012-06-15 01:02:10,analytics,Jobplotter.com Site Info,v2e31,jobplotter,1339711225.0,https://www.reddit.com/r/analytics/comments/v2e31/jobplottercom_site_info/,0.0,1.0,,en
1110106,2012-06-15 01:21:48,artificial,Planning: PDDL 3.1 and Drools - Has anyone tried PDDL e.g. GraphPlanner or JBoss Drools?,v2f7f,[deleted],,https://www.reddit.com/r/artificial/comments/v2f7f/planning_pddl_31_and_drools_has_anyone_tried_pddl/,1.0,0.0,,en
1110107,2012-06-15 04:11:37,statistics,"Hey Reddit. Could you please help me out and take this quick survey, I need data for a statistics class. Thanks!",v2nss,[deleted],,https://www.reddit.com/r/statistics/comments/v2nss/hey_reddit_could_you_please_help_me_out_and_take/,1.0,0.0,,en
1110108,2012-06-15 10:22:43,AskStatistics,What are the most useful concepts one can apply in everyday life?,v340g,homo-insurgo,1268403477.0,https://www.reddit.com/r/AskStatistics/comments/v340g/what_are_the_most_useful_concepts_one_can_apply/,9.0,13.0,,en
1110109,2012-06-15 12:48:06,MachineLearning,Machine Learning for Javascript Hackers,v37n9,elkos,1148041160.0,https://www.reddit.com/r/MachineLearning/comments/v37n9/machine_learning_for_javascript_hackers/,0.0,1.0,,en
1110110,2012-06-15 15:21:06,datasets,Live Chat: inside Chicago’s push to open up city data | Ars Technica,v3bgq,reidhoch,1209656850.0,https://www.reddit.com/r/datasets/comments/v3bgq/live_chat_inside_chicagos_push_to_open_up_city/,1.0,0.0,,en
1110111,2012-06-15 17:05:33,statistics,Anyone have book recommendations for clinical trial design?,v3fdq,synergy14,1298089580.0,https://www.reddit.com/r/statistics/comments/v3fdq/anyone_have_book_recommendations_for_clinical/,5.0,3.0,,en
1110112,2012-06-15 17:21:16,MachineLearning,Larry Wasserman (ML/stats CMU) now has a blog,v3g49,[deleted],,https://www.reddit.com/r/MachineLearning/comments/v3g49/larry_wasserman_mlstats_cmu_now_has_a_blog/,39.0,6.0,,en
1110113,2012-06-15 18:01:55,statistics,"Android holds 50% of the mobile phone market in the UK. Can I assume that 50% students in my university who own a phone, are using the Android operating system?",v3i3x,[deleted],,https://www.reddit.com/r/statistics/comments/v3i3x/android_holds_50_of_the_mobile_phone_market_in/,0.0,3.0,Can I make that assumption with any mathematical backing? are there any mathematical laws or theories that I can cite to back up that assumption?,en
1110114,2012-06-15 19:11:11,MachineLearning,What is a good way to learn the mathematical notation for algorithms?,v3ljz,H4L9000,1297796030.0,https://www.reddit.com/r/MachineLearning/comments/v3ljz/what_is_a_good_way_to_learn_the_mathematical/,6.0,7.0,"I understand the way an algorithm works after reading descriptions, but I can't seem to grasp how an algorithm works just by seeing the mathematical notation. For instance, I know how k-means works, and I have hand coded it to work in python. However, I have no idea how to do that from just looking at the k-means notation: arg min E ||Xj - Ui||^2 .",en
1110115,2012-06-15 19:34:14,MachineLearning,Basic question: Anomaly detection,v3mru,mistidoi,1327512893.0,https://www.reddit.com/r/MachineLearning/comments/v3mru/basic_question_anomaly_detection/,3.0,2.0,"This is probably mind-numbingly simple for most of you, but I have a project wherein I would like to flag days of heavy usage by users of a system.

I have no idea what I am doing, but here's what I was thinking so far:  looking at the data from the logs so far, it's clear that users' activity varies depending on the day of the week (In general, most people don't work on Sunday's and seem to work less on Fridays.  Some people do work on Saturdays and might have a weekday off.)

So, what I had envisioned was analyzing all of my data grouped by user and day of the week.  I envisioned then finding something like the mean and standard deviation for each user relative to each day of the week, and then basically ending up looking at daily usage as z-scores (distance from that user's daily mean in terms of standard deviations.)

Ideally, what I would end up with would be a flat line that jumped up only when that user had a spike in usage relative to their typical usage on that day of the week.

Does this seem reasonable? What would someone smarter than me/with a better background in this stuff do?

P.S.  Even as I write this I am realizing that the distribution of the data might not ideal for z-scores and am thinking about using IQR or something like that, but am less clear about what that might look like.  Thoughts?",en
1110116,2012-06-15 20:04:52,statistics,Accurate Estimation Methods with Small Sample Size,v3oe2,DeathbyToast,1324665050.0,https://www.reddit.com/r/statistics/comments/v3oe2/accurate_estimation_methods_with_small_sample_size/,8.0,39.0,"Hello r/statistics!

I have an interesting problem that I have been told is nigh impossible to solve.  I feel like if anyone on the Internets knows how to do this, its you guys!

I am an intern at a software company that works with data.  Lots of data.  I am talking about analyzing petabytes of data.  Reading in all of the data simply isn't an option for me because it would take days to complete that task.  I have been given a few hours for my script to run which means that I will have a sample size of at most 5% of the population.

My challenge is to use this 5% of the data on a specific hard drive and predict what the contents of the rest of the hard drive are.  For simplicity's sake I've modeled the ""data"" as random integers (for NDA reasons I can't go into why, but just trust that this is an accurate assumption for the data on the drives).  My job is to count the number of duplicate integers in my sample, and come up with a % that represents the % of duplicates on the entire hard drive.

I've made a basic simulation of this in excel, the file is [available here](https://www.dropbox.com/sh/0br99qhfvmnk42k/4B23NLVyhn/Example%20Data.xlsx)

There are two sheets in that file, the first is my analysis on the ""population"" aka the entire drive, which has 71% duplicate entries.  In the second page, I ""sampled"" the first 500 integers and ran the same analysis (sorted the entries in column B, then removed the duplicates in column C).  The trouble is, my sample results in 7% duplicates being counted.  This is...well...not correct.

That's where I'm hoping someone here can help me out.  I was looking at [Wikipedia's estimation theory entries](http://en.wikipedia.org/wiki/Estimation_theory#Estimators), and that was just plain confusing.  For my background, I've taken AP Stats (tutored it for a few years) as well as two college level Statistics courses, one was Probability Models for Economic Decisions (lots of excel modeling) and the other was Probability and Stochastic Processes for Electrical Engineers (I'm a Computer Engineering major from Cal Poly San Luis Obispo).  Anywho, I have a solid, but basic, understanding of statistics.  I have not however, done any sort of estimation theory (which as far as I can tell is where this needs to go).

I am not looking to hire a consultant for this project, I am just looking for some pointers as to what methods I should be researching (I have the next 10+ weeks to figure this out, I just need to know where to start my research!)

Thank you very much in advance for any and all help that you are willing to offer :)

And for those of who with short time (I can't blame you for not reading the above essay)




**tl;dr**
Need help with estimating population with a sample size of &lt;5% of the population.  Trying to count duplicate integers as shown in [this excel sheet](https://www.dropbox.com/sh/0br99qhfvmnk42k/4B23NLVyhn/Example%20Data.xlsx) 





**edit**
Thank you everyone for the responses!  I'll take a look at the methods suggested and post any feedback questions that arise.  I've tried to answer/respond to everyone, and please feel free to post any other questions you may have.  I greatly appreciate all of the help!

Also, my primary focus right now is getting the model in my excel sheet to work.  With the knowledge I have about my problem as it is right now, this is a close model to the actual data I will be analyzing.  Thanks again!

**SOLVED**
Turns out those who mentioned the birthday problem were on to something!
By solving for ""d"" in the equation given here: http://en.wikipedia.org/wiki/Birthday_problem#Collision_counting and then doing 1-d/total, where total is the total amount of numbers in my population, I can get within 5-10% accuracy the number of duplicates that exist in my population from my sample.

Example: My range of numbers is 1-4444, I used a sample size of 500, and a population size of 10,000.  Then d was estimated by my program to be 3992, and thus 1-(3992/10,000) = 60.08%.  The actual duplication rate should be 1-(4444/10,000) = 55%.

**THANK YOU EVERYONE FOR YOUR COMMENTS!**
I will be posting again soon, as this solution assumes that my data follows a uniform distribution, which it doesn't...",en
1110117,2012-06-16 03:44:50,statistics,Curve fitting and slopes adjusting,v4blv,MyNameCouldntBeAsLon,1311784142.0,https://www.reddit.com/r/statistics/comments/v4blv/curve_fitting_and_slopes_adjusting/,2.0,13.0,"I have a very large dataset (over 117k rows and 50 different columns), two of those variables form (by construction) a perfect logistic growth curve. I would like to fit a curve with those data points, and obtain the slopes for a particular group of points (time and percentage, I would like to obtain these slopes per hour).


I mainly use stata and excel at work, and I've been having trouble obtaining what I want from those programs, I'm open to try any other software provided that it can manage a dataset as big as I need it.",en
1110118,2012-06-16 06:22:29,statistics,z-test for Proportions vs. Chi-Squared for Binomial Data (counts)?,v4ibj,mvinformant,1283801629.0,https://www.reddit.com/r/statistics/comments/v4ibj/ztest_for_proportions_vs_chisquared_for_binomial/,1.0,2.0,"Say you flip a coin 100 times and get 60 heads and 40 tails. I learned that you're supposed to use a z-test and find that z=10/rt(24). However, can one also use a chi-squared goodness of fit test, with each expected value being 50? I think that produces X^2 = 4. Shouldn't the chi-squared value be the square of the z-value? What am I missing?",en
1110119,2012-06-16 08:29:43,statistics,Recommendations for volunteer data analysis work,v4nk6,zhilabug,1338315621.0,https://www.reddit.com/r/statistics/comments/v4nk6/recommendations_for_volunteer_data_analysis_work/,11.0,4.0,"Does anyone know of any organizations that need online volunteer analysts to do work?

I used to work as an entry-level analyst position for a university (used SPSS) and would like to get back into the data analysis world. I feel like I need a resume boost before applying for analyst jobs. Any tips? Thanks!",en
1110120,2012-06-16 17:49:53,artificial,"Jaron Lanier: The Death of Alan Turing
     ",v51g9,LECHEDEMIPALO,1313545138.0,https://www.reddit.com/r/artificial/comments/v51g9/jaron_lanier_the_death_of_alan_turing/,6.0,0.0,,en
1110121,2012-06-16 22:49:14,statistics,Is there a way to find statistics on a websites growth? x-post,v5e66,i_make_stories,1313768978.0,https://www.reddit.com/r/statistics/comments/v5e66/is_there_a_way_to_find_statistics_on_a_websites/,3.0,0.0,,en
1110122,2012-06-17 02:27:35,datascience,Towards a global lookup service for corporate ids,v5o5q,[deleted],,https://www.reddit.com/r/datascience/comments/v5o5q/towards_a_global_lookup_service_for_corporate_ids/,0.0,2.0,,en
1110123,2012-06-17 09:16:08,statistics,World maps showing social and economic data,v656v,FlowerOfTheHeart,1334013327.0,https://www.reddit.com/r/statistics/comments/v656v/world_maps_showing_social_and_economic_data/,15.0,3.0,,en
1110124,2012-06-17 10:39:26,statistics,What statistics book would you recommend for a researcher trying to analyze his data?,v67ij,joelthelion,1146260183.0,https://www.reddit.com/r/statistics/comments/v67ij/what_statistics_book_would_you_recommend_for_a/,1.0,12.0,"I work in the field of clinical trials, but I have a very limited understanding of statistical analysis. I would need to learn about statistical tests, ANOVA, regressions, adjustements, etc. My goal would be to gain a good understanding of these things so that I can perform them on a routine basis with the good hypotheses and without being terrified of doing mistakes.

Any advice?",en
1110125,2012-06-17 12:14:25,MachineLearning,Freedman’s Neglected Theorem,v69mx,greenrd,1195738321.0,https://www.reddit.com/r/MachineLearning/comments/v69mx/freedmans_neglected_theorem/,1.0,0.0,,en
1110126,2012-06-18 03:34:06,statistics,"Is using a classifier to determine if two sets of multivariate data are drawn from the same population a) naïve/stupid/obvious or b) a good idea/interesting?
",v787j,duckandcover,1196196596.0,https://www.reddit.com/r/statistics/comments/v787j/is_using_a_classifier_to_determine_if_two_sets_of/,4.0,15.0,"First : I'm going to use the word ""compatible"" to refer to whether two datasets are (apparently) drawn from the same population.  How the population is generated is unknown so all you u have to go on is the data and without a model. So:

I have a presentation to give to a group of engineers.  They know basic stats and most  have used a classifier, in a black box sense, to classify  with respect to some given predefined task (i.e. some a proiri annotated binary class)

Sometimes we get new sets of data and due to various issues the data might not be  the ""compatible"" and so shouldn't be used as either validation data or combining with old data to make an expanded data set.   As far as I can tell they don't do much if any analysis on new data to find this out which causes all kinds of problems.

So, it seemed to me to be logical to use a classifier to determine if two given multivariate data sets are ""compatible"".  This is accomplished, with some caveats, by simply labeling the old data as the negative class, the new data as the positive class and seeing how well the classifier can discriminate (as given by the AUC).  This seems to work well.

Though I've googled around and not seen it, I'm still not sure that this isn't naive; that there isn't a standard better statistical test for this and even if it is as good a way as any*, I'm not sure that it isn't so obvious as to be insulting even to non-statistician engineers who have used classifiers.

Any opinions?


* I know that I can do multiple univariate tests but I don't think that's as good.
",en
1110127,2012-06-18 04:45:14,statistics,Question about estimating a model parameter via Markov chain Monte Carlo ,v7bw1,roger_,1178076247.0,https://www.reddit.com/r/statistics/comments/v7bw1/question_about_estimating_a_model_parameter_via/,3.0,14.0,"Say I'm estimating a parameter θ from data x. Once I obtain the posterior distribution of θ (or the likelihood function L(θ)) and get some samples from it via MCMC, what's the ""best"" estimate of θ?

Should I pick the single value of the MCMC samples that maximizes L(θ) (the mode)? This would correspond to the ML/MAP. Or should I pick the average value of all of them? What kind of estimate would that be?


It seems like the latter would be ""better"" since it involves doing averaging. Perhaps it wouldn't matter much if the posterior distribution was symmetric? ",en
1110128,2012-06-18 07:27:39,statistics,Baseball anyone? /r/Sabermetrics/ wants you!,v7kgw,boilface,1332346665.0,https://www.reddit.com/r/statistics/comments/v7kgw/baseball_anyone_rsabermetrics_wants_you/,12.0,1.0,"/r/Sabermetrics has been dead for a few months, but there has been an active effort to revive it, and mathematically oriented baseball fans (particularly those interested in statistics) would be a welcome addition. If you have any questions I'll answer what I can, but please feel free to bring them to /r/Sabermetrics.",en
1110129,2012-06-18 16:37:33,MachineLearning,"Does Richter's ""4096 Colours"" painting fulfill the Restricted Isometry Property for Sparse Signal Recovery?",v81ut,[deleted],,https://www.reddit.com/r/MachineLearning/comments/v81ut/does_richters_4096_colours_painting_fulfill_the/,8.0,1.0,,en
1110130,2012-06-18 20:08:07,statistics,Jointly Distributed Random Variables,v8cea,[deleted],,https://www.reddit.com/r/statistics/comments/v8cea/jointly_distributed_random_variables/,0.0,2.0,"I would like help on a statistics problem I have: 

Annie and Alvie have agreed to meet between 5 pm and 6 pm for dinner. Let X=Annie's arrival time, and let Y=Alvie's arrival time. a) What is the joint pdf of X and Y? b)What is the probablility that they both arrive between 5:15 and 5:45? 

a) f(x,y)={ kxy     5&lt;x&lt;6, 5&lt;y&lt;6

I'm confused about part b. The other examples I have seen have variables defined as probabilities. Does evaluating the integral at a time when the variables aren't probabilities still find a probability on the given interval? Also, the first integral is normally evaluated at 1-y, since 1-y=x and the total probability is 1. I obviously can't do this, since I'm not evaluating probabilities like I mentioned before. 

Do you just find k and evaluate the integral on 5.25 and 5.75?",en
1110131,2012-06-18 20:08:58,statistics,"About how many people would see an average post in AskReddit (16 comments, 4 upvotes) before it gets buried?",v8cgc,[deleted],,https://www.reddit.com/r/statistics/comments/v8cgc/about_how_many_people_would_see_an_average_post/,1.0,0.0,,en
1110132,2012-06-18 21:51:52,statistics,An upcoming R-package for Bayesian updating with particle filters  ,v8ijm,quaternion,1252017693.0,https://www.reddit.com/r/statistics/comments/v8ijm/an_upcoming_rpackage_for_bayesian_updating_with/,13.0,0.0,,en
1110133,2012-06-18 22:45:07,datasets,Historical Nielsen Ratings for a given TV show?,v8lpy,[deleted],,https://www.reddit.com/r/datasets/comments/v8lpy/historical_nielsen_ratings_for_a_given_tv_show/,1.0,0.0,I'm looking for historical Nielsen ratings for Saturday Night Live (SNL) over its 35+ year history. All I can find are the last few seasons and bits and pieces of earlier broadcasts. Anyone know if they're available?,en
1110134,2012-06-18 23:16:38,MachineLearning,BigML’s Fancy Histograms:  Efficiently summarizing large amounts of streaming data,v8nns,jjdonald,1192132770.0,https://www.reddit.com/r/MachineLearning/comments/v8nns/bigmls_fancy_histograms_efficiently_summarizing/,0.0,0.0,,en
1110135,2012-06-19 01:17:42,statistics,George Casella passed away,v8uxd,whyilaugh,1275146029.0,https://www.reddit.com/r/statistics/comments/v8uxd/george_casella_passed_away/,27.0,3.0,,en
1110136,2012-06-19 01:28:10,MachineLearning,Do Androids Recall Dreams of Electric Sheeps ?,v8vkc,[deleted],,https://www.reddit.com/r/MachineLearning/comments/v8vkc/do_androids_recall_dreams_of_electric_sheeps/,13.0,2.0,,en
1110137,2012-06-19 05:13:54,statistics,Question about CRM databases,v97zc,[deleted],,https://www.reddit.com/r/statistics/comments/v97zc/question_about_crm_databases/,0.0,2.0,"I was offered an internship working with a CRM (customer relationship management) database system.  I am wondering if anyone here has any experience working with one and what your experience was like.  I was doing a bit of research on it, and it sounds like you just enter data, and the program (or the company selling the system) does all the work.  I know every job is different, but - based off your experiences - what sort of work would I be doing?  Coding?  Running analyses?

I will be addressing these topics at my interview, but I am hoping to hear from you guys so I don't have to go in completely cold.",en
1110138,2012-06-19 06:37:32,statistics,Help in justifying the creation of nominal categories based on a numerical distribution,v9coc,BearBeer,1311184240.0,https://www.reddit.com/r/statistics/comments/v9coc/help_in_justifying_the_creation_of_nominal/,2.0,3.0,"Hey /r/statistics, I have a question in regards to the creation of new variables. I currently have a distribution of n=33, with a numerical variable based on behavioral observations. I want to create 3 distinct groups of low, medium, and high, with n=11 in each group. The low/medium/high categories will be based on the 11 subjects in the lowest third of the distribution, in the middle third, and in the highest. My experiment compares these three nominal/ordinal groups.

I was given the go ahead by a stats professor on my campus who is very conservative with his testing, but before I attempt to schedule an appointment to simply ask about justification of this go-ahead, I'd like to see if you guys could shed some light on things.

Thanks again!",en
1110139,2012-06-19 07:38:10,MachineLearning,Question for those of you whose work has something to do with machine learning,v9fs8,llbeanfan,,https://www.reddit.com/r/MachineLearning/comments/v9fs8/question_for_those_of_you_whose_work_has/,4.0,7.0,What do you do? What do you like most and least about your job?,en
1110140,2012-06-19 09:05:59,statistics,Are exponential families taught in undergrad? When do you learn about them?,v9jmu,enfieldacademy,1302474908.0,https://www.reddit.com/r/statistics/comments/v9jmu/are_exponential_families_taught_in_undergrad_when/,2.0,7.0,,en
1110141,2012-06-19 16:00:04,MachineLearning,"A simple inference problem, need help",v9vi6,gholfali,1298136430.0,https://www.reddit.com/r/MachineLearning/comments/v9vi6/a_simple_inference_problem_need_help/,3.0,6.0,"Hi MLers. I have a problem in inference which might be quite easy for you. The problem is that: I have a simple Bayesian network, F -&gt; X -&gt; m, where F gets the binary values (0 or 1), X is normal distributed with different mean and covariance depending on F and m gets 3 different values depending on a Chi-squared test on X. I am going to find Pr(F=1|X,m). What I have got is:
Pr(F=1|X,m) = Pr(F)*Pr(X|F)*Pr(m|X)/Pr(X,m)
My problem is how to calculate  Pr(m|X). I am sure that you can help me for that.",en
1110142,2012-06-19 17:36:22,statistics,SAS Macro Simplifies SAS and R integration,v9zoa,bdobba,1258441408.0,https://www.reddit.com/r/statistics/comments/v9zoa/sas_macro_simplifies_sas_and_r_integration/,12.0,0.0,,en
1110143,2012-06-19 17:44:50,statistics,Secularization in America: part one of a statistical model using data from the GSS.,va037,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/va037/secularization_in_america_part_one_of_a/,5.0,2.0,,en
1110144,2012-06-19 22:51:34,MachineLearning,"Big Data, 30,000 Scientists and a Startup ",vahgl,elemenager,1273599729.0,https://www.reddit.com/r/MachineLearning/comments/vahgl/big_data_30000_scientists_and_a_startup/,22.0,3.0,,en
1110145,2012-06-20 00:08:21,rstats,The Fairest Way to Pick a Team,vam6c,cavedave,1128052800.0,https://www.reddit.com/r/rstats/comments/vam6c/the_fairest_way_to_pick_a_team/,7.0,2.0,,en
1110146,2012-06-20 00:21:30,statistics,Cleaning up data,vamxf,CURIOUS_ABOUT_SEX,1332639502.0,https://www.reddit.com/r/statistics/comments/vamxf/cleaning_up_data/,7.0,6.0,"I have a txt file with college football results that I'd like to work with.  The main thing that's getting in the way right now is that the data includes the team's rank each week, like below.

29,2,Aug 31 2002,Sat,Duke,23,,East Carolina,16,

30,2,Aug 31 2002,Sat,(6) Florida,51,,Alabama-Birmingham,3,

31,2,Aug 31 2002,Sat,(5) Florida State,40,,Virginia,19,

32,2,Aug 31 2002,Sat,(8) Georgia,31,,Clemson,28,


The numbers I want to remove are in parentheses.  There are about 8000 lines of data, so I need to automate it somehow.  I'm not very experienced with this though.  Any tips?


UPDATE:  thanks for the responses guys, I've got it sorted out.",en
1110147,2012-06-20 03:28:52,artificial,Fluid Concepts and Creative Analogies,vaxbx,explanabrag,1318283469.0,https://www.reddit.com/r/artificial/comments/vaxbx/fluid_concepts_and_creative_analogies/,9.0,16.0,"Has anyone else:

 * read this book?

 * (if you're a programmer) tried to apply the cognitive model described in the book to their own projects?

The cognitive model described in the book is very fun to study. What are your thoughts about it?

Links:

[The book](http://www.amazon.com/Fluid-Concepts-And-Creative-Analogies/dp/0465024750)

[A review and description of the book by Daniel Dennett](http://pp.kpnet.fi/seirioa/cdenn/hofstadt.htm)

[Fluid Analogies Research Group](http://www.cogsci.indiana.edu/)

Weighing in on my own question:

I avoided putting my personal thoughts on the matter here initially because I didn't want to colour the discussion. Here they are now in any case.

The cognitive model proposed in this book is the most compelling and illuminating theory about what processes occur in the mind that I have ever seen. When you compare it to almost all other AI research, everything else seems brittle and uninteresting in comparison.

Any description of the model I would write would not do it justice, since I am still trying to absorb as much information about it as I can. The review written by Daniel Dennett I posted above provides an excellent description. As well as [Fluid Concepts and Creative Analogies](http://www.amazon.com/Fluid-Concepts-And-Creative-Analogies/dp/0465024750/ref=pd_sim_b_4), I've been reading the following two books written by co-authors and collaborators:

[Analogy-Making as Perception - A Computer Model by Melanie Mitchell](http://www.amazon.com/Analogy-Making-Perception-Computer-Melanie-Mitchell/dp/0262132893)

[The Subtlety of Sameness by Robert French](http://www.amazon.com/The-Subtlety-Sameness-Computer-Analogy-Making/dp/0262061805)

Also, [here is an old java implementation you can try out.](http://itee.uq.edu.au/~scottb/_Copycat/)

I posted this here to get the opinions and thoughts of those smarter than me, and perhaps enthuse more people about this awesome cognitive model.

Edit: The Amazon reviews are a good read too:

&gt; This is the way AI should be done: by focusing on the right level of abstraction, situated above the level of neuroscience but below the level of simple input-output function mapping. Finally, a computer model that makes those first steps towards doing the same thing that people do.
",en
1110148,2012-06-20 06:51:57,MachineLearning,"What are some applications to Physics, Chemistry, Biology?",vb8jz,wtkm,1340160447.0,https://www.reddit.com/r/MachineLearning/comments/vb8jz/what_are_some_applications_to_physics_chemistry/,3.0,3.0,"Hi there, I am interested in ML looking to find something interesting except for robotics (AI/Vision) since I am a little tired of it. What are  useful / interesting applications to those fields of science? 

Regards",en
1110149,2012-06-20 11:03:49,statistics,The Fairest Way to Pick a Team,vbj78,cavedave,1128052800.0,https://www.reddit.com/r/statistics/comments/vbj78/the_fairest_way_to_pick_a_team/,11.0,0.0,,en
1110150,2012-06-20 17:00:29,statistics,How should you explain a model's coefficients to laymen?,vbtq6,internetrageguy,1332700653.0,https://www.reddit.com/r/statistics/comments/vbtq6/how_should_you_explain_a_models_coefficients_to/,6.0,10.0,"I've been studying statistics for a couple of years now, and professors, TA's, and book authors have been unable to give me a straight answer on how you can interpret a model, *in earnest*, for laymen, say for a management team that knows nothing about statistics.

This isn't about interpreting a specific model.  It's about knowing the limits of what you can and cannot reasonably conclude from a model.

The way I've grown to think of a model is basically as a black box.  When I explain a model to managers, I always say that a model's coefficients **cannot and do not** represent global reality.  

For example, say you are predicting sales in 2014 based on the age of the company's CEO and the number of employees in the company.

You get a model that says Sales2014 = 999000*Age - 19000*Employees.

Personally, I would say that, holding all else constant (i.e. number of employees), assuming that this model, which consists only of Age and Employees, accurately represents the entire, global reality of all features that predict Sales2014, increasing Age by 1 year will increase Sales2014 by $999,000.

Then I would add that there is no way anyone can ever capture the entire reality of Sales2014 beyond using CEO Age and #Employees because your database sucks ass anyway, so this is the best representation of reality we have.  But, since this is a very limited picture of reality, you can only use it as a magical black box that sort of predicts Sales 2014.  **BUT: you cannot draw any global conclusions about the goodness or badness of Age or #Employees based on this model's coefficients**.  E.g., you can't use this model as a reason to fire 1000 employees.

Am I thinking about modeling the right way?",en
1110151,2012-06-20 19:10:30,statistics,Need assistance in categorizing mean results for a Likert scale  ,vc0sw,tonecapone3001,1298277107.0,https://www.reddit.com/r/statistics/comments/vc0sw/need_assistance_in_categorizing_mean_results_for/,4.0,2.0,"Hi I need some advice regarding categorizing mean results in a Likert scale. Let me give you a quick background. We are asking respondents the importance and frequency of knowledge areas within a given domain on a 5 point Likert scale ranging from 1 (Not important at all/Not Frequent at all) to 5 (Very High Importance/Very High Frequency). We are taking the means for these domains to determine how to categorize them according to importance and frequency within 5 groups: ""Very low importance/Frequency"", ""Low Importance/Frequency"", ""Average Importance/Frequency"", ""High Importance/Frequency"", ""Very High Frequency"". My question is what is the best way to categorize mean results from a Likert Scale.

Note: My partner proposes that we use the following scale

1-1.49 Very Low Importance
1.5 -2.49 Low Importance
2.5 - 3.49 Average importance
3.5 - 4.49 High Importance
4.5 to 5 Very High Importance

Im not too fond of this as it seems really arbitrary and I dont like that a mean rating for a domain at 4.49 and at 3.51 would both be considered High Importance. Any resources or links you guys have can help alot.

Thanks ",en
1110152,2012-06-20 19:35:46,MachineLearning,Question about visualizing Locally Linear Embedding (or any other dimensionality reduction for that matter),vc2b7,zionsrogue,1210268911.0,https://www.reddit.com/r/MachineLearning/comments/vc2b7/question_about_visualizing_locally_linear/,9.0,6.0,"Over the past couple of weeks I have spent a lot of time reading up on dimensionality reduction papers, ranging from PCA, random projections, LLEs, Isomaps, Conformal eigenmaps, etc. LLEs really struck me as interesting and I came across an [introductory paper] (http://www.cs.nyu.edu/~roweis/lle/papers/lleintro.pdf) that really gave a nice, easy to understand overview. [Page 7 of the intro paper] (http://imgur.com/sS8OD) shows how LLEs can map images with corner faces to the corners of the two dimensional embedding. I understand how this works, but I am really struggling to mimic their visualization. If anyone can point me in the right direction or even has some sample code I would be really appreciative. I really want to start playing around with dimensionality reduction visualizations, but I'm honestly spinning my wheels right now.",en
1110153,2012-06-20 20:59:48,statistics,Have any ideas on a way to visualize time delay?,vc7is,phelpsben,1281300426.0,https://www.reddit.com/r/statistics/comments/vc7is/have_any_ideas_on_a_way_to_visualize_time_delay/,1.0,1.0,"I'm working on a website monitoring platform and aiming to be different than all the others, I'm looking to find a creative and intuitive way to chart a time delay.

I check a host on a timed interval and log the time it took the host to reply.

This is how I currently chart uptime per hour/day: http://jsbin.com/onegas

I have looked into horizon charts but I'm not sure if someone who has no knowledge of what a horizon chart is would know how to read it.

Anyone have some ideas on how I could chart delay?",en
1110154,2012-06-20 21:58:22,statistics,I'm great at math but I have no idea how to do statistics. Reddit can you point me in the right direction?,vcb94,TYPES_WITH_CHODE,1339095987.0,https://www.reddit.com/r/statistics/comments/vcb94/im_great_at_math_but_i_have_no_idea_how_to_do/,7.0,18.0,"This has been my deep dark secret for a long time. I've made it through school and jobs by winging it with respect to any statistics that we have to do, but in reality I don't understand it at all. Literally, I couldn't tell you what a linear regression does or the significance of a standard deviation. However, I'm confident I can learn if I have the right starting position. 

So, here's my situation. I work in research, and until now my contributions were basically on a high level (summaries, interpretations), with statistics that I could usually hand off/exchange with someone else. But I recently got a promotion to a position where help from others won't be possible. 

In the past during school, I was in research methods courses, but some how (and I swear to god this was not my fault), I would end up in these shitty ""theoretical"" type courses that would focus on the concepts of research, rather than it's practical application. I tried to find a couple books on statistics but honestly didn't know where to start. The few that I did find would usually have some suuuuuuuper complex explanation about the math behind statistics that made the material superfluous. 

I really just need a book, course, webinar, etc, SOMETHING that can sit down and walk me through some research methods and statistics. ""Here is a problem. For this type of problem you want to use this type of method, and here's why. Go into SPSS and click on Analyze - &gt; etc...""

I have money, time, tenacity, and patience but I don't know where to start. Oh dearest reddit, could someone please point me in the right direction? If it helps, the research we do mostly focuses on psychology/business/marketing type stuffs.

",en
1110155,2012-06-20 23:43:05,computervision,"KITTIN Vision Benchmark Suite. Free ground truth data for self driving vehicles (stereo, LIDAR, INS, GPS etc). ",vci0s,kscottz,1302126782.0,https://www.reddit.com/r/computervision/comments/vci0s/kittin_vision_benchmark_suite_free_ground_truth/,11.0,1.0,,en
1110156,2012-06-21 02:03:02,statistics,EILI5: The gamma distribution vs. the Weibull,vcq74,MetaStable14,1321821311.0,https://www.reddit.com/r/statistics/comments/vcq74/eili5_the_gamma_distribution_vs_the_weibull/,7.0,10.0,"Hello all, I am in the field of surface texture, and my superior declares that we must use the gamma distribution for surface texture estimates. I have googled the gamma distribution, but the wiki/wolfram explanations are lacking. So can anyone explain what its good for in English, and why it would be preferred over a Weibull distribution?

Thanks!",en
1110157,2012-06-21 02:38:06,MachineLearning,Question on Clustering with unknown clusters.,vcrvt,cyborgbrain,1319490298.0,https://www.reddit.com/r/MachineLearning/comments/vcrvt/question_on_clustering_with_unknown_clusters/,1.0,12.0,"Hey guys, quick question.
I have data that (in frequency) looks like : 
http://imgur.com/a89Sd

Upon visual inspection of the data I used K-Means clustering to separate peaks around A and C.

Is there such an algorithm that will detect the number of means to separate, ignoring false peaks B and D.

The final data has many more peaks than the sample data.",en
1110158,2012-06-21 09:07:07,statistics,Root-finding with noisy functions — The Endeavour,vdc7j,efrique,1219734906.0,https://www.reddit.com/r/statistics/comments/vdc7j/rootfinding_with_noisy_functions_the_endeavour/,5.0,0.0,,en
1110159,2012-06-21 11:10:30,statistics,Naive Bayes Classifiers,vdgcq,swvist,1327845280.0,https://www.reddit.com/r/statistics/comments/vdgcq/naive_bayes_classifiers/,2.0,0.0,,en
1110160,2012-06-21 11:24:07,analytics,How do you manage your Google Analytics campaign urls?,vdgpp,snowliondev,1330948696.0,https://www.reddit.com/r/analytics/comments/vdgpp/how_do_you_manage_your_google_analytics_campaign/,5.0,5.0,"I am trying to get my organisation in the habit of using GA campaign links on all print publicity, social media links and ads. 

Some of the urls (for print and social media) will need to go through a 301 redirect or a shortening services they are short enough to type. I have a Google spreadsheet that auto creates the campaign URLs but I am wondering how you guys create, store and manage the shorter link versions?

Do you just use 301 redirects?
Do you use a third party or a hosted service?",en
1110161,2012-06-21 13:48:33,MachineLearning,Algorithms for blind source seperation/dimensionality reduction,vdk7q,bsnyder788,1323537394.0,https://www.reddit.com/r/MachineLearning/comments/vdk7q/algorithms_for_blind_source/,13.0,8.0,"I am trying to extract a low amplitude signal hidden in a high amplitude signal.  (I don't know the true low amplitude signal but wan't to find the ""best"" approximation.  Spectrums of the two signals overlap so I cannot just use a filter.

What techniques should I try?  I have tried dimensionality reduction using PCA and SVC with OK results, but would like to obtain even better results  (specifically I can see residue of the high amplitude signal in the frequency domain)

Thanks!",en
1110162,2012-06-21 20:43:17,MachineLearning,General question about how to implement predictive models.,ve3bb,mistidoi,1327512893.0,https://www.reddit.com/r/MachineLearning/comments/ve3bb/general_question_about_how_to_implement/,6.0,5.0,"Say I have developed a great predictive model in R, and I want to implement it.  For my workflow, being able to use that model in Ruby would be great.  Has anyone made PMML interpreters for Ruby? What do folks do around here to get the stuff out of their analysis tools and into their daily workflows?",en
1110163,2012-06-21 22:19:20,statistics,Secularization in America: a generational model of changes in religious affiliation,ve9m4,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/ve9m4/secularization_in_america_a_generational_model_of/,20.0,5.0,,en
1110164,2012-06-22 01:34:56,statistics,What probability distribution(s) can describe (and help me analyse) events like these? ,veltt,PilarGore,1325303615.0,https://www.reddit.com/r/statistics/comments/veltt/what_probability_distributions_can_describe_and/,6.0,6.0,"Hello all; non-statistician here, who is trying to re-learn some school- and first-year-undergraduate level statistics to try and solve a problem which I think requires statistical analysis.  The trouble is, I don't think my data can be described by the distributions we learned at school.   Here's a hypothetical example:  
I go and stand on a busy street every day for a year, counting the number of people who trip and fall.  There will *almost always* be no one at all.  Occasionally there might be one, and even more rarely, two, or even three people tripping up on a single day.  Then someone says to me, ""Daniel, I think the number of people tripping up per day is showing an increasing trend.""  So I go and count the number of people tripping over per day for a few more days.  Now I still mostly see zero trip-ups per day, but there does seem to be some kind of trend.  Of course what I really need is to do is determine the significance (if any) in the difference between my data sets to be more confident of making any conclusions.  

My specific case:  
We test the cleanliness of our clean room at work with Petri dishes filled with agar.  We use one dish per day in each of several locations, but let's focus on a single location.  We *almost always* (about 95/100 tests) get zero organisms.  Every now and then we get one colony, and even more rarely, two--or sometimes three--colonies per dish.  
Now imagine that we start to see more colonies.  Not a greater number of colonies (like counts of five, six or seven), but a seeming increase in the number of one- two- and three-colony plates.  As before, most of the tests come up with zero still--there just seems to be a slight increase in the number of plates with colonies on them.  Normally, staff determine the presence of a trend purely by inspection.  But I am not quite satisfied with this.  Nor am I satisfied with saying, ""4% of plates had colonies on them last month; 7% have colonies on them this month, so there is a trend"".  I am sure that there must be a way to *describe* these data--even if just approximately; I am also sure that there is a way to compare null- and alternative hypotheses to determine, quantitatively, whether or not there is actually a trend.  
So, folks, what do you think?  Is there a distribution to describe data like these?  Assuming I'm re-learning statistics from scratch, (though I'm picking things up quickly again) will I need a solid background to know what to do with this distribution, or could I just focus on it heavily to make the inferences I need to make?  
Long post I know, and I know I probably seem hopelessly out of my depth!  But any name of a distribution which can be used to describe my data would be really useful, and I'll be eternally grateful.  

Thanks in advance!",en
1110165,2012-06-22 04:00:05,MachineLearning,Is there a good example of Gradient Boosting in R or Matlab? ,veu2a,kripaks,1326208924.0,https://www.reddit.com/r/MachineLearning/comments/veu2a/is_there_a_good_example_of_gradient_boosting_in_r/,8.0,4.0,"Reference: http://en.wikipedia.org/wiki/Gradient_boosting
(By an example, I do NOT mean one that is available as an API.) ",en
1110166,2012-06-22 04:03:37,statistics,NC officials declare that only LINEAR regression is valid for predicting sea level rise,veu8k,weichen91213,1340326885.0,https://www.reddit.com/r/statistics/comments/veu8k/nc_officials_declare_that_only_linear_regression/,20.0,2.0,,en
1110167,2012-06-22 06:24:37,MachineLearning,Simple Checklist for Malformed Taping Machine - BestPack.com,vf286,keithw1,1337840232.0,https://www.reddit.com/r/MachineLearning/comments/vf286/simple_checklist_for_malformed_taping_machine/,0.0,2.0,,en
1110168,2012-06-22 08:25:08,statistics,@Risk software,vf8tb,[deleted],,https://www.reddit.com/r/statistics/comments/vf8tb/risk_software/,3.0,4.0,Is anyone familiar with some good beginner guides to using @Risk?,en
1110169,2012-06-22 08:27:21,artificial,"Deep Learning works when recognizing 20,000 object categories - Google/Stanford",vf8wo,marshallp,1239903633.0,https://www.reddit.com/r/artificial/comments/vf8wo/deep_learning_works_when_recognizing_20000_object/,28.0,15.0,,en
1110170,2012-06-22 10:05:46,analytics,Track Yahoo Marketing &amp; Microsoft adCenter in Analytics,vfcuj,praroop,1274180434.0,https://www.reddit.com/r/analytics/comments/vfcuj/track_yahoo_marketing_microsoft_adcenter_in/,1.0,0.0,"To track Yahoo Search marketing &amp; Microsoft adcenter in Google Analytics all you have to do is to use Google Analytics URL Builder from there you will be able to track the result.

For Yahoo Search Marketing in Google Analytics

Step 1) Website url ==&gt; place your website url

Step 2)

Campaign Source :- yahoo
campaign Medium :- cpc
campaign term :- {OVKEY} (for yahoo only)
Camplign Content :- my ad group
Campaign Name :- what tou want

Step 3
click on generate URL

Click Generate URL then copy &amp; paste this into the Destination URL field when you create an ad in YSM as seen below.


For Microsoft adCenter in Google Analytics

Step 1) Website url ==&gt; place your website url

Step 2)

Campaign Source :- yahoo
campaign Medium :- cpc
campaign term :- {OrderItemId} (for Microsoft only)
Camplign Content :- my ad group
Campaign Name :- what tou want

Step 3
click on generate URL

Click Generate URL then copy &amp; paste this into the Destination URL field when you create an ad in Microsoft adcenter as seen below.",en
1110171,2012-06-22 11:55:02,statistics,"Logistic Regression, OR and Reference Groups",vffwi,pennywaltz,,https://www.reddit.com/r/statistics/comments/vffwi/logistic_regression_or_and_reference_groups/,6.0,10.0,"Hey,

I'm trying to remember my time in Epi-school (epidemiology), where we learned logistic regression and I can't seem to find the answer to this.

So I am working on a project looking at Patient waiting room times. One of my predictors is time of day (basically I split the day into 4 categories). So.. in a logistic regression model, how does one usually pick which category to be the reference group for a predictor?

Do I pick the category with the most people in it in the total? least? I can't remember the general practice of this. So I am turning to Reddit for help!",en
1110172,2012-06-22 14:42:31,MachineLearning,What is the biggest open problem in statistics or machine learning?,vfjub,cavedave,1128052800.0,https://www.reddit.com/r/MachineLearning/comments/vfjub/what_is_the_biggest_open_problem_in_statistics_or/,27.0,17.0,,en
1110173,2012-06-22 16:37:07,statistics,Statistics and the science club,vfnq5,t_rex_tullis,1318255134.0,https://www.reddit.com/r/statistics/comments/vfnq5/statistics_and_the_science_club/,17.0,0.0,,en
1110174,2012-06-22 20:42:09,statistics,Help with a chi square analysis,vg0qz,shloimtotheE,1292308445.0,https://www.reddit.com/r/statistics/comments/vg0qz/help_with_a_chi_square_analysis/,4.0,25.0,"I'm trying to compare the results of an experiment I conducted aimed at increasing the number of books borrowed from the college library. I'm told I should use a chi square analysis, but I can't figure out how!

Group A is baseline data showing the number of books borrowed each month in the months prior to the experiment. Group B is the two months after the experiment. 


Group A 299, 196, 310, 427, 202, 310, 252

Group B 345, 396",en
1110175,2012-06-22 22:34:46,statistics,Does anyone know of a method to adaptively reconstruct a reduced basis for pdfs...,vg7e1,gfunkland,1260507828.0,https://www.reddit.com/r/statistics/comments/vg7e1/does_anyone_know_of_a_method_to_adaptively/,2.0,3.0,"Suppose (as an example) you have N clusters of Gaussian data about N distinct points.  These points are not known a priori and you want to efficiently reconstruct an approximate pdf for this data by summing Gaussian's, needing to both compute the means and the variances of each point.  Does anyone know of tools to ascertain the means, number N, and variances about each reconstructed point? ",en
1110176,2012-06-22 23:43:59,statistics,Can I use estimates from the data to decide which groups to test?,vgbdi,showmethedata,1291957272.0,https://www.reddit.com/r/statistics/comments/vgbdi/can_i_use_estimates_from_the_data_to_decide_which/,1.0,3.0,"I'm performing a series of fisher's exact tests to see if I'm getting an enrichment for certain protein domains (contingency table: two groups as rows, number of proteins having domain versus number of proteins not having domain as columns).


The problem is people run a test for each domain and use Bonferroni or FDR correction for p-values. But I have thousands of domains and hence tests, making it difficult to get many significant hits. 


Just using a barplot, I can already see most domains do not have differences (group A has almost the same proportion as group B). Am I allowed to use these sample proportions to set a threshold (e.g. must have &gt;1.5 fold change) to decide which domains I should test for significance (and hence reduce the number of unnecessary tests to perform)? Or does using the estimated proportion violate something I'm not aware of?",en
1110177,2012-06-23 08:40:55,computervision,new better-than-sift descriptor from google research,vh1iq,marshallp,1239903633.0,https://www.reddit.com/r/computervision/comments/vh1iq/new_betterthansift_descriptor_from_google_research/,2.0,0.0,,en
1110178,2012-06-23 12:11:21,statistics,"Chinese Data Mask Depth of Slowdown, Executives Say",vh7gv,TanBoonTee,1241235970.0,https://www.reddit.com/r/statistics/comments/vh7gv/chinese_data_mask_depth_of_slowdown_executives_say/,1.0,1.0,,en
1110179,2012-06-23 14:58:41,statistics,Userstatistics of Tor (anonymity network),vhaqo,psYberspRe4Dd,1328723014.0,https://www.reddit.com/r/statistics/comments/vhaqo/userstatistics_of_tor_anonymity_network/,3.0,3.0,,en
1110180,2012-06-23 17:09:33,statistics,"Gay Population In U.S. Estimated At 4 Million, Gary Gates Says",vhe5s,tmn516,1340460485.0,https://www.reddit.com/r/statistics/comments/vhe5s/gay_population_in_us_estimated_at_4_million_gary/,1.0,0.0,,en
1110181,2012-06-23 18:05:12,statistics,Anyone have any experience with SAS certificates? ,vhg38,Poolmunch,,https://www.reddit.com/r/statistics/comments/vhg38/anyone_have_any_experience_with_sas_certificates/,5.0,5.0,"I have extensive experience with R but I have never touched SAS, I think I would benefit from the Base and advanced SAS certificate but I'm unsure if it's worth the cash. 
Anyone here have these or others and do you think the industry will continue to accept these? 
",en
1110182,2012-06-23 20:08:15,datascience,Towards a global lookup service for corporate ids,vhleg,mhermans,1169219262.0,https://www.reddit.com/r/datascience/comments/vhleg/towards_a_global_lookup_service_for_corporate_ids/,4.0,0.0,,en
1110183,2012-06-24 00:51:51,MachineLearning,The Manifest Destiny of Artificial Intelligence,vhyq6,qkdhfjdjdhd,1211920583.0,https://www.reddit.com/r/MachineLearning/comments/vhyq6/the_manifest_destiny_of_artificial_intelligence/,45.0,13.0,,en
1110184,2012-06-24 03:07:19,MachineLearning,Out of my league at SeaTac...,vi53w,mhink,1268334331.0,https://www.reddit.com/r/MachineLearning/comments/vi53w/out_of_my_league_at_seatac/,20.0,7.0,"Hey /r/machinelearning! I was reading the ""nicest celebrities"" thread on AskReddit, but I figured my story should probably go here instead, considering it's kind of a niche thing.

About a month ago, I was leaving Seattle after an interview (just graduated college in CS) and I decided to grab a beer at the bar.  As I was joking with the bartender, an older gentleman sat down next to me and we started talking.  At some point he mentions to the bartender that he has a PhD in Math and Computer Science, so naturally I'm interested and mention that I'm graduating.  He mentions that his company is involved in statistical programing.

Immediately I ask ""oh, so do you do a lot of work with R?""

Nope.

This guy was none other than Tom Lehman, the director of the SAS Advanced Analytics lab.  Whoops.  Turns out he had been in Seattle on business, and he had been meeting with the board of directors of *Nordstrom*.

After a bit of a talk about life and stuff, he gave me his card and mentioned that if I ended up in machine learning, to give him a call.  Finished his beer and left for his flight.  All around class-act kinda guy.  He also spent a lot of time talking up SAS itself, and left me with a business card.

Just a fun story, I suppose.  I really wish I was more into ML (or fly fishing) so we could've had more to talk about, but all in all, great guy, and although I ended up taking another job, for awhile I seriously considered applying at SAS.

Cool story, huh? ",en
1110185,2012-06-24 03:34:42,datascience,"R users conference highlights: o.a. IDE, visualization and report-generation developments ",vi6a0,mhermans,1169219262.0,https://www.reddit.com/r/datascience/comments/vi6a0/r_users_conference_highlights_oa_ide/,3.0,0.0,,en
1110186,2012-06-24 08:40:15,MachineLearning,zero-to-machine-learning-in-less-than-seven-minutes,vijte,iamsiva11,1338794177.0,https://www.reddit.com/r/MachineLearning/comments/vijte/zerotomachinelearninginlessthansevenminutes/,1.0,0.0,,en
1110187,2012-06-24 10:21:10,statistics,urn problem: when to stop sampling?,vin5h,martinus,1137128400.0,https://www.reddit.com/r/statistics/comments/vin5h/urn_problem_when_to_stop_sampling/,11.0,17.0,"I've got an urn with black and white marbles. I know the total number of marbles in the urn, but do not know how many black or white's are in it. I assume that there are n or more black marbles in the urn. What I want to do is randomly and repeatedly take one marble out of the urn, determine it's color (which is a costly operation), and then determine how probable it is that my assumption could still be correct. So I take out one marble (and don't put it back), look at it's color, and each time I do this I calculate how probable it is that there were n or more black marbles in the urn.

Here is an example: I know there are 100 marbles in the urn, and assume there are 50 black and 50 white marbles in it. I take 10 marbles out, and each of them was white. I think now it is safe to assume that it is highly unlikely that there are 50 or more black marbles in the urn, so my assumption was probably incorrect.

Sooo, my question is: how can I calculate this probability? I think this is related to the [hypergeometric distribution](http://en.wikipedia.org/wiki/Hypergeometric_distribution), at least it sounds quite similar. My goal is to decide as early as possible (with a given probability) if the assumption could still be correct, so that I can discard the urn and try another one, if it is too unlikely that the urn would still have enough black balls. I don't mind if for e.g. every 100 urn I wrongly decide that it does not have enough black balls in it.",en
1110188,2012-06-24 13:24:08,datasets,OCLC moves worldcat.org into the world of Linked Data and slaps on a open access licence,vir2o,mhermans,1169219262.0,https://www.reddit.com/r/datasets/comments/vir2o/oclc_moves_worldcatorg_into_the_world_of_linked/,8.0,0.0,,en
1110189,2012-06-24 20:06:39,statistics,"n+1: Death by Degrees (is higher education a ""credentials cartel""?)",vj2xq,joydom,1267493064.0,https://www.reddit.com/r/statistics/comments/vj2xq/n1_death_by_degrees_is_higher_education_a/,23.0,12.0,,en
1110190,2012-06-25 18:11:08,rstats,"Analytical Methods (read not too computationally intensive) for optimizing time series processes (Eg, computing optimal number of lags)",vkoun,ultraspeedz,,https://www.reddit.com/r/rstats/comments/vkoun/analytical_methods_read_not_too_computationally/,5.0,8.0,"How do you guys go about calculating the optimal number of MA lags or specifying the order?

Without having the problem of look ahead bias please.

",en
1110191,2012-06-25 19:20:23,rstats,Some functions fail to load from package,vkssr,Iamthelolrus,1251613077.0,https://www.reddit.com/r/rstats/comments/vkssr/some_functions_fail_to_load_from_package/,3.0,1.0,"I'm working with the plm panel data package. Most of the functions from the package show up but I can't access anything that acts as a suffix to an established command (lag.pseries, fixef.plm). I've updated to the most recent versions of R and R studio and I can't seem to get the commands recognized.

I'm assuming this is an issue with the way that I've installed the packages. Any ideas?",en
1110192,2012-06-25 20:15:52,statistics,Expert on multivariate time series wanted,vkw55,plf515,1271028320.0,https://www.reddit.com/r/statistics/comments/vkw55/expert_on_multivariate_time_series_wanted/,2.0,2.0,,en
1110193,2012-06-26 05:59:39,MachineLearning,My world of computer science: Prediction of  play- An Introduction,vly50,xyz1234X,1338611934.0,https://www.reddit.com/r/MachineLearning/comments/vly50/my_world_of_computer_science_prediction_of_play/,1.0,0.0,,en
1110194,2012-06-26 06:56:46,artificial,"Beginner here. I'm wonder if I could create something of 
the AI sort if I worked on creating a program that I would 
teach to edit itself.",vm1kt,mecrio,1329516141.0,https://www.reddit.com/r/artificial/comments/vm1kt/beginner_here_im_wonder_if_i_could_create/,0.0,2.0,*Eventually*	teach to edit itself...and predict patterns.,en
1110195,2012-06-26 14:57:48,statistics,How user friendly should statistical software be?,vmiq7,plf515,1271028320.0,https://www.reddit.com/r/statistics/comments/vmiq7/how_user_friendly_should_statistical_software_be/,1.0,1.0,,en
1110196,2012-06-26 15:07:59,statistics,Would people be interested in a reddit for statistics using SAS? ,vmj2u,plf515,1271028320.0,https://www.reddit.com/r/statistics/comments/vmj2u/would_people_be_interested_in_a_reddit_for/,1.0,10.0,,en
1110197,2012-06-26 17:03:06,computervision,FREAK now in OpenCV !,vmnr2,[deleted],,https://www.reddit.com/r/computervision/comments/vmnr2/freak_now_in_opencv/,1.0,0.0,,en
1110198,2012-06-26 19:39:40,MachineLearning,"In a Big Network of Computers, Evidence of Machine Learning - NYTimes.com",vmwnf,[deleted],,https://www.reddit.com/r/MachineLearning/comments/vmwnf/in_a_big_network_of_computers_evidence_of_machine/,1.0,0.0,,en
1110199,2012-06-26 19:49:57,MachineLearning,Large scale Machine Learning in Action: How Many Computers to Identify a Cat?,vmxa1,jackhammer2022,1280494843.0,https://www.reddit.com/r/MachineLearning/comments/vmxa1/large_scale_machine_learning_in_action_how_many/,4.0,2.0,,en
1110200,2012-06-26 20:30:08,MachineLearning,Required knowledge and education for a career in machine learning?,vmzua,BlameKanada,1290279507.0,https://www.reddit.com/r/MachineLearning/comments/vmzua/required_knowledge_and_education_for_a_career_in/,13.0,14.0,"I am wrapping up my undergraduate degree in CS, and I've done a bit of specialization in AI/machine learning through these courses:  IR, CompLing, Data Mining, Robotics.  I also have some research experience applying machine learning to computer vision/object recognition for robotics.  At this point I would say I have an ""OK"" general understanding of various ML concepts:  classifiers, clustering, association rules, ANNs, etc.  However, I don't really have a deep understanding of the math behind these things, as my undergrad courses mostly glossed over those details.

I have been accepted into a MS CS program at a highly ranked university (top-5 in AI).  It will take me an additional 3 semesters of school to get the Masters vs. leaving with just a BS.  I am trying to decide if I need to continue with the MS degree.  I really like learning, and would LIKE to do the degree, but the opportunity cost is weighing on the decision.

My career goal is to become more than a general software engineer.  I like ML and related (vision, robotics, NLP, etc), and it seems like it's both interesting and practical to industry.  Ideally I would have a job where I keep up with the state-of-the-art and apply it to the needs of the organization.  Another great job would be doing applied (or basic) research.  I'm more interested in solving technically hard problems that say going to Scrum meetings, talking about unit tests and deployments, etc.  I'm not saying those aren't necessary, but if that's ALL I do at work I could not stand it.  I should also mention that I have substantial ""general"" software engineering experience and I like coding.  I chose to go back to school to do something more interesting.  The question now is whether to continue to the Masters or stop with a Bachelors.  PhD is not realistic for me in my situation.

I would love to hear from people who work with ML, AI, etc in industry, including level of education required.  Thanks!

tl;dr:  How much education is required to get an ML job in industry?
",en
1110201,2012-06-26 23:01:57,MachineLearning,Could a ML algorithm learn to solve Sudoku?,vna2x,Pr0bability,1313521984.0,https://www.reddit.com/r/MachineLearning/comments/vna2x/could_a_ml_algorithm_learn_to_solve_sudoku/,9.0,12.0,"Problem: Given a large enough database of solved Sudokus (smaller than 6,670,903,752,021,072,936,960 - the number of all possible 9x9 solution grids), would a Machine Learning algorithm be able to solve any Sudoku fed to it?

I'm just curious, if it is at all possible / whether it was already tried.",en
1110202,2012-06-27 02:03:03,rstats,selecting values from multiple tables,vnllz,hillset,1318127882.0,https://www.reddit.com/r/rstats/comments/vnllz/selecting_values_from_multiple_tables/,1.0,0.0,"Hi everyone,

So I'm working with 2 datasets that describe the same non-directional social network.  The first dataset is an edgelist with 2 variables for the source and destination along with 3 other variables and the second dataset is a matrix listing the geodesic distances between nodes.  The 5 variables from the first table are:
-source_node
-destination_node
-mutual (denoting if the edge is is mutual, for example 1-&gt;3 and 3-&gt;1)
-Followers (denoting how many edges point into the node)
-Following (denoting how many edges point out from the node)

For example, the data look like this:

source_node: 1, 1, 1, 2, 2, 3
destination_node: 3, 4, 5, 4, 5, 1
mutual: 1, 0 , 0, 0, 0, 1
followers: 1, 0, 0, 0, 0, 1
following: 3, 3, 3, 2, 2, 1

I also have a matrix showing the geodesic distance between nodes (basically how many nodes you have to go through to get to the target node)

For the above example, it would be a 5x5 matrix with the distances in the cells.  (ex. source_node=5 would be on the 5th row and would have the values:

5: 1, 1, 2, (infinite), 0

(the infinite just means that you can't trace a line from source_node 5 to source_node 4).

I'd like to write an R function that applies the following algorithm:

1.  For a given source_node, find that source_node in the destination_node list (ex. if the given source_node=5, find observations 3 (1-&gt;5) and 5 (2-&gt;5).  if the given source_node=3, find observation 1 (1-&gt;3).

2.  List the observations found in the first step in order of the highest value for the following variable (ex. if the given source_node=5, observations 3 (1-&gt;5) and 5 (2-&gt;5) are found.  since the source_node in observation 1 has a higher value in the following variable (3 vs. 2), list the observation source_nodes as 1, 2.

3.  Once all observation source_nodes have been listed in step 2, find the given source_node in the geodesic distance table and select all cells who have a geodesic distance of 2 (ex. if the given source_node=5, find the row labeled 5 in the geodesic distance table and find all cells with a geodesic distance of 2)

4.  Take the selected cells in the above step and list the columns they are in in order of the highest following values. (ex.  source_node=5 has a value of 2 for column 3 so theres no need to sort.  if source_node=5 has a value of 2 for columns 3 and 4, the output would be 3, 4 because 3 has more followings than 4.)

5.  List the output from step 2 then from step 4 (for ex. source_node=5 would have an output of 1, 2, 3).

I have no idea how to even start implementing this algorithm so ANY help the reddit community could provide would be a huge help.  I'm not sure if it would be better to try to code this in another language besides R (possible python?) but I'm better at R than anything else so I thought I would try here first.  Thanks SO MUCH for your help everyone.



",en
1110203,2012-06-27 03:45:59,statistics,What kind of conclusions can I reach with limited survey/poll data?,vnrd5,oiler_not_youler,1325975541.0,https://www.reddit.com/r/statistics/comments/vnrd5/what_kind_of_conclusions_can_i_reach_with_limited/,2.0,1.0,"I am looking to analyze some surveys but do not need a rigorous approach. 

I have a stack of N surveys. I do not know what N is, but I know the average value for each response in the survey.

I can also sort my N surveys by gender or age group, or both. But I do not know how many surveys will be in any grouping. I do, still, know the average values for each response in that smaller group.

What kind of information can I obtain by comparing the results of the overall survey and the smaller groupings. 

For example, if the overall average response to a question is very close to the average response for men, but far away from the response from women - then is it safe to say a small number of women answered that question compared to men?

Can I do that? What other things can I do along those lines? Thanks.",en
1110204,2012-06-27 05:27:45,statistics,Incoming sophomore Statistics major.  What should my minor be?,vnxa1,beaverteeth92,1292728016.0,https://www.reddit.com/r/statistics/comments/vnxa1/incoming_sophomore_statistics_major_what_should/,8.0,36.0,"Hi r/statistics.

So I decided to switch majors from Molecular Biology to Statistics last semester, which was the second semester of my freshman year.  The Statistics major at my university is only 50 credits though (out of 120 needed to graduate), and I figure it would be good to have a minor that would work well with it.

Currently debating between math and computer science.  I figure with math, it gives me more leverage for grad school and understanding how various inference methods work.  Additionally, linear algebra is a requirement for the statistics major and can be fulfilled with either an applied or a theoretical course so for the 15-credit minor, I'd be making up a class for my major and two classes for the minor (linear algebra and theoretical math).

On the other hand, computer science and programming knowledge are pretty much necessary to do any kind of professional work with statistics and happens to be much more practical.  It's a 16-credit minor and nothing crosses over.

I also did the math, and I have 24 credits completed already (in addition to a bunch of AP credits and another 4 from Calc 2 at the end of the semester), putting me at another 96 credits to go before I can graduate.  Calc 1, 2, and 3 are necessary for my major, so I will have 12 (other four from my Intro Stat class) from my major already completed by the fall (38 major credits left).

Since I have some leeway, I have a few options.  I can minor in math or CS, minor in both, or add CS as a second major (40 credits) because some of the math classes cross over.  I'd love to have a versatile skillset.

Any advice r/statistics?


EDIT:  Oh yeah.  On a different note (I didn't feel like making another thread for this because it's not a detailed question), I'm buying one of the new Macbooks with retina display.  Is 8GB of RAM enough for statistical stuff or should I get 16?",en
1110205,2012-06-27 05:42:02,statistics,Need help determining if this repeated measures violates assumptions.,vny4r,thelisa,1316037108.0,https://www.reddit.com/r/statistics/comments/vny4r/need_help_determining_if_this_repeated_measures/,3.0,3.0,"I have a question that I'm hoping somebody here can help me with. I am a student working on my psychology thesis and am having some difficulty analyzing my data. 

I gave 300+ participants a questionnaire. They all answered a personality measure (continuous scale) and all gave their gender. I also gave them another set of questions that involved a categorical response to hypothetical situations. The hypothetical situations varied so that they had a 2x2x2 within-subjects design. Each participant either got ""easier"" versions of the situations or ""harder"" ones, so they each got 8 situations and difficulty was a between-subjects factor. Participants responded to a Yes/No question about the situations. 

To recap in list form:

Between Subjects Factors:

-Hypothetical Situation Difficulty Group (easy v. difficult; categorical)

-Participant gender (categorical)

Covariates:

-Personality questionnaire (continuous)

Conditions with yes or no answers (hypothetical situations each with 3 manipulations with 2 levels each) coded as:

Hypothetical Situation 1: 1,1,1

Hypothetical Situation 2: 1,1,2

Sit. 3: 1,2,1

Sit. 4: 1,2,2

Sit 5: 2,1,1

Sit 6: 2,1,2

Sit 7: 2,2,1

Sit 8: 2,2,2

I am interested in looking at how the personality measure and participant gender were related to the responses to the situations and also at how the different conditions within the situations relate to the responses. Ideally I would do this all in one analysis so that I can look at the interactions as easily as possible. 

Initially, I analyzed this in SYSTAT using a repeated measures GLM. However, I have had a couple people suggest that I should do it in a logistic regression instead. One said that the GLM violates an assumption. 

So, my questions are:
Does the repeated measures GLM violate any assumptions? If there isn't a problem with doing this as a repeated measures GLM, I would rather do it that way since I know how. 

Any help/input would be greatly appreciated! ",en
1110206,2012-06-27 07:14:49,artificial,How to build Artificial General Intelligence (Really),vo3d2,marshallp,1239903633.0,https://www.reddit.com/r/artificial/comments/vo3d2/how_to_build_artificial_general_intelligence/,17.0,9.0,,en
1110207,2012-06-27 09:21:18,MachineLearning,"Are there any books, articles or videos that explain 
the central concepts of machine learning from a 
layman's perspective?",vo9d2,habroptilus,1326800322.0,https://www.reddit.com/r/MachineLearning/comments/vo9d2/are_there_any_books_articles_or_videos_that/,16.0,9.0,"I have very little programming knowledge and even less math. I would like to be able to understand the concepts, methods and paradigms of ML, even at a very high, broad level. Later in my education I'm going to be dealing with it in more detail, so I'd like to have some context, which makes anything easier to learn.

EDIT: Thanks everyone! :) I have a lot of reading to do!",en
1110208,2012-06-27 11:11:42,statistics,"Subway systems of the world, presented on the same scale",vod3x,LXH,1330616671.0,https://www.reddit.com/r/statistics/comments/vod3x/subway_systems_of_the_world_presented_on_the_same/,1.0,0.0,,en
1110209,2012-06-27 16:48:03,statistics,r/statistics I knew I'd find you! I have a (probably simple) statistical problem here I could use some help with!,vonbm,blablubli,1338748134.0,https://www.reddit.com/r/statistics/comments/vonbm/rstatistics_i_knew_id_find_you_i_have_a_probably/,7.0,13.0,"[So here's what I have.](http://imgur.com/6jUzj).

I have to compare my ""Group 1"" with ""Group 2"" and ""Group 3"" seperately.

These are different cohorts of patients suffering from a disease - having different ages at the onset of the disease (""Age at Onset"") and disease durations.

What would be a suitable test, software and online tool to compare (and graphically display) the different frequencies of ages at onset, also comparing these blonging to the different groups?


Also, let's say I need to compare a group ""1"" with a control group, seeing if feature ""X"" is associated with my group ""1"" or not.

I tried to use [this](http://imgur.com/fpIcR)(imgur), and it gives me this P-value saying the association between the groups and outcomes is not statistically significant ([here](http://imgur.com/wjbwQ)).

I just need to say that my Group ""1"" isn't differently associated with the feature ""X"" than my control group.

How do I do this?


And is there a way to give a figure for how much these two are not associated, rather than how much they are associated? I'm not sure I can just 1 - P value.


I apologize for all of the statisticians who have until now killed themselves for how ignorant I am in statistics.",en
1110210,2012-06-27 20:33:37,statistics,Work Project: Forecast Coverage ,vp0bl,iamlilt,1316018448.0,https://www.reddit.com/r/statistics/comments/vp0bl/work_project_forecast_coverage/,1.0,2.0,"So this is for my work project and I need some advice on how to use statistics method to help me prove the following and make suggestions.

Here is the stiuation: My company is forecasting a purchase amount of a certain product from the customers 30 days prior to the acutal order from the customers.

Due to the lead time, my company asks the materials vendors to prepare for the materials based on our forecast and giving a 20% variance. (Means if we ask vendor to prepare 100k, they need to have up to 120k) Keep in mind that all this are happening 30 days prior to the acutal order.

Right now I have a bunch of past data, and it shows that my company is under-forecasting most of the time (means actual &gt; forecast). I cannot change the forecast system, because it is not my jurisdiction, however, I can change the variance (the 20%). So here comes the questions.

1.How to calculate current forecast accuracy (With 20%) - I'd like to know what is the current ""coverage level"", such as 95% of the time our forecast will cover X1% ~ X2% of the acutal order

2.If I were to increase the 20% variance, how to build a model that I can test on the coverage of different variance setting? Such as 30% variance will cover Y1%~Y2% of the actual order; 40% will cover Z1%~Z2% of the acutal order. All at 95% of confidence level.

Thank you!
",en
1110211,2012-06-27 20:48:22,artificial,Let's think about the applications of strong AI.,vp1a6,yuuray,1302683101.0,https://www.reddit.com/r/artificial/comments/vp1a6/lets_think_about_the_applications_of_strong_ai/,17.0,19.0,"The idea of replicating the way a human mind works is something I keep thinking more about every day. Figuring it out would probably be the biggest discovery in human history. But why? What tools would it put in our arms? How would it affect people, corporations, and governments?

Besides the obvious benefit of having something (someone?) to think for you, it also would also completely change the field of psychology, and would perhaps allow subjects like 'social science' to become a real science. Mental disorders would be reduced to formulas, and think about this: anti-thoughts. If a thought, idea or concept is reducible to a mechanical operation, the exact negative of that operation would completely cancel that thought. Want to forget a traumatic experience? Just administer the right pattern of electric stimulation and it's gone! Want to frame someone for a murder? Just plant the memory, along with a touch of guilt and a desire to turn himself in. 

And another one that speaks to my imagination:
Once the different parts of the strong AI are well enough understood, people can begin to build templates on their computer. Online repositories where you can download a language module so your home made AI can speak Turkish, or just download GeneralPurposeJoe, who is known for his reliable and clean way of thinking. General purpose
bits of 'brain machine' that you can easily combine to form a test villain for your video game. You'd be playing videogames where the final boss, isn't just fighting you, he actually *wants* to kill you.

Well, these are just some fantasies that make the search for strong AI so interesting to me. So please tell me what you think of them, and share some of your own ideas on the impact of strong AI.",en
1110212,2012-06-27 21:46:07,statistics,Looking for a better statistic than population density,vp576,dsampson92,1258961174.0,https://www.reddit.com/r/statistics/comments/vp576/looking_for_a_better_statistic_than_population/,2.0,10.0,"The standard two statistics given for population (of states and countries, not in the statistics sense per se) are total population and population density.  The problem is that population density does not tell the whole story, as people are not evenly spread out in any country.  Some countries have a very low population density, but the population is pretty densely packed in cities and suburbs, the countries just have a whole lot of nothing.  Countries like Canada or Sweden, for example.  On the other hand, the population of the eastern US is much more evenly distributed though there are obviously still population conglomerations in cities.  Is there a statistic that measures how skewed the population density of a country is?",en
1110213,2012-06-27 21:59:53,statistics,How do I analyze return rates for surveys?,vp63f,tonecapone3001,1298277107.0,https://www.reddit.com/r/statistics/comments/vp63f/how_do_i_analyze_return_rates_for_surveys/,4.0,5.0,"Need assistance. I am sure its an easy analysis and one I would slap my head once someone helps me. Here is my situation:

My company conducts yearly paper (w/ web option) surveys for our client. After 10 days (on avg), my company sends out a reminder postcard to all potential respondents with the web option only. wants to look at Response Rates for a yearly survey from a reminder postcard that was sent 10 days after the surveys were initially mailed. This year they decided to change the postcard significantly from the years previous. Now they want to know if the new reminder postcards increased web responses significantly from the year before.  I understand this is not the best approach. If I would have set up the study I would have sent the old and new postcards for the 2012 surveys and compare the RR for the two. This controls for other seasonal variables. Unfortunately this is not the case, Does anyone have an idea of the approach I should use?
",en
1110214,2012-06-27 22:12:38,MachineLearning,[AskML]  Which method of unicode normalization is best suited for natural language processing?,vp6xl,omginternets,,https://www.reddit.com/r/MachineLearning/comments/vp6xl/askml_which_method_of_unicode_normalization_is/,6.0,8.0,"I've been reading a lot on the subject of Unicode, but I remain very confused about normalization and its different forms. In short, I am working on a project that involves extracting text from PDF files and performing some semantic text analysis.

I've managed to satisfactorily extract the text using a simple python script, but now I need to make sure that all equivalent orthographic strings have one (and only one) representation. For example, the 'fi' typographic ligature should be decomposed into 'f' and 'i'.

I see that python's unicodedata.normalize function offers several algorithms for normalizing unicode code points. Could someone please explain the difference between:

- NFC
- NFKC
- NFD
- NFKD

I read the [relevant wikipedia article](http://en.wikipedia.org/wiki/Unicode_equivalence#Normalization), but it was far too opaque for my feeble brain to understand. Could someone kindly explain this to me in plain English?

Also, could you please make a recommendation for the normalization method best adapted to a natural language processing project?

Thank you very much in advance!",en
1110215,2012-06-28 00:11:10,statistics,Best calculator for undergrad statistics?,vpenk,intheaethyr,1319730063.0,https://www.reddit.com/r/statistics/comments/vpenk/best_calculator_for_undergrad_statistics/,4.0,18.0,"Can anyone possibly help me out with a suggestion towards the best calculator to invest in for undergrad level statistics? I don't want to spend a lot of money only to wish I had got something which had different functions.

I'm willing to spend the money to get a decent one but I just don't want buyers remorse when I see other peoples.",en
1110216,2012-06-28 05:46:10,artificial,"Rock Paper Robot: You Lose, Every Time",vpy7z,rydan,1296704387.0,https://www.reddit.com/r/artificial/comments/vpy7z/rock_paper_robot_you_lose_every_time/,0.0,0.0,,en
1110217,2012-06-28 07:21:43,statistics,why is it so hard to get an unpaid internship in statistics? have you encountered similar difficulties?,vq3n1,[deleted],,https://www.reddit.com/r/statistics/comments/vq3n1/why_is_it_so_hard_to_get_an_unpaid_internship_in/,0.0,24.0,"I apologize if the posts in this subreddit recently have taken to an undergrad trend, but i think what I'm asking is significantly different enough from the other statistics major post to warrant a new thread.

I have friends in various other majors who (with similar GPAs, which I know doesn't say much unless I specify the majors) have easily gotten paid/unpaid internships. I have applied to over 25 internships/jobs which I've found online. The majority of these were through my university's job database, but I also searched internships.com. I've contacted department heads to see if there were any need for help in the department and ""cold emailed"" relevant businesses. The only place to offer me an interview did not have a website or phone number and did not return my emails! 

I've been searching for positions for this summer or the coming fall and at this point it feels like I'm all but explicitly stating ""free work! please, take it!"". *i'm not looking for sympathy* though this might be a rant, because I'm still a college student (majoring in statistics if it wasn't obvious), but I would like to know whether other statisticians on Reddit had experienced these difficulties when beginning their job hunts? Some kind of assurance that I'm not missing a vital step to make this process easier would help as well.

I also understand that I may not be qualified to do much of anything with the education I have thus far attained. However, I feel that by applying to primarily unpaid internships I am compensating for the lack of an extensive skill set. 

**TLDR: Was it difficult for you at first when you began looking for jobs/internships? What are some things did to land your first job/internship?**",en
1110218,2012-06-28 09:32:50,MachineLearning,Decision forests: Tree ensembles for everything!,vq9t1,cypherx,1141075032.0,https://www.reddit.com/r/MachineLearning/comments/vq9t1/decision_forests_tree_ensembles_for_everything/,24.0,12.0,,en
1110219,2012-06-28 16:56:39,statistics,Margin of Error - Do Girls like ice cream less?,vqnv4,[deleted],,https://www.reddit.com/r/statistics/comments/vqnv4/margin_of_error_do_girls_like_ice_cream_less/,2.0,3.0,"Is the following accurate?  Are there other issues I need to consider?

If data shows that 55% of the total respondents like Ice Cream but only 49% of female respondents like Ice Cream, is there a significant difference between these two groups?

http://en.wikipedia.org/wiki/Margin_of_error

The maximum margin of error at 95% confidence level can be generalized as ~ 0.98/sqrt(n).

Assuming total group size 100 and the female group size is 40.

The maximum margin of error in the total group averages is (0.98/sqrt(100))=(0.98/10)=0.098=~10 percentage points. The full group is reported as 55% like ice cream but being more clear you can also say that 45-65% of people like ice cream with 95% confidence.

Since your group of females is smaller, the maximum error is higher (0.98/sqrt(40))=0.155=15.5pp.

Conclusion, no, there is not a significant difference in the groups at the 95% confidence level.

If you are less concerned about your precision, you can use the 90% confidence level instead, which changes your leading factor from 0.98 to 0.82. Also note, these are maximum levels of error. If your measured values are closer to the extremes (5% or 95%, the error rate is actually much lower.
",en
1110220,2012-06-28 17:05:18,MachineLearning,Is Machine Learning Losing Impact?,vqoay,[deleted],,https://www.reddit.com/r/MachineLearning/comments/vqoay/is_machine_learning_losing_impact/,1.0,0.0,"Thoughts on the ""ML that Matters"" paper and on the differences between ""ML-as-academic"" and ""ML-for-production"".",en
1110221,2012-06-28 17:53:07,statistics,Stupid statistical language question,vqr5d,[deleted],,https://www.reddit.com/r/statistics/comments/vqr5d/stupid_statistical_language_question/,3.0,6.0,"Ack, I don't know the term I need here. 

Multicollinearity is when the data with two explanatory variables is heavily correlated. However, I don't know what to say if an explanatory variable and dependent variable exhibit the same correlation.  Is there a specific name for the phenomena?

I'm just looking to say that I did not include an explanatory variable in my regression because I feel that it would be highly correlated and wanted to see if there was a more parsimonious way of doing so.
",en
1110222,2012-06-28 18:23:20,statistics,A little help with biostatistics,vqsz0,blablubli,1338748134.0,https://www.reddit.com/r/statistics/comments/vqsz0/a_little_help_with_biostatistics/,6.0,15.0,"I have practically no experience when it comes to statistics, so sorry if this is somehow uneasy to read.

I understand that I should run a t-test when variances are similar.
And a Welch-test if the different groups have different variances.
I'm using a software called ""MedCalc"", and it seems to be working fine but I'm having trouble interpreting the results.
I've run an F-test to compare the variances between two groups; followed by a t-test and a welch test.
I've compared cohorte 1 with cohorte 2 then cohorte 3 with cohorte 2 independently, both regarding the ""Age at Onset"" and the ""Disease duration""
I don't know how to interpret the F-test and which test to choose (t-test or Welch test).
Secondly, say I choose the t-test; which information regarding it are relevant when writing down the results?

Here are my results:

http://imgur.com/a/dpUF2",en
1110223,2012-06-28 18:26:02,statistics,How to identify the most important unsolved problems in statistics,vqt4s,t_rex_tullis,1318255134.0,https://www.reddit.com/r/statistics/comments/vqt4s/how_to_identify_the_most_important_unsolved/,16.0,0.0,,en
1110224,2012-06-28 19:04:43,statistics,"Secularization in America: do people get more religious as they age, or are old people more religious because they were born earlier?",vqvgt,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/vqvgt/secularization_in_america_do_people_get_more/,13.0,3.0,,en
1110225,2012-06-28 20:05:21,statistics,How would one find the correlation of a point with another lagged point within a periodic dataset?,vqz7u,SoularEclipse,1319986220.0,https://www.reddit.com/r/statistics/comments/vqz7u/how_would_one_find_the_correlation_of_a_point/,2.0,3.0,"As the title suggests, I am trying to determine the correlation of specific points with other specific points in my dataset, which is periodic. The problem I'm having is that I can get the correlation at lag 0 to be 1 but the correlation at all lags for all points is not always on [-1,1]. I figure it must be a problem with the way I am calculating the mean and variance.

I understand that the correlation of a point with another lagged by t steps should be


&gt;(x(0)-mean(0))(x(0+t)-mean(0+t))/
&gt;
&gt;sigma(0)sigma(0+t)


I have been calculating the ""local"" mean and sigma based on all values between x(0) and x(0+t), but when t=0, the mean is identically x(0), and therefore the correlation is 0, not 1. If this is the correct equation, how do you correctly calculate the ""local"" mean and sigma for each time?

(I am making these calculations using Fortran. If this ends up being the right way, I will look for an error in the code, but as many times as I have been through the code I have begun to think it is a method error.)",en
1110226,2012-06-28 21:01:57,MachineLearning,Machine Learning Summer School,vr2t6,rcrabb,1298711340.0,https://www.reddit.com/r/MachineLearning/comments/vr2t6/machine_learning_summer_school/,10.0,3.0,,en
1110227,2012-06-28 22:26:33,statistics,A Playful Interface for Linear Regression,vr883,tmcw,1146334336.0,https://www.reddit.com/r/statistics/comments/vr883/a_playful_interface_for_linear_regression/,1.0,0.0,,en
1110228,2012-06-29 03:03:54,statistics,Is 30-35 observations too small to do a survival analysis regression model?,vrp7d,sortizo,1288834170.0,https://www.reddit.com/r/statistics/comments/vrp7d/is_3035_observations_too_small_to_do_a_survival/,1.0,0.0,"I am doing a work about survival time of banks in Mexico, I am going to take information from 2000 to 2012 and I have about 30 to 35 observations between those years? 

I actually have 2 questions:

* What do you think about the period of time, is it too short or is good enough? I wanted to do my analysis since 1990 but I don't have any information.

* How about the 30 to 35 banks? Do you think that is large enough in order to do regression analysis?",en
1110229,2012-06-29 11:22:10,statistics,P = 1,vscuh,[deleted],,https://www.reddit.com/r/statistics/comments/vscuh/p_1/,0.0,8.0,"In fisher's exact test:


Is it possible that p= is 1,00000? Or should it read something like 0.99999? I mean the 
probability of a false positive error can not be exactly 1, can it?",en
1110230,2012-06-29 14:18:38,MachineLearning,Prerequisites for Machine Learning,vsh40,[deleted],,https://www.reddit.com/r/MachineLearning/comments/vsh40/prerequisites_for_machine_learning/,10.0,6.0,"Hello everyone, 

I know this question has been asked (and answered) many times already, but I decided to create a new post since I have a more specific query. I am currently doing a project on Machine Learning but I realized my Math and Stats background is too weak to grasp a lot of the concepts. I wish to get my prerequisites in shape. Can you please suggest me books that I can read to cover these prerequisites ? I am looking for recommendations for books on Linear Algebra, Basic Statistics and any other prerequisites that you think are required. Thank you!

P.S : My background is limited to basic high school math. My CS fundamentals are in better shape though.

EDIT : Thanks! This was exactly what I was asking for. I have also enrolled for the Intro to Statistics class at Udacity.",en
1110231,2012-06-29 16:58:02,statistics,Cross-validation (What is it good for?) ,vsmll,Professor_IR,1310512226.0,https://www.reddit.com/r/statistics/comments/vsmll/crossvalidation_what_is_it_good_for/,6.0,4.0,,en
1110232,2012-06-29 17:08:39,statistics,Stand Your Ground laws and homicides ,vsn2w,Professor_IR,1310512226.0,https://www.reddit.com/r/statistics/comments/vsn2w/stand_your_ground_laws_and_homicides/,1.0,2.0,,en
1110233,2012-06-29 17:24:35,rstats,Arduino library?,vsnt9,quatch,1294935914.0,https://www.reddit.com/r/rstats/comments/vsnt9/arduino_library/,1.0,0.0,"Does anyone know of an R library that would allow serial communication with an arduino?
Thanks.",en
1110234,2012-06-29 21:18:18,statistics,Trying to crack a tough problem involving the optimal number of floating software licenses to be shared. Don't know where else to ask this but it seems stats related. NOT homework help...,vt0x6,OuchWhatDoYouDo,1257031102.0,https://www.reddit.com/r/statistics/comments/vt0x6/trying_to_crack_a_tough_problem_involving_the/,2.0,6.0,"Being of more of a finance background, stats is interesting to me but not my best forte and I am hoping r/statistics could help me down the right path to solving this problem. If not, please point me in the right direction if you know a better place to get help! My company is not going to get a consultant to figure it out and so although I'm not the best person for it, I'm tasked with getting this resolved. 

Details:
We operate a facility with a frequently fluctuating number of employees. We do know however what our maximum capacity is.

Each employee has a specific job requiring the use of certain software tools. The tools reside on a networked license server and are checked out when in use. When the employee closes the program, the license becomes available again on the server.

2 or more jobs may use the same software package, but one job may require it 100% of the time while the other only uses it, say, 40% of the time.

We have reporting tools on the license server that return the following data for any selected time period: AVG, MIN, MAX, LAST
However, the reporting tool was installed only a few months ago, so the time period we can access is limited.

By reading these graphs it is readily apparent that some tools we have overpurchased and some are underpurchased

Question:
How can I use statistics to best determine -

* 1) The optimal number of licenses we should have available in our current state based on users and usage patterns?

* 2) A way to calculate the number of additional licenses needed if we were to expand beyond our current maximum capacity?

* 3) A usage threshold at constant headcount with increased software usage that would trigger us to increase the available license pool?

So far I have tried:

* Calculating equivalent annual headcount and then existing licenses per average headcount

* Calculating an adjusted headcount &amp; licenses per headcount based on usage of the tools. (ex: Tool A has 1 
user at 100% and 2 users at 50% - Adjusted Users are counted as 2)

* Calculating additional licenses needed based on keeping the ratio of licenses per adjusted headcount ratio constant with increased headcount.

My model I've tried to create does not work in that it cannot identify tools that are underpurchased to begin with.

I'm pretty comfortable in Excel which is where my work is taking place. If you have any pointers I would really really appreciate it.",en
1110235,2012-06-29 22:53:06,AskStatistics,"How do random processes like Brownian motion lead to mixing and diffusion in fluids? I'm under the impression that colloidal molecules are likely to be shoved about in any direction with equal probability, so why doesn't everything cancel out to produce particles that are stationary on average?",vt6p8,spirit_of_the_stairs,1307853630.0,https://www.reddit.com/r/AskStatistics/comments/vt6p8/how_do_random_processes_like_brownian_motion_lead/,7.0,3.0,,en
1110236,2012-06-30 02:03:27,rstats,Job openings,vthi6,[deleted],,https://www.reddit.com/r/rstats/comments/vthi6/job_openings/,1.0,0.0,"Hi, my company is looking to hire 2 statisticians, 0-5 years out of finishing a masters degree. If interested message me!",en
1110237,2012-06-30 03:21:39,AskStatistics,A question about the variance of a white noise ,vtlau,lajkabaus,1339447070.0,https://www.reddit.com/r/AskStatistics/comments/vtlau/a_question_about_the_variance_of_a_white_noise/,3.0,8.0,"Hi,

I have a pretty simple question which I thought I do not need to make a topic about, but Google is actually not helping, which is surprising. So here it goes:

How can white noise have infinite power if its variance is finite?

As far as I am aware, the following is always valid for a stationary zero-mean random process X which is classified as white noise (i.e. flat power spectrum)

R_x (0) = power = E[X^2 (t)] = sigma^2 · delta(0) = infinite = Var{X(t)} = sigma^2 = finite

assuming that the statisics of the random process are anything with the finite variance, for example, Gaussian distribution. So, yeah, I'm looking at the AWGN.

So, what gives?

Although I am aware of the physique of realistic white processes, I am purely interested in the theoretical POV here, so I assume that this white process indeed has an infinite power. How is that possible when at the same time its probability distribution has finite variance?

Many thanks in advance.",en
1110238,2012-06-30 04:34:34,MachineLearning,What classes should I take?,vtonx,tardisblue,1309281368.0,https://www.reddit.com/r/MachineLearning/comments/vtonx/what_classes_should_i_take/,0.0,9.0,"Some background info: I have a BA in math, but felt limited by my lack of coding skills so I'm back in school now for a BS in CS. My ultimate goal would be to get into machine learning in industry (Netflix, Twitter, etc.). My plan is to take a lot of CS with a lot of statistics. Are those the appropriate fields to focus on?

On this, am I taking the right classes?

Here's my first semester:

1. Grad level statistics: applied probability
2. Undergrad CS: intro to CS
3. Undergrad CS: intro to programming
4. Undergrad math: numerical analysis

Thanks!",en
1110239,2012-06-30 05:04:26,statistics,My company (Quicken Loans) is hiring so many stats people it's NOT EVEN FUNNY,vtpzn,garfieldsam,1328992729.0,https://www.reddit.com/r/statistics/comments/vtpzn/my_company_quicken_loans_is_hiring_so_many_stats/,18.0,24.0,"EDIT: Received a couple questions about how to find stats related jobs on the careers site. Almost any job that includes ""analytics,"" ""analysis,"" or ""data"" in the title or is database-oriented should fit the skills/interests on this subreddit. There are stats-related positions opening everywhere, but the most are in Data Ops, which is (strangely) a sub-team of Information Technology. Here's a link to that page:

https://www.quickenloanscareers.com/web/JobListings.aspx?Category=Information+Technology

Also, for your reference, I know in the Detroit office major boons to your application will be demonstrated knowledge or experience with *R*, *Minitab*, *SQL*, financial stats, and the mortgage business. There are some other really sought after skill-sets (particularly data mining and database stuff), but they are for people who are higher up in the food chain than me and I'm not familiar with them.

Anyway, that's not to say you should feel discouraged if you don't have any particular facility with those skill sets. It'll just be helpful if you do have them to emphasis those on resumes!

____________________________________________________________________

If you do happen to apply, let me know so I can get credit for a referral. :]

Here's the career site:

http://quickenloanscareers.com/

The company and its sister companies are growing like crazy. Quicken itself is tripling the size of some of its departments in the coming months, and we need a lot of people fluent in statistics to analyze the huge (and really cool) amounts of data we have. Everything from PhD- to BA-level stats knowledge and skills needed.

Positions across the country, but mostly in Detroit, Cleveland, Scottsdale AZ, and Charlotte NC.

Plus Quicken is often regarded as one of the best companies to work for. I can attest. It's the best job I've ever had.


#10th best place to work in the country, Forbes 2012 list:

http://money.cnn.com/magazines/fortune/best-companies/2012/snapshots/10.html

Example of an amazingly awesome job posting (and of the sweet culture at QL):

https://www.quickenloanscareers.com/web/ApplyNow.aspx?ReqID=52936",en
1110240,2012-06-30 18:34:31,statistics,"Hello r/statistics, I have a simple question about correlation coefficients.",vudwt,japko,1320357174.0,https://www.reddit.com/r/statistics/comments/vudwt/hello_rstatistics_i_have_a_simple_question_about/,2.0,12.0,"Hi, I'm analysing data for my master's thesis in psychology. I didthe Kołmogorow-Smirnow test for checking whether my variables have a normal distribution or not. My main dependent variable does not have a normal distribution, but the independent variables do.

I know that the Pearson correlation coefficient is not good for non-normal distributions, but does that apply to the dependent or independent variables? 

My gut tells me I should probably use Spearman correlation, but I want to be sure. Can you help me with that please?

Thanks!",en
1110241,2012-06-30 19:36:26,statistics,Monte Carlo Simulation - product of ratios doesn't come out right,vugfs,[deleted],,https://www.reddit.com/r/statistics/comments/vugfs/monte_carlo_simulation_product_of_ratios_doesnt/,6.0,6.0,"I'm trying to do a Monte Carlo simulation of a stock, using the closing price ratios P(t)/P(t-k), generally for k=22 (Avg. # of trading days in a month). I'm using a data base of daily closes for about 3 years.

In order to get the 22 day distribution, I randomly selected and multiplied 22 k=1 change ratios, and got a mean and sd.

To check it, I did a run just using ratios of points 22 days apart.(k=22)

They were entirely different.

The mean and sd of the 22 multiplied ratios was almost the same as that of the raw 1 day date, while the sd of the 22-day ratios was, as expected, 4 or 5 times greater.

I thought this must imply some kind of trend, which my random sampling was missing, but an autocorrelation for all values &gt;0 seemed to be distributed 'randomly' around .05.

Why would the product of n samples of a random value have a std. dev. similar to that of the single underlying values?
",en
1110242,2012-06-30 22:08:11,statistics,R functions for generating decision tree graphics?,vunca,[deleted],,https://www.reddit.com/r/statistics/comments/vunca/r_functions_for_generating_decision_tree_graphics/,20.0,2.0,"Hi all!

Is anyone familiar with whether there is some R utility for drawing DAGs  in the decision tree context? I am interested in something kind of like xtable where you can cat latex code either to files or the console that draws nodes and links to show inclusion / exclusion criteria for clinical studies.

If it doesn't exist, would anyone be interested in doing a github collab?",en
1110243,2012-07-01 00:11:23,MachineLearning,Is Machine Learning Losing Impact?,vut3d,kafka399,1335048442.0,https://www.reddit.com/r/MachineLearning/comments/vut3d/is_machine_learning_losing_impact/,7.0,3.0,,en
1110244,2012-07-01 02:43:52,statistics,Question about reality of being an actuary (school wise),vv08z,sunny_person,1338085966.0,https://www.reddit.com/r/statistics/comments/vv08z/question_about_reality_of_being_an_actuary_school/,2.0,6.0,"I have a few questions for any of you that became actuaries.  How much school really does it take to get a regular, run of the mill actuarial job?  I am 31, with a family, and am trying to get my first degree the hard way.  That's two classes a semester as time allows (because I work full time and have a family.) I deal with numbers and minor statistic things at work on a regular basis and do enjoy the number playing and I have done well in all the math classes I've taken (typically high A's).  After much undetermined major thought, I've decided I would like to become an actuary.  I'm still a couple years away from graduating (almost done with my sophomore year of college.  It's taken a while to even get here.) and from what I understand, to be an actuary, you don't necessarily have to major in statistics but it helps. Also, from what I understand, I will want to start taking some actuary tests junior year of college, so when I get my bachelors degree, I can go to a company and get an internship or something, and essentially learn while I keep taking these tests to become a full fledged actuary.  
Now is this accurate?  Can I become an actuary with just a bachelor's degree?  I really don't have the luxury of time to have double majors, or taking a bunch of statistics classes and then discovering I need a masters and a cherry on top to actually become an actuary.  My classes need to count.  Is this something I can do with just a major in statistics and maybe a minor in something else, at just two classes a semester?  Or should I just scrap it and get the traditional ""degree just to have it""?  
Thanks for any input on this.  With the price of college nowadays I don't want to waste any money on a hope and dream that won't happen.",en
1110245,2012-07-01 07:43:48,artificial,Easy to use GPU accelerated neural networks - torch 7,vv8mx,marshallp,1239903633.0,https://www.reddit.com/r/artificial/comments/vv8mx/easy_to_use_gpu_accelerated_neural_networks_torch/,19.0,1.0,,en
1110246,2012-07-01 17:43:38,MachineLearning,Can computers learn how to learn?,vvola,[deleted],,https://www.reddit.com/r/MachineLearning/comments/vvola/can_computers_learn_how_to_learn/,0.0,2.0,,en
1110247,2012-07-01 17:53:39,MachineLearning,Getting in the ML business,vvoxt,mfalcon,1264563760.0,https://www.reddit.com/r/MachineLearning/comments/vvoxt/getting_in_the_ml_business/,3.0,6.0,"I'm working as a web software developer but I'm really interested in the Machine Learning and Natural Language Processing fields. I've begun self studying(I took the ML class from Stanford) and I'm playing with the idea to start a business using this technologies.
The point is that I don't know how to start, what could be the potential clients and market, how much knowledge would be necessary to get clients...

There are a lot of stories in HN about web applications, but I didn't read anything about this kind of companies/startups, maybe because this technologies require building personalized solutions instead of the one to many from most startups.

I think that the path will get clearer as I keep reading and getting practice, but I'd nice to know some experiences from another people with similar interests.
",en
1110248,2012-07-01 18:03:26,MachineLearning,Is there a supervised learning algorithm that can inject randomness into its output in proportion to its uncertainty about the result?,vvpb2,sanity,1149365629.0,https://www.reddit.com/r/MachineLearning/comments/vvpb2/is_there_a_supervised_learning_algorithm_that_can/,15.0,20.0,"I'm wondering if there is a machine learning algorithm that produces a probability as an output, which can inject randomness into the output in proportion to uncertainty as to what the output should be.

The application is to decide what ad to show people when they visit a web page.  Our goal is to show people the ad they are most likely to click on, but for new ads we won't have much data, so initially there will be greater uncertainty as to what their click through rate will be.

A very simple version of this, which only looks at the number of impressions and clicks on each ad, is to use a beta distribution, this technique is described [here](http://blog.locut.us/2011/09/22/proportionate-ab-testing/) (it's also known as Thompson sampling, and it's a solution to the multi-armed bandit problem).

But I'd like to use a machine learning algorithm that uses a much wider range of attributes than simply the number of impressions and clicks on each ad.  Ideally the input attributes can be both categorical (where there could be thousands of possible values for each attribute, for example ""city/state/country""), and numeric (like time of day).

I was thinking perhaps a Bayesian Network learner, where we use a beta distribution to inject randomness into the probabilities used in the Bayesian Network when making a prediction, but I want to see whether others have looked into this problem.",en
1110249,2012-07-02 00:36:22,statistics,Good reference textbook for a biologist?,vw7ac,OneLegAtATime,1312819470.0,https://www.reddit.com/r/statistics/comments/vw7ac/good_reference_textbook_for_a_biologist/,7.0,7.0,"I'm a 4th year biology undergrad. I was only required to have one quarter of statistics, and the teacher was atrocious. Although I understand the bare fundamentals, I would like to have a better understanding of stats. Is there any old textbook that can be considered a 'bible' for fundamental statistics? I'm looking to buy a slightly older edition off of Amazon used.",en
1110250,2012-07-02 00:55:34,statistics,The perils of categorizing continuous variables,vw8ab,plf515,1271028320.0,https://www.reddit.com/r/statistics/comments/vw8ab/the_perils_of_categorizing_continuous_variables/,4.0,15.0,,en
1110251,2012-07-02 03:03:00,MachineLearning,What's the state of the analysis and generation of sheet music?,vwei8,Tiomaidh,1256863695.0,https://www.reddit.com/r/MachineLearning/comments/vwei8/whats_the_state_of_the_analysis_and_generation_of/,12.0,7.0,"## Background
I'm a CS undergrad particularly interested in AI, and have gotten my feet wet with ML through internships, Stanford's online AI course, and a job I have at a startup doing simple text mining and less-simple searching (and I'm also learning through osmosis as the software company I have a summer job at just got acquired by IBM's Big Data division). I'm also incredibly interested in 18th-century Scottish fiddle music, and my chief hobby for the past six years has been playing, composing, and otherwise studying it.

I've noticed that there's a lot of patterns in the tunes. The vast majority are 32 bars (measures), there's a handful of patterns that make ~5 unique bars get repeated in such a way as to fill the 32 (one common idiom is playing ABAC ABAC DEDC DEDC twice, where each letter represents a bar). And when playing a tune I've never seen or heard before, I sometimes have correct intuitions about what new content will come next--there's a lot of melodic and chordal patterns whose precise nature I can't quite articulate.

## Project
So as any sensible person would do when faced with vague theories and good data sources, I want to package up several thousand tunes in [ABC notation](http://en.wikipedia.org/wiki/ABC_notation), cause my computer to do incantations over them, look at the patterns my program finds out of pure musicological interest, and then use them to generate new tunes.

*Now it's time for the question*: do you know of any previously done work on something similar? I've spent a good amount of time trying to find things, but can only find audio-based projects. I'm not interested in analyzing the genre of an MP3 or contrasting the performance style of two artists. Nor am I interested in smashing together several loops of electronic sound and calling it music. This is a strictly sheet music-based project.

## Practical Details
From a logistics standpoint, I'd have a little bit of time this upcoming academic year to work on it (on the side), and if I wanted to I could have about twelve 20-40 hour weeks in the summer of 2013. In the summer and fall of 2013 I plan to be in Edinburgh studying ethnomusicology (yay, domain knowledge). I'm also thinking of applying to the [Singularity Institute](http://singularity.org/) and spending a week or two there in the beginning of the summer--if anyone has any thoughts on that I'd be interested in hearing them.

By asking about prior work I'm hoping someone will link me to a journal article where I'll learn what algorithms other people have used and how (un)successful that was--which would give me a place to start. If that doesn't work out:

## Current Strategy
First I'll create a hierarchy of things I'm interested in. One group might be broadly responsible for the overall structure of a tune, and I'll have one node recording data on length, another node recording data on which patterns of repeated bars are used, etc. Another top-level group might be broadly responsible for figuring out what type of note comes after what, with nodes that keep track of intervals (be it a minor third or a fifth or whatever), chord patterns, relative position in the bar, relative position in the tune, relationship to the key signature, etc. And so on and so forth. This obviously requires a lot of thought, and I'd like to try as much as possible to make things specific enough to be easily findable in the corpus, but abstract enough to be useful in a broader context. Then I'll run the corpus through them to get the actual data, inspect the results, probably iterate on that a few times as I realize what type of information is more valuable than others, and then feed it all to a glorified Markov chain generator for the composition aspect.

-----

I've done my best not to make an imposing wall of text, but...I wanted to make sure you had all the information you might want. The main question is about previous efforts, but anything else you'd like to add to the discussion is very welcome. Does this seem like a reasonable project? Any words of warning? I'm aware that even if everything goes perfectly according to plan and I have an automatic traditional tune spitter-outer the usefulness is questionable (not much of a market to monetize it with, and it seems a bit out there to go the academic route with)...it's purely for personal interest, pragmatism be darned.

Thanks!

**TL; DR**--See `Project` section.",en
1110252,2012-07-02 13:59:24,statistics,Here is a poem I wrote about the problems of being a data analyst,vx3xp,plf515,1271028320.0,https://www.reddit.com/r/statistics/comments/vx3xp/here_is_a_poem_i_wrote_about_the_problems_of/,0.0,0.0,,en
1110253,2012-07-02 17:50:20,AskStatistics,Correspondence between two versions of an assessment?,vxbve,DrClem,1324827830.0,https://www.reddit.com/r/AskStatistics/comments/vxbve/correspondence_between_two_versions_of_an/,5.0,3.0,"I have an old assessment that outputs a score of 0, 1, or 2. I updated the assessment with new images and text, but overall the same content. Then I took a bunch of participants and have them complete the old and the new assessments. How do I show that the two forms yield the same results (i.e. reliability of the new assessment)? A Pearson R doesn't make sense. Any thoughts?

Thanks in advance.",en
1110254,2012-07-02 18:06:24,statistics,Imputation Reference Text Help,vxcps,snoius,1265493835.0,https://www.reddit.com/r/statistics/comments/vxcps/imputation_reference_text_help/,4.0,3.0,"For my master's program I am required to complete a project of my choosing.

My project is examining Inpatient Survey Responses (healthcare survey data) in order to find new predictors of hospital scores. If I can find new predictors, if would mean that the Center for Medicare Services (CMS) would be able to more ""fairly"" compare hospitals of differing demographics.

As my dataset is based on survey responses, there is of course missing data. I have 11,000 individual responses, with a total of 300,000 data points containing around 40,000 missing points (ie 260,000 points of existing data).

My question lies here, is there a text you would recommend on reading about imputation, and methods of imputation? Or do you have any recommendations?

**edit:** Double words aren't double the fun.",en
1110255,2012-07-02 20:57:17,MachineLearning,What would you do with a bunch of greenhouse gas emissions data?,vxmm3,beanwolf,1341246095.0,https://www.reddit.com/r/MachineLearning/comments/vxmm3/what_would_you_do_with_a_bunch_of_greenhouse_gas/,2.0,3.0,"The data is from different organizations' buildings, trucks, data centers, etc. Really anything that emits greenhouse gases. Unfortunately the data only dates back to 2008, but could be as granular as monthly data in some places.

What would you do? Thanks!",en
1110256,2012-07-02 21:28:22,MachineLearning,L1 vs. L2 Regularization?,vxomj,nickponline,1285653350.0,https://www.reddit.com/r/MachineLearning/comments/vxomj/l1_vs_l2_regularization/,22.0,14.0,"I read that L1 favours sparse models where as L2 favours models with small coefficients. However I don't see how this is the case, sure with both L1 and L2 regularization (sum |theta| vs. sum theta^2) setting theta to zero will be favourable, so what makes the distinction between L1 being sparse and L2 having small coefficients. 

",en
1110257,2012-07-02 21:43:13,statistics,Help request for determination of rate (apologies for any blatant idiocy),vxpn7,Marzipan86,1333260693.0,https://www.reddit.com/r/statistics/comments/vxpn7/help_request_for_determination_of_rate_apologies/,0.0,9.0,"I want to start by apologizing for asking potentially inane statistical questions here, but I am way behind for a master's student in biology, and I seem to have developed a mental block on the topic...

My thesis is regarding metabolic rate of an aquatic organism in response to alteration in temperature and photoperiod. Right now, I have a lot of raw data regarding dissolved oxygen concentration and temperature all organized and edited in Excel. The next step is to determine the rate of oxygen consumption. 

The most obvious way I know of to do this is simply linear regression/best fit line for a graph of oxygen concentration over time. I've done this in Excel, but I'm not getting the resolution that I want (the rates are very low because the water volume was pretty high, which means I'm not getting an appropriate number of significant figures using default settings). 




What I want to know is:

*Is linear regression the best option to determine rate?

*Is there a way to make Excel give me more numbers? (I'm not sure how to ask this... I need more than the first non-zero digit)

*Is anyone versed enough in SAS to explain to me in plain old English how to run the correct procedure and how to read the output? Or is there a VERY clear, non-technical explanation on the internet somewhere?

*Can someone please suggest a book to help me figure out biostatistics? Preferably with basic instructions for using the more common stats programs (because, shocker, I'm pretty computer illiterate, too). Keep in mind that I need it explained to me without a lot of mathematical jargon or extra information.",en
1110258,2012-07-02 22:49:00,statistics,Error bars - standard error or standard deviation?,vxtyx,7ypo,1265929188.0,https://www.reddit.com/r/statistics/comments/vxtyx/error_bars_standard_error_or_standard_deviation/,9.0,10.0,"I'm no expert in statistics, by any means, but I have come across it in different settings while working in different labs. (Honestly, I should have a better grasp of stats given how many times I've used it)

My question has to do with some graphs I've seen lately - typically, error bars have been used to show the standard error of measurement used. However, I've recently seen a string of graphs that use standard deviation instead (this is real data, not idealized z- or t-distributions).

So, is there a convention about which to use, or is it based on the context specific to data type or preference?",en
1110259,2012-07-03 00:05:24,computervision,Help! Problem with SIFT implementation,vxz33,shitty_sift,1341262583.0,https://www.reddit.com/r/computervision/comments/vxz33/help_problem_with_sift_implementation/,9.0,6.0,"Hey all! I am trying to write an implementation of SIFT, just as an excercise. However, I'm running into problems that I haven't been able to figure out yet. As far as I can tell, what I'm getting is the opposite of SIFT: it finds uninteresting, flat areas of the image. I'm using the [VXL](http://vxl.sourceforge.net/).

Anyway, my understanding of the early stages of SIFT are as follows: 

* Build a guassian pyramid
* Using this pyramid, get a difference-of-gaussians pyramid
* Find all local extrema to get potential keypoints
* Doesn't matter, since I don't get this far.

I have a pastebin of my code [here](http://pastebin.com/JmpURcEC), if someone would be willing to help, I would be eternally grateful. For reference, so far, [this](http://imgur.com/qz8eu) is what my algorithm spits out, with a magenta pixel at the location of every detected ""keypoint"". 

For reference, [here](http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf) is Lowe's paper on SIFT (PDF Warning).

Finally, standard disclaimer, new to reddit, a friend recommended y'all to me, my apologies if I did something wrong or violated some rules of conduct.",en
1110260,2012-07-03 00:16:30,MachineLearning,Introduction to Conditional Random Fields,vxzsb,cypherx,1141075032.0,https://www.reddit.com/r/MachineLearning/comments/vxzsb/introduction_to_conditional_random_fields/,1.0,1.0,,en
1110261,2012-07-03 06:00:05,MachineLearning,L.A. Cops Embrace Crime-Predicting Algorithm - Technology Review,vyjtp,brente,1284170273.0,https://www.reddit.com/r/MachineLearning/comments/vyjtp/la_cops_embrace_crimepredicting_algorithm/,43.0,11.0,,en
1110262,2012-07-03 06:29:56,rstats,"Hey, rstats! ... why does R round *down* at 0.5",vylgv,datalies,1289234879.0,https://www.reddit.com/r/rstats/comments/vylgv/hey_rstats_why_does_r_round_down_at_05/,9.0,5.0,"... try it: round(0.5)

is there some proof that justifies this. If there is, i expect it to sound something like, ""2+2 == 5, for large enough values of 2"".",en
1110263,2012-07-03 07:49:45,statistics,Survey! (Saw this in /r/engineeringstudents and figured I'd give it a go here).,vypwa,byronhout,1318991022.0,https://www.reddit.com/r/statistics/comments/vypwa/survey_saw_this_in_rengineeringstudents_and/,2.0,6.0,"If there's any information you want to leave out then just leave it blank. Just copy and paste my format and fill it out.
1. Age:
2. Gender:
3. How many years in college so far:
4. Desired Degree\Degree Obtained if you're already done:
5. Desired job\Job Obtained if you already have one:
6. Operating System:",en
1110264,2012-07-03 18:29:21,rstats,help with dropping unused levels in a package,vzd02,tenurestudent,1337720339.0,https://www.reddit.com/r/rstats/comments/vzd02/help_with_dropping_unused_levels_in_a_package/,3.0,2.0,"I'm developing a package as part of my dissertation, early on I need to pull the number of levels for a variable in the data frame presented (unique site ids). I realized from my sample data, that the way I subset-ed retained all the levels - 6,076 vs the 154 in the sample data. Since this could be a potential bug for others, I'd like to prevent it from causing issues. 

as of now, I have:
data[,1] &lt;- as.character(data[,1])
data[,1] &lt;- as.factor(data[,1])

Which seems to work fine, but it means re-writing the data several times and when scaled up to larger data sets, this could be rather cumbersome. I know the gdata package has a function to address this and I can do b &lt;- a[1:5, drop=TRUE], but would these be any less gluttonous then what I've written?? I'm also trying to avoid requiring packages.
THANKS!!!
",en
1110265,2012-07-03 19:10:08,analytics,Whose Search Data is it Anyway? – Firefox to encrypt referring search strings,vzfe6,gadgetgranny,1303123294.0,https://www.reddit.com/r/analytics/comments/vzfe6/whose_search_data_is_it_anyway_firefox_to_encrypt/,1.0,2.0,One of the key reporting metrics returned by website analytics programs such as Google Analytics and Comscore is referral data.,en
1110266,2012-07-03 19:18:18,statistics,Optimistic Overconfidence in Data Analysis (crosspost from r/PhilosophyofScience),vzfuf,djaipel_samoylovich,1301277827.0,https://www.reddit.com/r/statistics/comments/vzfuf/optimistic_overconfidence_in_data_analysis/,4.0,1.0,,en
1110267,2012-07-03 21:15:24,MachineLearning,[AskML]I am looking for another genetic programming library comparable to this one without luck,vznag,ml_zealot,1301685204.0,https://www.reddit.com/r/MachineLearning/comments/vznag/askmli_am_looking_for_another_genetic_programming/,1.0,0.0,"I found pystep http://pystep.sourceforge.net/ awhile ago and love the syntax for setting up custom genetic programming problems.  The problem though is that it is dated, and not the fastest, would anyone know of any similar libraries for genetic programming that use similar syntax, in python or MATLAB?glance at the example page to see syntax, I have been unable to find a decent library in matlab for genetic programming, all are mostly genetic algorithms",en
1110268,2012-07-03 21:38:40,statistics,"How to interpret a confidence interval, *after* one has seen the data and calculated the interval?",vzoss,SkepticalEmpiricist,1285372750.0,https://www.reddit.com/r/statistics/comments/vzoss/how_to_interpret_a_confidence_interval_after_one/,13.0,33.0,"I feel a bit queasy when I hear talk of ""frequentist interpretations"" of statistical tests such as the [Confidence Interval](http://en.wikipedia.org/wiki/Confidence_interval). Let's me lay out a few non-controversial statements before building up to my specific question.

First, a simple medical example involving a blood test.  If the patient is sick(S), the test result will be positive(+) 95% of the time.  If the patient is healthy(H), the test result will be negative(-) 95% of the time.

    P(+|S) = 0.95
    P(-|H) = 0.95

Everybody is either Healthy or Sick, therefore the test is accurate 95% of the time, no matter whether you are healthy or sick.  This sounds fantastic.

Now, here's the tricky bit: Of all the people that get a positive test result, what proportion of those are sick?  It's tempting to think 95%, because we have a test that is accurate 95% of the time.  But that logic doesn't hold.  Given a positive test result, the conditional probability of being sick could be as little as zero.  Imagine if almost the entire population is healthy - in this case almost all of the positive test results are simply false positives and therefore very few of them are sick.  You would need to have some idea what proportion of your patients are sick (call this a 'prior') in order to calculate probabilities that condition on the data.

So, to summarize, we have a strong statement of accuracy (""accurate 95% of the time"") which is true for all values of the parameter (H or S).  But, when we condition on the data, the statement might not be true at all.  We lose all optimism as soon as we see the data.  We need to make further statements (priors) if we wish to have a basis to calculate conditional probabilities such as P(S|+).

We can replace Healthy and Sick with two conventional hypotheses, such as ""Electrons and positrons have the same mass"" and ""they have different mass"".   A frequentist wouldn't be happy putting a probability on such things, arguing that such things are either true or false and there is nothing random about it. Therefore it becomes more difficult to say, given a particular observed dataset, how reasonable the hypotheses are.  Does *this* data tell us that one hypothesis now looks better than it did?  I find it difficult to get a frequentist to make any such statement from a confidence interval.

We can change the setup further, so that we have a continuous parameter space rather than just a simple discrete space.  But the logic still holds. *Before* we look at the data, we can say ""the probability is at least 95% that the true value of the parameter will be within the interval that we compute.""

The parameter isn't a random variable, in the confidence interval framework, but the experimental data is a random variable.  The distribution of the data, and of the resulting interval, is a random variable whose distribution depends on the parameter.  It is meaningful to make statements, in advance of the experiments, about the probability that the interval will contain the true value of the parameter.

But we can make no such statements *after* we have looked at the data.  Once the data is seen and the confidence interval is calculated, you cannot say that the probability that it is still correct is 95%.  This is like the medical test; we were 95% confident; but now we can't say anything conditioning on the data, unless we are prepared to use priors.

Or can you? Clearly, if you're willing to specify a prior and use a [Credible Interval](http://en.wikipedia.org/wiki/Credible_interval), then you can still make strong statements after conditioning on the data.  But I'm curious about the prior-free frequentist Confidence Interval.  When I hear talk of a ""frequentist interpretation"" I think it just means:  ""I was optimistic before I saw the data, but I am unwilling to make any statement that is based on this data.""

As far as I can see, there is *no* frequentist interpretation of the confidence interval after the data and the interval are gathered and calculated.  They are willing and able to make statements about future experiments, *statements which even the most rabid Bayesian would agree with*, but they simply have ""no comment"" on the just-observed data.

I'm not really too interesting in *justifying* the frequentist interpretation.  We could argue all day about the validity or otherwise of writing down priors on hypotheses and treating hypotheses as random variables.

So, my question is: am I correct in saying that frequentists have no comment to make on the just-observed data and the interval associated with it? And that their statements are strictly related to future/alternative experiments?  That there is no ""frequentist interpretation"" associated with the just-observed data, the frequentist has no comment to make of any kind on that data? After you have calculated the interval, have you learned anything about the parameter that you didn't know before the experiment?  If not, why did you do the experiment?  If so, tell me what you have learned.

Contrast this with the use of p-values.  Given a low p-value, people will say ""either a rare event occured, or the null hypothesis is false."" Such wording is clearly intended to make the reader have 'more doubt' about the null hypothesis than they did before the experiment.  But with confidence intervals, I find it difficult to get anybody to say that they now feel that they have a better idea what the value of the parameter is, specifically that it is within the interval.  As far as I can see, p-values skirt close to being Bayesian - see [this pdf on the opposing and contradictory philosophies between Fisherian p-values and Neyman-Pearson error rates](http://www.uv.es/sestio/TechRep/tr14-03.pdf).

EDIT: Added a couple of paragraphs in the middle to take account of some discussions regarding whether the data is a random variable.  And also I added the section about p-values at the end.",en
1110269,2012-07-03 21:46:11,statistics,Job Opening for Masters in Statistics or Bright Undergrads,vzpa7,[deleted],,https://www.reddit.com/r/statistics/comments/vzpa7/job_opening_for_masters_in_statistics_or_bright/,2.0,0.0,,en
1110270,2012-07-03 22:53:47,MachineLearning,Speeding up greedy feature selection,vztld,cypherx,1141075032.0,https://www.reddit.com/r/MachineLearning/comments/vztld/speeding_up_greedy_feature_selection/,7.0,2.0,,en
1110271,2012-07-03 22:56:05,statistics,Speeding up greedy feature selection,vztqi,cypherx,1141075032.0,https://www.reddit.com/r/statistics/comments/vztqi/speeding_up_greedy_feature_selection/,3.0,0.0,,en
1110272,2012-07-04 00:12:58,rstats,"What is the difference between lmer and glmer? Also, heteroscedasticity.",vzykw,pepite,1287597167.0,https://www.reddit.com/r/rstats/comments/vzykw/what_is_the_difference_between_lmer_and_glmer/,6.0,1.0,"I don't know which I should be using. If it helps, my formula looks something like A~B*C*D+(1|E/F/G), family=poisson. C and D are continuous, B is categorical, and E, F, G represent the structure of my experiment.

Also, after checking my assumptions, I get this graph (I get the same thing in lmer and glmer) http://imgur.com/3Q2kZ, which, even to my very limited statistical know-how, doesn't quite seem right... Does it mean my data is just useless or is there something I can do to it that will make everything right or (even better) can I leave it alone and get on with my thesis?

Anything you Rstats gods can suggest will be much appreciated.

",en
1110273,2012-07-04 03:05:36,statistics,Probability of getting 2 specific cards of in a 5-card hand from a 52-card deck?,w08c1,Jahordon,1315585210.0,https://www.reddit.com/r/statistics/comments/w08c1/probability_of_getting_2_specific_cards_of_in_a/,0.0,7.0,"Let's say you draw a 5-card hand from a 52-card deck.

What is the probability that you draw at least 1 Ace and at least 1 King?

I know how to find the probability of getting at least 1 Ace in the hand, and the probability of at least 1 King, but I don't know how to find the probability of getting at least one of both, because the events seem to be dependent.

I am afraid that my knowledge of statistics is limited, so some insight would be much appreciated :).

EDIT: This is not a homework problem.  I'm a math student but I have yet to take a probability class.  I was trying to figure this out for playing Magic TCG (60 card deck, probability of drawing 1 of two kinds of cards, of which there are four each, in a 7-card hand.",en
1110274,2012-07-04 04:58:29,statistics,Sufficient Statistics help: Prove that the sum of two iid  Gaussians is a sufficient statistic,w0e9k,brownck,1268955199.0,https://www.reddit.com/r/statistics/comments/w0e9k/sufficient_statistics_help_prove_that_the_sum_of/,0.0,3.0,"Let [;X_1;] and [;X_2;] be iid Gaussian with mean [;\mu;] and variance [;1;]. What is the conditional density of [;(X_1,X_2);] given that [;X_1 + X_2 = t;]? In other words

[; p(X_1=x_1, X_2=x_2| X_1+X_2 = t) = ? ;]

I am trying to prove that [;X_1+X_2;] is a sufficient statistic directly from the definition, without using the Fisher factorization theorem. This means that the conditional density should not be a function of [;\mu;]. However, whenever I compute the density, I cannot get [;\mu;] to cancel out. 

I have determined (at least I think) that the density is 

[; \frac{f(x_1)f(t-x_1)}{\int_x f(x)f(t-x)dx };]

where [;f(x);] is the normal density with mean [;\mu;] and variance [;1;]. However, it doesn't look like this density is independent of the unknown parameter [;\mu;].  ",en
1110275,2012-07-04 05:57:55,MachineLearning,"Performance evaluation of learning algorithms [PDF, 120 pp]",w0h85,gtani,1136005200.0,https://www.reddit.com/r/MachineLearning/comments/w0h85/performance_evaluation_of_learning_algorithms_pdf/,15.0,0.0,,en
1110276,2012-07-04 06:43:48,statistics,"Is Amazon's ""average rating"" misleading?",w0ji3,mycatharsis,1294046118.0,https://www.reddit.com/r/statistics/comments/w0ji3/is_amazons_average_rating_misleading/,4.0,3.0,,en
1110277,2012-07-04 10:56:58,statistics,Why do they separate male and female divorce rates when I try to research divorce statistics?,w0ugi,gregorthebigmac,1286874908.0,https://www.reddit.com/r/statistics/comments/w0ugi/why_do_they_separate_male_and_female_divorce/,13.0,8.0,"Random curiosity led to me googling the current divorce rates in America, and I found it very difficult to see why there is a higher/lower divorce rate between genders. If we are assuming (correct me if I'm wrong) the statistics are not counting same-sex marriage (as their statistics go as far back as the 1960's), how is there a difference between men and women? Shouldn't the divorce rate be a mere percentage of marriages that fail, and be treated as one entity? Or is there some simple underlying reason I'm missing? Thanks in advance.",en
1110278,2012-07-04 14:59:52,AskStatistics,Non-parametric tests of parametric data,w10ve,slypsy,1249806998.0,https://www.reddit.com/r/AskStatistics/comments/w10ve/nonparametric_tests_of_parametric_data/,5.0,7.0,"I have seen a couple of papers recently where the authors applied non-parametric tests to their data on the basis that the data was not normally distributed and as the variances between groups were not equal.

I was under the impression (and I believe, taught), that data which is not normally distributed should be transformed, and that ANOVA is sufficiently robust that it can cope with non equal variances, even though it is not ideal. So at what point should you decide to stop with parametric tests and switch to non-parametric tests?",en
1110279,2012-07-04 17:12:50,rstats,John Deere is hiring statisticians,w1590,[deleted],,https://www.reddit.com/r/rstats/comments/w1590/john_deere_is_hiring_statisticians/,1.0,0.0,"http://www.deere.com/wps/dcom/en_US/corporate/our_company/careers/search_positions/searchpositions.page? Then click on the link for ""Experienced professional positions – United States"" You can then search for job Complete Good Forecast (Job number 50482625).",en
1110280,2012-07-04 17:18:46,MachineLearning,How can an ANOVA perform feature selection?,w15hn,nickponline,1285653350.0,https://www.reddit.com/r/MachineLearning/comments/w15hn/how_can_an_anova_perform_feature_selection/,4.0,1.0,"I thought an ANOVA was use to access the difference in means of two groups of data. So how can it be used to determine which features in a data set (X) are relevent in predicting labels the corresponding labels (t) ? And if so how is it possible agnostic of the type of classifier used?

For example is I make up some data where the second feature is clearly irrelevent, how can an anova show this?

X = [1 1 ; 1 -1 ; -1 1 ; -1 -1]

t = [1 1 -1 -1]'

Similarly with a Chi2 test?",en
1110281,2012-07-04 18:47:35,MachineLearning,"The future of data analysis, by Hadley Wickham",w19m9,agconway,1228006618.0,https://www.reddit.com/r/MachineLearning/comments/w19m9/the_future_of_data_analysis_by_hadley_wickham/,1.0,4.0,,en
1110282,2012-07-04 20:12:28,MachineLearning,Questions: many similar features and more features than data,w1dz3,TheInfelicitousDandy,1297303984.0,https://www.reddit.com/r/MachineLearning/comments/w1dz3/questions_many_similar_features_and_more_features/,2.0,7.0,"I'm somewhat new to machine learning and I have a few questions.

First is it a problem to have many similar highly correlated features?  Compute time isn't a problem, and prediction is better using all the features, so is there any reason to try and reduce them?

Second can some one explain to me why it's a bad thing to have more features than data.  I understand it in the case where your extra features are all higher polynomial terms and you end up with so many features that you can way over fit that data.  In the more general case is it still a bad thing?",en
1110283,2012-07-04 20:16:21,MachineLearning,Introducing BigML’s Free Machine Learning Sandbox,w1e6e,jjdonald,1192132770.0,https://www.reddit.com/r/MachineLearning/comments/w1e6e/introducing_bigmls_free_machine_learning_sandbox/,1.0,1.0,,en
1110284,2012-07-04 20:47:39,MachineLearning,Machine learning with structured outputs,w1ft7,cypherx,1141075032.0,https://www.reddit.com/r/MachineLearning/comments/w1ft7/machine_learning_with_structured_outputs/,2.0,0.0,,en
1110285,2012-07-04 23:32:58,statistics,Any leads for finding good recent statistics graduates?,w1ojc,[deleted],,https://www.reddit.com/r/statistics/comments/w1ojc/any_leads_for_finding_good_recent_statistics/,1.0,0.0,"I know it is wrong season to find graduates, but if you are interested: http://www.deere.com/wps/dcom/en_US/corporate/our_company/careers/search_positions/searchpositions.page? Then click on the link for ""Experienced professional positions – United States"" You can then search for job Complete Good Forecast (Job number 50482625).",en
1110286,2012-07-04 23:47:57,statistics,"Resample to ""equalize"" unequal cell sizes in ANOVA. Any problems with this method?",w1p9y,rottenborough,1257706820.0,https://www.reddit.com/r/statistics/comments/w1p9y/resample_to_equalize_unequal_cell_sizes_in_anova/,5.0,6.0,"In cases where it's quite certain that unequal cell sizes would cause a problem in an ANOVA (heterogeneity of variance, cell size confounded with another variable, etc.), would it be valid to:

1. For each cell, take N samples with replacements, where N is the size of the smallest cell.

2. Run the ANOVA analysis on the resampled cells to obtain the truth value of F_obs &gt;= F_crit

3. Repeat 1&amp;2 1000 times, take the percentage of ""true"" to be the p-value.",en
1110287,2012-07-05 00:51:10,AskStatistics,"Statistics question: relationship between two variables, non-normal data, and lots of ties.",w1sez,JoxerTheMighty,,https://www.reddit.com/r/AskStatistics/comments/w1sez/statistics_question_relationship_between_two/,1.0,0.0,"I have cognitive data from a natural population of monkeys. The monkeys self-segregate themselves into different groups of varying sizes, and I have a large number of monkeys tested from each of the groups (6 groups in total). What is the best statistical tool to explore the relationships between group size and their performance in the tests? The data is non-normal, but using spearman or kendalls runs into the problem of there being too many ties (each individual from a particular group is going to have identical numbers in terms of group size). I thought the goodman-kruskal gamma might be the right test, but I'm just not sure. Does anyone have any suggestion as to what to do? Thanks in advance for any help!
",en
1110288,2012-07-05 05:49:13,computervision,Home Automation Project,w25di,bge0,1319642036.0,https://www.reddit.com/r/computervision/comments/w25di/home_automation_project/,2.0,3.0,"Hey Everyone,
Trying to setup a home automation system. So far this is the plan:
1) Multi-threaded Boost setup: watchdog,cV,audio
2) Have 3 usb Cams on my box, 2 for stereo and one for the entrance

Version v0.1:
-Implement simple framework for detecting that a user has arrived, announcing it via some text-2-speech api

Ideas for the future: 
-Detect specifically which user has arrived home [face recognition,etc]
-Use gesture based controls to do simple tasks like increase volume,etc
-Use something like uWave to do audio recognition as well

Questions here are:
For user specific detection what would you recommend? This is to be done with only 1 cam. I can try something like Eigenfaces perhaps? Just need to ensure what the camera can see the user's face head on. Are there any more up to date algorithms that would provide better recognition? And does anyone have a link to a good paper/alg on 3d facial recog [i.e. implementable alg]
",en
1110289,2012-07-05 06:11:59,MachineLearning,How is a ROC curve and curve and not a point?,w26e1,[deleted],,https://www.reddit.com/r/MachineLearning/comments/w26e1/how_is_a_roc_curve_and_curve_and_not_a_point/,1.0,0.0,"If I have a classifier that I train and run on a dataset and compute the resulting TPR and FPR, then plot this in ROC space surely I just get a single point. How to people seem to get a ROC ""curve""?

TITLE SHOULD READ: How is a ROC curve a ""curve"" and not a point?",en
1110290,2012-07-05 06:34:50,MachineLearning,Is it possible to plot a ROC curve for an SVM?,w27gu,nickponline,1285653350.0,https://www.reddit.com/r/MachineLearning/comments/w27gu/is_it_possible_to_plot_a_roc_curve_for_an_svm/,7.0,9.0,Is it possible to plot a ROC curve for an SVM performing binary classification? It doesn't makes sense that you should be able to because there is no threshold value that you could vary to create the roc curve right? You would just get a single point representing the TPR vs. FPR of the classifier. ,en
1110291,2012-07-05 14:23:21,statistics,A Tutorial on Outlier Detection Techniques [r-bloggers.com],w2mdt,srkiboy83,1299056137.0,https://www.reddit.com/r/statistics/comments/w2mdt/a_tutorial_on_outlier_detection_techniques/,6.0,0.0,,en
1110292,2012-07-05 16:48:11,statistics,Is there an easy way to run a cross-validation training and testing with a reasonably large number of sets in Minitab?,w2r5z,garfieldsam,1328992729.0,https://www.reddit.com/r/statistics/comments/w2r5z/is_there_an_easy_way_to_run_a_crossvalidation/,0.0,0.0,I know how to in R but I like having a GUI. :/,en
1110293,2012-07-05 17:26:50,MachineLearning,Air Handling Unit Manufacturer,w2svc,gjkliol,1339581016.0,https://www.reddit.com/r/MachineLearning/comments/w2svc/air_handling_unit_manufacturer/,0.0,3.0,,en
1110294,2012-07-05 17:44:53,statistics,"Looking for an article I saw on here (maybe not on r/statistics) that was a scathing, sarcastic look at statistics in scientific review.",w2to5,Not_a_neuroscientist,1334355871.0,https://www.reddit.com/r/statistics/comments/w2to5/looking_for_an_article_i_saw_on_here_maybe_not_on/,6.0,4.0,"I know I saw it on reddit, but I can't find it. The author went through many different typical reviewers' comments and told possible responses for them. Example: If the reviewer said ""The N is too low"" the author should reply that ""Since the effect was so large a lower N is sufficient"" or something along those lines.

Thanks for your help!",en
1110295,2012-07-05 18:11:20,statistics,Does the chance of a type 1 error increase for a F-test for equal variances if the Ns of the two groups are different?,w2uz6,Not_a_neuroscientist,1334355871.0,https://www.reddit.com/r/statistics/comments/w2uz6/does_the_chance_of_a_type_1_error_increase_for_a/,3.0,3.0,"One group has 8, the other has 32. My F-test says they have unequal variances, what's the chance that if I had two groups of 32 I would get a different result, i.e. their variances are actually the same?",en
1110296,2012-07-05 19:25:48,MachineLearning,The Problem with Pre-Publication Peer Review,w2ywp,[deleted],,https://www.reddit.com/r/MachineLearning/comments/w2ywp/the_problem_with_prepublication_peer_review/,9.0,2.0,,en
1110297,2012-07-05 20:28:55,statistics,A new open journal on Data Science,w32cd,Professor_IR,1310512226.0,https://www.reddit.com/r/statistics/comments/w32cd/a_new_open_journal_on_data_science/,22.0,2.0,,en
1110298,2012-07-05 22:07:29,MachineLearning,"5 Ton, 980mm bore Piston Change on Marine Diesel Engine",w38cr,[deleted],,https://www.reddit.com/r/MachineLearning/comments/w38cr/5_ton_980mm_bore_piston_change_on_marine_diesel/,0.0,5.0,,en
1110299,2012-07-06 01:51:43,statistics,Need help with SAS for work-related problem,w3luf,Adamantus,1283897973.0,https://www.reddit.com/r/statistics/comments/w3luf/need_help_with_sas_for_workrelated_problem/,1.0,1.0,"I work for a loan company as a lead generation analyst (we buy potential customers from partners). Anyway, we have a database of our customers, which partner sold us to them (their PromoID), which marketing channel they went through (CampaignCode), and whether they defaulted (0 or 1). I'm trying to create a bad list of PromoIDs (partners) by finding the population mean and upper/lower confidence intervals for default rate within each CampaignCode and then comparing the default rate for the individual promoIDs and whether they are above the upper confidence limit. The data looks something like this:

Customer | PromoID | CampaignCode | Default

1 | 144023 | A | 0

2 | 144023 | A | 0

3 | 144023 | B | 1

4 | 273059 | D | 1

5 | 840385 | D | 0

6 | 840385 | B | 1

7 | 284734 | E | 1

So I was using a proc summary to sum up the data for the promoIDs within each campaign:
class = campaigncode promoid;
var = default total_apps
types campaigncode*promoid;

default_rate = default / total_apps

I then found the mean, upper confidence limit, and lower confidence limit of each campaign code with another proc summary:
proc summary mean uclm lclm;
class campaigncode;
var default_rate;
output out= y mean= , uclm=, lclm=;

I now realized that by summing the promoIDs, then finding the mean/uclm/lclm for the campaigncode, I'm getting an incorrect default rate.

My Question:
I need to find the population default mean/uclm/lclm for each campaigncode, but since I only have a default= 0 or 1, I first need to sum that up and divide by the total apps, which makes it impossible to find an upper/lower confidence interval since it's just one data point. How can I find the true mean/uclm/lclm for the campaigncode? Thanks a lot.

",en
1110300,2012-07-06 04:24:32,statistics,How would you improve the method used to calculate the London Inter-Bank Offered Rate (LIBOR) in light of the current scandal? ,w3u7k,mickey_kneecaps,1289184864.0,https://www.reddit.com/r/statistics/comments/w3u7k/how_would_you_improve_the_method_used_to/,5.0,0.0,,en
1110301,2012-07-06 07:10:08,MachineLearning,Introduction to Machine Learning Lectures: from Caltech Prof. Yaser S. Abu-Mostafa,w43hp,jrockIMSA08,1216693164.0,https://www.reddit.com/r/MachineLearning/comments/w43hp/introduction_to_machine_learning_lectures_from/,59.0,2.0,,en
1110302,2012-07-06 08:58:40,statistics,Recommendations for learning MCMC for Bayesian Inference,w48mf,brownck,1268955199.0,https://www.reddit.com/r/statistics/comments/w48mf/recommendations_for_learning_mcmc_for_bayesian/,12.0,6.0,"I am trying to apply MCMC techniques to sample the posterior, but I don't really understand how it needs to be implemented. In particular, I am trying to understand [this explanation of it](http://sciencehouse.wordpress.com/2010/06/23/mcmc-and-fitting-models-to-data/). It gives an example, but does not justify the steps. Any good references, explanations, etc? 

Thanks in advance! By the way, I just started a new job and I will need to learning a lot of Bayesian statistics so this is why I am asking. ",en
1110303,2012-07-06 10:47:46,statistics,Asymptotics Revisited.,w4cj2,srkiboy83,1299056137.0,https://www.reddit.com/r/statistics/comments/w4cj2/asymptotics_revisited/,1.0,0.0,,en
1110304,2012-07-06 12:38:48,statistics,"My idea of gathering statistics on Reddit, using askreddit. What do you think? ",w4ff3,Reddit_Statistician,1341563440.0,https://www.reddit.com/r/statistics/comments/w4ff3/my_idea_of_gathering_statistics_on_reddit_using/,1.0,2.0,,en
1110305,2012-07-06 13:44:19,statistics,Useful advice on plotting error bars when comparing sets of scores,w4gv7,[deleted],,https://www.reddit.com/r/statistics/comments/w4gv7/useful_advice_on_plotting_error_bars_when/,1.0,0.0,I read this one day and found a use for it the very next day. This sort of thing needs to be covered on statistics courses more often.,en
1110306,2012-07-06 16:28:32,statistics,ASK: simple function in R to compute a md5 hash on a single string? Does this exist in R?,w4lvu,Synes_Godt_Om,1336751767.0,https://www.reddit.com/r/statistics/comments/w4lvu/ask_simple_function_in_r_to_compute_a_md5_hash_on/,5.0,10.0,"Context:

I have a set of data.frames in which a few character columns act like keys for the rows. I need to use those character columns as identifiers for the rows.

The catch is that I can only trust the values immediately when the data.frame is created. Thus I need to compute a unique ID from the character values at this time.

(The reason I can't trust the values is that I have to convert the data.frames to csv-files, give them to somebody else and have them returned - and we are in a non-ascii locale).

To me the obvious solution is to compute a md5 hash and use this as unique id for future reference.

The problem is that I can't find any suitable md5 functions in R. 

So far I've found 'md5sum' in package 'tools' and 'md5' in package 'md5' (from rforge.net), none of these will compute a md5 hash on a simple string.

Thanks for any help.",en
1110307,2012-07-06 18:24:55,computerscience,Stanford's top major is computer science,w4rgq,[deleted],,https://www.reddit.com/r/computerscience/comments/w4rgq/stanfords_top_major_is_computer_science/,1.0,0.0,,en
1110308,2012-07-06 18:55:38,statistics,Can you define variance on any ordered system?,w4t6j,Traner,1278184133.0,https://www.reddit.com/r/statistics/comments/w4t6j/can_you_define_variance_on_any_ordered_system/,1.0,4.0,"Hi there r/statistics!

I'm a research assistant in speech pathology and we give out a form for the parents of children partaking in the research to evaluate the speech performance of the child.

We ask them to choose: very bad, bad, average, good, very good.

Someone got an idea whether it was useful to keep the standard deviation of these rankings. So my first idea was to convert them to 1 to 5 and then evaluate it.

Does anybody have tangentially related experience and/or can share some insight?",en
1110309,2012-07-06 20:52:14,MachineLearning,Quick classifiers for exploring medium-sized data (redux),w501n,cypherx,1141075032.0,https://www.reddit.com/r/MachineLearning/comments/w501n/quick_classifiers_for_exploring_mediumsized_data/,6.0,0.0,,en
1110310,2012-07-06 20:59:04,MachineLearning,Machine learning course to follow,w50hz,mfalcon,1264563760.0,https://www.reddit.com/r/MachineLearning/comments/w50hz/machine_learning_course_to_follow/,2.0,3.0,"I began the mlclass from coursera but I'd like a more advanced approach.

I found some courses(I don't know if there are more):

Andrew Ng Stanford CS229: http://cs229.stanford.edu/info.html

Caltech: http://work.caltech.edu/telecourse.html

Tom Mitchell Carnegie Mellon: http://www.cs.cmu.edu/~tom/10701_sp11/

I'm considering following the Tom Mitchell course as it seems to go deeper into the details, also because it uses a pretty cool bibliography.
What do you think, am I making the right choice?
",en
1110311,2012-07-06 21:22:37,statistics,Quick classifiers for exploring medium-sized data ,w51z7,cypherx,1141075032.0,https://www.reddit.com/r/statistics/comments/w51z7/quick_classifiers_for_exploring_mediumsized_data/,5.0,0.0,,en
1110312,2012-07-06 22:10:39,MachineLearning,Ideas for an E-Learning Recommendation System,w552e,skyflashings,1315159561.0,https://www.reddit.com/r/MachineLearning/comments/w552e/ideas_for_an_elearning_recommendation_system/,2.0,2.0,"I'm looking for suggestions, but not sure of a better place to ask this.

Simply put, I've been assigned to build a recommendation system which suggests learning materials to students after they take a quiz.

The current system:  Students anonymously go to the website, choose their particular book isbn, chapter and lesson, and take a self-assessment quiz.  If their score is below a certain percentage, they are 'remediated' and are given the textbook page number of an example associated with that question (this correlation is all in the database).  They can then email their scores to their teacher.

Now, there are thousands of digital learning materials collected from over the years, including video and other instructive materials.  The point of the project is to incorporate these materials into the old quiz system and 'intelligently' give suggestions to the student for remediation.  (These suggestions are in the form of links, to my knowledge).  Note, this whole project is more-or-less just a proof of concept that is intended to be applied to some of their more modern quiz systems.

Data available:  Each quiz contains the book title, chapter title, lesson title, the actual question and an optional hint for the student.  The new materials have metadata such as description, grade-level, remediary/basic/advanced, etc.  Each material is categorized into a certain group, for instance ""Expressions"" in mathematics.  The idea is that the questions will also be associated with these groups in order to narrow down the list of those materials retrieved.  That is, get all materials associated with the category that question is in.

For starters, a user registration system is needed to store individual quiz score history.  I suppose the kinds of additional data needed is dependent on the algorithm to learn off of the data.  I've basically been given free reign to choose what modifications need to be made and the kinds of data to store.

I've looked at various papers on the topic, and this one seems pretty relevant: http://www.ascilite.org.au/ajet/ajet26/ghauth.pdf

He uses a combination of cosine similarity for the materials and a user rating system.  In my case, I could preprocess the similarity rankings between each question and the materials in the same category (but I guess it would have to be done each time a new material is added).  This would be between data I know about the question against data I know about the materials.  I think I don't want students explicitly rating material, so instead I would give a rating implicitly based on whether or not that material helped them in their next quiz (eg they viewed material in ""expressions"" and the next time they encounter ""expressions"" in a quiz their score increased).

Is this a good approach to take?  Is there a way to use the additional metadata about students to do more than simple heuristics?  I would definitely appreciate any suggestions as this is the first machine-learning project that I've ever tried to tackle like this.  Thanks!",en
1110313,2012-07-06 23:13:08,MachineLearning,Need help with a clustering algorithm,w58wj,Sohakes,1207439461.0,https://www.reddit.com/r/MachineLearning/comments/w58wj/need_help_with_a_clustering_algorithm/,4.0,2.0,"Hello, fellow redditors! Sorry if it isn't the place to post this, but I'm not sure where to get help.

I'm an undergraduate student from Brazil doing some research with clustering algorithms. I have an instructor, and my job was simply to implement an clustering algorithm on a teacher's framework (that she did for her PhD). The problem is that it never returns the correct results, even for it's sample dataset. After much time debugging the code, I found there is no bug... the thing is, it's not returning the correct result because the algorithm idea looks strange, and wrong for me, and it returns just one cluster (out of as many as you want).

The algorithm is a variation of the CLIQUE algorithm. It's an subspace and density-based algorithm, that uses grids and works exploiting something called ""dimension monotonicity"". Basically, it divides the dataset in grids, and then try to find the cells (of the grid) that have more objects than a certain threshold. It marks these cells as dense, and then proceed to combine them to form larger dimensions candidate clusters (candidate because they may not form a cluster if they are not dense as well), and then repeat the process. To combine the clusters, the CLIQUE algorithm uses the dimension's monotonicity lemma:

**Lemma:** If a collection of points S is a cluster in a k-dimensional space, then S is also part of a cluster in any (k-1)-dimensional projections of this space.

**Proof:** A k-dimensional cluster C includes the points that fall inside a union of k-dimensional dense units. Since the units are dense, the selectivity (selectivity is the fraction of total data points contained in the unit) of each one is at least r (the threshold). All the projections of any unit u in C have at least as large selectivity, because they include all points inside u, and therefore are also dense. Since the units of the cluster are connected, their projections are also connected. It follows that the projections of the points in C lie in the same cluster in any (k-1)-dimensional projection. QED.

I can agree with that! The problem is that it specifies a fixed threshold (a input parameter). The algorithm that I'm working with don't use a fixed threshold, instead, it calculates a different threshold for each dimension, and then, for each candidate cluster created with the lower dimensions, it checks if it's dense for the threshold for each dimension in the cluster (i.e. it checks if the candidate cluster selectivity is higher than the threshold for each dimension that forms the cluster). The threshold formula is this (the paper doesn't prove it works, just point it works because of the lemma above, but I don't think the lemma apply because of the variable threshold):

    Threshold = N*a*1.5/D

Being 'N' the number of objects in the dataset, 'a' the bin size, 'D' the dimension size and 1.5 a constant. What it's trying to do is check if the bin is 1.5 times more dense that it was going to be if the dimension was uniform. It looks like a good idea. The problem is that the monotonicity lemma doesn't apply here, and he doesn't even prove it applies. I'm going to give an example to where I think it doesn't apply, i.e. there is a dense cluster on higher dimensions that isn't found on the lower dimensions:

Suppose I have a dataset with 2 clusters and 2 dimensions, both dimensions with size 10. The first cluster have 100 points uniformly distributed on 0~5 on the first dimension and 0~10 on the second. The second cluster have 8 objects, from 6~7 on the dimension 1, and from 0~4 on the second dimension. Now, both clusters have the same density (size 50 for 100 objects, size 4 for 8 objects), but the first cluster is so big that it ""hides"" the second cluster. The threshold for the first dimension and second cluster is 108\*1\*1.5/10. As we can see, it's a lot more than 8. Now the first cluster threshold and first dimension is 108\*5\*1.5/10=81, and so the first cluster would be considered dense, but it's density is the same as the second one. It look really flawed to make a formula like this, because it neglects that the cluster can be really big on the other dimensions, and so it will be really dense on some parts of the projection on lower dimensions (we don't want to find the biggest cluster, but the dense ones).

I think that is the problem, because the only cluster it finds is the biggest one, but all clusters have the same density. But it was some guy PhD thesis, and it's really intelligent on the other parts, so it's really strange to have such a error (and he made tests and all these things, showing good results)... someone could help me find the flaw in my logic? Or it's really kind of wrong?

Thanks in advance!

P.S.: Yeah, I avoided saying the name of the algorithm because it's not well known (just a few papers citing), and it would be bad if one of the first results was something bad about it, but I could say it if necessary. Sorry for the poor english and the wall of text.",en
1110314,2012-07-06 23:48:16,MachineLearning,Physics grad looking to attempt some kaggle events. Best/Quickest way to get up and running?,w5b1w,T3ppic,1297043762.0,https://www.reddit.com/r/MachineLearning/comments/w5b1w/physics_grad_looking_to_attempt_some_kaggle/,6.0,15.0,"Most of my third year (3 years in the UK for a BA) electives were in the field of statistical analysis and model testing and Im pretty comfortable with mathematics and python. 

I just can't find an ""easy"" in. I decided to put this in /r/machinelearning rather than /r/statistics because I think thats where the problem lies; Until looking through some of the kaggle interviews I have never heard the expression ""random forest"" which seems key to some of the entries. Wikipedia didn't help. Before then I had thought about Monte Carlo (obviously I know of it because its most relevant to physics) for some of the kaggle competitions but no interview mentions this as a relevant winning strategy probably because the learning and testing datasets are small.

I already have some experience with C and python with little to none in mathematica or R. Would you recommend a change to either of the latter? Or just struggle through with python? Most of my university experience with stats was done in Excel with the built in tools and a few custom sheets provided by the lecturer. 

A book Ive managed to look at is ""Data Mining Third Edition by Morgan Kaufman"" and it doesnt seem to be very relevant or conducive to get up and running quickly; I am under no illusions about winning any competition but I do want to at least try some of them as a learning exercise. 

TL:DR Can you recommend any books or websites that allow a ""quick and dirty"" introduction to kaggle related concepts such as random forests?
",en
1110315,2012-07-07 13:59:05,artificial,Deeplearning Q: Can DBNs learn algorithms/formulas?  ,w6aet,tluyben2,1192267383.0,https://www.reddit.com/r/artificial/comments/w6aet/deeplearning_q_can_dbns_learn_algorithmsformulas/,5.0,19.0,"As you see, I do not yet have a very full understanding of this matter (I just started reading about it in my spare time and will be playing around with Theano when done), however maybe someone already knows about this. 

With symbolic AI (ILP and such) we can derive rules from samples which can solve certain problems without explicitly programming them. 

I am wondering if a deep belief network can learn this theoretically and how? The only(? I could find) examples are pattern recognition; sound, images and text (NLP), so i'm wondering if it is theoretically possible to actual learn, for instance, a computer algorithm for the shortest path or a formula like Newton's laws. 

If not, is it known what would be required to make that step or is this just science fiction for now? ",en
1110316,2012-07-07 22:17:04,MachineLearning,A Quick Sentiment Analysis Experiment: Vorpal Wabbit vs state of the art results,w6sv5,rrenaud,1150739310.0,https://www.reddit.com/r/MachineLearning/comments/w6sv5/a_quick_sentiment_analysis_experiment_vorpal/,35.0,7.0,,en
1110317,2012-07-08 00:04:24,statistics,There appears to be some structure in stock data,w6ycj,[deleted],,https://www.reddit.com/r/statistics/comments/w6ycj/there_appears_to_be_some_structure_in_stock_data/,5.0,6.0,"I posted something about this a few days ago, but I've fixed some programming errors, and there still is a problem.

I'm analyzing ratios of daily closes of SPY for the past apx. 3 years, because I want to use them in a Monte Carlo simulation over periods of 25 trading days. (apx. 1 month)

The problem is that if I randomly select daily changes from the database, and multiply 25 of them, I get a very different result than if I multiply 25 sequential changes.

This suggests that there is some correlation between daily changes, but the autocorrelation, to my untutored eye, looks unremarkable.

If you're interested, here's an example:

-------------------------------------------

For 725 samples of the product of 25 **sequential** 1-day Changes

mean=1.0150, sd=0.0495, range = 0.8382 - 1.1519

probability of the modal change = 0.132

-------------------------------------------

For 725 samples of the product of 25 **random** 1-day Changes

mean=1.0126, sd=0.0618, range = 0.8268 - 1.2394

probability of the modal change = 0.067

-------------------------------------------

Does anyone have any ideas as to what might cause such a huge discrepancy? 

If you're still interested, here's the autocorrelation of daily price ratios, rounding errors and all:

dt=0; R=	 0.9993 ( essentially 1)

dt=1; R=	 -0.0601

dt=2; R=	 0.0563

dt=3; R=	 -0.0862

dt=4; R=	 0.0428

dt=5; R=	 -0.0964

dt=6; R=	 -0.0037

dt=7; R=	 0.0153 ( no this is not a week - I'm using Trading days)

dt=8; R=	 -0.0360

dt=9; R=	 -0.0333

dt=10; R=	 0.0475

dt=11; R=	 0.0166

dt=12; R=	 -0.0135

dt=13; R=	 0.0096

dt=14; R=	 -0.0319

dt=15; R=	 -0.0153

dt=16; R=	 0.0052

dt=17; R=	 0.0348

dt=18; R=	 -0.0597

dt=19; R=	 -0.0221

dt=20; R=	 0.0321

dt=21; R=	 -0.0198

dt=22; R=	 -0.0351

dt=23; R=	 0.0732

dt=24; R=	 0.0393

Thanks for any insight you might provide.
 ",en
1110318,2012-07-08 01:06:09,statistics,Does anybody have any MATLAB and WinBUGs programming experience (more emphasis on WinBUGs) and would be willing to help?,w71iq,[deleted],,https://www.reddit.com/r/statistics/comments/w71iq/does_anybody_have_any_matlab_and_winbugs/,0.0,2.0,"Hi,

I'm working on a thesis project and I'm having a bit of trouble with some coding. The coding involves hierarchical bayesians, which necessitates the use of WinBUGs. The data is stored in MATLAB and I use matbugs.m to run WinBUGs. The problem is the code isn't working. I have no experience with WinBUGs and neither does anyone around me. The code was originally run from R using R2Winbugs, however, my program works in MATLAB and it would be easier to just use MATLAB. You might want to know where I got the code: I got it from a professor who performed this analysis for a similar project. Therefore, the WinBUGs code is not my own. It would be really helpful if someone could help with the code. The code is posted below.
Thanks.



WINBUGS CODE: 
http://pastebin.com/nipkAgQv

MATLAB CODE:
http://pastebin.com/jJf83ZJe

",en
1110319,2012-07-08 07:48:42,statistics,"Want to beta-test a front end for the arXiv? Try this new webapp: it's fast, sharing-friendly, built for serendipity, and loves screens of all sizes. Comments are welcome anytime!",w7kjo,marxiv,1341528460.0,https://www.reddit.com/r/statistics/comments/w7kjo/want_to_betatest_a_front_end_for_the_arxiv_try/,11.0,6.0,,en
1110320,2012-07-08 12:05:29,MachineLearning,Time Series Data Library now on DataMarket,w7sq1,amair,,https://www.reddit.com/r/MachineLearning/comments/w7sq1/time_series_data_library_now_on_datamarket/,9.0,1.0,,en
1110321,2012-07-08 18:56:39,MachineLearning,Let's make an ML reading list,w832c,eloisius,1186189490.0,https://www.reddit.com/r/MachineLearning/comments/w832c/lets_make_an_ml_reading_list/,30.0,12.0,,en
1110322,2012-07-08 19:53:22,statistics,"What exactly do people mean when they say ""controlling for"" in terms of linear regression?",w85g9,statsn00b,,https://www.reddit.com/r/statistics/comments/w85g9/what_exactly_do_people_mean_when_they_say/,15.0,15.0,"Hi I have a strong background in math (linear algebra, analysis, etc.), but not so much applied statistics. Can someone please explain what researchers usually mean when the say regress Y on X, controlling for Z, or regress Y on X and Z, controlling for W, etc. thanks!",en
1110323,2012-07-08 23:15:12,statistics,Are those the right z-scores?,w8fk7,[deleted],,https://www.reddit.com/r/statistics/comments/w8fk7/are_those_the_right_zscores/,1.0,0.0,"Hey guys, 

don't worry I'm not asking you guys to do my work for me, just if you could maybe double check if I did it right, so I can make sure I got the hang of it.

Here's the problem:

1. For a population with μ = 70 and σ = 8, find the z-score that corresponds to each of the following X values.

X = Observation, μ = Mean, σ = Standard Deviation

z-score= (x- μ)/σ

X = 74		(74-70)/8= -9,25	z-score= -9.25

X = 68		(68-70)/8= -0.25	z-score= -0.25

X = 78		(78-70)/8=1		z-score= +1

X = 70		(70-70)/8=0		z-score=0",en
1110324,2012-07-09 04:05:18,MachineLearning,Am I shooting myself in the foot by going to a mid-tier university for a PhD? Should I try to transfer after my masters?,w8ved,thankyousir,1220896479.0,https://www.reddit.com/r/MachineLearning/comments/w8ved/am_i_shooting_myself_in_the_foot_by_going_to_a/,3.0,16.0,"I am currently in a graduate program ranked in the mid 50's in comp sci according to us news. I will start my program in the fall. I am really leaning towards getting a PhD because I like research and the relative freedom that comes with the degree, however, I am not sure if it would be wiser to try to stay here for my phd or leave after I get my masters (it would be very easy to do because I am currently funded only by a 2 year teaching assistantship). Would it be relatively easy to get funding to go to a higher ranked uni with a masters assuming I get good grades and perhaps a publication? Another factor: I am going to the same uni as my undergrad, is inbreeding a factor? 

Thanks!
",en
1110325,2012-07-09 05:26:16,AskStatistics,Going to graduate school-A few questions...,w901q,nkbxwb,1339633318.0,https://www.reddit.com/r/AskStatistics/comments/w901q/going_to_graduate_schoola_few_questions/,3.0,6.0,"Hello all,

This is my first post ever on reddit, so pardon me if I do anything wrong!

I am about to be a senior in college, and I want to go to grad school, because I heard that the more schooling you have in statistics, the better your chances for a good job are. I am majoring in statistics and mathematics, and minoring in computer science, but I want to focus more statistics, I really don't enjoy the mathematics as much as I do statistics.

I'm not really sure what I want to do. I know I like statistics, but I'm not sure what I want to do in the future. I'm hoping to go to grad school, and be able to find out what I like...

I go to school in Missouri, but I am from Texas. I want to go to grad school closer to Texas though, I miss my family and friends. The schools that I've been discussing with my adviser are : Rice, SMU, A&amp;M, Missouri, Iowa, Iowa State, Emory. Would you recommend me looking at any other schools?

I feel that I have a good enough resume to get me into most all schools. I am a winner of my schools statistics award for 2 years, I have a 3.75 GPA, I volunteer and research.

I am studying for the GRE, and I plan to take it in the fall...


Basically, if I could sum this post up, it would be this (tl;dr): I've spoke with my adviser about grad schools, and he has been helpful, but I'd like advice from reddit. What should I look for in a graduate school, where should I go, what do I need to do, etc.

Any advice would be helpful/appreciated.

Thanks!",en
1110326,2012-07-09 05:47:14,MachineLearning,OpenCyc version 4.0 has been released,w91av,jimktrains,,https://www.reddit.com/r/MachineLearning/comments/w91av/opencyc_version_40_has_been_released/,9.0,1.0,,en
1110327,2012-07-09 07:30:44,statistics,"Given a scatter plot with a sufficient amount of data, how would I determine the probability that a given X,Y pair falls on the plot?",w974c,Glorin,1289094363.0,https://www.reddit.com/r/statistics/comments/w974c/given_a_scatter_plot_with_a_sufficient_amount_of/,4.0,8.0,"Say I plot two variables like [this.](http://i.imgur.com/yPC28.png)

If the X variable is around 0.9, it's obvious that the chance I'll get a value of Y above 300 is near 0%.

If the X variable is around 0.4, then the chance I'll get a Y value of ~500 is between 5 and 25% (I'm guessing).

How do I actually get these probabilities?  

Was hoping to end up with something like [this.](http://imgur.com/v63FM)
Sorry for the crude MS paint.

Thanks!",en
1110328,2012-07-09 15:46:59,statistics,Need help trying to figure out which test to use to compare survival rates.,w9nz0,doctor_rabbit,1261633623.0,https://www.reddit.com/r/statistics/comments/w9nz0/need_help_trying_to_figure_out_which_test_to_use/,3.0,6.0,"I have the data for one hospital, and then some data for a network of hospitals to use as a benchmark. I see that we have a higher survival rate, but with a much smaller sampling size. I'm trying to figure out what test to use that factors in the sample size when testing for a statistical significance between the two survival rates.

Thanks!",en
1110329,2012-07-09 17:55:17,MachineLearning,What are the best NL processing and AI companies?,w9t4h,darkwave9,1341276511.0,https://www.reddit.com/r/MachineLearning/comments/w9t4h/what_are_the_best_nl_processing_and_ai_companies/,6.0,2.0,,en
1110330,2012-07-09 19:48:40,statistics,Secularization in America: what caused widespread religious disaffiliation in the 1990s?,w9zp4,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/w9zp4/secularization_in_america_what_caused_widespread/,27.0,6.0,,en
1110331,2012-07-09 20:51:38,artificial,A Phone that Knows Where You're Going - Technology Review,wa3sb,leapmind,1340728302.0,https://www.reddit.com/r/artificial/comments/wa3sb/a_phone_that_knows_where_youre_going_technology/,9.0,2.0,,en
1110332,2012-07-09 21:31:34,statistics,"need help finding appropriate test, human DNA analysis",wa6h1,smshah,1327166265.0,https://www.reddit.com/r/statistics/comments/wa6h1/need_help_finding_appropriate_test_human_dna/,3.0,1.0,"hi, i am working in a lab and need a bit of help.

to simplify what we are doing, we test each patient's DNA for sequence X and sequence Y. sequence X shows up at a value of around 25ct, whereas sequence Y shows up around 40ct. we do 42 repeats for each sequence, for each patient (for a total of 84 data points).

we want to determine in each patient if the amount of sequence X is significantly lower than sequence Y.

the problem now is that some times, one of the repeats will not have any sequence detected. a statistician told us to set that repeat's value at 50ct, and continue on with the analysis. we have been using a 1-tailed, unpaired t-test.

however, when sequence X has 20 undetermined values (which we would set to 50), but the remaining 22 repeats have good values around 25ct, the t-test has been failing (as the excessive 50ct values pushes X closer to Y). 

we want to be able to use the remaining 22 repeats (for example), without having to throw out the entire data set because the t-test is failing. to simply disregard all of the undetermined values does not seem statistically ""legal."" in addition, the number of X values would then be less than the number of Y values, which may also skew the results, which is why we do not know if it is okay to run the t-test with those values removed.

i would really appreciate it if anyone could give me advice about what type of test to use or what to do with the 50ct/undetermined values. i know this is a long post so thank you very much in advance!!

example data set: http://www.sendspace.com/file/ykpl4q

edits:
1. another problem that has been occurring is that the excessive number of 50ct values for X will push its average HIGHER than than the Y values' average. sometimes this allows the t-test to pass, but obviously the data will be extremely skewed and unusable.
2. let's just say that the ""concentration"" so to speak of sequence X is usually ""25"" on every repeat, which sequence Y is usually ""40.""
3. the important thing with the example data is that if you ignore the 50/undetermined values, the t-test passes!",en
1110333,2012-07-10 01:06:24,MachineLearning,"Can anyone give an intuitive explanation of when a classifier would give below-chance classification results i.e. ""anti-learning"" as I've heard it called?",wal0c,lpiloto,1310021871.0,https://www.reddit.com/r/MachineLearning/comments/wal0c/can_anyone_give_an_intuitive_explanation_of_when/,11.0,11.0,"By working, I mean that there isn't some issue with the data being labelled wrong or that there is insufficient data, etc.",en
1110334,2012-07-10 01:45:59,statistics,MAC versions of RExcel (or something similar)?,waniv,Whalen,1259294689.0,https://www.reddit.com/r/statistics/comments/waniv/mac_versions_of_rexcel_or_something_similar/,3.0,4.0,I am a public school teacher slowly learning how to incorporate the voluminous amount of data that our standardized test vendors provide us.  I want to find relationships between certain skill tests/types of questions and have a Mac only to work with.  Any ideas? ,en
1110335,2012-07-10 02:59:21,statistics,Question: How to make a portion of data ''more important'' in the calculation of average,was1o,ballstopicasso,1302808223.0,https://www.reddit.com/r/statistics/comments/was1o/question_how_to_make_a_portion_of_data_more/,2.0,10.0,"I have a 10 minute long data sample. I want to know what to do such that when I take the average, the values in the last 1 minute contribute more to the average than the rest of the data.

What is this called and what should I research? Any input is much appreciated.",en
1110336,2012-07-10 04:25:00,rstats,I am looking to hire a statistician,wax65,[deleted],,https://www.reddit.com/r/rstats/comments/wax65/i_am_looking_to_hire_a_statistician/,1.0,0.0,"I am involved with this job right now, message me with questions, or just apply. http://www.deere.com/wps/dcom/en_US/corporate/our_company/careers/search_positions/searchpositions.page? Then click on the link for ""Experienced professional positions – United States"" You can then search for job Complete Good Forecast (Job number 50482625).",en
1110337,2012-07-10 05:44:56,AskStatistics,Input on the validity of study purporting to speak for 83% of doctors (based on data from 699 returned surveys)?,wb26x,HoodyWarrelson,1328515123.0,https://www.reddit.com/r/AskStatistics/comments/wb26x/input_on_the_validity_of_study_purporting_to/,3.0,4.0,"I'm curious if anyone would like to weigh in on [this study](http://www.doctorsandpatients.org/images/files/DPMA_SurveyResults.pdf). It seems like it is wrong, but it is getting some headlines now..

In the ""About the Survey"" section, it explains that surveys were faxed to over 16,000 doctors, and 699 responded. It strikes me as problematic that the study goes on to make pronouncements like ""83% of doctors..."" based on how this was done. 

Am I wrong to think this? Would a generous genius in this subreddit care to break this one down?",en
1110338,2012-07-10 08:37:11,datascience,The Pathology of Big Data,wbc1j,SkyMarshal,1204709350.0,https://www.reddit.com/r/datascience/comments/wbc1j/the_pathology_of_big_data/,0.0,0.0,,en
1110339,2012-07-10 16:23:23,statistics,"How do I calculate how large the difference between two data sets can be, before there is a significant difference?",wbr10,unclear_plowerpants,1298887532.0,https://www.reddit.com/r/statistics/comments/wbr10/how_do_i_calculate_how_large_the_difference/,2.0,4.0,"PROBLEM  
I am looking at blood to plasma ratios (B:P) of a series of a drug under two different conditions (presence and absence of disease).  
I collected about 20 data points independently for each condition.
I have figured out that there is no statistical difference between them using t-test (alpha =0.05).  
What I want to know now is how big a difference between the two conditions could there exist without it showing up as a significant difference. The reason I need to know is that even a small difference could affect another parameter I am calculating from this data and I want to calculate a worst case for that parameter.  
  
EXAMPLE  
B:P healthy = 1.4 +/- 0.23 (n=20)  
B:P diseased = 1.4 +/- 0.25 (n=20)  
  
How much larger could ""B:P diseased"" be before the t-test shows a significant difference?  
  
WHAT I'VE DONE  
What I've done is just use the values of  
 (B:P healthy - 1\*SD) and   
(B:P diseased +1\*SD)  
for my secondary calculation as a ""worst case scenario"".  
  
And (B:P healthy)  
and (B:P healthy +5%)  
as a ""realistic scenario"".  
  
Both approaches feel a bit unsophisticated.

Thanks already to anyone who would like to give it a go!",en
1110340,2012-07-10 16:55:24,statistics,"Secularization in America: by 2030 (at the latest), None will be the second-largest religious affiliation.",wbsi1,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/wbsi1/secularization_in_america_by_2030_at_the_latest/,19.0,5.0,,en
1110341,2012-07-10 18:10:02,statistics,Incongruities between SAS and R implementing MDS ,wbwge,woodyallin,1301106545.0,https://www.reddit.com/r/statistics/comments/wbwge/incongruities_between_sas_and_r_implementing_mds/,1.0,2.0,"I'm an intern at a lab this summer and we have a problem. 

I have an interval similarity matrix (numbers between 0 and 1) and want to do MDS on this data.

I'm more familiar with R so I'll go from there.  

I found the Euclidean distances between pairs using dist() and then did metric MDS analysis using the cmdscale() function. 
This was the exact function: cmdscale(dist.matrix, k=2)

Now someone else in my lab took the same data and did metric MDS in SAS (I am not familiar with the exact methods). This is what the SAS output looks like:
&gt;Shape=TRIANGLE
&gt;Condition=MATRIX
&gt;Level=ABSOLUTE
&gt;Coef=IDENTITY
&gt;FORMULA=1
&gt;FIT=1
&gt;DIMENSION=2


The two sets of coordinates are different. 

What is going on here? Could it be that R and SAS use different ways to implement MDS? 

I tried reading the documentation, but I couldn't find anything that I didn't already know.

Thanks a lot!",en
1110342,2012-07-10 19:31:07,statistics,What type of regression would I run in SAS to answer this question using social survey data?,wc1gd,atomofconsumption,1205649727.0,https://www.reddit.com/r/statistics/comments/wc1gd/what_type_of_regression_would_i_run_in_sas_to/,1.0,6.0,"I'm used to doing crosstabs and using chi-square then calling it a day based on comparing the row percentages but I want to make this analysis more solid.

This is what I'm trying to figure out: 

""Out of females and males between 25-30, controlling for education level and province, who is more likely to be currently employed?""

* Variable 1: currently employed/not employed

* Variable 2: females between 25-30

* Variable 3: males between 25-20

* Variable 4: Education level (broken into 3 categories) 

* Variable 5: Province X

* Variable 6: Province Y




How can I throw these in a regression analysis?  I do not have a lot of experience with regression and therefore I am not sure which one to use.
",en
1110343,2012-07-10 21:58:38,artificial,Proceedings of the Artificial Life XIII  conference released,wcb1i,jmborg,1228697088.0,https://www.reddit.com/r/artificial/comments/wcb1i/proceedings_of_the_artificial_life_xiii/,23.0,3.0,,en
1110344,2012-07-10 22:12:17,MachineLearning,Air Machine &amp; Air Scrubbers for Abatement Projects,wcbyt,spidertime,1288727261.0,https://www.reddit.com/r/MachineLearning/comments/wcbyt/air_machine_air_scrubbers_for_abatement_projects/,1.0,1.0,,en
1110345,2012-07-11 02:23:50,statistics,Best way to test a die for fairness?,wcsl1,rwdalpe,1273291974.0,https://www.reddit.com/r/statistics/comments/wcsl1/best_way_to_test_a_die_for_fairness/,3.0,15.0,"Right now I'm having some trouble with a personal math project and was wondering if r/statistics might be able to help.

I have a dataset of 1000 rolls for a few 20 sided dice, and I'm trying to determine if they are likely fair or not. First, I tried a Pearson's chi-square test, but for a sample size of 1000 rolls it doesn't have a lot of statistical power for a 20 sided die. Someone suggested that I use Bayesian inference, which I think in this context is also called a ""posterior probability density function,"" to analyze my data. I'm trying to understand it, but I'm having difficulty wrapping my head around it.

Am I even going in the right direction? What would be the best way to analyze my data to help determine if the dice are fair?",en
1110346,2012-07-11 05:54:59,statistics,How do I derive the Standard Deviation formula?,wd55y,verdagon,1293741638.0,https://www.reddit.com/r/statistics/comments/wd55y/how_do_i_derive_the_standard_deviation_formula/,12.0,31.0,"Hi everyone, we all know that the formula for calculating a standard deviation is this:
http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Csqrt%7B%5Cfrac%7B%5CSigma%20(x_i%20-%20%5Cbar%20x)%5E2%7D%7Bn%7D%7D

My question: did someone just pull this formula out of thin air, and then the world standardized it?

If someone had pulled this formula and standardized on it, would it have made any less sense?
http://chart.apis.google.com/chart?cht=tx&amp;chl=%5Csqrt%5B4%5D%7B%5Cfrac%7B%5CSigma%20(x_i%20-%20%5Cbar%20x)%5E4%7D%7Bn%7D%7D

Another way to ask the same question: How did they come up with the formula for standard deviation? Why is there a square root, and why do we square the residuals? Why not have a fourth root, and raise the residuals to the fourth, or any other number for that matter?",en
1110347,2012-07-11 08:40:58,statistics,Linking R and Processing on a server: the free way to interactive dashboards for real-time analytics,wdedz,[deleted],,https://www.reddit.com/r/statistics/comments/wdedz/linking_r_and_processing_on_a_server_the_free_way/,18.0,7.0,,en
1110348,2012-07-11 19:36:03,statistics,Secularization in America: Internet use undermines religious affiliation (so does college).,we3pl,AllenDowney,1300587223.0,https://www.reddit.com/r/statistics/comments/we3pl/secularization_in_america_internet_use_undermines/,8.0,0.0,,en
1110349,2012-07-11 20:57:50,MachineLearning,Should you apply PCA to your data?,we90i,cypherx,1141075032.0,https://www.reddit.com/r/MachineLearning/comments/we90i/should_you_apply_pca_to_your_data/,20.0,9.0,,en
1110350,2012-07-11 22:41:43,statistics,I'm trying to predict accuracy over time. Apparently difference scores are a big statistical no-no- what do I use instead?,weg1j,colorful727,,https://www.reddit.com/r/statistics/comments/weg1j/im_trying_to_predict_accuracy_over_time/,7.0,10.0,"Hey r/statistics! So, I'm in psychology, and I have some longitudinal data on affective forecasting. Basically, people told me how happy they thought they would feel after finishing a particular exam, and then after the exam, they reported on how happy they actually felt. I need to examine who was more accurate in their emotional predictions. I'm expecting accuracy to be predicted by an interaction between a continuous variable and a dichotomous variable (so, regression).

The problem is what to use as the ""accuracy"" DV. Originally I thought I could just use difference scores. Subtract predicted happiness from actual happiness, and then regress that onto my independent variables and my interaction term. And I tried that, and it worked! Significant interaction, perfect simple effects results! But then, I read up on difference scores (e.g., Jeffrey Edwards), it looks like they have a number of statistical problems. Edwards proposes using polynomial regression instead. Not only do I not really get what this is or how it works, but it looks like it assumes that the ""difference"" variable is an IV, not a DV like in my case. 

So my question for r/statistics is, what's the right statistical test for me to use? Are difference scores okay to use as a DV, or are they too problematic? And if the latter, then what should I use instead (e.g., polynomial regression), and do you know of any resources I could use to learn how to do it? I'm revising this manuscript for a journal, and the editor has specifically asked me to justify the analyses I conduct here, so I want to make sure I do it right.

Thanks so much for reading!!

Edit: Wow, you guys have been so incredibly helpful!! Thank you so much for your time and for your insight. I definitely feel a lot more prepared/confident in tackling this paper now :)",en
1110351,2012-07-12 02:15:15,MachineLearning,Autonomous Race Car - I've been waiting for this!,wetra,HawkEgg,1307747609.0,https://www.reddit.com/r/MachineLearning/comments/wetra/autonomous_race_car_ive_been_waiting_for_this/,27.0,6.0,,en
